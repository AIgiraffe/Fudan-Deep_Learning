#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass ctex-article
\begin_preamble
\usepackage{amsmath}
\usepackage{bm}
\usepackage{ctex}

\usepackage{color}
\usepackage{fancyvrb}
\usepackage{CJKutf8}
\usepackage[overlap, CJK]{ruby}
\newenvironment{SChinese}{%
\CJKfamily{gbsn}%
\CJKtilde
\CJKnospace}{}
\newcommand{\cntxt}[1]{\begin{CJK}{UTF8}{}\begin{SChinese}#1\end{SChinese}\end{CJK}}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
% set default figure placement to htbp
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language chinese-simplified
\language_package default
\inputencoding utf8-plain
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\font_cjk gbsn
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
用Pointer Network网络模型给数字排序
\end_layout

\begin_layout Author
Linyang He
\end_layout

\begin_layout Standard
本文以为数字（包括整数、浮点数）情景为例，重点关注了Pointer Network模型作为一个attention机制特例的应用。本文先会讲述attention模
型，再描述了Pointer Ntework。接着展示在长度分别为5、10的整数和长度为5的浮点数上排序结果的展示。最后是实验的总结，并提出了一些自己的思考。
\end_layout

\begin_layout Section
Background
\end_layout

\begin_layout Subsection
Attention机制
\end_layout

\begin_layout Standard
基于Attention机制（注意力机制）神经网络是最近神经网络研究的一个热点。通常而言，Attention模型利用在seq2seq上。
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename pasted1.png
	scale 70

\end_inset


\end_layout

\begin_layout Standard
最基本的seq2seq模型包含一个encoder和一个decoder，一般的做法是将一个输入的句子编码成一个固定大小的state，然后作为decoder的初始状
态（当然也可以作为每一时刻的输入），但这样的一个状态对于decoder中的所有时刻都是一样的。 Attention即为注意力，人脑在对于的不同部分的注意力是不同
的。没有attention机制的encoder-decoder结构通常把encoder的最后一个状态作为decoder的输入（可能作为初始化，也可能作为每一时刻
的输入），但是encoder的state毕竟是有限的，存储不了太多的信息，对于decoder过程，每一个步骤都和之前的输入都没有关系了，只与这个传入的state
有关。attention机制的引入之后，decoder根据时刻的不同，让每一时刻的输入都有所不同。对于不同的decoder阶段，输入的hidden_state也
发生了变化，而不是仅仅一个c作为decoder的输入了。
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename pasted4.png
	scale 30

\end_inset


\end_layout

\begin_layout Standard
Attention-based Model其实就是一个相似性的度量，当前的输入与目标状态越相似，那么在当前的输入的权重就会越大，说明当前的输出越依赖于当前的输入
。
\end_layout

\begin_layout Standard
假设我们对encoder端的state标注为：
\begin_inset Formula $(h_{1},...,h_{T_{A}})$
\end_inset

， 而decoder端的state标注为：
\begin_inset Formula $(d_{1},...,d_{T_{B}})$
\end_inset

, 那么为了计算 在decoder端第t个时刻的attention 向量, 我们可以根据如下公式：
\end_layout

\begin_layout Standard
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout

$$s^t_i = v^T tanh(W_1h_i + W_2d_t)$$
\end_layout

\begin_layout Plain Layout

$$a^t_i = softmax(s^t_i )$$
\end_layout

\begin_layout Plain Layout

$$c_t = 
\backslash
sum^{T_A}_{i=1} a^t_ih_i$$
\end_layout

\end_inset

这里的
\begin_inset Formula $W_{1},$
\end_inset


\begin_inset Formula $W_{2}$
\end_inset

, 
\begin_inset Formula $v^{T}$
\end_inset

都是通过网络学习得来的参数。训练好这个含有attention机制的seq2seq网络之后，如果
\begin_inset ERT
status open

\begin_layout Plain Layout

$a_i^T$
\end_layout

\end_inset

越高，他们对相应的注意力就应该越高。
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename pasted3.png
	scale 60

\end_inset


\end_layout

\begin_layout Standard
如图，以机器翻译（Neural Translation Machine)为例，attention值被可视化了，如果越接近白色，则说明decoder端另外一门语言
的某些词语和encoder端源语言的对应的词越被“注意”。这说明了attention机制，确确实实起到了特别关注序列中某个对象。
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename pasted2.png
	scale 60

\end_inset


\end_layout

\begin_layout Subsection
Pointer Network与数字排序任务
\end_layout

\begin_layout Standard
考虑到普通attention机制的seq2seq不能处理变长的问题，且如果出现一个词典中不存在的词，普通attention也无能为力解决。这时候就需要一个可以处
理变长和处理词典中不存在的token的网络， Pointer Network就诞生了。Pointer Network 实际上是一种特殊的注意力机制。我们知道，事
实上，注意力机制可以分为两步：一是计算注意力分布
\begin_inset Formula $α$
\end_inset

，二是根据
\begin_inset Formula $α$
\end_inset

 来计算输入信息的加权平均。我们可以只利用注意力机制中的第一步，将注意 力分布作为一个软性的指针（pointer）来指出相关信息的位置。
\series bold
直观而言，是把普通的attention model中的加权平均变成了取最大值
\series default
。针网络（Pointer Network）[Vinyals et al., 2015]是一种序列到序列模型， 输入是长度为n的向量序列
\begin_inset ERT
status open

\begin_layout Plain Layout

$X = x_1 ,··· ,x_n$
\end_layout

\end_inset

 ，输出是下标序列
\begin_inset ERT
status open

\begin_layout Plain Layout

$c_{1:m} = c_1 ,c_2 ,··· ,c_m ， c_i 
\backslash
in [1,n],∀i$
\end_layout

\end_inset

。 和一般的序列到序列任务不同，这里的输出序列是输入序列的下标（索引）。 数字排序任务是指，输入一组乱序的数字，要按照规定的某个顺序进行对下标的排序，比如输
 入为20,5,10，输出为1,3,2。这里的数字可以是整数，也可以是浮点数。本文也正是探讨这样的应用。条件概率可以写成
\begin_inset ERT
status open

\begin_layout Plain Layout

$$p(c_{1:m} |X{1:n} ) 
\backslash
approx  
\backslash
Pi_{i=1}^mp(c_i|X_{c_1},..,X_{c_{i-1}},X_{1:n})$$
\end_layout

\end_inset


\end_layout

\begin_layout Standard
公式中的条件概率可以看成注意力分布来计算。假设用一个循环神经网络对
\begin_inset ERT
status open

\begin_layout Plain Layout

$x_{c1} ,··· ,x_{c_{i−1}} ,x_{1:n}$ 
\end_layout

\end_inset

进行编码得到向量
\begin_inset ERT
status open

\begin_layout Plain Layout

$h_i$
\end_layout

\end_inset

 ，则 
\begin_inset ERT
status open

\begin_layout Plain Layout

$$p(c_i |c_{1:i−1} ,x_{1:n} ) = softmax(s_i,j )$$
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

$s_{i,j}$
\end_layout

\end_inset

其实就是上文普通attention机制中的
\begin_inset ERT
status open

\begin_layout Plain Layout

$s^t_i$
\end_layout

\end_inset

。
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename pasted5.png
	scale 60

\end_inset


\end_layout

\begin_layout Standard
如图，我们通过左边的编码器得到了h3作为hidden_state输入到解码器中。此时，我们在根据attention分布指向了概率最大的那个位置。而我们的数据集是
已知这个位置信息的，这样通过位置的概率分布和真实的位置信息，通过计算CrossEntropy就可以得到loss并实现反向传播了。
\end_layout

\begin_layout Section
Experiment
\end_layout

\begin_layout Subsection
Dataset
\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
笔者在分析了提供的样例代码之后，认为样例代码的操作
\end_layout

\begin_layout Standard
可以看到，我们实际中在应用统计语言模型的时候，只能根据前n个词汇来计算，过长的依赖就不能得到解决。而这也正是RNN神经语言模型能够解决的问题。只需要把上面提到的
各种RNN模型的输入变成语料库中的lexicon序列，训练这个模型便可以得到神经语言模型了。
\end_layout

\begin_layout Subsection
Peom Generation
\end_layout

\begin_layout Standard
本文使用的是tensorflow版本的代码，这里分析一下各个函数的作用，以此来说明唐诗生成的过程。
\end_layout

\begin_layout Subsubsection
数据预处理部分
\end_layout

\begin_layout Standard
经过对函数process_poems的分析，我们可以得处三个返回值poems_vector, word_int_map, words分别为：存着诗（字符串）的v
ector索引集；每个字和索引的字典；诗歌语料库中出现的所有的字。
\end_layout

\begin_layout Subsubsection
rnn_lstm model
\end_layout

\begin_layout Standard
这部分代码定义了我们的RNN模型，我们使用的是MultiRNNCell，表示我们的模型是多层RNN堆叠而成的。同时，我们利用的是dynamic_rnn得到输出，
说明有多个时序被同时计算，这样提升了模型的训练效率。
\end_layout

\begin_layout Subsubsection
训练模型部分
\end_layout

\begin_layout Standard
这部分代码是清晰易懂的。唯一个值得关注的点是该程序将参数保存checkpoint中。如果训练被中断了，下次训练时可以直接从中断部分继续实验，极大地提升了效率。
\end_layout

\begin_layout Subsubsection
生成诗歌部分
\end_layout

\begin_layout Standard
2.2.3在代码上是训练模型，而生成诗歌则是验证模型。此外，要注意我们需要导入已经训练好的模型。注意如果需要训练好的参数能够成功导入，那么就要让网络模型等和训练的时
候的一致。而具体到诗歌生成，原理其实很简单，此时，实际上利用的就是语言模型的思想，给出一个begin_word，利用神经语言模型，预测出最可能的下一个word，
以此类推直到遇到了end_token为止，而这些预测出来的word组合起来也就是所谓的唐诗了。
\end_layout

\begin_layout Subsubsection
主函数
\end_layout

\begin_layout Standard
将上述的代码组合起来，按照预处理、训练、验证的流程进行实现。
\end_layout

\begin_layout Section
Result
\end_layout

\begin_layout Standard
本次实验中，我们训练了三个模型，情况如下，其中trraning_loss是最后一次迭代的值。
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
模型
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
learning_rate
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
epochs
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
training_loss
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
RNN
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
50
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.11
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
LSTM
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.001
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
112
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.73
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
LSTM
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
C
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.001
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
32
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.52
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
LSTM
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
生成的诗歌分别为：
\end_layout

\begin_layout Standard
结果分析：
\end_layout

\begin_layout Enumerate
我们发现在同样取learning_rate为0.001的时候，epoch多了之后，反而training_loss变高，这说明本身这个模型可能设计的就不太好，导致不
能完全收敛到全局最优解。
\end_layout

\begin_layout Enumerate
总体上而言，当training_loss变小之后，生成的诗歌的句子的长度也减小了。这说明神经语言模型得到了一定的提升，语言模型会较早的发现end_token，而
不会陷入生成很多很长且没有多少道理的诗句。
\end_layout

\begin_layout Standard
这些诗句中，还是有一些比较惊艳的，这里摘录出来，以飧读者：
\end_layout

\begin_layout Itemize
何当见君子，不见白头翁。
\end_layout

\begin_layout Itemize
日暮花如雪，春风入夜深。
\end_layout

\begin_layout Itemize
云门春色上，花落一枝香。
\end_layout

\begin_layout Itemize
莫问东风景，何人更有期。
\end_layout

\begin_layout Itemize
海上山头白，春风吹雨声。
\end_layout

\begin_layout Itemize
归来不可见，日暮一声风。
\end_layout

\begin_layout Itemize
欲待春光里，还随白露姿。
\end_layout

\begin_layout Itemize
相逢不可见，何处有残阳。
\end_layout

\begin_layout Section
Others
\end_layout

\begin_layout Standard
对数字排序生成这个任务的一些思考：
\end_layout

\begin_layout Enumerate
我们发现，在训练和测试的时候，实际采用的都是同样长度的数字序列。但是我们知道seq2seq model是可以处理变长问题的，如果测试的时候采用的是更长的数字序列
，结果又会如何呢。
\end_layout

\begin_layout Enumerate
实际上，普通的attention model已经可以对定长度的整数序列排序很好的work，但是如果是
\end_layout

\begin_layout Enumerate
一般而言，生成任务都有一些evaluation来评测我们的模型是好是坏，但是在诗歌生成的案例中，似乎并没有这样的一个评判方法。可否考虑大量的人工评分数据作为ev
aluation值得思考。艺术作品的好与坏，事实上，是很难用机器去定量评价的。艺术本就闪耀着人性的光辉，人性的东西应该回归由人性自身去评价，
\end_layout

\end_body
\end_document
