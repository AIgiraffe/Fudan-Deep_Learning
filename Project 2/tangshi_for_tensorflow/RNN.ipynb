{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import os\n",
    "\n",
    "start_token = 'G'\n",
    "end_token = 'E'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_poems(file_name):\n",
    "    poems = []\n",
    "    with open(file_name, \"r\", encoding='utf-8', ) as f:\n",
    "        for line in f.readlines():\n",
    "            try:\n",
    "                title, content = line.strip().split(':')\n",
    "                content = content.replace(' ', '')\n",
    "                if '_' in content or '(' in content or '（' in content or '《' in content or '[' in content or \\\n",
    "                                start_token in content or end_token in content:\n",
    "                    continue\n",
    "                if len(content) < 5 or len(content) > 80:\n",
    "                    continue\n",
    "                content = start_token + content + end_token\n",
    "                poems.append(content)\n",
    "            except ValueError as e:\n",
    "                pass\n",
    "    # 按诗的字数排序\n",
    "    poems = sorted(poems, key=lambda line: len(line))\n",
    "    # 统计每个字出现次数\n",
    "    all_words = []\n",
    "    for poem in poems:\n",
    "        all_words += [word for word in poem]  \n",
    "    counter = collections.Counter(all_words)  # 统计词和词频。\n",
    "    count_pairs = sorted(counter.items(), key=lambda x: -x[1])  # 排序\n",
    "    words, _ = zip(*count_pairs)\n",
    "    words = words[:len(words)] + (' ',)\n",
    "    word_int_map = dict(zip(words, range(len(words))))\n",
    "    poems_vector = [list(map(word_int_map.get, poem)) for poem in poems]\n",
    "    return poems_vector, word_int_map, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rnn_lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model(model, input_data, output_data, vocab_size, rnn_size=128, num_layers=2, batch_size=64,\n",
    "              learning_rate=0.01):\n",
    "    end_points = {}\n",
    "    # 构建RNN基本单元RNNcell\n",
    "    if model == 'rnn':\n",
    "        cell_fun = tf.contrib.rnn.BasicRNNCell\n",
    "    elif model == 'gru':\n",
    "        cell_fun = tf.contrib.rnn.GRUCell\n",
    "    else:\n",
    "        cell_fun = tf.contrib.rnn.BasicLSTMCell\n",
    "    #？？？？？？？？？？？？？？？？？？？？？？\n",
    "    # 每层128个小单元，一共有两层，输出的Ct 和 Ht 要分开放到两个tuple中\n",
    "    # 在下面补全代码 \n",
    "    #################################################\n",
    "    cell = cell_fun(rnn_size, state_is_tuple=True)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    #################################################\n",
    "    # 如果是训练模式，output_data不为None，则初始状态shape为[batch_size * rnn_size]\n",
    "    # 如果是生成模式，output_data为None，则初始状态shape为[1 * rnn_size]\n",
    "    if output_data is not None:\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    else:\n",
    "        initial_state = cell.zero_state(1, tf.float32)\n",
    "\n",
    "    # 构建隐层\n",
    "    for d in ['/gpu:0', '/gpu:1']:\n",
    "        with tf.device(d):\n",
    "            embedding = tf.Variable(tf.random_uniform([vocab_size + 1, rnn_size], -1.0, 1.0),name = 'embedding')\n",
    "            inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    #？？？？？？？？？？？？？？？？？？？？？？？？？？\n",
    "    ####################################################    \n",
    "    outputs, last_state = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state)# 填写里面的内容\n",
    "    ######################################################\n",
    "    output = tf.reshape(outputs, [-1, rnn_size])\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal([rnn_size, vocab_size + 1]))\n",
    "    bias = tf.Variable(tf.zeros(shape=[vocab_size + 1]))\n",
    "    logits = tf.nn.bias_add(tf.matmul(output, weights), bias=bias) # 一层全连接\n",
    "\n",
    "\n",
    "    if output_data is not None: # 训练模式\n",
    "        labels = tf.one_hot(tf.reshape(output_data, [-1]), depth=vocab_size + 1)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        total_loss = tf.reduce_mean(loss)\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)  # 优化器用的 adam\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['output'] = output\n",
    "        end_points['train_op'] = train_op\n",
    "        end_points['total_loss'] = total_loss\n",
    "        end_points['loss'] = loss\n",
    "        end_points['last_state'] = last_state\n",
    "    else: # 生成模式\n",
    "        prediction = tf.nn.softmax(logits)\n",
    "        end_points['initial_state'] = initial_state\n",
    "        end_points['last_state'] = last_state\n",
    "        end_points['prediction'] = prediction\n",
    "    return end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(save_file):\n",
    "    # 处理数据集\n",
    "    poems_vector, word_to_int, vocabularies = process_poems('./poems.txt')\n",
    "    # 生成batch\n",
    "    batches_inputs, batches_outputs = generate_batch(64, poems_vector, word_to_int)\n",
    "\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    output_targets = tf.placeholder(tf.int32, [batch_size, None])\n",
    "    # 构建模型\n",
    "    end_points = rnn_model(model='lstm', input_data=input_data, output_data=output_targets, vocab_size=len(\n",
    "        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=0.01)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        for epoch in range(500):\n",
    "            n = 0\n",
    "            n_chunk = len(poems_vector) // batch_size\n",
    "            for batch in range(n_chunk):\n",
    "                loss, _, _ = sess.run([\n",
    "                    end_points['total_loss'],\n",
    "                    end_points['last_state'],\n",
    "                    end_points['train_op']\n",
    "                ], feed_dict={input_data: batches_inputs[n], output_targets: batches_outputs[n]})\n",
    "                n += 1\n",
    "                print('[INFO] Epoch: %d , batch: %d , training loss: %.6f' % (epoch, batch, loss))\n",
    "            if loss < 3.0:\n",
    "                break\n",
    "#         saver.save(sess, './poem_generator')\n",
    "        saver.save(sess, save_file)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成 诗歌部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_poem(begin_word,save_file):\n",
    "    batch_size = 1\n",
    "    poems_vector, word_int_map, vocabularies = process_poems('./poems.txt')\n",
    "\n",
    "    input_data = tf.placeholder(tf.int32, [batch_size, None])\n",
    "\n",
    "    end_points = rnn_model(model='lstm', input_data=input_data, output_data=None, vocab_size=len(\n",
    "        vocabularies), rnn_size=128, num_layers=2, batch_size=64, learning_rate=0.01)\n",
    "\n",
    "    \n",
    "        \n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        saver.restore(sess, save_file)# 恢复之前训练好的模型 \n",
    "        poem = ''\n",
    "        #???????????????????????????????????????\n",
    "        # 下面部分代码主要功能是根据指定的开始字符来生成诗歌\n",
    "        #########################################\n",
    "        x = np.array([list(map(word_int_map.get, start_token))])\n",
    "        [predict, last_state] = sess.run([end_points['prediction'], end_points['last_state']],\n",
    "                                         feed_dict={input_data: x})\n",
    "        word = begin_word\n",
    "        while word != end_token:\n",
    "            poem += word\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = word_int_map[word]\n",
    "            [predict, last_state] = sess.run([end_points['prediction'], end_points['last_state']],\n",
    "                                             feed_dict={input_data: x, end_points['initial_state']: last_state})\n",
    "            word = to_word(predict, vocabularies)\n",
    "        \n",
    "        #########################################\n",
    "        return poem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他的一些处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, poems_vec, word_to_int):\n",
    "    # 每次取64首诗进行训练\n",
    "    n_chunk = len(poems_vec) // batch_size\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(n_chunk):\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "\n",
    "        batches = poems_vec[start_index:end_index]\n",
    "        # 找到这个batch的所有poem中最长的poem的长度\n",
    "        length = max(map(len, batches))\n",
    "        # 填充一个这么大小的空batch，空的地方放空格对应的index标号\n",
    "        x_data = np.full((batch_size, length), word_to_int[' '], np.int32)\n",
    "        for row in range(batch_size):\n",
    "            x_data[row, :len(batches[row])] = batches[row]\n",
    "        y_data = np.copy(x_data)\n",
    "        y_data[:, :-1] = x_data[:, 1:]\n",
    "        \"\"\"\n",
    "        x_data             y_data\n",
    "        [6,2,4,6,9]       [2,4,6,9,9]\n",
    "        [1,4,2,8,5]       [4,2,8,5,5]\n",
    "        \"\"\"\n",
    "        x_batches.append(x_data)\n",
    "        y_batches.append(y_data)\n",
    "    return x_batches, y_batches\n",
    "\n",
    "def to_word(predict, vocabs):# 预测的结果转化成汉字\n",
    "    sample = np.argmax(predict)\n",
    "    if sample > len(vocabs):\n",
    "        sample = len(vocabs) - 1\n",
    "    return vocabs[sample]\n",
    "def pretty_print_poem(poem):#  令打印的结果更工整\n",
    "    poem_sentences = poem.split('。')\n",
    "    for s in poem_sentences:\n",
    "        if s != '' and len(s) > 10:\n",
    "            print(s + '。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] train tang poem...\n",
      "WARNING:tensorflow:From <ipython-input-7-a7fbdb66f075>:15: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:At least two cells provided to MultiRNNCell are the same object and will share weights.\n",
      "WARNING:tensorflow:From <ipython-input-7-a7fbdb66f075>:43: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "[INFO] Epoch: 0 , batch: 0 , training loss: 8.797020\n",
      "[INFO] Epoch: 0 , batch: 1 , training loss: 7.284378\n",
      "[INFO] Epoch: 0 , batch: 2 , training loss: 10.441524\n",
      "[INFO] Epoch: 0 , batch: 3 , training loss: 6.526995\n",
      "[INFO] Epoch: 0 , batch: 4 , training loss: 7.754235\n",
      "[INFO] Epoch: 0 , batch: 5 , training loss: 6.691163\n",
      "[INFO] Epoch: 0 , batch: 6 , training loss: 7.491658\n",
      "[INFO] Epoch: 0 , batch: 7 , training loss: 7.384674\n",
      "[INFO] Epoch: 0 , batch: 8 , training loss: 7.370260\n",
      "[INFO] Epoch: 0 , batch: 9 , training loss: 7.287445\n",
      "[INFO] Epoch: 0 , batch: 10 , training loss: 7.139710\n",
      "[INFO] Epoch: 0 , batch: 11 , training loss: 6.904678\n",
      "[INFO] Epoch: 0 , batch: 12 , training loss: 6.835840\n",
      "[INFO] Epoch: 0 , batch: 13 , training loss: 6.773788\n",
      "[INFO] Epoch: 0 , batch: 14 , training loss: 6.677106\n",
      "[INFO] Epoch: 0 , batch: 15 , training loss: 6.583585\n",
      "[INFO] Epoch: 0 , batch: 16 , training loss: 6.514234\n",
      "[INFO] Epoch: 0 , batch: 17 , training loss: 6.434200\n",
      "[INFO] Epoch: 0 , batch: 18 , training loss: 6.298022\n",
      "[INFO] Epoch: 0 , batch: 19 , training loss: 6.235950\n",
      "[INFO] Epoch: 0 , batch: 20 , training loss: 6.152279\n",
      "[INFO] Epoch: 0 , batch: 21 , training loss: 6.049019\n",
      "[INFO] Epoch: 0 , batch: 22 , training loss: 6.190850\n",
      "[INFO] Epoch: 0 , batch: 23 , training loss: 6.030770\n",
      "[INFO] Epoch: 0 , batch: 24 , training loss: 5.939642\n",
      "[INFO] Epoch: 0 , batch: 25 , training loss: 5.973889\n",
      "[INFO] Epoch: 0 , batch: 26 , training loss: 5.859277\n",
      "[INFO] Epoch: 0 , batch: 27 , training loss: 5.830136\n",
      "[INFO] Epoch: 0 , batch: 28 , training loss: 5.985767\n",
      "[INFO] Epoch: 0 , batch: 29 , training loss: 5.744688\n",
      "[INFO] Epoch: 0 , batch: 30 , training loss: 5.703959\n",
      "[INFO] Epoch: 0 , batch: 31 , training loss: 5.798944\n",
      "[INFO] Epoch: 0 , batch: 32 , training loss: 5.780425\n",
      "[INFO] Epoch: 0 , batch: 33 , training loss: 5.674267\n",
      "[INFO] Epoch: 0 , batch: 34 , training loss: 5.842118\n",
      "[INFO] Epoch: 0 , batch: 35 , training loss: 5.727913\n",
      "[INFO] Epoch: 0 , batch: 36 , training loss: 5.673300\n",
      "[INFO] Epoch: 0 , batch: 37 , training loss: 5.588216\n",
      "[INFO] Epoch: 0 , batch: 38 , training loss: 5.671690\n",
      "[INFO] Epoch: 0 , batch: 39 , training loss: 5.609049\n",
      "[INFO] Epoch: 0 , batch: 40 , training loss: 5.500533\n",
      "[INFO] Epoch: 0 , batch: 41 , training loss: 5.769216\n",
      "[INFO] Epoch: 0 , batch: 42 , training loss: 6.086535\n",
      "[INFO] Epoch: 0 , batch: 43 , training loss: 5.989388\n",
      "[INFO] Epoch: 0 , batch: 44 , training loss: 5.948249\n",
      "[INFO] Epoch: 0 , batch: 45 , training loss: 6.160097\n",
      "[INFO] Epoch: 0 , batch: 46 , training loss: 6.917189\n",
      "[INFO] Epoch: 0 , batch: 47 , training loss: 6.130607\n",
      "[INFO] Epoch: 0 , batch: 48 , training loss: 6.221280\n",
      "[INFO] Epoch: 0 , batch: 49 , training loss: 6.193332\n",
      "[INFO] Epoch: 0 , batch: 50 , training loss: 6.106942\n",
      "[INFO] Epoch: 0 , batch: 51 , training loss: 5.920323\n",
      "[INFO] Epoch: 0 , batch: 52 , training loss: 5.797057\n",
      "[INFO] Epoch: 0 , batch: 53 , training loss: 5.974998\n",
      "[INFO] Epoch: 0 , batch: 54 , training loss: 5.894680\n",
      "[INFO] Epoch: 0 , batch: 55 , training loss: 5.981586\n",
      "[INFO] Epoch: 0 , batch: 56 , training loss: 5.853033\n",
      "[INFO] Epoch: 0 , batch: 57 , training loss: 5.800897\n",
      "[INFO] Epoch: 0 , batch: 58 , training loss: 5.735938\n",
      "[INFO] Epoch: 0 , batch: 59 , training loss: 5.889225\n",
      "[INFO] Epoch: 0 , batch: 60 , training loss: 5.707302\n",
      "[INFO] Epoch: 0 , batch: 61 , training loss: 5.784279\n",
      "[INFO] Epoch: 0 , batch: 62 , training loss: 5.753554\n",
      "[INFO] Epoch: 0 , batch: 63 , training loss: 5.815161\n",
      "[INFO] Epoch: 0 , batch: 64 , training loss: 5.776046\n",
      "[INFO] Epoch: 0 , batch: 65 , training loss: 5.809192\n",
      "[INFO] Epoch: 0 , batch: 66 , training loss: 5.754939\n",
      "[INFO] Epoch: 0 , batch: 67 , training loss: 5.741594\n",
      "[INFO] Epoch: 0 , batch: 68 , training loss: 5.965006\n",
      "[INFO] Epoch: 0 , batch: 69 , training loss: 5.787127\n",
      "[INFO] Epoch: 0 , batch: 70 , training loss: 5.913239\n",
      "[INFO] Epoch: 0 , batch: 71 , training loss: 5.807652\n",
      "[INFO] Epoch: 0 , batch: 72 , training loss: 5.884068\n",
      "[INFO] Epoch: 0 , batch: 73 , training loss: 5.814391\n",
      "[INFO] Epoch: 0 , batch: 74 , training loss: 5.778812\n",
      "[INFO] Epoch: 0 , batch: 75 , training loss: 5.569670\n",
      "[INFO] Epoch: 0 , batch: 76 , training loss: 5.758635\n",
      "[INFO] Epoch: 0 , batch: 77 , training loss: 5.659961\n",
      "[INFO] Epoch: 0 , batch: 78 , training loss: 5.654167\n",
      "[INFO] Epoch: 0 , batch: 79 , training loss: 5.542978\n",
      "[INFO] Epoch: 0 , batch: 80 , training loss: 5.671426\n",
      "[INFO] Epoch: 0 , batch: 81 , training loss: 5.743264\n",
      "[INFO] Epoch: 0 , batch: 82 , training loss: 5.728453\n",
      "[INFO] Epoch: 0 , batch: 83 , training loss: 5.820283\n",
      "[INFO] Epoch: 0 , batch: 84 , training loss: 5.694282\n",
      "[INFO] Epoch: 0 , batch: 85 , training loss: 5.749132\n",
      "[INFO] Epoch: 0 , batch: 86 , training loss: 5.740663\n",
      "[INFO] Epoch: 0 , batch: 87 , training loss: 5.751971\n",
      "[INFO] Epoch: 0 , batch: 88 , training loss: 5.856467\n",
      "[INFO] Epoch: 0 , batch: 89 , training loss: 5.694307\n",
      "[INFO] Epoch: 0 , batch: 90 , training loss: 5.650473\n",
      "[INFO] Epoch: 0 , batch: 91 , training loss: 5.678339\n",
      "[INFO] Epoch: 0 , batch: 92 , training loss: 5.675999\n",
      "[INFO] Epoch: 0 , batch: 93 , training loss: 5.718160\n",
      "[INFO] Epoch: 0 , batch: 94 , training loss: 5.806783\n",
      "[INFO] Epoch: 0 , batch: 95 , training loss: 5.707954\n",
      "[INFO] Epoch: 0 , batch: 96 , training loss: 5.618710\n",
      "[INFO] Epoch: 0 , batch: 97 , training loss: 5.707788\n",
      "[INFO] Epoch: 0 , batch: 98 , training loss: 5.697275\n",
      "[INFO] Epoch: 0 , batch: 99 , training loss: 5.672901\n",
      "[INFO] Epoch: 0 , batch: 100 , training loss: 5.567207\n",
      "[INFO] Epoch: 0 , batch: 101 , training loss: 5.572729\n",
      "[INFO] Epoch: 0 , batch: 102 , training loss: 5.772269\n",
      "[INFO] Epoch: 0 , batch: 103 , training loss: 5.526446\n",
      "[INFO] Epoch: 0 , batch: 104 , training loss: 5.579637\n",
      "[INFO] Epoch: 0 , batch: 105 , training loss: 5.772787\n",
      "[INFO] Epoch: 0 , batch: 106 , training loss: 5.743403\n",
      "[INFO] Epoch: 0 , batch: 107 , training loss: 5.714691\n",
      "[INFO] Epoch: 0 , batch: 108 , training loss: 5.517457\n",
      "[INFO] Epoch: 0 , batch: 109 , training loss: 5.468442\n",
      "[INFO] Epoch: 0 , batch: 110 , training loss: 5.679997\n",
      "[INFO] Epoch: 0 , batch: 111 , training loss: 5.628764\n",
      "[INFO] Epoch: 0 , batch: 112 , training loss: 5.697312\n",
      "[INFO] Epoch: 0 , batch: 113 , training loss: 5.553575\n",
      "[INFO] Epoch: 0 , batch: 114 , training loss: 5.735790\n",
      "[INFO] Epoch: 0 , batch: 115 , training loss: 5.675331\n",
      "[INFO] Epoch: 0 , batch: 116 , training loss: 5.751850\n",
      "[INFO] Epoch: 0 , batch: 117 , training loss: 5.863317\n",
      "[INFO] Epoch: 0 , batch: 118 , training loss: 5.869548\n",
      "[INFO] Epoch: 0 , batch: 119 , training loss: 5.789380\n",
      "[INFO] Epoch: 0 , batch: 120 , training loss: 5.847636\n",
      "[INFO] Epoch: 0 , batch: 121 , training loss: 5.672972\n",
      "[INFO] Epoch: 0 , batch: 122 , training loss: 5.662012\n",
      "[INFO] Epoch: 0 , batch: 123 , training loss: 5.898416\n",
      "[INFO] Epoch: 0 , batch: 124 , training loss: 5.954186\n",
      "[INFO] Epoch: 0 , batch: 125 , training loss: 5.536176\n",
      "[INFO] Epoch: 0 , batch: 126 , training loss: 5.625772\n",
      "[INFO] Epoch: 0 , batch: 127 , training loss: 5.642140\n",
      "[INFO] Epoch: 0 , batch: 128 , training loss: 5.824414\n",
      "[INFO] Epoch: 0 , batch: 129 , training loss: 5.694044\n",
      "[INFO] Epoch: 0 , batch: 130 , training loss: 5.721042\n",
      "[INFO] Epoch: 0 , batch: 131 , training loss: 5.692877\n",
      "[INFO] Epoch: 0 , batch: 132 , training loss: 5.736342\n",
      "[INFO] Epoch: 0 , batch: 133 , training loss: 5.637088\n",
      "[INFO] Epoch: 0 , batch: 134 , training loss: 5.563984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 0 , batch: 135 , training loss: 5.556635\n",
      "[INFO] Epoch: 0 , batch: 136 , training loss: 5.706733\n",
      "[INFO] Epoch: 0 , batch: 137 , training loss: 5.623903\n",
      "[INFO] Epoch: 0 , batch: 138 , training loss: 5.784577\n",
      "[INFO] Epoch: 0 , batch: 139 , training loss: 6.247742\n",
      "[INFO] Epoch: 0 , batch: 140 , training loss: 6.260918\n",
      "[INFO] Epoch: 0 , batch: 141 , training loss: 6.037811\n",
      "[INFO] Epoch: 0 , batch: 142 , training loss: 5.755075\n",
      "[INFO] Epoch: 0 , batch: 143 , training loss: 5.724460\n",
      "[INFO] Epoch: 0 , batch: 144 , training loss: 5.766144\n",
      "[INFO] Epoch: 0 , batch: 145 , training loss: 5.838947\n",
      "[INFO] Epoch: 0 , batch: 146 , training loss: 5.862205\n",
      "[INFO] Epoch: 0 , batch: 147 , training loss: 5.574625\n",
      "[INFO] Epoch: 0 , batch: 148 , training loss: 5.574781\n",
      "[INFO] Epoch: 0 , batch: 149 , training loss: 5.642587\n",
      "[INFO] Epoch: 0 , batch: 150 , training loss: 5.891544\n",
      "[INFO] Epoch: 0 , batch: 151 , training loss: 5.503241\n",
      "[INFO] Epoch: 0 , batch: 152 , training loss: 5.512493\n",
      "[INFO] Epoch: 0 , batch: 153 , training loss: 5.708337\n",
      "[INFO] Epoch: 0 , batch: 154 , training loss: 5.791903\n",
      "[INFO] Epoch: 0 , batch: 155 , training loss: 5.954174\n",
      "[INFO] Epoch: 0 , batch: 156 , training loss: 5.687247\n",
      "[INFO] Epoch: 0 , batch: 157 , training loss: 5.664893\n",
      "[INFO] Epoch: 0 , batch: 158 , training loss: 5.991777\n",
      "[INFO] Epoch: 0 , batch: 159 , training loss: 6.086532\n",
      "[INFO] Epoch: 0 , batch: 160 , training loss: 6.861397\n",
      "[INFO] Epoch: 0 , batch: 161 , training loss: 6.353748\n",
      "[INFO] Epoch: 0 , batch: 162 , training loss: 5.823614\n",
      "[INFO] Epoch: 0 , batch: 163 , training loss: 5.756057\n",
      "[INFO] Epoch: 0 , batch: 164 , training loss: 5.877524\n",
      "[INFO] Epoch: 0 , batch: 165 , training loss: 5.896591\n",
      "[INFO] Epoch: 0 , batch: 166 , training loss: 7.104649\n",
      "[INFO] Epoch: 0 , batch: 167 , training loss: 6.885495\n",
      "[INFO] Epoch: 0 , batch: 168 , training loss: 6.976417\n",
      "[INFO] Epoch: 0 , batch: 169 , training loss: 6.858795\n",
      "[INFO] Epoch: 0 , batch: 170 , training loss: 6.720753\n",
      "[INFO] Epoch: 0 , batch: 171 , training loss: 6.538097\n",
      "[INFO] Epoch: 0 , batch: 172 , training loss: 6.544641\n",
      "[INFO] Epoch: 0 , batch: 173 , training loss: 6.281071\n",
      "[INFO] Epoch: 0 , batch: 174 , training loss: 6.246119\n",
      "[INFO] Epoch: 0 , batch: 175 , training loss: 6.257083\n",
      "[INFO] Epoch: 0 , batch: 176 , training loss: 6.422307\n",
      "[INFO] Epoch: 0 , batch: 177 , training loss: 6.256381\n",
      "[INFO] Epoch: 0 , batch: 178 , training loss: 6.169193\n",
      "[INFO] Epoch: 0 , batch: 179 , training loss: 6.098085\n",
      "[INFO] Epoch: 0 , batch: 180 , training loss: 6.110112\n",
      "[INFO] Epoch: 0 , batch: 181 , training loss: 6.182212\n",
      "[INFO] Epoch: 0 , batch: 182 , training loss: 6.042868\n",
      "[INFO] Epoch: 0 , batch: 183 , training loss: 5.983688\n",
      "[INFO] Epoch: 0 , batch: 184 , training loss: 5.986148\n",
      "[INFO] Epoch: 0 , batch: 185 , training loss: 5.960606\n",
      "[INFO] Epoch: 0 , batch: 186 , training loss: 5.995785\n",
      "[INFO] Epoch: 0 , batch: 187 , training loss: 6.088806\n",
      "[INFO] Epoch: 0 , batch: 188 , training loss: 6.022603\n",
      "[INFO] Epoch: 0 , batch: 189 , training loss: 5.962326\n",
      "[INFO] Epoch: 0 , batch: 190 , training loss: 5.924724\n",
      "[INFO] Epoch: 0 , batch: 191 , training loss: 6.019054\n",
      "[INFO] Epoch: 0 , batch: 192 , training loss: 5.860965\n",
      "[INFO] Epoch: 0 , batch: 193 , training loss: 5.852736\n",
      "[INFO] Epoch: 0 , batch: 194 , training loss: 5.812779\n",
      "[INFO] Epoch: 0 , batch: 195 , training loss: 5.937230\n",
      "[INFO] Epoch: 0 , batch: 196 , training loss: 5.729728\n",
      "[INFO] Epoch: 0 , batch: 197 , training loss: 5.917633\n",
      "[INFO] Epoch: 0 , batch: 198 , training loss: 5.729703\n",
      "[INFO] Epoch: 0 , batch: 199 , training loss: 5.764259\n",
      "[INFO] Epoch: 0 , batch: 200 , training loss: 5.722116\n",
      "[INFO] Epoch: 0 , batch: 201 , training loss: 5.730073\n",
      "[INFO] Epoch: 0 , batch: 202 , training loss: 5.656033\n",
      "[INFO] Epoch: 0 , batch: 203 , training loss: 5.545916\n",
      "[INFO] Epoch: 0 , batch: 204 , training loss: 5.650774\n",
      "[INFO] Epoch: 0 , batch: 205 , training loss: 5.539873\n",
      "[INFO] Epoch: 0 , batch: 206 , training loss: 5.421491\n",
      "[INFO] Epoch: 0 , batch: 207 , training loss: 5.444344\n",
      "[INFO] Epoch: 0 , batch: 208 , training loss: 5.678513\n",
      "[INFO] Epoch: 0 , batch: 209 , training loss: 5.666903\n",
      "[INFO] Epoch: 0 , batch: 210 , training loss: 5.699848\n",
      "[INFO] Epoch: 0 , batch: 211 , training loss: 5.536661\n",
      "[INFO] Epoch: 0 , batch: 212 , training loss: 5.497498\n",
      "[INFO] Epoch: 0 , batch: 213 , training loss: 5.638882\n",
      "[INFO] Epoch: 0 , batch: 214 , training loss: 5.546143\n",
      "[INFO] Epoch: 0 , batch: 215 , training loss: 5.655568\n",
      "[INFO] Epoch: 0 , batch: 216 , training loss: 5.621190\n",
      "[INFO] Epoch: 0 , batch: 217 , training loss: 5.519243\n",
      "[INFO] Epoch: 0 , batch: 218 , training loss: 5.548802\n",
      "[INFO] Epoch: 0 , batch: 219 , training loss: 5.557595\n",
      "[INFO] Epoch: 0 , batch: 220 , training loss: 5.520381\n",
      "[INFO] Epoch: 0 , batch: 221 , training loss: 5.478214\n",
      "[INFO] Epoch: 0 , batch: 222 , training loss: 5.585298\n",
      "[INFO] Epoch: 0 , batch: 223 , training loss: 5.657775\n",
      "[INFO] Epoch: 0 , batch: 224 , training loss: 5.687758\n",
      "[INFO] Epoch: 0 , batch: 225 , training loss: 5.542666\n",
      "[INFO] Epoch: 0 , batch: 226 , training loss: 5.649870\n",
      "[INFO] Epoch: 0 , batch: 227 , training loss: 5.614994\n",
      "[INFO] Epoch: 0 , batch: 228 , training loss: 5.712738\n",
      "[INFO] Epoch: 0 , batch: 229 , training loss: 5.615736\n",
      "[INFO] Epoch: 0 , batch: 230 , training loss: 5.442762\n",
      "[INFO] Epoch: 0 , batch: 231 , training loss: 5.341162\n",
      "[INFO] Epoch: 0 , batch: 232 , training loss: 5.383584\n",
      "[INFO] Epoch: 0 , batch: 233 , training loss: 5.459996\n",
      "[INFO] Epoch: 0 , batch: 234 , training loss: 5.255807\n",
      "[INFO] Epoch: 0 , batch: 235 , training loss: 5.349234\n",
      "[INFO] Epoch: 0 , batch: 236 , training loss: 5.502666\n",
      "[INFO] Epoch: 0 , batch: 237 , training loss: 5.597805\n",
      "[INFO] Epoch: 0 , batch: 238 , training loss: 5.360462\n",
      "[INFO] Epoch: 0 , batch: 239 , training loss: 5.385631\n",
      "[INFO] Epoch: 0 , batch: 240 , training loss: 5.496130\n",
      "[INFO] Epoch: 0 , batch: 241 , training loss: 5.337217\n",
      "[INFO] Epoch: 0 , batch: 242 , training loss: 5.302911\n",
      "[INFO] Epoch: 0 , batch: 243 , training loss: 5.574899\n",
      "[INFO] Epoch: 0 , batch: 244 , training loss: 5.430728\n",
      "[INFO] Epoch: 0 , batch: 245 , training loss: 5.418640\n",
      "[INFO] Epoch: 0 , batch: 246 , training loss: 5.237959\n",
      "[INFO] Epoch: 0 , batch: 247 , training loss: 5.331371\n",
      "[INFO] Epoch: 0 , batch: 248 , training loss: 5.357149\n",
      "[INFO] Epoch: 0 , batch: 249 , training loss: 5.309711\n",
      "[INFO] Epoch: 0 , batch: 250 , training loss: 5.181030\n",
      "[INFO] Epoch: 0 , batch: 251 , training loss: 5.494230\n",
      "[INFO] Epoch: 0 , batch: 252 , training loss: 5.300856\n",
      "[INFO] Epoch: 0 , batch: 253 , training loss: 5.396721\n",
      "[INFO] Epoch: 0 , batch: 254 , training loss: 5.600909\n",
      "[INFO] Epoch: 0 , batch: 255 , training loss: 5.610522\n",
      "[INFO] Epoch: 0 , batch: 256 , training loss: 5.535328\n",
      "[INFO] Epoch: 0 , batch: 257 , training loss: 5.557053\n",
      "[INFO] Epoch: 0 , batch: 258 , training loss: 5.587304\n",
      "[INFO] Epoch: 0 , batch: 259 , training loss: 5.647980\n",
      "[INFO] Epoch: 0 , batch: 260 , training loss: 5.386436\n",
      "[INFO] Epoch: 0 , batch: 261 , training loss: 5.541240\n",
      "[INFO] Epoch: 0 , batch: 262 , training loss: 5.634642\n",
      "[INFO] Epoch: 0 , batch: 263 , training loss: 5.671267\n",
      "[INFO] Epoch: 0 , batch: 264 , training loss: 5.166973\n",
      "[INFO] Epoch: 0 , batch: 265 , training loss: 5.315955\n",
      "[INFO] Epoch: 0 , batch: 266 , training loss: 5.726477\n",
      "[INFO] Epoch: 0 , batch: 267 , training loss: 5.548788\n",
      "[INFO] Epoch: 0 , batch: 268 , training loss: 5.407629\n",
      "[INFO] Epoch: 0 , batch: 269 , training loss: 5.565707\n",
      "[INFO] Epoch: 0 , batch: 270 , training loss: 5.502481\n",
      "[INFO] Epoch: 0 , batch: 271 , training loss: 5.527688\n",
      "[INFO] Epoch: 0 , batch: 272 , training loss: 5.462937\n",
      "[INFO] Epoch: 0 , batch: 273 , training loss: 5.475097\n",
      "[INFO] Epoch: 0 , batch: 274 , training loss: 5.604213\n",
      "[INFO] Epoch: 0 , batch: 275 , training loss: 5.490768\n",
      "[INFO] Epoch: 0 , batch: 276 , training loss: 5.586099\n",
      "[INFO] Epoch: 0 , batch: 277 , training loss: 5.613021\n",
      "[INFO] Epoch: 0 , batch: 278 , training loss: 5.227694\n",
      "[INFO] Epoch: 0 , batch: 279 , training loss: 5.229732\n",
      "[INFO] Epoch: 0 , batch: 280 , training loss: 5.243927\n",
      "[INFO] Epoch: 0 , batch: 281 , training loss: 5.314506\n",
      "[INFO] Epoch: 0 , batch: 282 , training loss: 5.239724\n",
      "[INFO] Epoch: 0 , batch: 283 , training loss: 5.392947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 0 , batch: 284 , training loss: 5.326487\n",
      "[INFO] Epoch: 0 , batch: 285 , training loss: 5.403841\n",
      "[INFO] Epoch: 0 , batch: 286 , training loss: 5.340783\n",
      "[INFO] Epoch: 0 , batch: 287 , training loss: 5.139217\n",
      "[INFO] Epoch: 0 , batch: 288 , training loss: 5.309906\n",
      "[INFO] Epoch: 0 , batch: 289 , training loss: 5.307545\n",
      "[INFO] Epoch: 0 , batch: 290 , training loss: 5.211391\n",
      "[INFO] Epoch: 0 , batch: 291 , training loss: 5.172017\n",
      "[INFO] Epoch: 0 , batch: 292 , training loss: 5.217531\n",
      "[INFO] Epoch: 0 , batch: 293 , training loss: 5.219745\n",
      "[INFO] Epoch: 0 , batch: 294 , training loss: 5.708445\n",
      "[INFO] Epoch: 0 , batch: 295 , training loss: 5.514990\n",
      "[INFO] Epoch: 0 , batch: 296 , training loss: 5.409861\n",
      "[INFO] Epoch: 0 , batch: 297 , training loss: 5.319449\n",
      "[INFO] Epoch: 0 , batch: 298 , training loss: 5.254310\n",
      "[INFO] Epoch: 0 , batch: 299 , training loss: 5.144796\n",
      "[INFO] Epoch: 0 , batch: 300 , training loss: 5.164272\n",
      "[INFO] Epoch: 0 , batch: 301 , training loss: 5.200540\n",
      "[INFO] Epoch: 0 , batch: 302 , training loss: 5.293703\n",
      "[INFO] Epoch: 0 , batch: 303 , training loss: 5.305413\n",
      "[INFO] Epoch: 0 , batch: 304 , training loss: 5.465493\n",
      "[INFO] Epoch: 0 , batch: 305 , training loss: 5.222346\n",
      "[INFO] Epoch: 0 , batch: 306 , training loss: 5.287805\n",
      "[INFO] Epoch: 0 , batch: 307 , training loss: 5.229120\n",
      "[INFO] Epoch: 0 , batch: 308 , training loss: 5.300698\n",
      "[INFO] Epoch: 0 , batch: 309 , training loss: 5.249557\n",
      "[INFO] Epoch: 0 , batch: 310 , training loss: 5.050998\n",
      "[INFO] Epoch: 0 , batch: 311 , training loss: 5.061057\n",
      "[INFO] Epoch: 0 , batch: 312 , training loss: 4.939730\n",
      "[INFO] Epoch: 0 , batch: 313 , training loss: 5.127640\n",
      "[INFO] Epoch: 0 , batch: 314 , training loss: 5.169509\n",
      "[INFO] Epoch: 0 , batch: 315 , training loss: 5.149474\n",
      "[INFO] Epoch: 0 , batch: 316 , training loss: 5.508866\n",
      "[INFO] Epoch: 0 , batch: 317 , training loss: 5.991540\n",
      "[INFO] Epoch: 0 , batch: 318 , training loss: 6.038472\n",
      "[INFO] Epoch: 0 , batch: 319 , training loss: 5.572490\n",
      "[INFO] Epoch: 0 , batch: 320 , training loss: 5.253638\n",
      "[INFO] Epoch: 0 , batch: 321 , training loss: 5.069094\n",
      "[INFO] Epoch: 0 , batch: 322 , training loss: 5.233257\n",
      "[INFO] Epoch: 0 , batch: 323 , training loss: 5.168578\n",
      "[INFO] Epoch: 0 , batch: 324 , training loss: 5.180926\n",
      "[INFO] Epoch: 0 , batch: 325 , training loss: 5.375503\n",
      "[INFO] Epoch: 0 , batch: 326 , training loss: 5.430490\n",
      "[INFO] Epoch: 0 , batch: 327 , training loss: 5.245728\n",
      "[INFO] Epoch: 0 , batch: 328 , training loss: 5.333739\n",
      "[INFO] Epoch: 0 , batch: 329 , training loss: 5.160117\n",
      "[INFO] Epoch: 0 , batch: 330 , training loss: 5.167471\n",
      "[INFO] Epoch: 0 , batch: 331 , training loss: 5.348111\n",
      "[INFO] Epoch: 0 , batch: 332 , training loss: 4.989815\n",
      "[INFO] Epoch: 0 , batch: 333 , training loss: 4.952466\n",
      "[INFO] Epoch: 0 , batch: 334 , training loss: 5.246655\n",
      "[INFO] Epoch: 0 , batch: 335 , training loss: 5.253675\n",
      "[INFO] Epoch: 0 , batch: 336 , training loss: 5.142162\n",
      "[INFO] Epoch: 0 , batch: 337 , training loss: 5.325726\n",
      "[INFO] Epoch: 0 , batch: 338 , training loss: 5.348820\n",
      "[INFO] Epoch: 0 , batch: 339 , training loss: 5.239395\n",
      "[INFO] Epoch: 0 , batch: 340 , training loss: 5.397663\n",
      "[INFO] Epoch: 0 , batch: 341 , training loss: 5.168120\n",
      "[INFO] Epoch: 0 , batch: 342 , training loss: 5.092515\n",
      "[INFO] Epoch: 0 , batch: 343 , training loss: 5.199718\n",
      "[INFO] Epoch: 0 , batch: 344 , training loss: 5.116462\n",
      "[INFO] Epoch: 0 , batch: 345 , training loss: 5.240200\n",
      "[INFO] Epoch: 0 , batch: 346 , training loss: 5.317267\n",
      "[INFO] Epoch: 0 , batch: 347 , training loss: 5.132996\n",
      "[INFO] Epoch: 0 , batch: 348 , training loss: 5.379694\n",
      "[INFO] Epoch: 0 , batch: 349 , training loss: 5.560922\n",
      "[INFO] Epoch: 0 , batch: 350 , training loss: 5.126981\n",
      "[INFO] Epoch: 0 , batch: 351 , training loss: 5.098551\n",
      "[INFO] Epoch: 0 , batch: 352 , training loss: 5.112033\n",
      "[INFO] Epoch: 0 , batch: 353 , training loss: 5.086594\n",
      "[INFO] Epoch: 0 , batch: 354 , training loss: 5.155366\n",
      "[INFO] Epoch: 0 , batch: 355 , training loss: 5.290741\n",
      "[INFO] Epoch: 0 , batch: 356 , training loss: 5.208629\n",
      "[INFO] Epoch: 0 , batch: 357 , training loss: 5.173837\n",
      "[INFO] Epoch: 0 , batch: 358 , training loss: 5.186548\n",
      "[INFO] Epoch: 0 , batch: 359 , training loss: 5.211380\n",
      "[INFO] Epoch: 0 , batch: 360 , training loss: 5.140478\n",
      "[INFO] Epoch: 0 , batch: 361 , training loss: 5.133972\n",
      "[INFO] Epoch: 0 , batch: 362 , training loss: 5.185626\n",
      "[INFO] Epoch: 0 , batch: 363 , training loss: 5.098313\n",
      "[INFO] Epoch: 0 , batch: 364 , training loss: 5.175344\n",
      "[INFO] Epoch: 0 , batch: 365 , training loss: 5.019956\n",
      "[INFO] Epoch: 0 , batch: 366 , training loss: 5.210622\n",
      "[INFO] Epoch: 0 , batch: 367 , training loss: 5.218359\n",
      "[INFO] Epoch: 0 , batch: 368 , training loss: 5.778165\n",
      "[INFO] Epoch: 0 , batch: 369 , training loss: 5.506629\n",
      "[INFO] Epoch: 0 , batch: 370 , training loss: 5.321323\n",
      "[INFO] Epoch: 0 , batch: 371 , training loss: 5.880864\n",
      "[INFO] Epoch: 0 , batch: 372 , training loss: 6.334737\n",
      "[INFO] Epoch: 0 , batch: 373 , training loss: 6.211669\n",
      "[INFO] Epoch: 0 , batch: 374 , training loss: 6.061953\n",
      "[INFO] Epoch: 0 , batch: 375 , training loss: 6.183575\n",
      "[INFO] Epoch: 0 , batch: 376 , training loss: 6.311574\n",
      "[INFO] Epoch: 0 , batch: 377 , training loss: 5.863168\n",
      "[INFO] Epoch: 0 , batch: 378 , training loss: 5.722993\n",
      "[INFO] Epoch: 0 , batch: 379 , training loss: 5.711116\n",
      "[INFO] Epoch: 0 , batch: 380 , training loss: 5.725246\n",
      "[INFO] Epoch: 0 , batch: 381 , training loss: 5.659390\n",
      "[INFO] Epoch: 0 , batch: 382 , training loss: 5.589860\n",
      "[INFO] Epoch: 0 , batch: 383 , training loss: 6.048483\n",
      "[INFO] Epoch: 0 , batch: 384 , training loss: 6.636466\n",
      "[INFO] Epoch: 0 , batch: 385 , training loss: 6.268784\n",
      "[INFO] Epoch: 0 , batch: 386 , training loss: 6.253969\n",
      "[INFO] Epoch: 0 , batch: 387 , training loss: 6.124063\n",
      "[INFO] Epoch: 0 , batch: 388 , training loss: 5.981903\n",
      "[INFO] Epoch: 0 , batch: 389 , training loss: 5.830017\n",
      "[INFO] Epoch: 0 , batch: 390 , training loss: 5.838004\n",
      "[INFO] Epoch: 0 , batch: 391 , training loss: 5.896564\n",
      "[INFO] Epoch: 0 , batch: 392 , training loss: 6.057751\n",
      "[INFO] Epoch: 0 , batch: 393 , training loss: 5.963992\n",
      "[INFO] Epoch: 0 , batch: 394 , training loss: 5.957837\n",
      "[INFO] Epoch: 0 , batch: 395 , training loss: 5.818413\n",
      "[INFO] Epoch: 0 , batch: 396 , training loss: 5.680428\n",
      "[INFO] Epoch: 0 , batch: 397 , training loss: 5.789879\n",
      "[INFO] Epoch: 0 , batch: 398 , training loss: 5.682828\n",
      "[INFO] Epoch: 0 , batch: 399 , training loss: 5.703964\n",
      "[INFO] Epoch: 0 , batch: 400 , training loss: 5.637330\n",
      "[INFO] Epoch: 0 , batch: 401 , training loss: 5.925577\n",
      "[INFO] Epoch: 0 , batch: 402 , training loss: 5.732773\n",
      "[INFO] Epoch: 0 , batch: 403 , training loss: 5.756558\n",
      "[INFO] Epoch: 0 , batch: 404 , training loss: 5.808180\n",
      "[INFO] Epoch: 0 , batch: 405 , training loss: 5.838082\n",
      "[INFO] Epoch: 0 , batch: 406 , training loss: 5.796576\n",
      "[INFO] Epoch: 0 , batch: 407 , training loss: 5.756178\n",
      "[INFO] Epoch: 0 , batch: 408 , training loss: 5.764165\n",
      "[INFO] Epoch: 0 , batch: 409 , training loss: 5.644609\n",
      "[INFO] Epoch: 0 , batch: 410 , training loss: 5.601307\n",
      "[INFO] Epoch: 0 , batch: 411 , training loss: 5.810784\n",
      "[INFO] Epoch: 0 , batch: 412 , training loss: 5.718990\n",
      "[INFO] Epoch: 0 , batch: 413 , training loss: 5.634911\n",
      "[INFO] Epoch: 0 , batch: 414 , training loss: 5.650491\n",
      "[INFO] Epoch: 0 , batch: 415 , training loss: 5.671470\n",
      "[INFO] Epoch: 0 , batch: 416 , training loss: 5.730609\n",
      "[INFO] Epoch: 0 , batch: 417 , training loss: 5.645059\n",
      "[INFO] Epoch: 0 , batch: 418 , training loss: 5.655812\n",
      "[INFO] Epoch: 0 , batch: 419 , training loss: 5.666058\n",
      "[INFO] Epoch: 0 , batch: 420 , training loss: 5.569130\n",
      "[INFO] Epoch: 0 , batch: 421 , training loss: 5.520663\n",
      "[INFO] Epoch: 0 , batch: 422 , training loss: 5.517304\n",
      "[INFO] Epoch: 0 , batch: 423 , training loss: 5.736814\n",
      "[INFO] Epoch: 0 , batch: 424 , training loss: 5.762856\n",
      "[INFO] Epoch: 0 , batch: 425 , training loss: 5.728074\n",
      "[INFO] Epoch: 0 , batch: 426 , training loss: 5.391970\n",
      "[INFO] Epoch: 0 , batch: 427 , training loss: 5.676169\n",
      "[INFO] Epoch: 0 , batch: 428 , training loss: 5.625719\n",
      "[INFO] Epoch: 0 , batch: 429 , training loss: 5.399272\n",
      "[INFO] Epoch: 0 , batch: 430 , training loss: 5.662609\n",
      "[INFO] Epoch: 0 , batch: 431 , training loss: 5.336567\n",
      "[INFO] Epoch: 0 , batch: 432 , training loss: 5.423318\n",
      "[INFO] Epoch: 0 , batch: 433 , training loss: 5.455752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 0 , batch: 434 , training loss: 5.308324\n",
      "[INFO] Epoch: 0 , batch: 435 , training loss: 5.556766\n",
      "[INFO] Epoch: 0 , batch: 436 , training loss: 5.649966\n",
      "[INFO] Epoch: 0 , batch: 437 , training loss: 5.509487\n",
      "[INFO] Epoch: 0 , batch: 438 , training loss: 5.226362\n",
      "[INFO] Epoch: 0 , batch: 439 , training loss: 5.436584\n",
      "[INFO] Epoch: 0 , batch: 440 , training loss: 5.532819\n",
      "[INFO] Epoch: 0 , batch: 441 , training loss: 5.536446\n",
      "[INFO] Epoch: 0 , batch: 442 , training loss: 5.455103\n",
      "[INFO] Epoch: 0 , batch: 443 , training loss: 5.587167\n",
      "[INFO] Epoch: 0 , batch: 444 , training loss: 5.359671\n",
      "[INFO] Epoch: 0 , batch: 445 , training loss: 5.197024\n",
      "[INFO] Epoch: 0 , batch: 446 , training loss: 5.040317\n",
      "[INFO] Epoch: 0 , batch: 447 , training loss: 5.320418\n",
      "[INFO] Epoch: 0 , batch: 448 , training loss: 5.463908\n",
      "[INFO] Epoch: 0 , batch: 449 , training loss: 5.751895\n",
      "[INFO] Epoch: 0 , batch: 450 , training loss: 5.734023\n",
      "[INFO] Epoch: 0 , batch: 451 , training loss: 5.621028\n",
      "[INFO] Epoch: 0 , batch: 452 , training loss: 5.417502\n",
      "[INFO] Epoch: 0 , batch: 453 , training loss: 5.301630\n",
      "[INFO] Epoch: 0 , batch: 454 , training loss: 5.406677\n",
      "[INFO] Epoch: 0 , batch: 455 , training loss: 5.446931\n",
      "[INFO] Epoch: 0 , batch: 456 , training loss: 5.425339\n",
      "[INFO] Epoch: 0 , batch: 457 , training loss: 5.492913\n",
      "[INFO] Epoch: 0 , batch: 458 , training loss: 5.249519\n",
      "[INFO] Epoch: 0 , batch: 459 , training loss: 5.264835\n",
      "[INFO] Epoch: 0 , batch: 460 , training loss: 5.416912\n",
      "[INFO] Epoch: 0 , batch: 461 , training loss: 5.420007\n",
      "[INFO] Epoch: 0 , batch: 462 , training loss: 5.457999\n",
      "[INFO] Epoch: 0 , batch: 463 , training loss: 5.299725\n",
      "[INFO] Epoch: 0 , batch: 464 , training loss: 5.424336\n",
      "[INFO] Epoch: 0 , batch: 465 , training loss: 5.370548\n",
      "[INFO] Epoch: 0 , batch: 466 , training loss: 5.439229\n",
      "[INFO] Epoch: 0 , batch: 467 , training loss: 5.537354\n",
      "[INFO] Epoch: 0 , batch: 468 , training loss: 5.481805\n",
      "[INFO] Epoch: 0 , batch: 469 , training loss: 5.419820\n",
      "[INFO] Epoch: 0 , batch: 470 , training loss: 5.186059\n",
      "[INFO] Epoch: 0 , batch: 471 , training loss: 5.278153\n",
      "[INFO] Epoch: 0 , batch: 472 , training loss: 5.345905\n",
      "[INFO] Epoch: 0 , batch: 473 , training loss: 5.257362\n",
      "[INFO] Epoch: 0 , batch: 474 , training loss: 5.185334\n",
      "[INFO] Epoch: 0 , batch: 475 , training loss: 5.047013\n",
      "[INFO] Epoch: 0 , batch: 476 , training loss: 5.324317\n",
      "[INFO] Epoch: 0 , batch: 477 , training loss: 5.390473\n",
      "[INFO] Epoch: 0 , batch: 478 , training loss: 5.533730\n",
      "[INFO] Epoch: 0 , batch: 479 , training loss: 5.491871\n",
      "[INFO] Epoch: 0 , batch: 480 , training loss: 5.626534\n",
      "[INFO] Epoch: 0 , batch: 481 , training loss: 5.353749\n",
      "[INFO] Epoch: 0 , batch: 482 , training loss: 5.507417\n",
      "[INFO] Epoch: 0 , batch: 483 , training loss: 5.350543\n",
      "[INFO] Epoch: 0 , batch: 484 , training loss: 5.232359\n",
      "[INFO] Epoch: 0 , batch: 485 , training loss: 5.407444\n",
      "[INFO] Epoch: 0 , batch: 486 , training loss: 5.247360\n",
      "[INFO] Epoch: 0 , batch: 487 , training loss: 5.251112\n",
      "[INFO] Epoch: 0 , batch: 488 , training loss: 5.396562\n",
      "[INFO] Epoch: 0 , batch: 489 , training loss: 5.269806\n",
      "[INFO] Epoch: 0 , batch: 490 , training loss: 5.357079\n",
      "[INFO] Epoch: 0 , batch: 491 , training loss: 5.369587\n",
      "[INFO] Epoch: 0 , batch: 492 , training loss: 5.123779\n",
      "[INFO] Epoch: 0 , batch: 493 , training loss: 5.329222\n",
      "[INFO] Epoch: 0 , batch: 494 , training loss: 5.276390\n",
      "[INFO] Epoch: 0 , batch: 495 , training loss: 5.370169\n",
      "[INFO] Epoch: 0 , batch: 496 , training loss: 5.226426\n",
      "[INFO] Epoch: 0 , batch: 497 , training loss: 5.239431\n",
      "[INFO] Epoch: 0 , batch: 498 , training loss: 5.332176\n",
      "[INFO] Epoch: 0 , batch: 499 , training loss: 5.334336\n",
      "[INFO] Epoch: 0 , batch: 500 , training loss: 5.727369\n",
      "[INFO] Epoch: 0 , batch: 501 , training loss: 6.235823\n",
      "[INFO] Epoch: 0 , batch: 502 , training loss: 6.632020\n",
      "[INFO] Epoch: 0 , batch: 503 , training loss: 6.359384\n",
      "[INFO] Epoch: 0 , batch: 504 , training loss: 6.294011\n",
      "[INFO] Epoch: 0 , batch: 505 , training loss: 6.166213\n",
      "[INFO] Epoch: 0 , batch: 506 , training loss: 5.896966\n",
      "[INFO] Epoch: 0 , batch: 507 , training loss: 5.748521\n",
      "[INFO] Epoch: 0 , batch: 508 , training loss: 5.780246\n",
      "[INFO] Epoch: 0 , batch: 509 , training loss: 5.579730\n",
      "[INFO] Epoch: 0 , batch: 510 , training loss: 5.662968\n",
      "[INFO] Epoch: 0 , batch: 511 , training loss: 5.597622\n",
      "[INFO] Epoch: 0 , batch: 512 , training loss: 5.577209\n",
      "[INFO] Epoch: 0 , batch: 513 , training loss: 5.726940\n",
      "[INFO] Epoch: 0 , batch: 514 , training loss: 5.531164\n",
      "[INFO] Epoch: 0 , batch: 515 , training loss: 5.691113\n",
      "[INFO] Epoch: 0 , batch: 516 , training loss: 5.559297\n",
      "[INFO] Epoch: 0 , batch: 517 , training loss: 5.576736\n",
      "[INFO] Epoch: 0 , batch: 518 , training loss: 5.521777\n",
      "[INFO] Epoch: 0 , batch: 519 , training loss: 5.460995\n",
      "[INFO] Epoch: 0 , batch: 520 , training loss: 5.554604\n",
      "[INFO] Epoch: 0 , batch: 521 , training loss: 5.636849\n",
      "[INFO] Epoch: 0 , batch: 522 , training loss: 5.693303\n",
      "[INFO] Epoch: 0 , batch: 523 , training loss: 5.513823\n",
      "[INFO] Epoch: 0 , batch: 524 , training loss: 5.592955\n",
      "[INFO] Epoch: 0 , batch: 525 , training loss: 5.598414\n",
      "[INFO] Epoch: 0 , batch: 526 , training loss: 5.405928\n",
      "[INFO] Epoch: 0 , batch: 527 , training loss: 5.429510\n",
      "[INFO] Epoch: 0 , batch: 528 , training loss: 5.551191\n",
      "[INFO] Epoch: 0 , batch: 529 , training loss: 5.493014\n",
      "[INFO] Epoch: 0 , batch: 530 , training loss: 5.334831\n",
      "[INFO] Epoch: 0 , batch: 531 , training loss: 5.475454\n",
      "[INFO] Epoch: 0 , batch: 532 , training loss: 5.313682\n",
      "[INFO] Epoch: 0 , batch: 533 , training loss: 5.380096\n",
      "[INFO] Epoch: 0 , batch: 534 , training loss: 5.425632\n",
      "[INFO] Epoch: 0 , batch: 535 , training loss: 5.476324\n",
      "[INFO] Epoch: 0 , batch: 536 , training loss: 5.412553\n",
      "[INFO] Epoch: 0 , batch: 537 , training loss: 5.351199\n",
      "[INFO] Epoch: 0 , batch: 538 , training loss: 5.390596\n",
      "[INFO] Epoch: 0 , batch: 539 , training loss: 5.575782\n",
      "[INFO] Epoch: 0 , batch: 540 , training loss: 6.526034\n",
      "[INFO] Epoch: 0 , batch: 541 , training loss: 6.483457\n",
      "[INFO] Epoch: 0 , batch: 542 , training loss: 6.045586\n",
      "[INFO] Epoch: 1 , batch: 0 , training loss: 5.797071\n",
      "[INFO] Epoch: 1 , batch: 1 , training loss: 5.464664\n",
      "[INFO] Epoch: 1 , batch: 2 , training loss: 5.498168\n",
      "[INFO] Epoch: 1 , batch: 3 , training loss: 4.932796\n",
      "[INFO] Epoch: 1 , batch: 4 , training loss: 5.562271\n",
      "[INFO] Epoch: 1 , batch: 5 , training loss: 5.315383\n",
      "[INFO] Epoch: 1 , batch: 6 , training loss: 6.608071\n",
      "[INFO] Epoch: 1 , batch: 7 , training loss: 5.667200\n",
      "[INFO] Epoch: 1 , batch: 8 , training loss: 5.443540\n",
      "[INFO] Epoch: 1 , batch: 9 , training loss: 5.314698\n",
      "[INFO] Epoch: 1 , batch: 10 , training loss: 5.117959\n",
      "[INFO] Epoch: 1 , batch: 11 , training loss: 4.977564\n",
      "[INFO] Epoch: 1 , batch: 12 , training loss: 5.145503\n",
      "[INFO] Epoch: 1 , batch: 13 , training loss: 5.024447\n",
      "[INFO] Epoch: 1 , batch: 14 , training loss: 4.858847\n",
      "[INFO] Epoch: 1 , batch: 15 , training loss: 5.066943\n",
      "[INFO] Epoch: 1 , batch: 16 , training loss: 4.929743\n",
      "[INFO] Epoch: 1 , batch: 17 , training loss: 4.982884\n",
      "[INFO] Epoch: 1 , batch: 18 , training loss: 4.922761\n",
      "[INFO] Epoch: 1 , batch: 19 , training loss: 4.807066\n",
      "[INFO] Epoch: 1 , batch: 20 , training loss: 4.687367\n",
      "[INFO] Epoch: 1 , batch: 21 , training loss: 4.781824\n",
      "[INFO] Epoch: 1 , batch: 22 , training loss: 4.995444\n",
      "[INFO] Epoch: 1 , batch: 23 , training loss: 4.975909\n",
      "[INFO] Epoch: 1 , batch: 24 , training loss: 4.829988\n",
      "[INFO] Epoch: 1 , batch: 25 , training loss: 4.975054\n",
      "[INFO] Epoch: 1 , batch: 26 , training loss: 4.757607\n",
      "[INFO] Epoch: 1 , batch: 27 , training loss: 4.752320\n",
      "[INFO] Epoch: 1 , batch: 28 , training loss: 5.017166\n",
      "[INFO] Epoch: 1 , batch: 29 , training loss: 4.771260\n",
      "[INFO] Epoch: 1 , batch: 30 , training loss: 4.743510\n",
      "[INFO] Epoch: 1 , batch: 31 , training loss: 4.820051\n",
      "[INFO] Epoch: 1 , batch: 32 , training loss: 4.835415\n",
      "[INFO] Epoch: 1 , batch: 33 , training loss: 4.879002\n",
      "[INFO] Epoch: 1 , batch: 34 , training loss: 4.962563\n",
      "[INFO] Epoch: 1 , batch: 35 , training loss: 4.811480\n",
      "[INFO] Epoch: 1 , batch: 36 , training loss: 4.781435\n",
      "[INFO] Epoch: 1 , batch: 37 , training loss: 4.681462\n",
      "[INFO] Epoch: 1 , batch: 38 , training loss: 4.818538\n",
      "[INFO] Epoch: 1 , batch: 39 , training loss: 4.661597\n",
      "[INFO] Epoch: 1 , batch: 40 , training loss: 4.657676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 1 , batch: 41 , training loss: 4.976492\n",
      "[INFO] Epoch: 1 , batch: 42 , training loss: 5.557801\n",
      "[INFO] Epoch: 1 , batch: 43 , training loss: 5.455974\n",
      "[INFO] Epoch: 1 , batch: 44 , training loss: 5.417092\n",
      "[INFO] Epoch: 1 , batch: 45 , training loss: 5.852530\n",
      "[INFO] Epoch: 1 , batch: 46 , training loss: 6.655379\n",
      "[INFO] Epoch: 1 , batch: 47 , training loss: 5.337350\n",
      "[INFO] Epoch: 1 , batch: 48 , training loss: 5.490172\n",
      "[INFO] Epoch: 1 , batch: 49 , training loss: 5.376513\n",
      "[INFO] Epoch: 1 , batch: 50 , training loss: 5.337482\n",
      "[INFO] Epoch: 1 , batch: 51 , training loss: 5.164062\n",
      "[INFO] Epoch: 1 , batch: 52 , training loss: 4.954906\n",
      "[INFO] Epoch: 1 , batch: 53 , training loss: 5.132703\n",
      "[INFO] Epoch: 1 , batch: 54 , training loss: 5.074963\n",
      "[INFO] Epoch: 1 , batch: 55 , training loss: 5.135817\n",
      "[INFO] Epoch: 1 , batch: 56 , training loss: 4.960893\n",
      "[INFO] Epoch: 1 , batch: 57 , training loss: 4.794795\n",
      "[INFO] Epoch: 1 , batch: 58 , training loss: 4.819805\n",
      "[INFO] Epoch: 1 , batch: 59 , training loss: 5.037355\n",
      "[INFO] Epoch: 1 , batch: 60 , training loss: 4.762548\n",
      "[INFO] Epoch: 1 , batch: 61 , training loss: 4.888406\n",
      "[INFO] Epoch: 1 , batch: 62 , training loss: 4.779168\n",
      "[INFO] Epoch: 1 , batch: 63 , training loss: 4.957585\n",
      "[INFO] Epoch: 1 , batch: 64 , training loss: 5.040301\n",
      "[INFO] Epoch: 1 , batch: 65 , training loss: 4.918789\n",
      "[INFO] Epoch: 1 , batch: 66 , training loss: 4.740561\n",
      "[INFO] Epoch: 1 , batch: 67 , training loss: 4.710878\n",
      "[INFO] Epoch: 1 , batch: 68 , training loss: 5.094639\n",
      "[INFO] Epoch: 1 , batch: 69 , training loss: 4.808112\n",
      "[INFO] Epoch: 1 , batch: 70 , training loss: 5.068082\n",
      "[INFO] Epoch: 1 , batch: 71 , training loss: 4.871087\n",
      "[INFO] Epoch: 1 , batch: 72 , training loss: 4.947216\n",
      "[INFO] Epoch: 1 , batch: 73 , training loss: 4.864516\n",
      "[INFO] Epoch: 1 , batch: 74 , training loss: 4.955230\n",
      "[INFO] Epoch: 1 , batch: 75 , training loss: 4.634733\n",
      "[INFO] Epoch: 1 , batch: 76 , training loss: 4.894482\n",
      "[INFO] Epoch: 1 , batch: 77 , training loss: 4.736438\n",
      "[INFO] Epoch: 1 , batch: 78 , training loss: 4.775064\n",
      "[INFO] Epoch: 1 , batch: 79 , training loss: 4.641807\n",
      "[INFO] Epoch: 1 , batch: 80 , training loss: 4.855776\n",
      "[INFO] Epoch: 1 , batch: 81 , training loss: 4.894210\n",
      "[INFO] Epoch: 1 , batch: 82 , training loss: 4.910728\n",
      "[INFO] Epoch: 1 , batch: 83 , training loss: 4.963792\n",
      "[INFO] Epoch: 1 , batch: 84 , training loss: 4.841816\n",
      "[INFO] Epoch: 1 , batch: 85 , training loss: 4.942035\n",
      "[INFO] Epoch: 1 , batch: 86 , training loss: 4.962492\n",
      "[INFO] Epoch: 1 , batch: 87 , training loss: 4.916025\n",
      "[INFO] Epoch: 1 , batch: 88 , training loss: 5.030522\n",
      "[INFO] Epoch: 1 , batch: 89 , training loss: 4.827107\n",
      "[INFO] Epoch: 1 , batch: 90 , training loss: 4.804585\n",
      "[INFO] Epoch: 1 , batch: 91 , training loss: 4.803823\n",
      "[INFO] Epoch: 1 , batch: 92 , training loss: 4.794117\n",
      "[INFO] Epoch: 1 , batch: 93 , training loss: 4.833796\n",
      "[INFO] Epoch: 1 , batch: 94 , training loss: 5.078552\n",
      "[INFO] Epoch: 1 , batch: 95 , training loss: 4.828187\n",
      "[INFO] Epoch: 1 , batch: 96 , training loss: 4.796416\n",
      "[INFO] Epoch: 1 , batch: 97 , training loss: 4.805555\n",
      "[INFO] Epoch: 1 , batch: 98 , training loss: 4.829762\n",
      "[INFO] Epoch: 1 , batch: 99 , training loss: 4.821961\n",
      "[INFO] Epoch: 1 , batch: 100 , training loss: 4.654214\n",
      "[INFO] Epoch: 1 , batch: 101 , training loss: 4.698869\n",
      "[INFO] Epoch: 1 , batch: 102 , training loss: 4.906598\n",
      "[INFO] Epoch: 1 , batch: 103 , training loss: 4.581833\n",
      "[INFO] Epoch: 1 , batch: 104 , training loss: 4.641345\n",
      "[INFO] Epoch: 1 , batch: 105 , training loss: 4.879467\n",
      "[INFO] Epoch: 1 , batch: 106 , training loss: 4.875341\n",
      "[INFO] Epoch: 1 , batch: 107 , training loss: 4.828452\n",
      "[INFO] Epoch: 1 , batch: 108 , training loss: 4.630216\n",
      "[INFO] Epoch: 1 , batch: 109 , training loss: 4.513110\n",
      "[INFO] Epoch: 1 , batch: 110 , training loss: 4.890831\n",
      "[INFO] Epoch: 1 , batch: 111 , training loss: 4.821745\n",
      "[INFO] Epoch: 1 , batch: 112 , training loss: 4.851618\n",
      "[INFO] Epoch: 1 , batch: 113 , training loss: 4.719603\n",
      "[INFO] Epoch: 1 , batch: 114 , training loss: 4.926890\n",
      "[INFO] Epoch: 1 , batch: 115 , training loss: 4.846489\n",
      "[INFO] Epoch: 1 , batch: 116 , training loss: 4.849262\n",
      "[INFO] Epoch: 1 , batch: 117 , training loss: 5.055081\n",
      "[INFO] Epoch: 1 , batch: 118 , training loss: 5.047984\n",
      "[INFO] Epoch: 1 , batch: 119 , training loss: 5.086840\n",
      "[INFO] Epoch: 1 , batch: 120 , training loss: 5.026861\n",
      "[INFO] Epoch: 1 , batch: 121 , training loss: 4.778233\n",
      "[INFO] Epoch: 1 , batch: 122 , training loss: 4.772081\n",
      "[INFO] Epoch: 1 , batch: 123 , training loss: 4.930509\n",
      "[INFO] Epoch: 1 , batch: 124 , training loss: 5.091779\n",
      "[INFO] Epoch: 1 , batch: 125 , training loss: 4.644147\n",
      "[INFO] Epoch: 1 , batch: 126 , training loss: 4.685957\n",
      "[INFO] Epoch: 1 , batch: 127 , training loss: 4.720119\n",
      "[INFO] Epoch: 1 , batch: 128 , training loss: 4.938480\n",
      "[INFO] Epoch: 1 , batch: 129 , training loss: 4.814409\n",
      "[INFO] Epoch: 1 , batch: 130 , training loss: 4.901100\n",
      "[INFO] Epoch: 1 , batch: 131 , training loss: 4.856575\n",
      "[INFO] Epoch: 1 , batch: 132 , training loss: 4.889099\n",
      "[INFO] Epoch: 1 , batch: 133 , training loss: 4.768250\n",
      "[INFO] Epoch: 1 , batch: 134 , training loss: 4.605518\n",
      "[INFO] Epoch: 1 , batch: 135 , training loss: 4.598067\n",
      "[INFO] Epoch: 1 , batch: 136 , training loss: 4.855147\n",
      "[INFO] Epoch: 1 , batch: 137 , training loss: 4.768743\n",
      "[INFO] Epoch: 1 , batch: 138 , training loss: 4.978267\n",
      "[INFO] Epoch: 1 , batch: 139 , training loss: 5.682472\n",
      "[INFO] Epoch: 1 , batch: 140 , training loss: 5.608721\n",
      "[INFO] Epoch: 1 , batch: 141 , training loss: 5.305738\n",
      "[INFO] Epoch: 1 , batch: 142 , training loss: 4.917936\n",
      "[INFO] Epoch: 1 , batch: 143 , training loss: 4.979096\n",
      "[INFO] Epoch: 1 , batch: 144 , training loss: 4.833349\n",
      "[INFO] Epoch: 1 , batch: 145 , training loss: 4.947953\n",
      "[INFO] Epoch: 1 , batch: 146 , training loss: 5.076877\n",
      "[INFO] Epoch: 1 , batch: 147 , training loss: 4.639763\n",
      "[INFO] Epoch: 1 , batch: 148 , training loss: 4.637111\n",
      "[INFO] Epoch: 1 , batch: 149 , training loss: 4.792377\n",
      "[INFO] Epoch: 1 , batch: 150 , training loss: 5.067361\n",
      "[INFO] Epoch: 1 , batch: 151 , training loss: 4.653582\n",
      "[INFO] Epoch: 1 , batch: 152 , training loss: 4.735936\n",
      "[INFO] Epoch: 1 , batch: 153 , training loss: 4.919880\n",
      "[INFO] Epoch: 1 , batch: 154 , training loss: 4.959173\n",
      "[INFO] Epoch: 1 , batch: 155 , training loss: 5.223652\n",
      "[INFO] Epoch: 1 , batch: 156 , training loss: 4.823119\n",
      "[INFO] Epoch: 1 , batch: 157 , training loss: 4.882548\n",
      "[INFO] Epoch: 1 , batch: 158 , training loss: 5.300896\n",
      "[INFO] Epoch: 1 , batch: 159 , training loss: 5.323895\n",
      "[INFO] Epoch: 1 , batch: 160 , training loss: 6.275224\n",
      "[INFO] Epoch: 1 , batch: 161 , training loss: 5.783983\n",
      "[INFO] Epoch: 1 , batch: 162 , training loss: 5.400766\n",
      "[INFO] Epoch: 1 , batch: 163 , training loss: 5.333279\n",
      "[INFO] Epoch: 1 , batch: 164 , training loss: 5.132477\n",
      "[INFO] Epoch: 1 , batch: 165 , training loss: 5.164289\n",
      "[INFO] Epoch: 1 , batch: 166 , training loss: 5.921214\n",
      "[INFO] Epoch: 1 , batch: 167 , training loss: 6.498914\n",
      "[INFO] Epoch: 1 , batch: 168 , training loss: 6.351655\n",
      "[INFO] Epoch: 1 , batch: 169 , training loss: 6.094164\n",
      "[INFO] Epoch: 1 , batch: 170 , training loss: 5.982265\n",
      "[INFO] Epoch: 1 , batch: 171 , training loss: 5.849955\n",
      "[INFO] Epoch: 1 , batch: 172 , training loss: 5.832456\n",
      "[INFO] Epoch: 1 , batch: 173 , training loss: 5.862838\n",
      "[INFO] Epoch: 1 , batch: 174 , training loss: 5.863490\n",
      "[INFO] Epoch: 1 , batch: 175 , training loss: 5.824378\n",
      "[INFO] Epoch: 1 , batch: 176 , training loss: 5.815336\n",
      "[INFO] Epoch: 1 , batch: 177 , training loss: 5.599515\n",
      "[INFO] Epoch: 1 , batch: 178 , training loss: 5.482870\n",
      "[INFO] Epoch: 1 , batch: 179 , training loss: 5.474790\n",
      "[INFO] Epoch: 1 , batch: 180 , training loss: 5.440108\n",
      "[INFO] Epoch: 1 , batch: 181 , training loss: 5.566834\n",
      "[INFO] Epoch: 1 , batch: 182 , training loss: 5.439600\n",
      "[INFO] Epoch: 1 , batch: 183 , training loss: 5.338859\n",
      "[INFO] Epoch: 1 , batch: 184 , training loss: 5.251675\n",
      "[INFO] Epoch: 1 , batch: 185 , training loss: 5.201762\n",
      "[INFO] Epoch: 1 , batch: 186 , training loss: 5.289815\n",
      "[INFO] Epoch: 1 , batch: 187 , training loss: 5.398491\n",
      "[INFO] Epoch: 1 , batch: 188 , training loss: 5.342190\n",
      "[INFO] Epoch: 1 , batch: 189 , training loss: 5.275311\n",
      "[INFO] Epoch: 1 , batch: 190 , training loss: 5.208954\n",
      "[INFO] Epoch: 1 , batch: 191 , training loss: 5.358002\n",
      "[INFO] Epoch: 1 , batch: 192 , training loss: 5.111986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 1 , batch: 193 , training loss: 5.215825\n",
      "[INFO] Epoch: 1 , batch: 194 , training loss: 5.129432\n",
      "[INFO] Epoch: 1 , batch: 195 , training loss: 5.299258\n",
      "[INFO] Epoch: 1 , batch: 196 , training loss: 5.010823\n",
      "[INFO] Epoch: 1 , batch: 197 , training loss: 5.261991\n",
      "[INFO] Epoch: 1 , batch: 198 , training loss: 5.002031\n",
      "[INFO] Epoch: 1 , batch: 199 , training loss: 5.087886\n",
      "[INFO] Epoch: 1 , batch: 200 , training loss: 5.029490\n",
      "[INFO] Epoch: 1 , batch: 201 , training loss: 4.981647\n",
      "[INFO] Epoch: 1 , batch: 202 , training loss: 4.924783\n",
      "[INFO] Epoch: 1 , batch: 203 , training loss: 4.866120\n",
      "[INFO] Epoch: 1 , batch: 204 , training loss: 5.065695\n",
      "[INFO] Epoch: 1 , batch: 205 , training loss: 4.677141\n",
      "[INFO] Epoch: 1 , batch: 206 , training loss: 4.531775\n",
      "[INFO] Epoch: 1 , batch: 207 , training loss: 4.599157\n",
      "[INFO] Epoch: 1 , batch: 208 , training loss: 5.054193\n",
      "[INFO] Epoch: 1 , batch: 209 , training loss: 4.953005\n",
      "[INFO] Epoch: 1 , batch: 210 , training loss: 4.994736\n",
      "[INFO] Epoch: 1 , batch: 211 , training loss: 4.883929\n",
      "[INFO] Epoch: 1 , batch: 212 , training loss: 4.920228\n",
      "[INFO] Epoch: 1 , batch: 213 , training loss: 4.998723\n",
      "[INFO] Epoch: 1 , batch: 214 , training loss: 4.903633\n",
      "[INFO] Epoch: 1 , batch: 215 , training loss: 5.113014\n",
      "[INFO] Epoch: 1 , batch: 216 , training loss: 4.954527\n",
      "[INFO] Epoch: 1 , batch: 217 , training loss: 4.834467\n",
      "[INFO] Epoch: 1 , batch: 218 , training loss: 4.899756\n",
      "[INFO] Epoch: 1 , batch: 219 , training loss: 4.927310\n",
      "[INFO] Epoch: 1 , batch: 220 , training loss: 4.785283\n",
      "[INFO] Epoch: 1 , batch: 221 , training loss: 4.775012\n",
      "[INFO] Epoch: 1 , batch: 222 , training loss: 4.930720\n",
      "[INFO] Epoch: 1 , batch: 223 , training loss: 5.028471\n",
      "[INFO] Epoch: 1 , batch: 224 , training loss: 5.059828\n",
      "[INFO] Epoch: 1 , batch: 225 , training loss: 4.915427\n",
      "[INFO] Epoch: 1 , batch: 226 , training loss: 5.046066\n",
      "[INFO] Epoch: 1 , batch: 227 , training loss: 4.994468\n",
      "[INFO] Epoch: 1 , batch: 228 , training loss: 5.073761\n",
      "[INFO] Epoch: 1 , batch: 229 , training loss: 4.956979\n",
      "[INFO] Epoch: 1 , batch: 230 , training loss: 4.764780\n",
      "[INFO] Epoch: 1 , batch: 231 , training loss: 4.597447\n",
      "[INFO] Epoch: 1 , batch: 232 , training loss: 4.705979\n",
      "[INFO] Epoch: 1 , batch: 233 , training loss: 4.805788\n",
      "[INFO] Epoch: 1 , batch: 234 , training loss: 4.449045\n",
      "[INFO] Epoch: 1 , batch: 235 , training loss: 4.613215\n",
      "[INFO] Epoch: 1 , batch: 236 , training loss: 4.812057\n",
      "[INFO] Epoch: 1 , batch: 237 , training loss: 4.919277\n",
      "[INFO] Epoch: 1 , batch: 238 , training loss: 4.662322\n",
      "[INFO] Epoch: 1 , batch: 239 , training loss: 4.708586\n",
      "[INFO] Epoch: 1 , batch: 240 , training loss: 4.805205\n",
      "[INFO] Epoch: 1 , batch: 241 , training loss: 4.579439\n",
      "[INFO] Epoch: 1 , batch: 242 , training loss: 4.579072\n",
      "[INFO] Epoch: 1 , batch: 243 , training loss: 4.938546\n",
      "[INFO] Epoch: 1 , batch: 244 , training loss: 4.803668\n",
      "[INFO] Epoch: 1 , batch: 245 , training loss: 4.804964\n",
      "[INFO] Epoch: 1 , batch: 246 , training loss: 4.443419\n",
      "[INFO] Epoch: 1 , batch: 247 , training loss: 4.593440\n",
      "[INFO] Epoch: 1 , batch: 248 , training loss: 4.696141\n",
      "[INFO] Epoch: 1 , batch: 249 , training loss: 4.632594\n",
      "[INFO] Epoch: 1 , batch: 250 , training loss: 4.446521\n",
      "[INFO] Epoch: 1 , batch: 251 , training loss: 4.904651\n",
      "[INFO] Epoch: 1 , batch: 252 , training loss: 4.610681\n",
      "[INFO] Epoch: 1 , batch: 253 , training loss: 4.638158\n",
      "[INFO] Epoch: 1 , batch: 254 , training loss: 4.968190\n",
      "[INFO] Epoch: 1 , batch: 255 , training loss: 4.958783\n",
      "[INFO] Epoch: 1 , batch: 256 , training loss: 4.831637\n",
      "[INFO] Epoch: 1 , batch: 257 , training loss: 4.995136\n",
      "[INFO] Epoch: 1 , batch: 258 , training loss: 5.112263\n",
      "[INFO] Epoch: 1 , batch: 259 , training loss: 5.126482\n",
      "[INFO] Epoch: 1 , batch: 260 , training loss: 4.760556\n",
      "[INFO] Epoch: 1 , batch: 261 , training loss: 4.989079\n",
      "[INFO] Epoch: 1 , batch: 262 , training loss: 5.188575\n",
      "[INFO] Epoch: 1 , batch: 263 , training loss: 5.252199\n",
      "[INFO] Epoch: 1 , batch: 264 , training loss: 4.556880\n",
      "[INFO] Epoch: 1 , batch: 265 , training loss: 4.733051\n",
      "[INFO] Epoch: 1 , batch: 266 , training loss: 5.275461\n",
      "[INFO] Epoch: 1 , batch: 267 , training loss: 4.942086\n",
      "[INFO] Epoch: 1 , batch: 268 , training loss: 4.831552\n",
      "[INFO] Epoch: 1 , batch: 269 , training loss: 4.927297\n",
      "[INFO] Epoch: 1 , batch: 270 , training loss: 4.873309\n",
      "[INFO] Epoch: 1 , batch: 271 , training loss: 4.899729\n",
      "[INFO] Epoch: 1 , batch: 272 , training loss: 4.873654\n",
      "[INFO] Epoch: 1 , batch: 273 , training loss: 4.885998\n",
      "[INFO] Epoch: 1 , batch: 274 , training loss: 4.996035\n",
      "[INFO] Epoch: 1 , batch: 275 , training loss: 4.887373\n",
      "[INFO] Epoch: 1 , batch: 276 , training loss: 4.954815\n",
      "[INFO] Epoch: 1 , batch: 277 , training loss: 5.070177\n",
      "[INFO] Epoch: 1 , batch: 278 , training loss: 4.591212\n",
      "[INFO] Epoch: 1 , batch: 279 , training loss: 4.634318\n",
      "[INFO] Epoch: 1 , batch: 280 , training loss: 4.593537\n",
      "[INFO] Epoch: 1 , batch: 281 , training loss: 4.717145\n",
      "[INFO] Epoch: 1 , batch: 282 , training loss: 4.620362\n",
      "[INFO] Epoch: 1 , batch: 283 , training loss: 4.749983\n",
      "[INFO] Epoch: 1 , batch: 284 , training loss: 4.702045\n",
      "[INFO] Epoch: 1 , batch: 285 , training loss: 4.788382\n",
      "[INFO] Epoch: 1 , batch: 286 , training loss: 4.723680\n",
      "[INFO] Epoch: 1 , batch: 287 , training loss: 4.527646\n",
      "[INFO] Epoch: 1 , batch: 288 , training loss: 4.658883\n",
      "[INFO] Epoch: 1 , batch: 289 , training loss: 4.666526\n",
      "[INFO] Epoch: 1 , batch: 290 , training loss: 4.501437\n",
      "[INFO] Epoch: 1 , batch: 291 , training loss: 4.467497\n",
      "[INFO] Epoch: 1 , batch: 292 , training loss: 4.540071\n",
      "[INFO] Epoch: 1 , batch: 293 , training loss: 4.535503\n",
      "[INFO] Epoch: 1 , batch: 294 , training loss: 5.172793\n",
      "[INFO] Epoch: 1 , batch: 295 , training loss: 4.923126\n",
      "[INFO] Epoch: 1 , batch: 296 , training loss: 4.867396\n",
      "[INFO] Epoch: 1 , batch: 297 , training loss: 4.750641\n",
      "[INFO] Epoch: 1 , batch: 298 , training loss: 4.626187\n",
      "[INFO] Epoch: 1 , batch: 299 , training loss: 4.571883\n",
      "[INFO] Epoch: 1 , batch: 300 , training loss: 4.582142\n",
      "[INFO] Epoch: 1 , batch: 301 , training loss: 4.580134\n",
      "[INFO] Epoch: 1 , batch: 302 , training loss: 4.696578\n",
      "[INFO] Epoch: 1 , batch: 303 , training loss: 4.715105\n",
      "[INFO] Epoch: 1 , batch: 304 , training loss: 4.957485\n",
      "[INFO] Epoch: 1 , batch: 305 , training loss: 4.611970\n",
      "[INFO] Epoch: 1 , batch: 306 , training loss: 4.739645\n",
      "[INFO] Epoch: 1 , batch: 307 , training loss: 4.703554\n",
      "[INFO] Epoch: 1 , batch: 308 , training loss: 4.676976\n",
      "[INFO] Epoch: 1 , batch: 309 , training loss: 4.700272\n",
      "[INFO] Epoch: 1 , batch: 310 , training loss: 4.426532\n",
      "[INFO] Epoch: 1 , batch: 311 , training loss: 4.469759\n",
      "[INFO] Epoch: 1 , batch: 312 , training loss: 4.363155\n",
      "[INFO] Epoch: 1 , batch: 313 , training loss: 4.553424\n",
      "[INFO] Epoch: 1 , batch: 314 , training loss: 4.578676\n",
      "[INFO] Epoch: 1 , batch: 315 , training loss: 4.622469\n",
      "[INFO] Epoch: 1 , batch: 316 , training loss: 5.027728\n",
      "[INFO] Epoch: 1 , batch: 317 , training loss: 5.658118\n",
      "[INFO] Epoch: 1 , batch: 318 , training loss: 5.740561\n",
      "[INFO] Epoch: 1 , batch: 319 , training loss: 5.184752\n",
      "[INFO] Epoch: 1 , batch: 320 , training loss: 4.693681\n",
      "[INFO] Epoch: 1 , batch: 321 , training loss: 4.489510\n",
      "[INFO] Epoch: 1 , batch: 322 , training loss: 4.632712\n",
      "[INFO] Epoch: 1 , batch: 323 , training loss: 4.594218\n",
      "[INFO] Epoch: 1 , batch: 324 , training loss: 4.619158\n",
      "[INFO] Epoch: 1 , batch: 325 , training loss: 4.808349\n",
      "[INFO] Epoch: 1 , batch: 326 , training loss: 4.863733\n",
      "[INFO] Epoch: 1 , batch: 327 , training loss: 4.725111\n",
      "[INFO] Epoch: 1 , batch: 328 , training loss: 4.763490\n",
      "[INFO] Epoch: 1 , batch: 329 , training loss: 4.599164\n",
      "[INFO] Epoch: 1 , batch: 330 , training loss: 4.601307\n",
      "[INFO] Epoch: 1 , batch: 331 , training loss: 4.793582\n",
      "[INFO] Epoch: 1 , batch: 332 , training loss: 4.488401\n",
      "[INFO] Epoch: 1 , batch: 333 , training loss: 4.434938\n",
      "[INFO] Epoch: 1 , batch: 334 , training loss: 4.660577\n",
      "[INFO] Epoch: 1 , batch: 335 , training loss: 4.717565\n",
      "[INFO] Epoch: 1 , batch: 336 , training loss: 4.662865\n",
      "[INFO] Epoch: 1 , batch: 337 , training loss: 4.798014\n",
      "[INFO] Epoch: 1 , batch: 338 , training loss: 4.900302\n",
      "[INFO] Epoch: 1 , batch: 339 , training loss: 4.772433\n",
      "[INFO] Epoch: 1 , batch: 340 , training loss: 4.948190\n",
      "[INFO] Epoch: 1 , batch: 341 , training loss: 4.659156\n",
      "[INFO] Epoch: 1 , batch: 342 , training loss: 4.560108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 1 , batch: 343 , training loss: 4.645177\n",
      "[INFO] Epoch: 1 , batch: 344 , training loss: 4.490201\n",
      "[INFO] Epoch: 1 , batch: 345 , training loss: 4.646793\n",
      "[INFO] Epoch: 1 , batch: 346 , training loss: 4.764553\n",
      "[INFO] Epoch: 1 , batch: 347 , training loss: 4.591020\n",
      "[INFO] Epoch: 1 , batch: 348 , training loss: 4.864195\n",
      "[INFO] Epoch: 1 , batch: 349 , training loss: 4.950174\n",
      "[INFO] Epoch: 1 , batch: 350 , training loss: 4.577372\n",
      "[INFO] Epoch: 1 , batch: 351 , training loss: 4.594094\n",
      "[INFO] Epoch: 1 , batch: 352 , training loss: 4.630466\n",
      "[INFO] Epoch: 1 , batch: 353 , training loss: 4.593746\n",
      "[INFO] Epoch: 1 , batch: 354 , training loss: 4.686266\n",
      "[INFO] Epoch: 1 , batch: 355 , training loss: 4.795295\n",
      "[INFO] Epoch: 1 , batch: 356 , training loss: 4.667994\n",
      "[INFO] Epoch: 1 , batch: 357 , training loss: 4.689497\n",
      "[INFO] Epoch: 1 , batch: 358 , training loss: 4.674110\n",
      "[INFO] Epoch: 1 , batch: 359 , training loss: 4.682270\n",
      "[INFO] Epoch: 1 , batch: 360 , training loss: 4.630938\n",
      "[INFO] Epoch: 1 , batch: 361 , training loss: 4.620566\n",
      "[INFO] Epoch: 1 , batch: 362 , training loss: 4.713404\n",
      "[INFO] Epoch: 1 , batch: 363 , training loss: 4.583311\n",
      "[INFO] Epoch: 1 , batch: 364 , training loss: 4.647066\n",
      "[INFO] Epoch: 1 , batch: 365 , training loss: 4.530420\n",
      "[INFO] Epoch: 1 , batch: 366 , training loss: 4.727255\n",
      "[INFO] Epoch: 1 , batch: 367 , training loss: 4.791904\n",
      "[INFO] Epoch: 1 , batch: 368 , training loss: 5.448262\n",
      "[INFO] Epoch: 1 , batch: 369 , training loss: 5.017340\n",
      "[INFO] Epoch: 1 , batch: 370 , training loss: 4.741946\n",
      "[INFO] Epoch: 1 , batch: 371 , training loss: 5.395237\n",
      "[INFO] Epoch: 1 , batch: 372 , training loss: 5.734930\n",
      "[INFO] Epoch: 1 , batch: 373 , training loss: 5.667736\n",
      "[INFO] Epoch: 1 , batch: 374 , training loss: 5.696176\n",
      "[INFO] Epoch: 1 , batch: 375 , training loss: 5.626520\n",
      "[INFO] Epoch: 1 , batch: 376 , training loss: 5.623108\n",
      "[INFO] Epoch: 1 , batch: 377 , training loss: 5.245676\n",
      "[INFO] Epoch: 1 , batch: 378 , training loss: 5.320441\n",
      "[INFO] Epoch: 1 , batch: 379 , training loss: 5.337428\n",
      "[INFO] Epoch: 1 , batch: 380 , training loss: 5.369505\n",
      "[INFO] Epoch: 1 , batch: 381 , training loss: 5.142536\n",
      "[INFO] Epoch: 1 , batch: 382 , training loss: 5.290949\n",
      "[INFO] Epoch: 1 , batch: 383 , training loss: 5.542557\n",
      "[INFO] Epoch: 1 , batch: 384 , training loss: 5.932162\n",
      "[INFO] Epoch: 1 , batch: 385 , training loss: 5.727858\n",
      "[INFO] Epoch: 1 , batch: 386 , training loss: 5.813857\n",
      "[INFO] Epoch: 1 , batch: 387 , training loss: 5.703286\n",
      "[INFO] Epoch: 1 , batch: 388 , training loss: 5.477580\n",
      "[INFO] Epoch: 1 , batch: 389 , training loss: 5.235260\n",
      "[INFO] Epoch: 1 , batch: 390 , training loss: 5.174789\n",
      "[INFO] Epoch: 1 , batch: 391 , training loss: 5.237371\n",
      "[INFO] Epoch: 1 , batch: 392 , training loss: 5.520646\n",
      "[INFO] Epoch: 1 , batch: 393 , training loss: 5.431865\n",
      "[INFO] Epoch: 1 , batch: 394 , training loss: 5.470222\n",
      "[INFO] Epoch: 1 , batch: 395 , training loss: 5.267831\n",
      "[INFO] Epoch: 1 , batch: 396 , training loss: 5.090866\n",
      "[INFO] Epoch: 1 , batch: 397 , training loss: 5.215708\n",
      "[INFO] Epoch: 1 , batch: 398 , training loss: 5.097765\n",
      "[INFO] Epoch: 1 , batch: 399 , training loss: 5.122388\n",
      "[INFO] Epoch: 1 , batch: 400 , training loss: 5.052350\n",
      "[INFO] Epoch: 1 , batch: 401 , training loss: 5.435135\n",
      "[INFO] Epoch: 1 , batch: 402 , training loss: 5.175606\n",
      "[INFO] Epoch: 1 , batch: 403 , training loss: 5.145339\n",
      "[INFO] Epoch: 1 , batch: 404 , training loss: 5.221913\n",
      "[INFO] Epoch: 1 , batch: 405 , training loss: 5.287933\n",
      "[INFO] Epoch: 1 , batch: 406 , training loss: 5.171987\n",
      "[INFO] Epoch: 1 , batch: 407 , training loss: 5.225182\n",
      "[INFO] Epoch: 1 , batch: 408 , training loss: 5.204242\n",
      "[INFO] Epoch: 1 , batch: 409 , training loss: 5.135513\n",
      "[INFO] Epoch: 1 , batch: 410 , training loss: 5.134860\n",
      "[INFO] Epoch: 1 , batch: 411 , training loss: 5.332530\n",
      "[INFO] Epoch: 1 , batch: 412 , training loss: 5.198555\n",
      "[INFO] Epoch: 1 , batch: 413 , training loss: 5.072202\n",
      "[INFO] Epoch: 1 , batch: 414 , training loss: 5.116071\n",
      "[INFO] Epoch: 1 , batch: 415 , training loss: 5.141759\n",
      "[INFO] Epoch: 1 , batch: 416 , training loss: 5.206574\n",
      "[INFO] Epoch: 1 , batch: 417 , training loss: 5.104858\n",
      "[INFO] Epoch: 1 , batch: 418 , training loss: 5.138258\n",
      "[INFO] Epoch: 1 , batch: 419 , training loss: 5.107230\n",
      "[INFO] Epoch: 1 , batch: 420 , training loss: 5.034652\n",
      "[INFO] Epoch: 1 , batch: 421 , training loss: 5.003330\n",
      "[INFO] Epoch: 1 , batch: 422 , training loss: 4.933498\n",
      "[INFO] Epoch: 1 , batch: 423 , training loss: 5.178396\n",
      "[INFO] Epoch: 1 , batch: 424 , training loss: 5.286695\n",
      "[INFO] Epoch: 1 , batch: 425 , training loss: 5.206993\n",
      "[INFO] Epoch: 1 , batch: 426 , training loss: 4.835657\n",
      "[INFO] Epoch: 1 , batch: 427 , training loss: 5.165524\n",
      "[INFO] Epoch: 1 , batch: 428 , training loss: 5.065775\n",
      "[INFO] Epoch: 1 , batch: 429 , training loss: 4.837156\n",
      "[INFO] Epoch: 1 , batch: 430 , training loss: 5.119588\n",
      "[INFO] Epoch: 1 , batch: 431 , training loss: 4.720480\n",
      "[INFO] Epoch: 1 , batch: 432 , training loss: 4.830596\n",
      "[INFO] Epoch: 1 , batch: 433 , training loss: 4.830690\n",
      "[INFO] Epoch: 1 , batch: 434 , training loss: 4.693019\n",
      "[INFO] Epoch: 1 , batch: 435 , training loss: 5.002769\n",
      "[INFO] Epoch: 1 , batch: 436 , training loss: 5.120698\n",
      "[INFO] Epoch: 1 , batch: 437 , training loss: 4.976742\n",
      "[INFO] Epoch: 1 , batch: 438 , training loss: 4.687299\n",
      "[INFO] Epoch: 1 , batch: 439 , training loss: 4.920592\n",
      "[INFO] Epoch: 1 , batch: 440 , training loss: 5.051019\n",
      "[INFO] Epoch: 1 , batch: 441 , training loss: 5.042292\n",
      "[INFO] Epoch: 1 , batch: 442 , training loss: 4.913691\n",
      "[INFO] Epoch: 1 , batch: 443 , training loss: 5.117254\n",
      "[INFO] Epoch: 1 , batch: 444 , training loss: 4.766389\n",
      "[INFO] Epoch: 1 , batch: 445 , training loss: 4.606822\n",
      "[INFO] Epoch: 1 , batch: 446 , training loss: 4.470144\n",
      "[INFO] Epoch: 1 , batch: 447 , training loss: 4.765251\n",
      "[INFO] Epoch: 1 , batch: 448 , training loss: 4.922462\n",
      "[INFO] Epoch: 1 , batch: 449 , training loss: 5.322596\n",
      "[INFO] Epoch: 1 , batch: 450 , training loss: 5.351275\n",
      "[INFO] Epoch: 1 , batch: 451 , training loss: 5.201406\n",
      "[INFO] Epoch: 1 , batch: 452 , training loss: 4.971570\n",
      "[INFO] Epoch: 1 , batch: 453 , training loss: 4.736215\n",
      "[INFO] Epoch: 1 , batch: 454 , training loss: 4.896196\n",
      "[INFO] Epoch: 1 , batch: 455 , training loss: 4.935594\n",
      "[INFO] Epoch: 1 , batch: 456 , training loss: 4.935593\n",
      "[INFO] Epoch: 1 , batch: 457 , training loss: 5.033216\n",
      "[INFO] Epoch: 1 , batch: 458 , training loss: 4.756407\n",
      "[INFO] Epoch: 1 , batch: 459 , training loss: 4.734454\n",
      "[INFO] Epoch: 1 , batch: 460 , training loss: 4.879344\n",
      "[INFO] Epoch: 1 , batch: 461 , training loss: 4.906884\n",
      "[INFO] Epoch: 1 , batch: 462 , training loss: 4.955620\n",
      "[INFO] Epoch: 1 , batch: 463 , training loss: 4.774355\n",
      "[INFO] Epoch: 1 , batch: 464 , training loss: 4.958286\n",
      "[INFO] Epoch: 1 , batch: 465 , training loss: 4.903569\n",
      "[INFO] Epoch: 1 , batch: 466 , training loss: 4.983145\n",
      "[INFO] Epoch: 1 , batch: 467 , training loss: 5.095411\n",
      "[INFO] Epoch: 1 , batch: 468 , training loss: 5.011999\n",
      "[INFO] Epoch: 1 , batch: 469 , training loss: 4.977286\n",
      "[INFO] Epoch: 1 , batch: 470 , training loss: 4.700128\n",
      "[INFO] Epoch: 1 , batch: 471 , training loss: 4.825539\n",
      "[INFO] Epoch: 1 , batch: 472 , training loss: 4.897576\n",
      "[INFO] Epoch: 1 , batch: 473 , training loss: 4.821890\n",
      "[INFO] Epoch: 1 , batch: 474 , training loss: 4.662054\n",
      "[INFO] Epoch: 1 , batch: 475 , training loss: 4.520122\n",
      "[INFO] Epoch: 1 , batch: 476 , training loss: 4.868270\n",
      "[INFO] Epoch: 1 , batch: 477 , training loss: 4.957224\n",
      "[INFO] Epoch: 1 , batch: 478 , training loss: 5.099144\n",
      "[INFO] Epoch: 1 , batch: 479 , training loss: 5.030173\n",
      "[INFO] Epoch: 1 , batch: 480 , training loss: 5.179111\n",
      "[INFO] Epoch: 1 , batch: 481 , training loss: 4.941590\n",
      "[INFO] Epoch: 1 , batch: 482 , training loss: 5.091825\n",
      "[INFO] Epoch: 1 , batch: 483 , training loss: 4.903185\n",
      "[INFO] Epoch: 1 , batch: 484 , training loss: 4.758248\n",
      "[INFO] Epoch: 1 , batch: 485 , training loss: 4.947267\n",
      "[INFO] Epoch: 1 , batch: 486 , training loss: 4.768229\n",
      "[INFO] Epoch: 1 , batch: 487 , training loss: 4.763755\n",
      "[INFO] Epoch: 1 , batch: 488 , training loss: 4.957393\n",
      "[INFO] Epoch: 1 , batch: 489 , training loss: 4.814257\n",
      "[INFO] Epoch: 1 , batch: 490 , training loss: 4.927992\n",
      "[INFO] Epoch: 1 , batch: 491 , training loss: 4.937782\n",
      "[INFO] Epoch: 1 , batch: 492 , training loss: 4.729061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 1 , batch: 493 , training loss: 4.938041\n",
      "[INFO] Epoch: 1 , batch: 494 , training loss: 4.844097\n",
      "[INFO] Epoch: 1 , batch: 495 , training loss: 4.969440\n",
      "[INFO] Epoch: 1 , batch: 496 , training loss: 4.833888\n",
      "[INFO] Epoch: 1 , batch: 497 , training loss: 4.842844\n",
      "[INFO] Epoch: 1 , batch: 498 , training loss: 4.935579\n",
      "[INFO] Epoch: 1 , batch: 499 , training loss: 4.960608\n",
      "[INFO] Epoch: 1 , batch: 500 , training loss: 5.328450\n",
      "[INFO] Epoch: 1 , batch: 501 , training loss: 5.716793\n",
      "[INFO] Epoch: 1 , batch: 502 , training loss: 5.926220\n",
      "[INFO] Epoch: 1 , batch: 503 , training loss: 5.694429\n",
      "[INFO] Epoch: 1 , batch: 504 , training loss: 5.644473\n",
      "[INFO] Epoch: 1 , batch: 505 , training loss: 5.458888\n",
      "[INFO] Epoch: 1 , batch: 506 , training loss: 5.339753\n",
      "[INFO] Epoch: 1 , batch: 507 , training loss: 5.351998\n",
      "[INFO] Epoch: 1 , batch: 508 , training loss: 5.380705\n",
      "[INFO] Epoch: 1 , batch: 509 , training loss: 5.100219\n",
      "[INFO] Epoch: 1 , batch: 510 , training loss: 5.183724\n",
      "[INFO] Epoch: 1 , batch: 511 , training loss: 5.126267\n",
      "[INFO] Epoch: 1 , batch: 512 , training loss: 5.192003\n",
      "[INFO] Epoch: 1 , batch: 513 , training loss: 5.417308\n",
      "[INFO] Epoch: 1 , batch: 514 , training loss: 5.102254\n",
      "[INFO] Epoch: 1 , batch: 515 , training loss: 5.338263\n",
      "[INFO] Epoch: 1 , batch: 516 , training loss: 5.103312\n",
      "[INFO] Epoch: 1 , batch: 517 , training loss: 5.135528\n",
      "[INFO] Epoch: 1 , batch: 518 , training loss: 5.065129\n",
      "[INFO] Epoch: 1 , batch: 519 , training loss: 4.973187\n",
      "[INFO] Epoch: 1 , batch: 520 , training loss: 5.151450\n",
      "[INFO] Epoch: 1 , batch: 521 , training loss: 5.201350\n",
      "[INFO] Epoch: 1 , batch: 522 , training loss: 5.277921\n",
      "[INFO] Epoch: 1 , batch: 523 , training loss: 5.112131\n",
      "[INFO] Epoch: 1 , batch: 524 , training loss: 5.287745\n",
      "[INFO] Epoch: 1 , batch: 525 , training loss: 5.255609\n",
      "[INFO] Epoch: 1 , batch: 526 , training loss: 5.028571\n",
      "[INFO] Epoch: 1 , batch: 527 , training loss: 5.047815\n",
      "[INFO] Epoch: 1 , batch: 528 , training loss: 5.164887\n",
      "[INFO] Epoch: 1 , batch: 529 , training loss: 5.101456\n",
      "[INFO] Epoch: 1 , batch: 530 , training loss: 4.932643\n",
      "[INFO] Epoch: 1 , batch: 531 , training loss: 5.093527\n",
      "[INFO] Epoch: 1 , batch: 532 , training loss: 4.929557\n",
      "[INFO] Epoch: 1 , batch: 533 , training loss: 5.041241\n",
      "[INFO] Epoch: 1 , batch: 534 , training loss: 5.069873\n",
      "[INFO] Epoch: 1 , batch: 535 , training loss: 5.087587\n",
      "[INFO] Epoch: 1 , batch: 536 , training loss: 5.022807\n",
      "[INFO] Epoch: 1 , batch: 537 , training loss: 4.949956\n",
      "[INFO] Epoch: 1 , batch: 538 , training loss: 5.026509\n",
      "[INFO] Epoch: 1 , batch: 539 , training loss: 5.195112\n",
      "[INFO] Epoch: 1 , batch: 540 , training loss: 5.908092\n",
      "[INFO] Epoch: 1 , batch: 541 , training loss: 5.924212\n",
      "[INFO] Epoch: 1 , batch: 542 , training loss: 5.617044\n",
      "[INFO] Epoch: 2 , batch: 0 , training loss: 5.649794\n",
      "[INFO] Epoch: 2 , batch: 1 , training loss: 5.174370\n",
      "[INFO] Epoch: 2 , batch: 2 , training loss: 4.974780\n",
      "[INFO] Epoch: 2 , batch: 3 , training loss: 4.789288\n",
      "[INFO] Epoch: 2 , batch: 4 , training loss: 5.297029\n",
      "[INFO] Epoch: 2 , batch: 5 , training loss: 4.878715\n",
      "[INFO] Epoch: 2 , batch: 6 , training loss: 5.687302\n",
      "[INFO] Epoch: 2 , batch: 7 , training loss: 5.051749\n",
      "[INFO] Epoch: 2 , batch: 8 , training loss: 4.810361\n",
      "[INFO] Epoch: 2 , batch: 9 , training loss: 4.823349\n",
      "[INFO] Epoch: 2 , batch: 10 , training loss: 4.667217\n",
      "[INFO] Epoch: 2 , batch: 11 , training loss: 4.485800\n",
      "[INFO] Epoch: 2 , batch: 12 , training loss: 4.626091\n",
      "[INFO] Epoch: 2 , batch: 13 , training loss: 4.521614\n",
      "[INFO] Epoch: 2 , batch: 14 , training loss: 4.274891\n",
      "[INFO] Epoch: 2 , batch: 15 , training loss: 4.581600\n",
      "[INFO] Epoch: 2 , batch: 16 , training loss: 4.445610\n",
      "[INFO] Epoch: 2 , batch: 17 , training loss: 4.588732\n",
      "[INFO] Epoch: 2 , batch: 18 , training loss: 4.524462\n",
      "[INFO] Epoch: 2 , batch: 19 , training loss: 4.285695\n",
      "[INFO] Epoch: 2 , batch: 20 , training loss: 4.232240\n",
      "[INFO] Epoch: 2 , batch: 21 , training loss: 4.297860\n",
      "[INFO] Epoch: 2 , batch: 22 , training loss: 4.491834\n",
      "[INFO] Epoch: 2 , batch: 23 , training loss: 4.591726\n",
      "[INFO] Epoch: 2 , batch: 24 , training loss: 4.420785\n",
      "[INFO] Epoch: 2 , batch: 25 , training loss: 4.571174\n",
      "[INFO] Epoch: 2 , batch: 26 , training loss: 4.378745\n",
      "[INFO] Epoch: 2 , batch: 27 , training loss: 4.365752\n",
      "[INFO] Epoch: 2 , batch: 28 , training loss: 4.641881\n",
      "[INFO] Epoch: 2 , batch: 29 , training loss: 4.357689\n",
      "[INFO] Epoch: 2 , batch: 30 , training loss: 4.333122\n",
      "[INFO] Epoch: 2 , batch: 31 , training loss: 4.413366\n",
      "[INFO] Epoch: 2 , batch: 32 , training loss: 4.405565\n",
      "[INFO] Epoch: 2 , batch: 33 , training loss: 4.546942\n",
      "[INFO] Epoch: 2 , batch: 34 , training loss: 4.578800\n",
      "[INFO] Epoch: 2 , batch: 35 , training loss: 4.469729\n",
      "[INFO] Epoch: 2 , batch: 36 , training loss: 4.382164\n",
      "[INFO] Epoch: 2 , batch: 37 , training loss: 4.290018\n",
      "[INFO] Epoch: 2 , batch: 38 , training loss: 4.472300\n",
      "[INFO] Epoch: 2 , batch: 39 , training loss: 4.287605\n",
      "[INFO] Epoch: 2 , batch: 40 , training loss: 4.335865\n",
      "[INFO] Epoch: 2 , batch: 41 , training loss: 4.672320\n",
      "[INFO] Epoch: 2 , batch: 42 , training loss: 5.302917\n",
      "[INFO] Epoch: 2 , batch: 43 , training loss: 5.103168\n",
      "[INFO] Epoch: 2 , batch: 44 , training loss: 5.062316\n",
      "[INFO] Epoch: 2 , batch: 45 , training loss: 5.479341\n",
      "[INFO] Epoch: 2 , batch: 46 , training loss: 6.067944\n",
      "[INFO] Epoch: 2 , batch: 47 , training loss: 4.935869\n",
      "[INFO] Epoch: 2 , batch: 48 , training loss: 5.115369\n",
      "[INFO] Epoch: 2 , batch: 49 , training loss: 4.983401\n",
      "[INFO] Epoch: 2 , batch: 50 , training loss: 4.845634\n",
      "[INFO] Epoch: 2 , batch: 51 , training loss: 4.750398\n",
      "[INFO] Epoch: 2 , batch: 52 , training loss: 4.574161\n",
      "[INFO] Epoch: 2 , batch: 53 , training loss: 4.732793\n",
      "[INFO] Epoch: 2 , batch: 54 , training loss: 4.707847\n",
      "[INFO] Epoch: 2 , batch: 55 , training loss: 4.786379\n",
      "[INFO] Epoch: 2 , batch: 56 , training loss: 4.606449\n",
      "[INFO] Epoch: 2 , batch: 57 , training loss: 4.405415\n",
      "[INFO] Epoch: 2 , batch: 58 , training loss: 4.418081\n",
      "[INFO] Epoch: 2 , batch: 59 , training loss: 4.627888\n",
      "[INFO] Epoch: 2 , batch: 60 , training loss: 4.376234\n",
      "[INFO] Epoch: 2 , batch: 61 , training loss: 4.535642\n",
      "[INFO] Epoch: 2 , batch: 62 , training loss: 4.418145\n",
      "[INFO] Epoch: 2 , batch: 63 , training loss: 4.623803\n",
      "[INFO] Epoch: 2 , batch: 64 , training loss: 4.763944\n",
      "[INFO] Epoch: 2 , batch: 65 , training loss: 4.525819\n",
      "[INFO] Epoch: 2 , batch: 66 , training loss: 4.340622\n",
      "[INFO] Epoch: 2 , batch: 67 , training loss: 4.329597\n",
      "[INFO] Epoch: 2 , batch: 68 , training loss: 4.727691\n",
      "[INFO] Epoch: 2 , batch: 69 , training loss: 4.430865\n",
      "[INFO] Epoch: 2 , batch: 70 , training loss: 4.771555\n",
      "[INFO] Epoch: 2 , batch: 71 , training loss: 4.521140\n",
      "[INFO] Epoch: 2 , batch: 72 , training loss: 4.575937\n",
      "[INFO] Epoch: 2 , batch: 73 , training loss: 4.512743\n",
      "[INFO] Epoch: 2 , batch: 74 , training loss: 4.643551\n",
      "[INFO] Epoch: 2 , batch: 75 , training loss: 4.348952\n",
      "[INFO] Epoch: 2 , batch: 76 , training loss: 4.565990\n",
      "[INFO] Epoch: 2 , batch: 77 , training loss: 4.415809\n",
      "[INFO] Epoch: 2 , batch: 78 , training loss: 4.472444\n",
      "[INFO] Epoch: 2 , batch: 79 , training loss: 4.289153\n",
      "[INFO] Epoch: 2 , batch: 80 , training loss: 4.523047\n",
      "[INFO] Epoch: 2 , batch: 81 , training loss: 4.549363\n",
      "[INFO] Epoch: 2 , batch: 82 , training loss: 4.546898\n",
      "[INFO] Epoch: 2 , batch: 83 , training loss: 4.611344\n",
      "[INFO] Epoch: 2 , batch: 84 , training loss: 4.559529\n",
      "[INFO] Epoch: 2 , batch: 85 , training loss: 4.629241\n",
      "[INFO] Epoch: 2 , batch: 86 , training loss: 4.652670\n",
      "[INFO] Epoch: 2 , batch: 87 , training loss: 4.586370\n",
      "[INFO] Epoch: 2 , batch: 88 , training loss: 4.704830\n",
      "[INFO] Epoch: 2 , batch: 89 , training loss: 4.496755\n",
      "[INFO] Epoch: 2 , batch: 90 , training loss: 4.473283\n",
      "[INFO] Epoch: 2 , batch: 91 , training loss: 4.462721\n",
      "[INFO] Epoch: 2 , batch: 92 , training loss: 4.445711\n",
      "[INFO] Epoch: 2 , batch: 93 , training loss: 4.532309\n",
      "[INFO] Epoch: 2 , batch: 94 , training loss: 4.757153\n",
      "[INFO] Epoch: 2 , batch: 95 , training loss: 4.515464\n",
      "[INFO] Epoch: 2 , batch: 96 , training loss: 4.483728\n",
      "[INFO] Epoch: 2 , batch: 97 , training loss: 4.474159\n",
      "[INFO] Epoch: 2 , batch: 98 , training loss: 4.473245\n",
      "[INFO] Epoch: 2 , batch: 99 , training loss: 4.492529\n",
      "[INFO] Epoch: 2 , batch: 100 , training loss: 4.330206\n",
      "[INFO] Epoch: 2 , batch: 101 , training loss: 4.387208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 2 , batch: 102 , training loss: 4.582658\n",
      "[INFO] Epoch: 2 , batch: 103 , training loss: 4.262622\n",
      "[INFO] Epoch: 2 , batch: 104 , training loss: 4.296089\n",
      "[INFO] Epoch: 2 , batch: 105 , training loss: 4.562405\n",
      "[INFO] Epoch: 2 , batch: 106 , training loss: 4.546305\n",
      "[INFO] Epoch: 2 , batch: 107 , training loss: 4.461832\n",
      "[INFO] Epoch: 2 , batch: 108 , training loss: 4.307612\n",
      "[INFO] Epoch: 2 , batch: 109 , training loss: 4.196157\n",
      "[INFO] Epoch: 2 , batch: 110 , training loss: 4.540655\n",
      "[INFO] Epoch: 2 , batch: 111 , training loss: 4.513011\n",
      "[INFO] Epoch: 2 , batch: 112 , training loss: 4.521683\n",
      "[INFO] Epoch: 2 , batch: 113 , training loss: 4.436672\n",
      "[INFO] Epoch: 2 , batch: 114 , training loss: 4.609158\n",
      "[INFO] Epoch: 2 , batch: 115 , training loss: 4.515985\n",
      "[INFO] Epoch: 2 , batch: 116 , training loss: 4.484770\n",
      "[INFO] Epoch: 2 , batch: 117 , training loss: 4.713336\n",
      "[INFO] Epoch: 2 , batch: 118 , training loss: 4.700827\n",
      "[INFO] Epoch: 2 , batch: 119 , training loss: 4.783840\n",
      "[INFO] Epoch: 2 , batch: 120 , training loss: 4.720577\n",
      "[INFO] Epoch: 2 , batch: 121 , training loss: 4.492754\n",
      "[INFO] Epoch: 2 , batch: 122 , training loss: 4.441135\n",
      "[INFO] Epoch: 2 , batch: 123 , training loss: 4.597421\n",
      "[INFO] Epoch: 2 , batch: 124 , training loss: 4.772326\n",
      "[INFO] Epoch: 2 , batch: 125 , training loss: 4.358950\n",
      "[INFO] Epoch: 2 , batch: 126 , training loss: 4.363954\n",
      "[INFO] Epoch: 2 , batch: 127 , training loss: 4.418556\n",
      "[INFO] Epoch: 2 , batch: 128 , training loss: 4.636203\n",
      "[INFO] Epoch: 2 , batch: 129 , training loss: 4.507540\n",
      "[INFO] Epoch: 2 , batch: 130 , training loss: 4.567453\n",
      "[INFO] Epoch: 2 , batch: 131 , training loss: 4.532669\n",
      "[INFO] Epoch: 2 , batch: 132 , training loss: 4.584212\n",
      "[INFO] Epoch: 2 , batch: 133 , training loss: 4.481004\n",
      "[INFO] Epoch: 2 , batch: 134 , training loss: 4.266508\n",
      "[INFO] Epoch: 2 , batch: 135 , training loss: 4.270340\n",
      "[INFO] Epoch: 2 , batch: 136 , training loss: 4.581302\n",
      "[INFO] Epoch: 2 , batch: 137 , training loss: 4.518119\n",
      "[INFO] Epoch: 2 , batch: 138 , training loss: 4.693851\n",
      "[INFO] Epoch: 2 , batch: 139 , training loss: 5.409781\n",
      "[INFO] Epoch: 2 , batch: 140 , training loss: 5.341012\n",
      "[INFO] Epoch: 2 , batch: 141 , training loss: 5.002893\n",
      "[INFO] Epoch: 2 , batch: 142 , training loss: 4.564028\n",
      "[INFO] Epoch: 2 , batch: 143 , training loss: 4.651132\n",
      "[INFO] Epoch: 2 , batch: 144 , training loss: 4.487003\n",
      "[INFO] Epoch: 2 , batch: 145 , training loss: 4.625231\n",
      "[INFO] Epoch: 2 , batch: 146 , training loss: 4.808324\n",
      "[INFO] Epoch: 2 , batch: 147 , training loss: 4.365841\n",
      "[INFO] Epoch: 2 , batch: 148 , training loss: 4.366012\n",
      "[INFO] Epoch: 2 , batch: 149 , training loss: 4.468030\n",
      "[INFO] Epoch: 2 , batch: 150 , training loss: 4.750901\n",
      "[INFO] Epoch: 2 , batch: 151 , training loss: 4.385675\n",
      "[INFO] Epoch: 2 , batch: 152 , training loss: 4.413469\n",
      "[INFO] Epoch: 2 , batch: 153 , training loss: 4.621869\n",
      "[INFO] Epoch: 2 , batch: 154 , training loss: 4.668321\n",
      "[INFO] Epoch: 2 , batch: 155 , training loss: 4.929025\n",
      "[INFO] Epoch: 2 , batch: 156 , training loss: 4.554590\n",
      "[INFO] Epoch: 2 , batch: 157 , training loss: 4.541108\n",
      "[INFO] Epoch: 2 , batch: 158 , training loss: 5.038917\n",
      "[INFO] Epoch: 2 , batch: 159 , training loss: 4.977668\n",
      "[INFO] Epoch: 2 , batch: 160 , training loss: 5.623907\n",
      "[INFO] Epoch: 2 , batch: 161 , training loss: 5.237799\n",
      "[INFO] Epoch: 2 , batch: 162 , training loss: 5.043657\n",
      "[INFO] Epoch: 2 , batch: 163 , training loss: 5.094871\n",
      "[INFO] Epoch: 2 , batch: 164 , training loss: 4.935886\n",
      "[INFO] Epoch: 2 , batch: 165 , training loss: 4.936210\n",
      "[INFO] Epoch: 2 , batch: 166 , training loss: 5.485852\n",
      "[INFO] Epoch: 2 , batch: 167 , training loss: 6.250790\n",
      "[INFO] Epoch: 2 , batch: 168 , training loss: 5.888715\n",
      "[INFO] Epoch: 2 , batch: 169 , training loss: 5.593416\n",
      "[INFO] Epoch: 2 , batch: 170 , training loss: 5.612477\n",
      "[INFO] Epoch: 2 , batch: 171 , training loss: 5.401343\n",
      "[INFO] Epoch: 2 , batch: 172 , training loss: 5.455757\n",
      "[INFO] Epoch: 2 , batch: 173 , training loss: 5.607153\n",
      "[INFO] Epoch: 2 , batch: 174 , training loss: 5.630902\n",
      "[INFO] Epoch: 2 , batch: 175 , training loss: 5.609420\n",
      "[INFO] Epoch: 2 , batch: 176 , training loss: 5.517807\n",
      "[INFO] Epoch: 2 , batch: 177 , training loss: 5.213224\n",
      "[INFO] Epoch: 2 , batch: 178 , training loss: 5.090957\n",
      "[INFO] Epoch: 2 , batch: 179 , training loss: 5.082829\n",
      "[INFO] Epoch: 2 , batch: 180 , training loss: 5.043130\n",
      "[INFO] Epoch: 2 , batch: 181 , training loss: 5.209734\n",
      "[INFO] Epoch: 2 , batch: 182 , training loss: 5.125002\n",
      "[INFO] Epoch: 2 , batch: 183 , training loss: 5.043231\n",
      "[INFO] Epoch: 2 , batch: 184 , training loss: 4.913352\n",
      "[INFO] Epoch: 2 , batch: 185 , training loss: 4.871240\n",
      "[INFO] Epoch: 2 , batch: 186 , training loss: 4.971658\n",
      "[INFO] Epoch: 2 , batch: 187 , training loss: 5.095166\n",
      "[INFO] Epoch: 2 , batch: 188 , training loss: 5.058800\n",
      "[INFO] Epoch: 2 , batch: 189 , training loss: 4.968572\n",
      "[INFO] Epoch: 2 , batch: 190 , training loss: 4.907118\n",
      "[INFO] Epoch: 2 , batch: 191 , training loss: 5.048868\n",
      "[INFO] Epoch: 2 , batch: 192 , training loss: 4.806972\n",
      "[INFO] Epoch: 2 , batch: 193 , training loss: 4.957131\n",
      "[INFO] Epoch: 2 , batch: 194 , training loss: 4.836336\n",
      "[INFO] Epoch: 2 , batch: 195 , training loss: 5.020545\n",
      "[INFO] Epoch: 2 , batch: 196 , training loss: 4.701629\n",
      "[INFO] Epoch: 2 , batch: 197 , training loss: 4.939955\n",
      "[INFO] Epoch: 2 , batch: 198 , training loss: 4.702217\n",
      "[INFO] Epoch: 2 , batch: 199 , training loss: 4.787305\n",
      "[INFO] Epoch: 2 , batch: 200 , training loss: 4.745782\n",
      "[INFO] Epoch: 2 , batch: 201 , training loss: 4.663508\n",
      "[INFO] Epoch: 2 , batch: 202 , training loss: 4.607672\n",
      "[INFO] Epoch: 2 , batch: 203 , training loss: 4.581055\n",
      "[INFO] Epoch: 2 , batch: 204 , training loss: 4.780038\n",
      "[INFO] Epoch: 2 , batch: 205 , training loss: 4.340886\n",
      "[INFO] Epoch: 2 , batch: 206 , training loss: 4.206790\n",
      "[INFO] Epoch: 2 , batch: 207 , training loss: 4.263689\n",
      "[INFO] Epoch: 2 , batch: 208 , training loss: 4.748473\n",
      "[INFO] Epoch: 2 , batch: 209 , training loss: 4.615514\n",
      "[INFO] Epoch: 2 , batch: 210 , training loss: 4.649096\n",
      "[INFO] Epoch: 2 , batch: 211 , training loss: 4.583877\n",
      "[INFO] Epoch: 2 , batch: 212 , training loss: 4.683130\n",
      "[INFO] Epoch: 2 , batch: 213 , training loss: 4.715681\n",
      "[INFO] Epoch: 2 , batch: 214 , training loss: 4.693341\n",
      "[INFO] Epoch: 2 , batch: 215 , training loss: 4.941968\n",
      "[INFO] Epoch: 2 , batch: 216 , training loss: 4.654051\n",
      "[INFO] Epoch: 2 , batch: 217 , training loss: 4.570815\n",
      "[INFO] Epoch: 2 , batch: 218 , training loss: 4.603262\n",
      "[INFO] Epoch: 2 , batch: 219 , training loss: 4.656441\n",
      "[INFO] Epoch: 2 , batch: 220 , training loss: 4.494524\n",
      "[INFO] Epoch: 2 , batch: 221 , training loss: 4.511039\n",
      "[INFO] Epoch: 2 , batch: 222 , training loss: 4.653970\n",
      "[INFO] Epoch: 2 , batch: 223 , training loss: 4.765066\n",
      "[INFO] Epoch: 2 , batch: 224 , training loss: 4.787288\n",
      "[INFO] Epoch: 2 , batch: 225 , training loss: 4.627149\n",
      "[INFO] Epoch: 2 , batch: 226 , training loss: 4.790240\n",
      "[INFO] Epoch: 2 , batch: 227 , training loss: 4.732215\n",
      "[INFO] Epoch: 2 , batch: 228 , training loss: 4.794886\n",
      "[INFO] Epoch: 2 , batch: 229 , training loss: 4.683864\n",
      "[INFO] Epoch: 2 , batch: 230 , training loss: 4.479949\n",
      "[INFO] Epoch: 2 , batch: 231 , training loss: 4.306862\n",
      "[INFO] Epoch: 2 , batch: 232 , training loss: 4.453015\n",
      "[INFO] Epoch: 2 , batch: 233 , training loss: 4.528186\n",
      "[INFO] Epoch: 2 , batch: 234 , training loss: 4.160133\n",
      "[INFO] Epoch: 2 , batch: 235 , training loss: 4.316090\n",
      "[INFO] Epoch: 2 , batch: 236 , training loss: 4.538861\n",
      "[INFO] Epoch: 2 , batch: 237 , training loss: 4.655828\n",
      "[INFO] Epoch: 2 , batch: 238 , training loss: 4.377119\n",
      "[INFO] Epoch: 2 , batch: 239 , training loss: 4.446394\n",
      "[INFO] Epoch: 2 , batch: 240 , training loss: 4.535322\n",
      "[INFO] Epoch: 2 , batch: 241 , training loss: 4.296857\n",
      "[INFO] Epoch: 2 , batch: 242 , training loss: 4.303221\n",
      "[INFO] Epoch: 2 , batch: 243 , training loss: 4.661952\n",
      "[INFO] Epoch: 2 , batch: 244 , training loss: 4.563809\n",
      "[INFO] Epoch: 2 , batch: 245 , training loss: 4.556669\n",
      "[INFO] Epoch: 2 , batch: 246 , training loss: 4.182554\n",
      "[INFO] Epoch: 2 , batch: 247 , training loss: 4.341862\n",
      "[INFO] Epoch: 2 , batch: 248 , training loss: 4.477710\n",
      "[INFO] Epoch: 2 , batch: 249 , training loss: 4.406915\n",
      "[INFO] Epoch: 2 , batch: 250 , training loss: 4.213280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 2 , batch: 251 , training loss: 4.699518\n",
      "[INFO] Epoch: 2 , batch: 252 , training loss: 4.373791\n",
      "[INFO] Epoch: 2 , batch: 253 , training loss: 4.357702\n",
      "[INFO] Epoch: 2 , batch: 254 , training loss: 4.690124\n",
      "[INFO] Epoch: 2 , batch: 255 , training loss: 4.683517\n",
      "[INFO] Epoch: 2 , batch: 256 , training loss: 4.582189\n",
      "[INFO] Epoch: 2 , batch: 257 , training loss: 4.778986\n",
      "[INFO] Epoch: 2 , batch: 258 , training loss: 4.863129\n",
      "[INFO] Epoch: 2 , batch: 259 , training loss: 4.885915\n",
      "[INFO] Epoch: 2 , batch: 260 , training loss: 4.528090\n",
      "[INFO] Epoch: 2 , batch: 261 , training loss: 4.761834\n",
      "[INFO] Epoch: 2 , batch: 262 , training loss: 4.972789\n",
      "[INFO] Epoch: 2 , batch: 263 , training loss: 5.068800\n",
      "[INFO] Epoch: 2 , batch: 264 , training loss: 4.355857\n",
      "[INFO] Epoch: 2 , batch: 265 , training loss: 4.511557\n",
      "[INFO] Epoch: 2 , batch: 266 , training loss: 5.068907\n",
      "[INFO] Epoch: 2 , batch: 267 , training loss: 4.725434\n",
      "[INFO] Epoch: 2 , batch: 268 , training loss: 4.609267\n",
      "[INFO] Epoch: 2 , batch: 269 , training loss: 4.673361\n",
      "[INFO] Epoch: 2 , batch: 270 , training loss: 4.631514\n",
      "[INFO] Epoch: 2 , batch: 271 , training loss: 4.681098\n",
      "[INFO] Epoch: 2 , batch: 272 , training loss: 4.647868\n",
      "[INFO] Epoch: 2 , batch: 273 , training loss: 4.660211\n",
      "[INFO] Epoch: 2 , batch: 274 , training loss: 4.780118\n",
      "[INFO] Epoch: 2 , batch: 275 , training loss: 4.667214\n",
      "[INFO] Epoch: 2 , batch: 276 , training loss: 4.715927\n",
      "[INFO] Epoch: 2 , batch: 277 , training loss: 4.856786\n",
      "[INFO] Epoch: 2 , batch: 278 , training loss: 4.384118\n",
      "[INFO] Epoch: 2 , batch: 279 , training loss: 4.420200\n",
      "[INFO] Epoch: 2 , batch: 280 , training loss: 4.360837\n",
      "[INFO] Epoch: 2 , batch: 281 , training loss: 4.517203\n",
      "[INFO] Epoch: 2 , batch: 282 , training loss: 4.427058\n",
      "[INFO] Epoch: 2 , batch: 283 , training loss: 4.529335\n",
      "[INFO] Epoch: 2 , batch: 284 , training loss: 4.509853\n",
      "[INFO] Epoch: 2 , batch: 285 , training loss: 4.575163\n",
      "[INFO] Epoch: 2 , batch: 286 , training loss: 4.509291\n",
      "[INFO] Epoch: 2 , batch: 287 , training loss: 4.331604\n",
      "[INFO] Epoch: 2 , batch: 288 , training loss: 4.416853\n",
      "[INFO] Epoch: 2 , batch: 289 , training loss: 4.442104\n",
      "[INFO] Epoch: 2 , batch: 290 , training loss: 4.251842\n",
      "[INFO] Epoch: 2 , batch: 291 , training loss: 4.215728\n",
      "[INFO] Epoch: 2 , batch: 292 , training loss: 4.311971\n",
      "[INFO] Epoch: 2 , batch: 293 , training loss: 4.295146\n",
      "[INFO] Epoch: 2 , batch: 294 , training loss: 4.991885\n",
      "[INFO] Epoch: 2 , batch: 295 , training loss: 4.707625\n",
      "[INFO] Epoch: 2 , batch: 296 , training loss: 4.662599\n",
      "[INFO] Epoch: 2 , batch: 297 , training loss: 4.544267\n",
      "[INFO] Epoch: 2 , batch: 298 , training loss: 4.416100\n",
      "[INFO] Epoch: 2 , batch: 299 , training loss: 4.385926\n",
      "[INFO] Epoch: 2 , batch: 300 , training loss: 4.377053\n",
      "[INFO] Epoch: 2 , batch: 301 , training loss: 4.347494\n",
      "[INFO] Epoch: 2 , batch: 302 , training loss: 4.498779\n",
      "[INFO] Epoch: 2 , batch: 303 , training loss: 4.528391\n",
      "[INFO] Epoch: 2 , batch: 304 , training loss: 4.733625\n",
      "[INFO] Epoch: 2 , batch: 305 , training loss: 4.416008\n",
      "[INFO] Epoch: 2 , batch: 306 , training loss: 4.564069\n",
      "[INFO] Epoch: 2 , batch: 307 , training loss: 4.549499\n",
      "[INFO] Epoch: 2 , batch: 308 , training loss: 4.482413\n",
      "[INFO] Epoch: 2 , batch: 309 , training loss: 4.499516\n",
      "[INFO] Epoch: 2 , batch: 310 , training loss: 4.253729\n",
      "[INFO] Epoch: 2 , batch: 311 , training loss: 4.308743\n",
      "[INFO] Epoch: 2 , batch: 312 , training loss: 4.170549\n",
      "[INFO] Epoch: 2 , batch: 313 , training loss: 4.372597\n",
      "[INFO] Epoch: 2 , batch: 314 , training loss: 4.409397\n",
      "[INFO] Epoch: 2 , batch: 315 , training loss: 4.438066\n",
      "[INFO] Epoch: 2 , batch: 316 , training loss: 4.844882\n",
      "[INFO] Epoch: 2 , batch: 317 , training loss: 5.437034\n",
      "[INFO] Epoch: 2 , batch: 318 , training loss: 5.526010\n",
      "[INFO] Epoch: 2 , batch: 319 , training loss: 5.010394\n",
      "[INFO] Epoch: 2 , batch: 320 , training loss: 4.502162\n",
      "[INFO] Epoch: 2 , batch: 321 , training loss: 4.283694\n",
      "[INFO] Epoch: 2 , batch: 322 , training loss: 4.423088\n",
      "[INFO] Epoch: 2 , batch: 323 , training loss: 4.410174\n",
      "[INFO] Epoch: 2 , batch: 324 , training loss: 4.418853\n",
      "[INFO] Epoch: 2 , batch: 325 , training loss: 4.601061\n",
      "[INFO] Epoch: 2 , batch: 326 , training loss: 4.647515\n",
      "[INFO] Epoch: 2 , batch: 327 , training loss: 4.528266\n",
      "[INFO] Epoch: 2 , batch: 328 , training loss: 4.546390\n",
      "[INFO] Epoch: 2 , batch: 329 , training loss: 4.407901\n",
      "[INFO] Epoch: 2 , batch: 330 , training loss: 4.396316\n",
      "[INFO] Epoch: 2 , batch: 331 , training loss: 4.602058\n",
      "[INFO] Epoch: 2 , batch: 332 , training loss: 4.321428\n",
      "[INFO] Epoch: 2 , batch: 333 , training loss: 4.284604\n",
      "[INFO] Epoch: 2 , batch: 334 , training loss: 4.430879\n",
      "[INFO] Epoch: 2 , batch: 335 , training loss: 4.544832\n",
      "[INFO] Epoch: 2 , batch: 336 , training loss: 4.508756\n",
      "[INFO] Epoch: 2 , batch: 337 , training loss: 4.619485\n",
      "[INFO] Epoch: 2 , batch: 338 , training loss: 4.749954\n",
      "[INFO] Epoch: 2 , batch: 339 , training loss: 4.599715\n",
      "[INFO] Epoch: 2 , batch: 340 , training loss: 4.796531\n",
      "[INFO] Epoch: 2 , batch: 341 , training loss: 4.504485\n",
      "[INFO] Epoch: 2 , batch: 342 , training loss: 4.372924\n",
      "[INFO] Epoch: 2 , batch: 343 , training loss: 4.440792\n",
      "[INFO] Epoch: 2 , batch: 344 , training loss: 4.284995\n",
      "[INFO] Epoch: 2 , batch: 345 , training loss: 4.417059\n",
      "[INFO] Epoch: 2 , batch: 346 , training loss: 4.523664\n",
      "[INFO] Epoch: 2 , batch: 347 , training loss: 4.380693\n",
      "[INFO] Epoch: 2 , batch: 348 , training loss: 4.636303\n",
      "[INFO] Epoch: 2 , batch: 349 , training loss: 4.709951\n",
      "[INFO] Epoch: 2 , batch: 350 , training loss: 4.394454\n",
      "[INFO] Epoch: 2 , batch: 351 , training loss: 4.441850\n",
      "[INFO] Epoch: 2 , batch: 352 , training loss: 4.463046\n",
      "[INFO] Epoch: 2 , batch: 353 , training loss: 4.430902\n",
      "[INFO] Epoch: 2 , batch: 354 , training loss: 4.531889\n",
      "[INFO] Epoch: 2 , batch: 355 , training loss: 4.599416\n",
      "[INFO] Epoch: 2 , batch: 356 , training loss: 4.467930\n",
      "[INFO] Epoch: 2 , batch: 357 , training loss: 4.512164\n",
      "[INFO] Epoch: 2 , batch: 358 , training loss: 4.477893\n",
      "[INFO] Epoch: 2 , batch: 359 , training loss: 4.487987\n",
      "[INFO] Epoch: 2 , batch: 360 , training loss: 4.456158\n",
      "[INFO] Epoch: 2 , batch: 361 , training loss: 4.459568\n",
      "[INFO] Epoch: 2 , batch: 362 , training loss: 4.540460\n",
      "[INFO] Epoch: 2 , batch: 363 , training loss: 4.425541\n",
      "[INFO] Epoch: 2 , batch: 364 , training loss: 4.472458\n",
      "[INFO] Epoch: 2 , batch: 365 , training loss: 4.364384\n",
      "[INFO] Epoch: 2 , batch: 366 , training loss: 4.560594\n",
      "[INFO] Epoch: 2 , batch: 367 , training loss: 4.615413\n",
      "[INFO] Epoch: 2 , batch: 368 , training loss: 5.248777\n",
      "[INFO] Epoch: 2 , batch: 369 , training loss: 4.819747\n",
      "[INFO] Epoch: 2 , batch: 370 , training loss: 4.536516\n",
      "[INFO] Epoch: 2 , batch: 371 , training loss: 5.184449\n",
      "[INFO] Epoch: 2 , batch: 372 , training loss: 5.470573\n",
      "[INFO] Epoch: 2 , batch: 373 , training loss: 5.423540\n",
      "[INFO] Epoch: 2 , batch: 374 , training loss: 5.442726\n",
      "[INFO] Epoch: 2 , batch: 375 , training loss: 5.401625\n",
      "[INFO] Epoch: 2 , batch: 376 , training loss: 5.378637\n",
      "[INFO] Epoch: 2 , batch: 377 , training loss: 5.018319\n",
      "[INFO] Epoch: 2 , batch: 378 , training loss: 5.113042\n",
      "[INFO] Epoch: 2 , batch: 379 , training loss: 5.148584\n",
      "[INFO] Epoch: 2 , batch: 380 , training loss: 5.207552\n",
      "[INFO] Epoch: 2 , batch: 381 , training loss: 4.978790\n",
      "[INFO] Epoch: 2 , batch: 382 , training loss: 5.176909\n",
      "[INFO] Epoch: 2 , batch: 383 , training loss: 5.318730\n",
      "[INFO] Epoch: 2 , batch: 384 , training loss: 5.563649\n",
      "[INFO] Epoch: 2 , batch: 385 , training loss: 5.374369\n",
      "[INFO] Epoch: 2 , batch: 386 , training loss: 5.461733\n",
      "[INFO] Epoch: 2 , batch: 387 , training loss: 5.363449\n",
      "[INFO] Epoch: 2 , batch: 388 , training loss: 5.121800\n",
      "[INFO] Epoch: 2 , batch: 389 , training loss: 4.875155\n",
      "[INFO] Epoch: 2 , batch: 390 , training loss: 4.853567\n",
      "[INFO] Epoch: 2 , batch: 391 , training loss: 4.889636\n",
      "[INFO] Epoch: 2 , batch: 392 , training loss: 5.220899\n",
      "[INFO] Epoch: 2 , batch: 393 , training loss: 5.135921\n",
      "[INFO] Epoch: 2 , batch: 394 , training loss: 5.190254\n",
      "[INFO] Epoch: 2 , batch: 395 , training loss: 4.986411\n",
      "[INFO] Epoch: 2 , batch: 396 , training loss: 4.785878\n",
      "[INFO] Epoch: 2 , batch: 397 , training loss: 4.929626\n",
      "[INFO] Epoch: 2 , batch: 398 , training loss: 4.798248\n",
      "[INFO] Epoch: 2 , batch: 399 , training loss: 4.841319\n",
      "[INFO] Epoch: 2 , batch: 400 , training loss: 4.767852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 2 , batch: 401 , training loss: 5.196766\n",
      "[INFO] Epoch: 2 , batch: 402 , training loss: 4.916399\n",
      "[INFO] Epoch: 2 , batch: 403 , training loss: 4.822325\n",
      "[INFO] Epoch: 2 , batch: 404 , training loss: 4.948773\n",
      "[INFO] Epoch: 2 , batch: 405 , training loss: 5.015165\n",
      "[INFO] Epoch: 2 , batch: 406 , training loss: 4.880709\n",
      "[INFO] Epoch: 2 , batch: 407 , training loss: 4.949287\n",
      "[INFO] Epoch: 2 , batch: 408 , training loss: 4.896284\n",
      "[INFO] Epoch: 2 , batch: 409 , training loss: 4.866525\n",
      "[INFO] Epoch: 2 , batch: 410 , training loss: 4.899643\n",
      "[INFO] Epoch: 2 , batch: 411 , training loss: 5.109206\n",
      "[INFO] Epoch: 2 , batch: 412 , training loss: 4.954717\n",
      "[INFO] Epoch: 2 , batch: 413 , training loss: 4.785472\n",
      "[INFO] Epoch: 2 , batch: 414 , training loss: 4.837750\n",
      "[INFO] Epoch: 2 , batch: 415 , training loss: 4.872927\n",
      "[INFO] Epoch: 2 , batch: 416 , training loss: 4.932543\n",
      "[INFO] Epoch: 2 , batch: 417 , training loss: 4.842337\n",
      "[INFO] Epoch: 2 , batch: 418 , training loss: 4.888344\n",
      "[INFO] Epoch: 2 , batch: 419 , training loss: 4.840117\n",
      "[INFO] Epoch: 2 , batch: 420 , training loss: 4.776543\n",
      "[INFO] Epoch: 2 , batch: 421 , training loss: 4.786153\n",
      "[INFO] Epoch: 2 , batch: 422 , training loss: 4.688127\n",
      "[INFO] Epoch: 2 , batch: 423 , training loss: 4.919563\n",
      "[INFO] Epoch: 2 , batch: 424 , training loss: 5.055266\n",
      "[INFO] Epoch: 2 , batch: 425 , training loss: 4.982396\n",
      "[INFO] Epoch: 2 , batch: 426 , training loss: 4.592331\n",
      "[INFO] Epoch: 2 , batch: 427 , training loss: 4.915194\n",
      "[INFO] Epoch: 2 , batch: 428 , training loss: 4.819262\n",
      "[INFO] Epoch: 2 , batch: 429 , training loss: 4.592192\n",
      "[INFO] Epoch: 2 , batch: 430 , training loss: 4.900833\n",
      "[INFO] Epoch: 2 , batch: 431 , training loss: 4.460712\n",
      "[INFO] Epoch: 2 , batch: 432 , training loss: 4.560182\n",
      "[INFO] Epoch: 2 , batch: 433 , training loss: 4.541524\n",
      "[INFO] Epoch: 2 , batch: 434 , training loss: 4.434871\n",
      "[INFO] Epoch: 2 , batch: 435 , training loss: 4.775509\n",
      "[INFO] Epoch: 2 , batch: 436 , training loss: 4.894986\n",
      "[INFO] Epoch: 2 , batch: 437 , training loss: 4.733749\n",
      "[INFO] Epoch: 2 , batch: 438 , training loss: 4.449897\n",
      "[INFO] Epoch: 2 , batch: 439 , training loss: 4.682216\n",
      "[INFO] Epoch: 2 , batch: 440 , training loss: 4.856440\n",
      "[INFO] Epoch: 2 , batch: 441 , training loss: 4.845652\n",
      "[INFO] Epoch: 2 , batch: 442 , training loss: 4.676091\n",
      "[INFO] Epoch: 2 , batch: 443 , training loss: 4.925007\n",
      "[INFO] Epoch: 2 , batch: 444 , training loss: 4.533150\n",
      "[INFO] Epoch: 2 , batch: 445 , training loss: 4.371084\n",
      "[INFO] Epoch: 2 , batch: 446 , training loss: 4.247147\n",
      "[INFO] Epoch: 2 , batch: 447 , training loss: 4.536164\n",
      "[INFO] Epoch: 2 , batch: 448 , training loss: 4.706822\n",
      "[INFO] Epoch: 2 , batch: 449 , training loss: 5.130096\n",
      "[INFO] Epoch: 2 , batch: 450 , training loss: 5.172954\n",
      "[INFO] Epoch: 2 , batch: 451 , training loss: 5.024347\n",
      "[INFO] Epoch: 2 , batch: 452 , training loss: 4.768827\n",
      "[INFO] Epoch: 2 , batch: 453 , training loss: 4.540428\n",
      "[INFO] Epoch: 2 , batch: 454 , training loss: 4.708040\n",
      "[INFO] Epoch: 2 , batch: 455 , training loss: 4.732700\n",
      "[INFO] Epoch: 2 , batch: 456 , training loss: 4.713456\n",
      "[INFO] Epoch: 2 , batch: 457 , training loss: 4.834795\n",
      "[INFO] Epoch: 2 , batch: 458 , training loss: 4.544025\n",
      "[INFO] Epoch: 2 , batch: 459 , training loss: 4.528762\n",
      "[INFO] Epoch: 2 , batch: 460 , training loss: 4.671468\n",
      "[INFO] Epoch: 2 , batch: 461 , training loss: 4.690477\n",
      "[INFO] Epoch: 2 , batch: 462 , training loss: 4.729526\n",
      "[INFO] Epoch: 2 , batch: 463 , training loss: 4.585061\n",
      "[INFO] Epoch: 2 , batch: 464 , training loss: 4.766418\n",
      "[INFO] Epoch: 2 , batch: 465 , training loss: 4.710751\n",
      "[INFO] Epoch: 2 , batch: 466 , training loss: 4.798545\n",
      "[INFO] Epoch: 2 , batch: 467 , training loss: 4.883197\n",
      "[INFO] Epoch: 2 , batch: 468 , training loss: 4.789307\n",
      "[INFO] Epoch: 2 , batch: 469 , training loss: 4.774545\n",
      "[INFO] Epoch: 2 , batch: 470 , training loss: 4.523861\n",
      "[INFO] Epoch: 2 , batch: 471 , training loss: 4.656971\n",
      "[INFO] Epoch: 2 , batch: 472 , training loss: 4.735434\n",
      "[INFO] Epoch: 2 , batch: 473 , training loss: 4.635595\n",
      "[INFO] Epoch: 2 , batch: 474 , training loss: 4.458777\n",
      "[INFO] Epoch: 2 , batch: 475 , training loss: 4.311454\n",
      "[INFO] Epoch: 2 , batch: 476 , training loss: 4.698601\n",
      "[INFO] Epoch: 2 , batch: 477 , training loss: 4.799426\n",
      "[INFO] Epoch: 2 , batch: 478 , training loss: 4.913334\n",
      "[INFO] Epoch: 2 , batch: 479 , training loss: 4.842988\n",
      "[INFO] Epoch: 2 , batch: 480 , training loss: 4.988468\n",
      "[INFO] Epoch: 2 , batch: 481 , training loss: 4.778355\n",
      "[INFO] Epoch: 2 , batch: 482 , training loss: 4.921867\n",
      "[INFO] Epoch: 2 , batch: 483 , training loss: 4.739593\n",
      "[INFO] Epoch: 2 , batch: 484 , training loss: 4.568161\n",
      "[INFO] Epoch: 2 , batch: 485 , training loss: 4.744889\n",
      "[INFO] Epoch: 2 , batch: 486 , training loss: 4.563903\n",
      "[INFO] Epoch: 2 , batch: 487 , training loss: 4.575065\n",
      "[INFO] Epoch: 2 , batch: 488 , training loss: 4.771465\n",
      "[INFO] Epoch: 2 , batch: 489 , training loss: 4.631319\n",
      "[INFO] Epoch: 2 , batch: 490 , training loss: 4.736459\n",
      "[INFO] Epoch: 2 , batch: 491 , training loss: 4.732151\n",
      "[INFO] Epoch: 2 , batch: 492 , training loss: 4.559822\n",
      "[INFO] Epoch: 2 , batch: 493 , training loss: 4.763864\n",
      "[INFO] Epoch: 2 , batch: 494 , training loss: 4.667128\n",
      "[INFO] Epoch: 2 , batch: 495 , training loss: 4.797344\n",
      "[INFO] Epoch: 2 , batch: 496 , training loss: 4.675322\n",
      "[INFO] Epoch: 2 , batch: 497 , training loss: 4.682436\n",
      "[INFO] Epoch: 2 , batch: 498 , training loss: 4.762473\n",
      "[INFO] Epoch: 2 , batch: 499 , training loss: 4.780513\n",
      "[INFO] Epoch: 2 , batch: 500 , training loss: 5.121962\n",
      "[INFO] Epoch: 2 , batch: 501 , training loss: 5.507546\n",
      "[INFO] Epoch: 2 , batch: 502 , training loss: 5.673305\n",
      "[INFO] Epoch: 2 , batch: 503 , training loss: 5.423126\n",
      "[INFO] Epoch: 2 , batch: 504 , training loss: 5.443881\n",
      "[INFO] Epoch: 2 , batch: 505 , training loss: 5.301152\n",
      "[INFO] Epoch: 2 , batch: 506 , training loss: 5.179121\n",
      "[INFO] Epoch: 2 , batch: 507 , training loss: 5.187218\n",
      "[INFO] Epoch: 2 , batch: 508 , training loss: 5.175975\n",
      "[INFO] Epoch: 2 , batch: 509 , training loss: 4.896028\n",
      "[INFO] Epoch: 2 , batch: 510 , training loss: 4.962868\n",
      "[INFO] Epoch: 2 , batch: 511 , training loss: 4.895621\n",
      "[INFO] Epoch: 2 , batch: 512 , training loss: 4.968307\n",
      "[INFO] Epoch: 2 , batch: 513 , training loss: 5.231459\n",
      "[INFO] Epoch: 2 , batch: 514 , training loss: 4.886472\n",
      "[INFO] Epoch: 2 , batch: 515 , training loss: 5.147186\n",
      "[INFO] Epoch: 2 , batch: 516 , training loss: 4.912282\n",
      "[INFO] Epoch: 2 , batch: 517 , training loss: 4.947413\n",
      "[INFO] Epoch: 2 , batch: 518 , training loss: 4.874122\n",
      "[INFO] Epoch: 2 , batch: 519 , training loss: 4.762247\n",
      "[INFO] Epoch: 2 , batch: 520 , training loss: 4.974538\n",
      "[INFO] Epoch: 2 , batch: 521 , training loss: 4.997345\n",
      "[INFO] Epoch: 2 , batch: 522 , training loss: 5.074412\n",
      "[INFO] Epoch: 2 , batch: 523 , training loss: 4.928040\n",
      "[INFO] Epoch: 2 , batch: 524 , training loss: 5.161480\n",
      "[INFO] Epoch: 2 , batch: 525 , training loss: 5.120443\n",
      "[INFO] Epoch: 2 , batch: 526 , training loss: 4.855090\n",
      "[INFO] Epoch: 2 , batch: 527 , training loss: 4.874193\n",
      "[INFO] Epoch: 2 , batch: 528 , training loss: 4.983078\n",
      "[INFO] Epoch: 2 , batch: 529 , training loss: 4.906145\n",
      "[INFO] Epoch: 2 , batch: 530 , training loss: 4.742607\n",
      "[INFO] Epoch: 2 , batch: 531 , training loss: 4.907222\n",
      "[INFO] Epoch: 2 , batch: 532 , training loss: 4.734719\n",
      "[INFO] Epoch: 2 , batch: 533 , training loss: 4.884853\n",
      "[INFO] Epoch: 2 , batch: 534 , training loss: 4.909393\n",
      "[INFO] Epoch: 2 , batch: 535 , training loss: 4.915731\n",
      "[INFO] Epoch: 2 , batch: 536 , training loss: 4.829050\n",
      "[INFO] Epoch: 2 , batch: 537 , training loss: 4.765860\n",
      "[INFO] Epoch: 2 , batch: 538 , training loss: 4.857274\n",
      "[INFO] Epoch: 2 , batch: 539 , training loss: 5.015366\n",
      "[INFO] Epoch: 2 , batch: 540 , training loss: 5.656140\n",
      "[INFO] Epoch: 2 , batch: 541 , training loss: 5.634117\n",
      "[INFO] Epoch: 2 , batch: 542 , training loss: 5.372370\n",
      "[INFO] Epoch: 3 , batch: 0 , training loss: 5.337770\n",
      "[INFO] Epoch: 3 , batch: 1 , training loss: 5.008045\n",
      "[INFO] Epoch: 3 , batch: 2 , training loss: 4.757611\n",
      "[INFO] Epoch: 3 , batch: 3 , training loss: 4.672517\n",
      "[INFO] Epoch: 3 , batch: 4 , training loss: 5.125762\n",
      "[INFO] Epoch: 3 , batch: 5 , training loss: 4.736997\n",
      "[INFO] Epoch: 3 , batch: 6 , training loss: 5.311874\n",
      "[INFO] Epoch: 3 , batch: 7 , training loss: 4.739514\n",
      "[INFO] Epoch: 3 , batch: 8 , training loss: 4.453074\n",
      "[INFO] Epoch: 3 , batch: 9 , training loss: 4.557843\n",
      "[INFO] Epoch: 3 , batch: 10 , training loss: 4.475604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 3 , batch: 11 , training loss: 4.330787\n",
      "[INFO] Epoch: 3 , batch: 12 , training loss: 4.415783\n",
      "[INFO] Epoch: 3 , batch: 13 , training loss: 4.298196\n",
      "[INFO] Epoch: 3 , batch: 14 , training loss: 4.042514\n",
      "[INFO] Epoch: 3 , batch: 15 , training loss: 4.343400\n",
      "[INFO] Epoch: 3 , batch: 16 , training loss: 4.242870\n",
      "[INFO] Epoch: 3 , batch: 17 , training loss: 4.392701\n",
      "[INFO] Epoch: 3 , batch: 18 , training loss: 4.361091\n",
      "[INFO] Epoch: 3 , batch: 19 , training loss: 4.056495\n",
      "[INFO] Epoch: 3 , batch: 20 , training loss: 4.048830\n",
      "[INFO] Epoch: 3 , batch: 21 , training loss: 4.139276\n",
      "[INFO] Epoch: 3 , batch: 22 , training loss: 4.285148\n",
      "[INFO] Epoch: 3 , batch: 23 , training loss: 4.422351\n",
      "[INFO] Epoch: 3 , batch: 24 , training loss: 4.258763\n",
      "[INFO] Epoch: 3 , batch: 25 , training loss: 4.382382\n",
      "[INFO] Epoch: 3 , batch: 26 , training loss: 4.203030\n",
      "[INFO] Epoch: 3 , batch: 27 , training loss: 4.193867\n",
      "[INFO] Epoch: 3 , batch: 28 , training loss: 4.462840\n",
      "[INFO] Epoch: 3 , batch: 29 , training loss: 4.163274\n",
      "[INFO] Epoch: 3 , batch: 30 , training loss: 4.173798\n",
      "[INFO] Epoch: 3 , batch: 31 , training loss: 4.273635\n",
      "[INFO] Epoch: 3 , batch: 32 , training loss: 4.244531\n",
      "[INFO] Epoch: 3 , batch: 33 , training loss: 4.395383\n",
      "[INFO] Epoch: 3 , batch: 34 , training loss: 4.416498\n",
      "[INFO] Epoch: 3 , batch: 35 , training loss: 4.294502\n",
      "[INFO] Epoch: 3 , batch: 36 , training loss: 4.232504\n",
      "[INFO] Epoch: 3 , batch: 37 , training loss: 4.168920\n",
      "[INFO] Epoch: 3 , batch: 38 , training loss: 4.309687\n",
      "[INFO] Epoch: 3 , batch: 39 , training loss: 4.140493\n",
      "[INFO] Epoch: 3 , batch: 40 , training loss: 4.214726\n",
      "[INFO] Epoch: 3 , batch: 41 , training loss: 4.506217\n",
      "[INFO] Epoch: 3 , batch: 42 , training loss: 5.157363\n",
      "[INFO] Epoch: 3 , batch: 43 , training loss: 4.881571\n",
      "[INFO] Epoch: 3 , batch: 44 , training loss: 4.938080\n",
      "[INFO] Epoch: 3 , batch: 45 , training loss: 5.174688\n",
      "[INFO] Epoch: 3 , batch: 46 , training loss: 5.677016\n",
      "[INFO] Epoch: 3 , batch: 47 , training loss: 4.712784\n",
      "[INFO] Epoch: 3 , batch: 48 , training loss: 4.901218\n",
      "[INFO] Epoch: 3 , batch: 49 , training loss: 4.796847\n",
      "[INFO] Epoch: 3 , batch: 50 , training loss: 4.601270\n",
      "[INFO] Epoch: 3 , batch: 51 , training loss: 4.581603\n",
      "[INFO] Epoch: 3 , batch: 52 , training loss: 4.402841\n",
      "[INFO] Epoch: 3 , batch: 53 , training loss: 4.558444\n",
      "[INFO] Epoch: 3 , batch: 54 , training loss: 4.525095\n",
      "[INFO] Epoch: 3 , batch: 55 , training loss: 4.634746\n",
      "[INFO] Epoch: 3 , batch: 56 , training loss: 4.423258\n",
      "[INFO] Epoch: 3 , batch: 57 , training loss: 4.260878\n",
      "[INFO] Epoch: 3 , batch: 58 , training loss: 4.264833\n",
      "[INFO] Epoch: 3 , batch: 59 , training loss: 4.447176\n",
      "[INFO] Epoch: 3 , batch: 60 , training loss: 4.203751\n",
      "[INFO] Epoch: 3 , batch: 61 , training loss: 4.361294\n",
      "[INFO] Epoch: 3 , batch: 62 , training loss: 4.234810\n",
      "[INFO] Epoch: 3 , batch: 63 , training loss: 4.434609\n",
      "[INFO] Epoch: 3 , batch: 64 , training loss: 4.576336\n",
      "[INFO] Epoch: 3 , batch: 65 , training loss: 4.310367\n",
      "[INFO] Epoch: 3 , batch: 66 , training loss: 4.124815\n",
      "[INFO] Epoch: 3 , batch: 67 , training loss: 4.147551\n",
      "[INFO] Epoch: 3 , batch: 68 , training loss: 4.496900\n",
      "[INFO] Epoch: 3 , batch: 69 , training loss: 4.240772\n",
      "[INFO] Epoch: 3 , batch: 70 , training loss: 4.572063\n",
      "[INFO] Epoch: 3 , batch: 71 , training loss: 4.319751\n",
      "[INFO] Epoch: 3 , batch: 72 , training loss: 4.391459\n",
      "[INFO] Epoch: 3 , batch: 73 , training loss: 4.323627\n",
      "[INFO] Epoch: 3 , batch: 74 , training loss: 4.479606\n",
      "[INFO] Epoch: 3 , batch: 75 , training loss: 4.179676\n",
      "[INFO] Epoch: 3 , batch: 76 , training loss: 4.381433\n",
      "[INFO] Epoch: 3 , batch: 77 , training loss: 4.252601\n",
      "[INFO] Epoch: 3 , batch: 78 , training loss: 4.335021\n",
      "[INFO] Epoch: 3 , batch: 79 , training loss: 4.128634\n",
      "[INFO] Epoch: 3 , batch: 80 , training loss: 4.394810\n",
      "[INFO] Epoch: 3 , batch: 81 , training loss: 4.374813\n",
      "[INFO] Epoch: 3 , batch: 82 , training loss: 4.375528\n",
      "[INFO] Epoch: 3 , batch: 83 , training loss: 4.450646\n",
      "[INFO] Epoch: 3 , batch: 84 , training loss: 4.392434\n",
      "[INFO] Epoch: 3 , batch: 85 , training loss: 4.458648\n",
      "[INFO] Epoch: 3 , batch: 86 , training loss: 4.477942\n",
      "[INFO] Epoch: 3 , batch: 87 , training loss: 4.398587\n",
      "[INFO] Epoch: 3 , batch: 88 , training loss: 4.554245\n",
      "[INFO] Epoch: 3 , batch: 89 , training loss: 4.334505\n",
      "[INFO] Epoch: 3 , batch: 90 , training loss: 4.359780\n",
      "[INFO] Epoch: 3 , batch: 91 , training loss: 4.304111\n",
      "[INFO] Epoch: 3 , batch: 92 , training loss: 4.288651\n",
      "[INFO] Epoch: 3 , batch: 93 , training loss: 4.396970\n",
      "[INFO] Epoch: 3 , batch: 94 , training loss: 4.617079\n",
      "[INFO] Epoch: 3 , batch: 95 , training loss: 4.356519\n",
      "[INFO] Epoch: 3 , batch: 96 , training loss: 4.332065\n",
      "[INFO] Epoch: 3 , batch: 97 , training loss: 4.323423\n",
      "[INFO] Epoch: 3 , batch: 98 , training loss: 4.305838\n",
      "[INFO] Epoch: 3 , batch: 99 , training loss: 4.345463\n",
      "[INFO] Epoch: 3 , batch: 100 , training loss: 4.175906\n",
      "[INFO] Epoch: 3 , batch: 101 , training loss: 4.214375\n",
      "[INFO] Epoch: 3 , batch: 102 , training loss: 4.432834\n",
      "[INFO] Epoch: 3 , batch: 103 , training loss: 4.137545\n",
      "[INFO] Epoch: 3 , batch: 104 , training loss: 4.122626\n",
      "[INFO] Epoch: 3 , batch: 105 , training loss: 4.430007\n",
      "[INFO] Epoch: 3 , batch: 106 , training loss: 4.404349\n",
      "[INFO] Epoch: 3 , batch: 107 , training loss: 4.296948\n",
      "[INFO] Epoch: 3 , batch: 108 , training loss: 4.156650\n",
      "[INFO] Epoch: 3 , batch: 109 , training loss: 4.056651\n",
      "[INFO] Epoch: 3 , batch: 110 , training loss: 4.385617\n",
      "[INFO] Epoch: 3 , batch: 111 , training loss: 4.358845\n",
      "[INFO] Epoch: 3 , batch: 112 , training loss: 4.367776\n",
      "[INFO] Epoch: 3 , batch: 113 , training loss: 4.272830\n",
      "[INFO] Epoch: 3 , batch: 114 , training loss: 4.424069\n",
      "[INFO] Epoch: 3 , batch: 115 , training loss: 4.326879\n",
      "[INFO] Epoch: 3 , batch: 116 , training loss: 4.317682\n",
      "[INFO] Epoch: 3 , batch: 117 , training loss: 4.553482\n",
      "[INFO] Epoch: 3 , batch: 118 , training loss: 4.531383\n",
      "[INFO] Epoch: 3 , batch: 119 , training loss: 4.641688\n",
      "[INFO] Epoch: 3 , batch: 120 , training loss: 4.564615\n",
      "[INFO] Epoch: 3 , batch: 121 , training loss: 4.344925\n",
      "[INFO] Epoch: 3 , batch: 122 , training loss: 4.267354\n",
      "[INFO] Epoch: 3 , batch: 123 , training loss: 4.419239\n",
      "[INFO] Epoch: 3 , batch: 124 , training loss: 4.585666\n",
      "[INFO] Epoch: 3 , batch: 125 , training loss: 4.203084\n",
      "[INFO] Epoch: 3 , batch: 126 , training loss: 4.202364\n",
      "[INFO] Epoch: 3 , batch: 127 , training loss: 4.270916\n",
      "[INFO] Epoch: 3 , batch: 128 , training loss: 4.465630\n",
      "[INFO] Epoch: 3 , batch: 129 , training loss: 4.356139\n",
      "[INFO] Epoch: 3 , batch: 130 , training loss: 4.411264\n",
      "[INFO] Epoch: 3 , batch: 131 , training loss: 4.366714\n",
      "[INFO] Epoch: 3 , batch: 132 , training loss: 4.428339\n",
      "[INFO] Epoch: 3 , batch: 133 , training loss: 4.331401\n",
      "[INFO] Epoch: 3 , batch: 134 , training loss: 4.097063\n",
      "[INFO] Epoch: 3 , batch: 135 , training loss: 4.125245\n",
      "[INFO] Epoch: 3 , batch: 136 , training loss: 4.445601\n",
      "[INFO] Epoch: 3 , batch: 137 , training loss: 4.361496\n",
      "[INFO] Epoch: 3 , batch: 138 , training loss: 4.523664\n",
      "[INFO] Epoch: 3 , batch: 139 , training loss: 5.237748\n",
      "[INFO] Epoch: 3 , batch: 140 , training loss: 5.150206\n",
      "[INFO] Epoch: 3 , batch: 141 , training loss: 4.819495\n",
      "[INFO] Epoch: 3 , batch: 142 , training loss: 4.360879\n",
      "[INFO] Epoch: 3 , batch: 143 , training loss: 4.476043\n",
      "[INFO] Epoch: 3 , batch: 144 , training loss: 4.299926\n",
      "[INFO] Epoch: 3 , batch: 145 , training loss: 4.449296\n",
      "[INFO] Epoch: 3 , batch: 146 , training loss: 4.659252\n",
      "[INFO] Epoch: 3 , batch: 147 , training loss: 4.214109\n",
      "[INFO] Epoch: 3 , batch: 148 , training loss: 4.213551\n",
      "[INFO] Epoch: 3 , batch: 149 , training loss: 4.290922\n",
      "[INFO] Epoch: 3 , batch: 150 , training loss: 4.585062\n",
      "[INFO] Epoch: 3 , batch: 151 , training loss: 4.243750\n",
      "[INFO] Epoch: 3 , batch: 152 , training loss: 4.277136\n",
      "[INFO] Epoch: 3 , batch: 153 , training loss: 4.451976\n",
      "[INFO] Epoch: 3 , batch: 154 , training loss: 4.497255\n",
      "[INFO] Epoch: 3 , batch: 155 , training loss: 4.773882\n",
      "[INFO] Epoch: 3 , batch: 156 , training loss: 4.391239\n",
      "[INFO] Epoch: 3 , batch: 157 , training loss: 4.375815\n",
      "[INFO] Epoch: 3 , batch: 158 , training loss: 4.848110\n",
      "[INFO] Epoch: 3 , batch: 159 , training loss: 4.761096\n",
      "[INFO] Epoch: 3 , batch: 160 , training loss: 5.273834\n",
      "[INFO] Epoch: 3 , batch: 161 , training loss: 4.977396\n",
      "[INFO] Epoch: 3 , batch: 162 , training loss: 4.842651\n",
      "[INFO] Epoch: 3 , batch: 163 , training loss: 4.939812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 3 , batch: 164 , training loss: 4.769733\n",
      "[INFO] Epoch: 3 , batch: 165 , training loss: 4.769100\n",
      "[INFO] Epoch: 3 , batch: 166 , training loss: 5.134786\n",
      "[INFO] Epoch: 3 , batch: 167 , training loss: 5.827849\n",
      "[INFO] Epoch: 3 , batch: 168 , training loss: 5.529895\n",
      "[INFO] Epoch: 3 , batch: 169 , training loss: 5.241143\n",
      "[INFO] Epoch: 3 , batch: 170 , training loss: 5.225587\n",
      "[INFO] Epoch: 3 , batch: 171 , training loss: 4.961725\n",
      "[INFO] Epoch: 3 , batch: 172 , training loss: 5.131972\n",
      "[INFO] Epoch: 3 , batch: 173 , training loss: 5.294974\n",
      "[INFO] Epoch: 3 , batch: 174 , training loss: 5.472019\n",
      "[INFO] Epoch: 3 , batch: 175 , training loss: 5.537471\n",
      "[INFO] Epoch: 3 , batch: 176 , training loss: 5.388339\n",
      "[INFO] Epoch: 3 , batch: 177 , training loss: 4.993545\n",
      "[INFO] Epoch: 3 , batch: 178 , training loss: 4.861241\n",
      "[INFO] Epoch: 3 , batch: 179 , training loss: 4.865458\n",
      "[INFO] Epoch: 3 , batch: 180 , training loss: 4.803716\n",
      "[INFO] Epoch: 3 , batch: 181 , training loss: 5.023821\n",
      "[INFO] Epoch: 3 , batch: 182 , training loss: 4.915936\n",
      "[INFO] Epoch: 3 , batch: 183 , training loss: 4.888306\n",
      "[INFO] Epoch: 3 , batch: 184 , training loss: 4.730169\n",
      "[INFO] Epoch: 3 , batch: 185 , training loss: 4.702085\n",
      "[INFO] Epoch: 3 , batch: 186 , training loss: 4.811793\n",
      "[INFO] Epoch: 3 , batch: 187 , training loss: 4.942763\n",
      "[INFO] Epoch: 3 , batch: 188 , training loss: 4.890729\n",
      "[INFO] Epoch: 3 , batch: 189 , training loss: 4.813547\n",
      "[INFO] Epoch: 3 , batch: 190 , training loss: 4.751131\n",
      "[INFO] Epoch: 3 , batch: 191 , training loss: 4.921139\n",
      "[INFO] Epoch: 3 , batch: 192 , training loss: 4.656728\n",
      "[INFO] Epoch: 3 , batch: 193 , training loss: 4.808020\n",
      "[INFO] Epoch: 3 , batch: 194 , training loss: 4.690311\n",
      "[INFO] Epoch: 3 , batch: 195 , training loss: 4.838300\n",
      "[INFO] Epoch: 3 , batch: 196 , training loss: 4.547632\n",
      "[INFO] Epoch: 3 , batch: 197 , training loss: 4.786172\n",
      "[INFO] Epoch: 3 , batch: 198 , training loss: 4.557479\n",
      "[INFO] Epoch: 3 , batch: 199 , training loss: 4.642065\n",
      "[INFO] Epoch: 3 , batch: 200 , training loss: 4.585122\n",
      "[INFO] Epoch: 3 , batch: 201 , training loss: 4.515389\n",
      "[INFO] Epoch: 3 , batch: 202 , training loss: 4.471010\n",
      "[INFO] Epoch: 3 , batch: 203 , training loss: 4.453402\n",
      "[INFO] Epoch: 3 , batch: 204 , training loss: 4.656593\n",
      "[INFO] Epoch: 3 , batch: 205 , training loss: 4.210339\n",
      "[INFO] Epoch: 3 , batch: 206 , training loss: 4.093265\n",
      "[INFO] Epoch: 3 , batch: 207 , training loss: 4.130580\n",
      "[INFO] Epoch: 3 , batch: 208 , training loss: 4.573979\n",
      "[INFO] Epoch: 3 , batch: 209 , training loss: 4.448370\n",
      "[INFO] Epoch: 3 , batch: 210 , training loss: 4.504503\n",
      "[INFO] Epoch: 3 , batch: 211 , training loss: 4.459505\n",
      "[INFO] Epoch: 3 , batch: 212 , training loss: 4.556821\n",
      "[INFO] Epoch: 3 , batch: 213 , training loss: 4.580997\n",
      "[INFO] Epoch: 3 , batch: 214 , training loss: 4.561161\n",
      "[INFO] Epoch: 3 , batch: 215 , training loss: 4.813250\n",
      "[INFO] Epoch: 3 , batch: 216 , training loss: 4.516452\n",
      "[INFO] Epoch: 3 , batch: 217 , training loss: 4.428268\n",
      "[INFO] Epoch: 3 , batch: 218 , training loss: 4.455742\n",
      "[INFO] Epoch: 3 , batch: 219 , training loss: 4.543583\n",
      "[INFO] Epoch: 3 , batch: 220 , training loss: 4.350905\n",
      "[INFO] Epoch: 3 , batch: 221 , training loss: 4.370691\n",
      "[INFO] Epoch: 3 , batch: 222 , training loss: 4.537156\n",
      "[INFO] Epoch: 3 , batch: 223 , training loss: 4.617985\n",
      "[INFO] Epoch: 3 , batch: 224 , training loss: 4.657812\n",
      "[INFO] Epoch: 3 , batch: 225 , training loss: 4.494401\n",
      "[INFO] Epoch: 3 , batch: 226 , training loss: 4.654554\n",
      "[INFO] Epoch: 3 , batch: 227 , training loss: 4.618005\n",
      "[INFO] Epoch: 3 , batch: 228 , training loss: 4.646791\n",
      "[INFO] Epoch: 3 , batch: 229 , training loss: 4.542449\n",
      "[INFO] Epoch: 3 , batch: 230 , training loss: 4.361245\n",
      "[INFO] Epoch: 3 , batch: 231 , training loss: 4.175199\n",
      "[INFO] Epoch: 3 , batch: 232 , training loss: 4.337801\n",
      "[INFO] Epoch: 3 , batch: 233 , training loss: 4.411114\n",
      "[INFO] Epoch: 3 , batch: 234 , training loss: 4.028461\n",
      "[INFO] Epoch: 3 , batch: 235 , training loss: 4.185785\n",
      "[INFO] Epoch: 3 , batch: 236 , training loss: 4.391282\n",
      "[INFO] Epoch: 3 , batch: 237 , training loss: 4.525943\n",
      "[INFO] Epoch: 3 , batch: 238 , training loss: 4.253444\n",
      "[INFO] Epoch: 3 , batch: 239 , training loss: 4.326969\n",
      "[INFO] Epoch: 3 , batch: 240 , training loss: 4.399991\n",
      "[INFO] Epoch: 3 , batch: 241 , training loss: 4.158655\n",
      "[INFO] Epoch: 3 , batch: 242 , training loss: 4.174210\n",
      "[INFO] Epoch: 3 , batch: 243 , training loss: 4.522025\n",
      "[INFO] Epoch: 3 , batch: 244 , training loss: 4.448392\n",
      "[INFO] Epoch: 3 , batch: 245 , training loss: 4.439441\n",
      "[INFO] Epoch: 3 , batch: 246 , training loss: 4.068327\n",
      "[INFO] Epoch: 3 , batch: 247 , training loss: 4.225995\n",
      "[INFO] Epoch: 3 , batch: 248 , training loss: 4.362119\n",
      "[INFO] Epoch: 3 , batch: 249 , training loss: 4.311119\n",
      "[INFO] Epoch: 3 , batch: 250 , training loss: 4.090378\n",
      "[INFO] Epoch: 3 , batch: 251 , training loss: 4.601853\n",
      "[INFO] Epoch: 3 , batch: 252 , training loss: 4.264345\n",
      "[INFO] Epoch: 3 , batch: 253 , training loss: 4.227816\n",
      "[INFO] Epoch: 3 , batch: 254 , training loss: 4.552535\n",
      "[INFO] Epoch: 3 , batch: 255 , training loss: 4.523349\n",
      "[INFO] Epoch: 3 , batch: 256 , training loss: 4.460167\n",
      "[INFO] Epoch: 3 , batch: 257 , training loss: 4.656782\n",
      "[INFO] Epoch: 3 , batch: 258 , training loss: 4.719324\n",
      "[INFO] Epoch: 3 , batch: 259 , training loss: 4.747453\n",
      "[INFO] Epoch: 3 , batch: 260 , training loss: 4.408844\n",
      "[INFO] Epoch: 3 , batch: 261 , training loss: 4.638266\n",
      "[INFO] Epoch: 3 , batch: 262 , training loss: 4.858800\n",
      "[INFO] Epoch: 3 , batch: 263 , training loss: 4.968875\n",
      "[INFO] Epoch: 3 , batch: 264 , training loss: 4.251172\n",
      "[INFO] Epoch: 3 , batch: 265 , training loss: 4.408924\n",
      "[INFO] Epoch: 3 , batch: 266 , training loss: 4.947713\n",
      "[INFO] Epoch: 3 , batch: 267 , training loss: 4.590942\n",
      "[INFO] Epoch: 3 , batch: 268 , training loss: 4.489667\n",
      "[INFO] Epoch: 3 , batch: 269 , training loss: 4.535218\n",
      "[INFO] Epoch: 3 , batch: 270 , training loss: 4.504215\n",
      "[INFO] Epoch: 3 , batch: 271 , training loss: 4.540630\n",
      "[INFO] Epoch: 3 , batch: 272 , training loss: 4.513350\n",
      "[INFO] Epoch: 3 , batch: 273 , training loss: 4.524971\n",
      "[INFO] Epoch: 3 , batch: 274 , training loss: 4.639171\n",
      "[INFO] Epoch: 3 , batch: 275 , training loss: 4.522469\n",
      "[INFO] Epoch: 3 , batch: 276 , training loss: 4.575178\n",
      "[INFO] Epoch: 3 , batch: 277 , training loss: 4.728739\n",
      "[INFO] Epoch: 3 , batch: 278 , training loss: 4.283747\n",
      "[INFO] Epoch: 3 , batch: 279 , training loss: 4.313495\n",
      "[INFO] Epoch: 3 , batch: 280 , training loss: 4.244945\n",
      "[INFO] Epoch: 3 , batch: 281 , training loss: 4.421994\n",
      "[INFO] Epoch: 3 , batch: 282 , training loss: 4.321512\n",
      "[INFO] Epoch: 3 , batch: 283 , training loss: 4.410302\n",
      "[INFO] Epoch: 3 , batch: 284 , training loss: 4.419762\n",
      "[INFO] Epoch: 3 , batch: 285 , training loss: 4.449588\n",
      "[INFO] Epoch: 3 , batch: 286 , training loss: 4.397577\n",
      "[INFO] Epoch: 3 , batch: 287 , training loss: 4.231241\n",
      "[INFO] Epoch: 3 , batch: 288 , training loss: 4.294955\n",
      "[INFO] Epoch: 3 , batch: 289 , training loss: 4.334431\n",
      "[INFO] Epoch: 3 , batch: 290 , training loss: 4.126559\n",
      "[INFO] Epoch: 3 , batch: 291 , training loss: 4.101809\n",
      "[INFO] Epoch: 3 , batch: 292 , training loss: 4.186777\n",
      "[INFO] Epoch: 3 , batch: 293 , training loss: 4.167136\n",
      "[INFO] Epoch: 3 , batch: 294 , training loss: 4.865555\n",
      "[INFO] Epoch: 3 , batch: 295 , training loss: 4.602051\n",
      "[INFO] Epoch: 3 , batch: 296 , training loss: 4.545565\n",
      "[INFO] Epoch: 3 , batch: 297 , training loss: 4.442001\n",
      "[INFO] Epoch: 3 , batch: 298 , training loss: 4.310317\n",
      "[INFO] Epoch: 3 , batch: 299 , training loss: 4.292414\n",
      "[INFO] Epoch: 3 , batch: 300 , training loss: 4.272077\n",
      "[INFO] Epoch: 3 , batch: 301 , training loss: 4.222303\n",
      "[INFO] Epoch: 3 , batch: 302 , training loss: 4.400370\n",
      "[INFO] Epoch: 3 , batch: 303 , training loss: 4.411560\n",
      "[INFO] Epoch: 3 , batch: 304 , training loss: 4.630335\n",
      "[INFO] Epoch: 3 , batch: 305 , training loss: 4.323916\n",
      "[INFO] Epoch: 3 , batch: 306 , training loss: 4.468846\n",
      "[INFO] Epoch: 3 , batch: 307 , training loss: 4.464818\n",
      "[INFO] Epoch: 3 , batch: 308 , training loss: 4.370304\n",
      "[INFO] Epoch: 3 , batch: 309 , training loss: 4.381191\n",
      "[INFO] Epoch: 3 , batch: 310 , training loss: 4.155506\n",
      "[INFO] Epoch: 3 , batch: 311 , training loss: 4.216530\n",
      "[INFO] Epoch: 3 , batch: 312 , training loss: 4.080042\n",
      "[INFO] Epoch: 3 , batch: 313 , training loss: 4.264275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 3 , batch: 314 , training loss: 4.308389\n",
      "[INFO] Epoch: 3 , batch: 315 , training loss: 4.347925\n",
      "[INFO] Epoch: 3 , batch: 316 , training loss: 4.721253\n",
      "[INFO] Epoch: 3 , batch: 317 , training loss: 5.289963\n",
      "[INFO] Epoch: 3 , batch: 318 , training loss: 5.391107\n",
      "[INFO] Epoch: 3 , batch: 319 , training loss: 4.902664\n",
      "[INFO] Epoch: 3 , batch: 320 , training loss: 4.388143\n",
      "[INFO] Epoch: 3 , batch: 321 , training loss: 4.177179\n",
      "[INFO] Epoch: 3 , batch: 322 , training loss: 4.303124\n",
      "[INFO] Epoch: 3 , batch: 323 , training loss: 4.300228\n",
      "[INFO] Epoch: 3 , batch: 324 , training loss: 4.310980\n",
      "[INFO] Epoch: 3 , batch: 325 , training loss: 4.481746\n",
      "[INFO] Epoch: 3 , batch: 326 , training loss: 4.529106\n",
      "[INFO] Epoch: 3 , batch: 327 , training loss: 4.413432\n",
      "[INFO] Epoch: 3 , batch: 328 , training loss: 4.423798\n",
      "[INFO] Epoch: 3 , batch: 329 , training loss: 4.305496\n",
      "[INFO] Epoch: 3 , batch: 330 , training loss: 4.278306\n",
      "[INFO] Epoch: 3 , batch: 331 , training loss: 4.487283\n",
      "[INFO] Epoch: 3 , batch: 332 , training loss: 4.215648\n",
      "[INFO] Epoch: 3 , batch: 333 , training loss: 4.201621\n",
      "[INFO] Epoch: 3 , batch: 334 , training loss: 4.331522\n",
      "[INFO] Epoch: 3 , batch: 335 , training loss: 4.443033\n",
      "[INFO] Epoch: 3 , batch: 336 , training loss: 4.412693\n",
      "[INFO] Epoch: 3 , batch: 337 , training loss: 4.506010\n",
      "[INFO] Epoch: 3 , batch: 338 , training loss: 4.669536\n",
      "[INFO] Epoch: 3 , batch: 339 , training loss: 4.501818\n",
      "[INFO] Epoch: 3 , batch: 340 , training loss: 4.709891\n",
      "[INFO] Epoch: 3 , batch: 341 , training loss: 4.420481\n",
      "[INFO] Epoch: 3 , batch: 342 , training loss: 4.263670\n",
      "[INFO] Epoch: 3 , batch: 343 , training loss: 4.331793\n",
      "[INFO] Epoch: 3 , batch: 344 , training loss: 4.166383\n",
      "[INFO] Epoch: 3 , batch: 345 , training loss: 4.295357\n",
      "[INFO] Epoch: 3 , batch: 346 , training loss: 4.381761\n",
      "[INFO] Epoch: 3 , batch: 347 , training loss: 4.261816\n",
      "[INFO] Epoch: 3 , batch: 348 , training loss: 4.507798\n",
      "[INFO] Epoch: 3 , batch: 349 , training loss: 4.568142\n",
      "[INFO] Epoch: 3 , batch: 350 , training loss: 4.287187\n",
      "[INFO] Epoch: 3 , batch: 351 , training loss: 4.350156\n",
      "[INFO] Epoch: 3 , batch: 352 , training loss: 4.366093\n",
      "[INFO] Epoch: 3 , batch: 353 , training loss: 4.325303\n",
      "[INFO] Epoch: 3 , batch: 354 , training loss: 4.446744\n",
      "[INFO] Epoch: 3 , batch: 355 , training loss: 4.493316\n",
      "[INFO] Epoch: 3 , batch: 356 , training loss: 4.361780\n",
      "[INFO] Epoch: 3 , batch: 357 , training loss: 4.409374\n",
      "[INFO] Epoch: 3 , batch: 358 , training loss: 4.372435\n",
      "[INFO] Epoch: 3 , batch: 359 , training loss: 4.361934\n",
      "[INFO] Epoch: 3 , batch: 360 , training loss: 4.373238\n",
      "[INFO] Epoch: 3 , batch: 361 , training loss: 4.359888\n",
      "[INFO] Epoch: 3 , batch: 362 , training loss: 4.445634\n",
      "[INFO] Epoch: 3 , batch: 363 , training loss: 4.349214\n",
      "[INFO] Epoch: 3 , batch: 364 , training loss: 4.376936\n",
      "[INFO] Epoch: 3 , batch: 365 , training loss: 4.278321\n",
      "[INFO] Epoch: 3 , batch: 366 , training loss: 4.453160\n",
      "[INFO] Epoch: 3 , batch: 367 , training loss: 4.505717\n",
      "[INFO] Epoch: 3 , batch: 368 , training loss: 5.112123\n",
      "[INFO] Epoch: 3 , batch: 369 , training loss: 4.702184\n",
      "[INFO] Epoch: 3 , batch: 370 , training loss: 4.421228\n",
      "[INFO] Epoch: 3 , batch: 371 , training loss: 5.016541\n",
      "[INFO] Epoch: 3 , batch: 372 , training loss: 5.294357\n",
      "[INFO] Epoch: 3 , batch: 373 , training loss: 5.269485\n",
      "[INFO] Epoch: 3 , batch: 374 , training loss: 5.320712\n",
      "[INFO] Epoch: 3 , batch: 375 , training loss: 5.282272\n",
      "[INFO] Epoch: 3 , batch: 376 , training loss: 5.223400\n",
      "[INFO] Epoch: 3 , batch: 377 , training loss: 4.881014\n",
      "[INFO] Epoch: 3 , batch: 378 , training loss: 5.000686\n",
      "[INFO] Epoch: 3 , batch: 379 , training loss: 5.036404\n",
      "[INFO] Epoch: 3 , batch: 380 , training loss: 5.102641\n",
      "[INFO] Epoch: 3 , batch: 381 , training loss: 4.868588\n",
      "[INFO] Epoch: 3 , batch: 382 , training loss: 5.060642\n",
      "[INFO] Epoch: 3 , batch: 383 , training loss: 5.207535\n",
      "[INFO] Epoch: 3 , batch: 384 , training loss: 5.373142\n",
      "[INFO] Epoch: 3 , batch: 385 , training loss: 5.164834\n",
      "[INFO] Epoch: 3 , batch: 386 , training loss: 5.260335\n",
      "[INFO] Epoch: 3 , batch: 387 , training loss: 5.192378\n",
      "[INFO] Epoch: 3 , batch: 388 , training loss: 4.947120\n",
      "[INFO] Epoch: 3 , batch: 389 , training loss: 4.717849\n",
      "[INFO] Epoch: 3 , batch: 390 , training loss: 4.700568\n",
      "[INFO] Epoch: 3 , batch: 391 , training loss: 4.722788\n",
      "[INFO] Epoch: 3 , batch: 392 , training loss: 5.047193\n",
      "[INFO] Epoch: 3 , batch: 393 , training loss: 4.969801\n",
      "[INFO] Epoch: 3 , batch: 394 , training loss: 5.043645\n",
      "[INFO] Epoch: 3 , batch: 395 , training loss: 4.826548\n",
      "[INFO] Epoch: 3 , batch: 396 , training loss: 4.625484\n",
      "[INFO] Epoch: 3 , batch: 397 , training loss: 4.778244\n",
      "[INFO] Epoch: 3 , batch: 398 , training loss: 4.642894\n",
      "[INFO] Epoch: 3 , batch: 399 , training loss: 4.692371\n",
      "[INFO] Epoch: 3 , batch: 400 , training loss: 4.626935\n",
      "[INFO] Epoch: 3 , batch: 401 , training loss: 5.054320\n",
      "[INFO] Epoch: 3 , batch: 402 , training loss: 4.776178\n",
      "[INFO] Epoch: 3 , batch: 403 , training loss: 4.658330\n",
      "[INFO] Epoch: 3 , batch: 404 , training loss: 4.788869\n",
      "[INFO] Epoch: 3 , batch: 405 , training loss: 4.877991\n",
      "[INFO] Epoch: 3 , batch: 406 , training loss: 4.731439\n",
      "[INFO] Epoch: 3 , batch: 407 , training loss: 4.801554\n",
      "[INFO] Epoch: 3 , batch: 408 , training loss: 4.739463\n",
      "[INFO] Epoch: 3 , batch: 409 , training loss: 4.735512\n",
      "[INFO] Epoch: 3 , batch: 410 , training loss: 4.772339\n",
      "[INFO] Epoch: 3 , batch: 411 , training loss: 4.989104\n",
      "[INFO] Epoch: 3 , batch: 412 , training loss: 4.826035\n",
      "[INFO] Epoch: 3 , batch: 413 , training loss: 4.660202\n",
      "[INFO] Epoch: 3 , batch: 414 , training loss: 4.713536\n",
      "[INFO] Epoch: 3 , batch: 415 , training loss: 4.740554\n",
      "[INFO] Epoch: 3 , batch: 416 , training loss: 4.807165\n",
      "[INFO] Epoch: 3 , batch: 417 , training loss: 4.717509\n",
      "[INFO] Epoch: 3 , batch: 418 , training loss: 4.767943\n",
      "[INFO] Epoch: 3 , batch: 419 , training loss: 4.714730\n",
      "[INFO] Epoch: 3 , batch: 420 , training loss: 4.667717\n",
      "[INFO] Epoch: 3 , batch: 421 , training loss: 4.677559\n",
      "[INFO] Epoch: 3 , batch: 422 , training loss: 4.563864\n",
      "[INFO] Epoch: 3 , batch: 423 , training loss: 4.808655\n",
      "[INFO] Epoch: 3 , batch: 424 , training loss: 4.936839\n",
      "[INFO] Epoch: 3 , batch: 425 , training loss: 4.862693\n",
      "[INFO] Epoch: 3 , batch: 426 , training loss: 4.475975\n",
      "[INFO] Epoch: 3 , batch: 427 , training loss: 4.796385\n",
      "[INFO] Epoch: 3 , batch: 428 , training loss: 4.692441\n",
      "[INFO] Epoch: 3 , batch: 429 , training loss: 4.487284\n",
      "[INFO] Epoch: 3 , batch: 430 , training loss: 4.796617\n",
      "[INFO] Epoch: 3 , batch: 431 , training loss: 4.335500\n",
      "[INFO] Epoch: 3 , batch: 432 , training loss: 4.438849\n",
      "[INFO] Epoch: 3 , batch: 433 , training loss: 4.418378\n",
      "[INFO] Epoch: 3 , batch: 434 , training loss: 4.319214\n",
      "[INFO] Epoch: 3 , batch: 435 , training loss: 4.673442\n",
      "[INFO] Epoch: 3 , batch: 436 , training loss: 4.789236\n",
      "[INFO] Epoch: 3 , batch: 437 , training loss: 4.592280\n",
      "[INFO] Epoch: 3 , batch: 438 , training loss: 4.338671\n",
      "[INFO] Epoch: 3 , batch: 439 , training loss: 4.573596\n",
      "[INFO] Epoch: 3 , batch: 440 , training loss: 4.760014\n",
      "[INFO] Epoch: 3 , batch: 441 , training loss: 4.756822\n",
      "[INFO] Epoch: 3 , batch: 442 , training loss: 4.574110\n",
      "[INFO] Epoch: 3 , batch: 443 , training loss: 4.814695\n",
      "[INFO] Epoch: 3 , batch: 444 , training loss: 4.411389\n",
      "[INFO] Epoch: 3 , batch: 445 , training loss: 4.246333\n",
      "[INFO] Epoch: 3 , batch: 446 , training loss: 4.152503\n",
      "[INFO] Epoch: 3 , batch: 447 , training loss: 4.415987\n",
      "[INFO] Epoch: 3 , batch: 448 , training loss: 4.594084\n",
      "[INFO] Epoch: 3 , batch: 449 , training loss: 5.021442\n",
      "[INFO] Epoch: 3 , batch: 450 , training loss: 5.064404\n",
      "[INFO] Epoch: 3 , batch: 451 , training loss: 4.936012\n",
      "[INFO] Epoch: 3 , batch: 452 , training loss: 4.665199\n",
      "[INFO] Epoch: 3 , batch: 453 , training loss: 4.444160\n",
      "[INFO] Epoch: 3 , batch: 454 , training loss: 4.606623\n",
      "[INFO] Epoch: 3 , batch: 455 , training loss: 4.631880\n",
      "[INFO] Epoch: 3 , batch: 456 , training loss: 4.618372\n",
      "[INFO] Epoch: 3 , batch: 457 , training loss: 4.728462\n",
      "[INFO] Epoch: 3 , batch: 458 , training loss: 4.443812\n",
      "[INFO] Epoch: 3 , batch: 459 , training loss: 4.423824\n",
      "[INFO] Epoch: 3 , batch: 460 , training loss: 4.558689\n",
      "[INFO] Epoch: 3 , batch: 461 , training loss: 4.581001\n",
      "[INFO] Epoch: 3 , batch: 462 , training loss: 4.617652\n",
      "[INFO] Epoch: 3 , batch: 463 , training loss: 4.492112\n",
      "[INFO] Epoch: 3 , batch: 464 , training loss: 4.660257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 3 , batch: 465 , training loss: 4.590314\n",
      "[INFO] Epoch: 3 , batch: 466 , training loss: 4.690012\n",
      "[INFO] Epoch: 3 , batch: 467 , training loss: 4.767781\n",
      "[INFO] Epoch: 3 , batch: 468 , training loss: 4.676887\n",
      "[INFO] Epoch: 3 , batch: 469 , training loss: 4.677932\n",
      "[INFO] Epoch: 3 , batch: 470 , training loss: 4.423688\n",
      "[INFO] Epoch: 3 , batch: 471 , training loss: 4.571836\n",
      "[INFO] Epoch: 3 , batch: 472 , training loss: 4.643684\n",
      "[INFO] Epoch: 3 , batch: 473 , training loss: 4.544292\n",
      "[INFO] Epoch: 3 , batch: 474 , training loss: 4.364108\n",
      "[INFO] Epoch: 3 , batch: 475 , training loss: 4.208919\n",
      "[INFO] Epoch: 3 , batch: 476 , training loss: 4.605682\n",
      "[INFO] Epoch: 3 , batch: 477 , training loss: 4.703862\n",
      "[INFO] Epoch: 3 , batch: 478 , training loss: 4.804848\n",
      "[INFO] Epoch: 3 , batch: 479 , training loss: 4.742289\n",
      "[INFO] Epoch: 3 , batch: 480 , training loss: 4.877678\n",
      "[INFO] Epoch: 3 , batch: 481 , training loss: 4.690413\n",
      "[INFO] Epoch: 3 , batch: 482 , training loss: 4.827378\n",
      "[INFO] Epoch: 3 , batch: 483 , training loss: 4.642974\n",
      "[INFO] Epoch: 3 , batch: 484 , training loss: 4.460302\n",
      "[INFO] Epoch: 3 , batch: 485 , training loss: 4.626226\n",
      "[INFO] Epoch: 3 , batch: 486 , training loss: 4.457305\n",
      "[INFO] Epoch: 3 , batch: 487 , training loss: 4.477679\n",
      "[INFO] Epoch: 3 , batch: 488 , training loss: 4.658939\n",
      "[INFO] Epoch: 3 , batch: 489 , training loss: 4.535860\n",
      "[INFO] Epoch: 3 , batch: 490 , training loss: 4.622239\n",
      "[INFO] Epoch: 3 , batch: 491 , training loss: 4.600217\n",
      "[INFO] Epoch: 3 , batch: 492 , training loss: 4.467033\n",
      "[INFO] Epoch: 3 , batch: 493 , training loss: 4.661296\n",
      "[INFO] Epoch: 3 , batch: 494 , training loss: 4.562385\n",
      "[INFO] Epoch: 3 , batch: 495 , training loss: 4.701531\n",
      "[INFO] Epoch: 3 , batch: 496 , training loss: 4.581115\n",
      "[INFO] Epoch: 3 , batch: 497 , training loss: 4.587923\n",
      "[INFO] Epoch: 3 , batch: 498 , training loss: 4.655693\n",
      "[INFO] Epoch: 3 , batch: 499 , training loss: 4.688206\n",
      "[INFO] Epoch: 3 , batch: 500 , training loss: 4.955608\n",
      "[INFO] Epoch: 3 , batch: 501 , training loss: 5.361061\n",
      "[INFO] Epoch: 3 , batch: 502 , training loss: 5.516088\n",
      "[INFO] Epoch: 3 , batch: 503 , training loss: 5.256627\n",
      "[INFO] Epoch: 3 , batch: 504 , training loss: 5.321024\n",
      "[INFO] Epoch: 3 , batch: 505 , training loss: 5.199497\n",
      "[INFO] Epoch: 3 , batch: 506 , training loss: 5.087906\n",
      "[INFO] Epoch: 3 , batch: 507 , training loss: 5.120486\n",
      "[INFO] Epoch: 3 , batch: 508 , training loss: 5.063827\n",
      "[INFO] Epoch: 3 , batch: 509 , training loss: 4.802542\n",
      "[INFO] Epoch: 3 , batch: 510 , training loss: 4.862359\n",
      "[INFO] Epoch: 3 , batch: 511 , training loss: 4.784377\n",
      "[INFO] Epoch: 3 , batch: 512 , training loss: 4.856683\n",
      "[INFO] Epoch: 3 , batch: 513 , training loss: 5.115482\n",
      "[INFO] Epoch: 3 , batch: 514 , training loss: 4.766188\n",
      "[INFO] Epoch: 3 , batch: 515 , training loss: 5.038423\n",
      "[INFO] Epoch: 3 , batch: 516 , training loss: 4.816667\n",
      "[INFO] Epoch: 3 , batch: 517 , training loss: 4.837538\n",
      "[INFO] Epoch: 3 , batch: 518 , training loss: 4.770119\n",
      "[INFO] Epoch: 3 , batch: 519 , training loss: 4.660164\n",
      "[INFO] Epoch: 3 , batch: 520 , training loss: 4.876455\n",
      "[INFO] Epoch: 3 , batch: 521 , training loss: 4.884098\n",
      "[INFO] Epoch: 3 , batch: 522 , training loss: 4.975322\n",
      "[INFO] Epoch: 3 , batch: 523 , training loss: 4.825297\n",
      "[INFO] Epoch: 3 , batch: 524 , training loss: 5.076530\n",
      "[INFO] Epoch: 3 , batch: 525 , training loss: 5.021258\n",
      "[INFO] Epoch: 3 , batch: 526 , training loss: 4.761134\n",
      "[INFO] Epoch: 3 , batch: 527 , training loss: 4.773319\n",
      "[INFO] Epoch: 3 , batch: 528 , training loss: 4.875148\n",
      "[INFO] Epoch: 3 , batch: 529 , training loss: 4.802253\n",
      "[INFO] Epoch: 3 , batch: 530 , training loss: 4.636426\n",
      "[INFO] Epoch: 3 , batch: 531 , training loss: 4.812727\n",
      "[INFO] Epoch: 3 , batch: 532 , training loss: 4.640304\n",
      "[INFO] Epoch: 3 , batch: 533 , training loss: 4.805166\n",
      "[INFO] Epoch: 3 , batch: 534 , training loss: 4.812652\n",
      "[INFO] Epoch: 3 , batch: 535 , training loss: 4.818915\n",
      "[INFO] Epoch: 3 , batch: 536 , training loss: 4.713462\n",
      "[INFO] Epoch: 3 , batch: 537 , training loss: 4.656563\n",
      "[INFO] Epoch: 3 , batch: 538 , training loss: 4.747309\n",
      "[INFO] Epoch: 3 , batch: 539 , training loss: 4.916718\n",
      "[INFO] Epoch: 3 , batch: 540 , training loss: 5.585855\n",
      "[INFO] Epoch: 3 , batch: 541 , training loss: 5.556380\n",
      "[INFO] Epoch: 3 , batch: 542 , training loss: 5.296548\n",
      "[INFO] Epoch: 4 , batch: 0 , training loss: 4.837407\n",
      "[INFO] Epoch: 4 , batch: 1 , training loss: 4.691060\n",
      "[INFO] Epoch: 4 , batch: 2 , training loss: 4.527102\n",
      "[INFO] Epoch: 4 , batch: 3 , training loss: 4.368615\n",
      "[INFO] Epoch: 4 , batch: 4 , training loss: 4.696784\n",
      "[INFO] Epoch: 4 , batch: 5 , training loss: 4.435219\n",
      "[INFO] Epoch: 4 , batch: 6 , training loss: 5.187088\n",
      "[INFO] Epoch: 4 , batch: 7 , training loss: 4.718683\n",
      "[INFO] Epoch: 4 , batch: 8 , training loss: 4.400486\n",
      "[INFO] Epoch: 4 , batch: 9 , training loss: 4.495337\n",
      "[INFO] Epoch: 4 , batch: 10 , training loss: 4.340919\n",
      "[INFO] Epoch: 4 , batch: 11 , training loss: 4.198153\n",
      "[INFO] Epoch: 4 , batch: 12 , training loss: 4.265635\n",
      "[INFO] Epoch: 4 , batch: 13 , training loss: 4.186759\n",
      "[INFO] Epoch: 4 , batch: 14 , training loss: 3.980622\n",
      "[INFO] Epoch: 4 , batch: 15 , training loss: 4.255543\n",
      "[INFO] Epoch: 4 , batch: 16 , training loss: 4.147566\n",
      "[INFO] Epoch: 4 , batch: 17 , training loss: 4.290585\n",
      "[INFO] Epoch: 4 , batch: 18 , training loss: 4.236765\n",
      "[INFO] Epoch: 4 , batch: 19 , training loss: 3.949924\n",
      "[INFO] Epoch: 4 , batch: 20 , training loss: 3.905972\n",
      "[INFO] Epoch: 4 , batch: 21 , training loss: 4.024949\n",
      "[INFO] Epoch: 4 , batch: 22 , training loss: 4.147738\n",
      "[INFO] Epoch: 4 , batch: 23 , training loss: 4.270960\n",
      "[INFO] Epoch: 4 , batch: 24 , training loss: 4.127356\n",
      "[INFO] Epoch: 4 , batch: 25 , training loss: 4.274453\n",
      "[INFO] Epoch: 4 , batch: 26 , training loss: 4.075288\n",
      "[INFO] Epoch: 4 , batch: 27 , training loss: 4.071213\n",
      "[INFO] Epoch: 4 , batch: 28 , training loss: 4.328761\n",
      "[INFO] Epoch: 4 , batch: 29 , training loss: 4.057448\n",
      "[INFO] Epoch: 4 , batch: 30 , training loss: 4.039227\n",
      "[INFO] Epoch: 4 , batch: 31 , training loss: 4.158808\n",
      "[INFO] Epoch: 4 , batch: 32 , training loss: 4.132138\n",
      "[INFO] Epoch: 4 , batch: 33 , training loss: 4.265022\n",
      "[INFO] Epoch: 4 , batch: 34 , training loss: 4.275252\n",
      "[INFO] Epoch: 4 , batch: 35 , training loss: 4.169952\n",
      "[INFO] Epoch: 4 , batch: 36 , training loss: 4.117918\n",
      "[INFO] Epoch: 4 , batch: 37 , training loss: 4.061751\n",
      "[INFO] Epoch: 4 , batch: 38 , training loss: 4.198545\n",
      "[INFO] Epoch: 4 , batch: 39 , training loss: 3.992527\n",
      "[INFO] Epoch: 4 , batch: 40 , training loss: 4.105711\n",
      "[INFO] Epoch: 4 , batch: 41 , training loss: 4.343877\n",
      "[INFO] Epoch: 4 , batch: 42 , training loss: 5.000954\n",
      "[INFO] Epoch: 4 , batch: 43 , training loss: 4.672811\n",
      "[INFO] Epoch: 4 , batch: 44 , training loss: 4.746595\n",
      "[INFO] Epoch: 4 , batch: 45 , training loss: 4.950758\n",
      "[INFO] Epoch: 4 , batch: 46 , training loss: 5.366638\n",
      "[INFO] Epoch: 4 , batch: 47 , training loss: 4.534206\n",
      "[INFO] Epoch: 4 , batch: 48 , training loss: 4.677997\n",
      "[INFO] Epoch: 4 , batch: 49 , training loss: 4.602148\n",
      "[INFO] Epoch: 4 , batch: 50 , training loss: 4.403793\n",
      "[INFO] Epoch: 4 , batch: 51 , training loss: 4.401793\n",
      "[INFO] Epoch: 4 , batch: 52 , training loss: 4.214382\n",
      "[INFO] Epoch: 4 , batch: 53 , training loss: 4.403339\n",
      "[INFO] Epoch: 4 , batch: 54 , training loss: 4.362105\n",
      "[INFO] Epoch: 4 , batch: 55 , training loss: 4.440667\n",
      "[INFO] Epoch: 4 , batch: 56 , training loss: 4.269571\n",
      "[INFO] Epoch: 4 , batch: 57 , training loss: 4.129861\n",
      "[INFO] Epoch: 4 , batch: 58 , training loss: 4.160953\n",
      "[INFO] Epoch: 4 , batch: 59 , training loss: 4.279725\n",
      "[INFO] Epoch: 4 , batch: 60 , training loss: 4.074528\n",
      "[INFO] Epoch: 4 , batch: 61 , training loss: 4.210163\n",
      "[INFO] Epoch: 4 , batch: 62 , training loss: 4.091485\n",
      "[INFO] Epoch: 4 , batch: 63 , training loss: 4.292257\n",
      "[INFO] Epoch: 4 , batch: 64 , training loss: 4.435523\n",
      "[INFO] Epoch: 4 , batch: 65 , training loss: 4.134009\n",
      "[INFO] Epoch: 4 , batch: 66 , training loss: 3.971803\n",
      "[INFO] Epoch: 4 , batch: 67 , training loss: 4.007911\n",
      "[INFO] Epoch: 4 , batch: 68 , training loss: 4.359074\n",
      "[INFO] Epoch: 4 , batch: 69 , training loss: 4.108196\n",
      "[INFO] Epoch: 4 , batch: 70 , training loss: 4.426837\n",
      "[INFO] Epoch: 4 , batch: 71 , training loss: 4.188543\n",
      "[INFO] Epoch: 4 , batch: 72 , training loss: 4.283182\n",
      "[INFO] Epoch: 4 , batch: 73 , training loss: 4.212770\n",
      "[INFO] Epoch: 4 , batch: 74 , training loss: 4.360909\n",
      "[INFO] Epoch: 4 , batch: 75 , training loss: 4.080412\n",
      "[INFO] Epoch: 4 , batch: 76 , training loss: 4.263511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 4 , batch: 77 , training loss: 4.133512\n",
      "[INFO] Epoch: 4 , batch: 78 , training loss: 4.252681\n",
      "[INFO] Epoch: 4 , batch: 79 , training loss: 4.041174\n",
      "[INFO] Epoch: 4 , batch: 80 , training loss: 4.270640\n",
      "[INFO] Epoch: 4 , batch: 81 , training loss: 4.270917\n",
      "[INFO] Epoch: 4 , batch: 82 , training loss: 4.258350\n",
      "[INFO] Epoch: 4 , batch: 83 , training loss: 4.341034\n",
      "[INFO] Epoch: 4 , batch: 84 , training loss: 4.294547\n",
      "[INFO] Epoch: 4 , batch: 85 , training loss: 4.364056\n",
      "[INFO] Epoch: 4 , batch: 86 , training loss: 4.357769\n",
      "[INFO] Epoch: 4 , batch: 87 , training loss: 4.290096\n",
      "[INFO] Epoch: 4 , batch: 88 , training loss: 4.456582\n",
      "[INFO] Epoch: 4 , batch: 89 , training loss: 4.217693\n",
      "[INFO] Epoch: 4 , batch: 90 , training loss: 4.269317\n",
      "[INFO] Epoch: 4 , batch: 91 , training loss: 4.212384\n",
      "[INFO] Epoch: 4 , batch: 92 , training loss: 4.198821\n",
      "[INFO] Epoch: 4 , batch: 93 , training loss: 4.309466\n",
      "[INFO] Epoch: 4 , batch: 94 , training loss: 4.505835\n",
      "[INFO] Epoch: 4 , batch: 95 , training loss: 4.262411\n",
      "[INFO] Epoch: 4 , batch: 96 , training loss: 4.221572\n",
      "[INFO] Epoch: 4 , batch: 97 , training loss: 4.226583\n",
      "[INFO] Epoch: 4 , batch: 98 , training loss: 4.187568\n",
      "[INFO] Epoch: 4 , batch: 99 , training loss: 4.254767\n",
      "[INFO] Epoch: 4 , batch: 100 , training loss: 4.081904\n",
      "[INFO] Epoch: 4 , batch: 101 , training loss: 4.111430\n",
      "[INFO] Epoch: 4 , batch: 102 , training loss: 4.339224\n",
      "[INFO] Epoch: 4 , batch: 103 , training loss: 4.040012\n",
      "[INFO] Epoch: 4 , batch: 104 , training loss: 4.016179\n",
      "[INFO] Epoch: 4 , batch: 105 , training loss: 4.325122\n",
      "[INFO] Epoch: 4 , batch: 106 , training loss: 4.313865\n",
      "[INFO] Epoch: 4 , batch: 107 , training loss: 4.191167\n",
      "[INFO] Epoch: 4 , batch: 108 , training loss: 4.065311\n",
      "[INFO] Epoch: 4 , batch: 109 , training loss: 3.969283\n",
      "[INFO] Epoch: 4 , batch: 110 , training loss: 4.261081\n",
      "[INFO] Epoch: 4 , batch: 111 , training loss: 4.254938\n",
      "[INFO] Epoch: 4 , batch: 112 , training loss: 4.261476\n",
      "[INFO] Epoch: 4 , batch: 113 , training loss: 4.170481\n",
      "[INFO] Epoch: 4 , batch: 114 , training loss: 4.303321\n",
      "[INFO] Epoch: 4 , batch: 115 , training loss: 4.203453\n",
      "[INFO] Epoch: 4 , batch: 116 , training loss: 4.208071\n",
      "[INFO] Epoch: 4 , batch: 117 , training loss: 4.455245\n",
      "[INFO] Epoch: 4 , batch: 118 , training loss: 4.404882\n",
      "[INFO] Epoch: 4 , batch: 119 , training loss: 4.534052\n",
      "[INFO] Epoch: 4 , batch: 120 , training loss: 4.464755\n",
      "[INFO] Epoch: 4 , batch: 121 , training loss: 4.253354\n",
      "[INFO] Epoch: 4 , batch: 122 , training loss: 4.177884\n",
      "[INFO] Epoch: 4 , batch: 123 , training loss: 4.308699\n",
      "[INFO] Epoch: 4 , batch: 124 , training loss: 4.478465\n",
      "[INFO] Epoch: 4 , batch: 125 , training loss: 4.084351\n",
      "[INFO] Epoch: 4 , batch: 126 , training loss: 4.113002\n",
      "[INFO] Epoch: 4 , batch: 127 , training loss: 4.163240\n",
      "[INFO] Epoch: 4 , batch: 128 , training loss: 4.350864\n",
      "[INFO] Epoch: 4 , batch: 129 , training loss: 4.251385\n",
      "[INFO] Epoch: 4 , batch: 130 , training loss: 4.312645\n",
      "[INFO] Epoch: 4 , batch: 131 , training loss: 4.270319\n",
      "[INFO] Epoch: 4 , batch: 132 , training loss: 4.317374\n",
      "[INFO] Epoch: 4 , batch: 133 , training loss: 4.235229\n",
      "[INFO] Epoch: 4 , batch: 134 , training loss: 3.996072\n",
      "[INFO] Epoch: 4 , batch: 135 , training loss: 4.026178\n",
      "[INFO] Epoch: 4 , batch: 136 , training loss: 4.354112\n",
      "[INFO] Epoch: 4 , batch: 137 , training loss: 4.251138\n",
      "[INFO] Epoch: 4 , batch: 138 , training loss: 4.404920\n",
      "[INFO] Epoch: 4 , batch: 139 , training loss: 5.101160\n",
      "[INFO] Epoch: 4 , batch: 140 , training loss: 4.999325\n",
      "[INFO] Epoch: 4 , batch: 141 , training loss: 4.676014\n",
      "[INFO] Epoch: 4 , batch: 142 , training loss: 4.239559\n",
      "[INFO] Epoch: 4 , batch: 143 , training loss: 4.346025\n",
      "[INFO] Epoch: 4 , batch: 144 , training loss: 4.187457\n",
      "[INFO] Epoch: 4 , batch: 145 , training loss: 4.328926\n",
      "[INFO] Epoch: 4 , batch: 146 , training loss: 4.543553\n",
      "[INFO] Epoch: 4 , batch: 147 , training loss: 4.117440\n",
      "[INFO] Epoch: 4 , batch: 148 , training loss: 4.116557\n",
      "[INFO] Epoch: 4 , batch: 149 , training loss: 4.217186\n",
      "[INFO] Epoch: 4 , batch: 150 , training loss: 4.481945\n",
      "[INFO] Epoch: 4 , batch: 151 , training loss: 4.176702\n",
      "[INFO] Epoch: 4 , batch: 152 , training loss: 4.190955\n",
      "[INFO] Epoch: 4 , batch: 153 , training loss: 4.345773\n",
      "[INFO] Epoch: 4 , batch: 154 , training loss: 4.393395\n",
      "[INFO] Epoch: 4 , batch: 155 , training loss: 4.627769\n",
      "[INFO] Epoch: 4 , batch: 156 , training loss: 4.309886\n",
      "[INFO] Epoch: 4 , batch: 157 , training loss: 4.273784\n",
      "[INFO] Epoch: 4 , batch: 158 , training loss: 4.709791\n",
      "[INFO] Epoch: 4 , batch: 159 , training loss: 4.612080\n",
      "[INFO] Epoch: 4 , batch: 160 , training loss: 5.093250\n",
      "[INFO] Epoch: 4 , batch: 161 , training loss: 4.873992\n",
      "[INFO] Epoch: 4 , batch: 162 , training loss: 4.750282\n",
      "[INFO] Epoch: 4 , batch: 163 , training loss: 4.874503\n",
      "[INFO] Epoch: 4 , batch: 164 , training loss: 4.720307\n",
      "[INFO] Epoch: 4 , batch: 165 , training loss: 4.714012\n",
      "[INFO] Epoch: 4 , batch: 166 , training loss: 4.960902\n",
      "[INFO] Epoch: 4 , batch: 167 , training loss: 5.584446\n",
      "[INFO] Epoch: 4 , batch: 168 , training loss: 5.295075\n",
      "[INFO] Epoch: 4 , batch: 169 , training loss: 5.061475\n",
      "[INFO] Epoch: 4 , batch: 170 , training loss: 5.054704\n",
      "[INFO] Epoch: 4 , batch: 171 , training loss: 4.676798\n",
      "[INFO] Epoch: 4 , batch: 172 , training loss: 4.846951\n",
      "[INFO] Epoch: 4 , batch: 173 , training loss: 5.016917\n",
      "[INFO] Epoch: 4 , batch: 174 , training loss: 5.272396\n",
      "[INFO] Epoch: 4 , batch: 175 , training loss: 5.383652\n",
      "[INFO] Epoch: 4 , batch: 176 , training loss: 5.247118\n",
      "[INFO] Epoch: 4 , batch: 177 , training loss: 4.820079\n",
      "[INFO] Epoch: 4 , batch: 178 , training loss: 4.719570\n",
      "[INFO] Epoch: 4 , batch: 179 , training loss: 4.767307\n",
      "[INFO] Epoch: 4 , batch: 180 , training loss: 4.667640\n",
      "[INFO] Epoch: 4 , batch: 181 , training loss: 4.889083\n",
      "[INFO] Epoch: 4 , batch: 182 , training loss: 4.775730\n",
      "[INFO] Epoch: 4 , batch: 183 , training loss: 4.760361\n",
      "[INFO] Epoch: 4 , batch: 184 , training loss: 4.622210\n",
      "[INFO] Epoch: 4 , batch: 185 , training loss: 4.604217\n",
      "[INFO] Epoch: 4 , batch: 186 , training loss: 4.692941\n",
      "[INFO] Epoch: 4 , batch: 187 , training loss: 4.845584\n",
      "[INFO] Epoch: 4 , batch: 188 , training loss: 4.801935\n",
      "[INFO] Epoch: 4 , batch: 189 , training loss: 4.713681\n",
      "[INFO] Epoch: 4 , batch: 190 , training loss: 4.667903\n",
      "[INFO] Epoch: 4 , batch: 191 , training loss: 4.823559\n",
      "[INFO] Epoch: 4 , batch: 192 , training loss: 4.569142\n",
      "[INFO] Epoch: 4 , batch: 193 , training loss: 4.707963\n",
      "[INFO] Epoch: 4 , batch: 194 , training loss: 4.610501\n",
      "[INFO] Epoch: 4 , batch: 195 , training loss: 4.730812\n",
      "[INFO] Epoch: 4 , batch: 196 , training loss: 4.463124\n",
      "[INFO] Epoch: 4 , batch: 197 , training loss: 4.658885\n",
      "[INFO] Epoch: 4 , batch: 198 , training loss: 4.462115\n",
      "[INFO] Epoch: 4 , batch: 199 , training loss: 4.573709\n",
      "[INFO] Epoch: 4 , batch: 200 , training loss: 4.485221\n",
      "[INFO] Epoch: 4 , batch: 201 , training loss: 4.404028\n",
      "[INFO] Epoch: 4 , batch: 202 , training loss: 4.382631\n",
      "[INFO] Epoch: 4 , batch: 203 , training loss: 4.388147\n",
      "[INFO] Epoch: 4 , batch: 204 , training loss: 4.562171\n",
      "[INFO] Epoch: 4 , batch: 205 , training loss: 4.117524\n",
      "[INFO] Epoch: 4 , batch: 206 , training loss: 4.021794\n",
      "[INFO] Epoch: 4 , batch: 207 , training loss: 4.049333\n",
      "[INFO] Epoch: 4 , batch: 208 , training loss: 4.465878\n",
      "[INFO] Epoch: 4 , batch: 209 , training loss: 4.357914\n",
      "[INFO] Epoch: 4 , batch: 210 , training loss: 4.415663\n",
      "[INFO] Epoch: 4 , batch: 211 , training loss: 4.384837\n",
      "[INFO] Epoch: 4 , batch: 212 , training loss: 4.471973\n",
      "[INFO] Epoch: 4 , batch: 213 , training loss: 4.477897\n",
      "[INFO] Epoch: 4 , batch: 214 , training loss: 4.483778\n",
      "[INFO] Epoch: 4 , batch: 215 , training loss: 4.737163\n",
      "[INFO] Epoch: 4 , batch: 216 , training loss: 4.437135\n",
      "[INFO] Epoch: 4 , batch: 217 , training loss: 4.359915\n",
      "[INFO] Epoch: 4 , batch: 218 , training loss: 4.369219\n",
      "[INFO] Epoch: 4 , batch: 219 , training loss: 4.465528\n",
      "[INFO] Epoch: 4 , batch: 220 , training loss: 4.266377\n",
      "[INFO] Epoch: 4 , batch: 221 , training loss: 4.288120\n",
      "[INFO] Epoch: 4 , batch: 222 , training loss: 4.459890\n",
      "[INFO] Epoch: 4 , batch: 223 , training loss: 4.541536\n",
      "[INFO] Epoch: 4 , batch: 224 , training loss: 4.570746\n",
      "[INFO] Epoch: 4 , batch: 225 , training loss: 4.416887\n",
      "[INFO] Epoch: 4 , batch: 226 , training loss: 4.578879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 4 , batch: 227 , training loss: 4.540604\n",
      "[INFO] Epoch: 4 , batch: 228 , training loss: 4.568965\n",
      "[INFO] Epoch: 4 , batch: 229 , training loss: 4.454895\n",
      "[INFO] Epoch: 4 , batch: 230 , training loss: 4.279500\n",
      "[INFO] Epoch: 4 , batch: 231 , training loss: 4.106141\n",
      "[INFO] Epoch: 4 , batch: 232 , training loss: 4.270735\n",
      "[INFO] Epoch: 4 , batch: 233 , training loss: 4.321524\n",
      "[INFO] Epoch: 4 , batch: 234 , training loss: 3.956441\n",
      "[INFO] Epoch: 4 , batch: 235 , training loss: 4.106476\n",
      "[INFO] Epoch: 4 , batch: 236 , training loss: 4.298220\n",
      "[INFO] Epoch: 4 , batch: 237 , training loss: 4.453947\n",
      "[INFO] Epoch: 4 , batch: 238 , training loss: 4.171088\n",
      "[INFO] Epoch: 4 , batch: 239 , training loss: 4.249459\n",
      "[INFO] Epoch: 4 , batch: 240 , training loss: 4.323929\n",
      "[INFO] Epoch: 4 , batch: 241 , training loss: 4.087678\n",
      "[INFO] Epoch: 4 , batch: 242 , training loss: 4.097831\n",
      "[INFO] Epoch: 4 , batch: 243 , training loss: 4.450806\n",
      "[INFO] Epoch: 4 , batch: 244 , training loss: 4.363815\n",
      "[INFO] Epoch: 4 , batch: 245 , training loss: 4.367423\n",
      "[INFO] Epoch: 4 , batch: 246 , training loss: 4.005113\n",
      "[INFO] Epoch: 4 , batch: 247 , training loss: 4.169780\n",
      "[INFO] Epoch: 4 , batch: 248 , training loss: 4.292600\n",
      "[INFO] Epoch: 4 , batch: 249 , training loss: 4.249983\n",
      "[INFO] Epoch: 4 , batch: 250 , training loss: 4.021044\n",
      "[INFO] Epoch: 4 , batch: 251 , training loss: 4.542044\n",
      "[INFO] Epoch: 4 , batch: 252 , training loss: 4.193677\n",
      "[INFO] Epoch: 4 , batch: 253 , training loss: 4.152290\n",
      "[INFO] Epoch: 4 , batch: 254 , training loss: 4.472830\n",
      "[INFO] Epoch: 4 , batch: 255 , training loss: 4.436983\n",
      "[INFO] Epoch: 4 , batch: 256 , training loss: 4.386647\n",
      "[INFO] Epoch: 4 , batch: 257 , training loss: 4.596203\n",
      "[INFO] Epoch: 4 , batch: 258 , training loss: 4.638488\n",
      "[INFO] Epoch: 4 , batch: 259 , training loss: 4.673534\n",
      "[INFO] Epoch: 4 , batch: 260 , training loss: 4.351017\n",
      "[INFO] Epoch: 4 , batch: 261 , training loss: 4.556978\n",
      "[INFO] Epoch: 4 , batch: 262 , training loss: 4.776974\n",
      "[INFO] Epoch: 4 , batch: 263 , training loss: 4.893967\n",
      "[INFO] Epoch: 4 , batch: 264 , training loss: 4.188212\n",
      "[INFO] Epoch: 4 , batch: 265 , training loss: 4.326663\n",
      "[INFO] Epoch: 4 , batch: 266 , training loss: 4.861648\n",
      "[INFO] Epoch: 4 , batch: 267 , training loss: 4.510786\n",
      "[INFO] Epoch: 4 , batch: 268 , training loss: 4.402884\n",
      "[INFO] Epoch: 4 , batch: 269 , training loss: 4.451562\n",
      "[INFO] Epoch: 4 , batch: 270 , training loss: 4.418928\n",
      "[INFO] Epoch: 4 , batch: 271 , training loss: 4.463828\n",
      "[INFO] Epoch: 4 , batch: 272 , training loss: 4.432982\n",
      "[INFO] Epoch: 4 , batch: 273 , training loss: 4.439863\n",
      "[INFO] Epoch: 4 , batch: 274 , training loss: 4.579010\n",
      "[INFO] Epoch: 4 , batch: 275 , training loss: 4.445181\n",
      "[INFO] Epoch: 4 , batch: 276 , training loss: 4.495140\n",
      "[INFO] Epoch: 4 , batch: 277 , training loss: 4.645410\n",
      "[INFO] Epoch: 4 , batch: 278 , training loss: 4.211395\n",
      "[INFO] Epoch: 4 , batch: 279 , training loss: 4.248599\n",
      "[INFO] Epoch: 4 , batch: 280 , training loss: 4.195031\n",
      "[INFO] Epoch: 4 , batch: 281 , training loss: 4.363528\n",
      "[INFO] Epoch: 4 , batch: 282 , training loss: 4.254578\n",
      "[INFO] Epoch: 4 , batch: 283 , training loss: 4.334969\n",
      "[INFO] Epoch: 4 , batch: 284 , training loss: 4.353883\n",
      "[INFO] Epoch: 4 , batch: 285 , training loss: 4.368670\n",
      "[INFO] Epoch: 4 , batch: 286 , training loss: 4.330795\n",
      "[INFO] Epoch: 4 , batch: 287 , training loss: 4.163821\n",
      "[INFO] Epoch: 4 , batch: 288 , training loss: 4.224374\n",
      "[INFO] Epoch: 4 , batch: 289 , training loss: 4.274286\n",
      "[INFO] Epoch: 4 , batch: 290 , training loss: 4.051872\n",
      "[INFO] Epoch: 4 , batch: 291 , training loss: 4.032022\n",
      "[INFO] Epoch: 4 , batch: 292 , training loss: 4.122739\n",
      "[INFO] Epoch: 4 , batch: 293 , training loss: 4.086251\n",
      "[INFO] Epoch: 4 , batch: 294 , training loss: 4.781084\n",
      "[INFO] Epoch: 4 , batch: 295 , training loss: 4.527108\n",
      "[INFO] Epoch: 4 , batch: 296 , training loss: 4.467919\n",
      "[INFO] Epoch: 4 , batch: 297 , training loss: 4.373259\n",
      "[INFO] Epoch: 4 , batch: 298 , training loss: 4.240485\n",
      "[INFO] Epoch: 4 , batch: 299 , training loss: 4.226506\n",
      "[INFO] Epoch: 4 , batch: 300 , training loss: 4.206454\n",
      "[INFO] Epoch: 4 , batch: 301 , training loss: 4.139336\n",
      "[INFO] Epoch: 4 , batch: 302 , training loss: 4.336617\n",
      "[INFO] Epoch: 4 , batch: 303 , training loss: 4.348303\n",
      "[INFO] Epoch: 4 , batch: 304 , training loss: 4.536746\n",
      "[INFO] Epoch: 4 , batch: 305 , training loss: 4.258972\n",
      "[INFO] Epoch: 4 , batch: 306 , training loss: 4.404253\n",
      "[INFO] Epoch: 4 , batch: 307 , training loss: 4.391604\n",
      "[INFO] Epoch: 4 , batch: 308 , training loss: 4.287394\n",
      "[INFO] Epoch: 4 , batch: 309 , training loss: 4.294162\n",
      "[INFO] Epoch: 4 , batch: 310 , training loss: 4.097604\n",
      "[INFO] Epoch: 4 , batch: 311 , training loss: 4.157450\n",
      "[INFO] Epoch: 4 , batch: 312 , training loss: 4.021700\n",
      "[INFO] Epoch: 4 , batch: 313 , training loss: 4.197868\n",
      "[INFO] Epoch: 4 , batch: 314 , training loss: 4.252934\n",
      "[INFO] Epoch: 4 , batch: 315 , training loss: 4.290271\n",
      "[INFO] Epoch: 4 , batch: 316 , training loss: 4.641681\n",
      "[INFO] Epoch: 4 , batch: 317 , training loss: 5.189250\n",
      "[INFO] Epoch: 4 , batch: 318 , training loss: 5.289039\n",
      "[INFO] Epoch: 4 , batch: 319 , training loss: 4.836960\n",
      "[INFO] Epoch: 4 , batch: 320 , training loss: 4.307446\n",
      "[INFO] Epoch: 4 , batch: 321 , training loss: 4.107493\n",
      "[INFO] Epoch: 4 , batch: 322 , training loss: 4.233947\n",
      "[INFO] Epoch: 4 , batch: 323 , training loss: 4.227581\n",
      "[INFO] Epoch: 4 , batch: 324 , training loss: 4.229820\n",
      "[INFO] Epoch: 4 , batch: 325 , training loss: 4.399083\n",
      "[INFO] Epoch: 4 , batch: 326 , training loss: 4.436124\n",
      "[INFO] Epoch: 4 , batch: 327 , training loss: 4.336438\n",
      "[INFO] Epoch: 4 , batch: 328 , training loss: 4.337344\n",
      "[INFO] Epoch: 4 , batch: 329 , training loss: 4.219327\n",
      "[INFO] Epoch: 4 , batch: 330 , training loss: 4.212402\n",
      "[INFO] Epoch: 4 , batch: 331 , training loss: 4.411226\n",
      "[INFO] Epoch: 4 , batch: 332 , training loss: 4.160897\n",
      "[INFO] Epoch: 4 , batch: 333 , training loss: 4.143476\n",
      "[INFO] Epoch: 4 , batch: 334 , training loss: 4.269252\n",
      "[INFO] Epoch: 4 , batch: 335 , training loss: 4.372540\n",
      "[INFO] Epoch: 4 , batch: 336 , training loss: 4.356215\n",
      "[INFO] Epoch: 4 , batch: 337 , training loss: 4.428577\n",
      "[INFO] Epoch: 4 , batch: 338 , training loss: 4.601423\n",
      "[INFO] Epoch: 4 , batch: 339 , training loss: 4.435604\n",
      "[INFO] Epoch: 4 , batch: 340 , training loss: 4.641062\n",
      "[INFO] Epoch: 4 , batch: 341 , training loss: 4.353933\n",
      "[INFO] Epoch: 4 , batch: 342 , training loss: 4.183059\n",
      "[INFO] Epoch: 4 , batch: 343 , training loss: 4.262704\n",
      "[INFO] Epoch: 4 , batch: 344 , training loss: 4.098366\n",
      "[INFO] Epoch: 4 , batch: 345 , training loss: 4.232183\n",
      "[INFO] Epoch: 4 , batch: 346 , training loss: 4.290872\n",
      "[INFO] Epoch: 4 , batch: 347 , training loss: 4.194541\n",
      "[INFO] Epoch: 4 , batch: 348 , training loss: 4.421012\n",
      "[INFO] Epoch: 4 , batch: 349 , training loss: 4.473711\n",
      "[INFO] Epoch: 4 , batch: 350 , training loss: 4.219989\n",
      "[INFO] Epoch: 4 , batch: 351 , training loss: 4.297573\n",
      "[INFO] Epoch: 4 , batch: 352 , training loss: 4.318538\n",
      "[INFO] Epoch: 4 , batch: 353 , training loss: 4.265020\n",
      "[INFO] Epoch: 4 , batch: 354 , training loss: 4.378050\n",
      "[INFO] Epoch: 4 , batch: 355 , training loss: 4.416490\n",
      "[INFO] Epoch: 4 , batch: 356 , training loss: 4.289625\n",
      "[INFO] Epoch: 4 , batch: 357 , training loss: 4.354937\n",
      "[INFO] Epoch: 4 , batch: 358 , training loss: 4.296260\n",
      "[INFO] Epoch: 4 , batch: 359 , training loss: 4.289756\n",
      "[INFO] Epoch: 4 , batch: 360 , training loss: 4.314559\n",
      "[INFO] Epoch: 4 , batch: 361 , training loss: 4.287718\n",
      "[INFO] Epoch: 4 , batch: 362 , training loss: 4.382457\n",
      "[INFO] Epoch: 4 , batch: 363 , training loss: 4.291849\n",
      "[INFO] Epoch: 4 , batch: 364 , training loss: 4.317048\n",
      "[INFO] Epoch: 4 , batch: 365 , training loss: 4.216835\n",
      "[INFO] Epoch: 4 , batch: 366 , training loss: 4.381828\n",
      "[INFO] Epoch: 4 , batch: 367 , training loss: 4.439778\n",
      "[INFO] Epoch: 4 , batch: 368 , training loss: 5.006913\n",
      "[INFO] Epoch: 4 , batch: 369 , training loss: 4.617215\n",
      "[INFO] Epoch: 4 , batch: 370 , training loss: 4.340010\n",
      "[INFO] Epoch: 4 , batch: 371 , training loss: 4.897159\n",
      "[INFO] Epoch: 4 , batch: 372 , training loss: 5.183051\n",
      "[INFO] Epoch: 4 , batch: 373 , training loss: 5.168736\n",
      "[INFO] Epoch: 4 , batch: 374 , training loss: 5.248893\n",
      "[INFO] Epoch: 4 , batch: 375 , training loss: 5.189899\n",
      "[INFO] Epoch: 4 , batch: 376 , training loss: 5.115783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 4 , batch: 377 , training loss: 4.794878\n",
      "[INFO] Epoch: 4 , batch: 378 , training loss: 4.911782\n",
      "[INFO] Epoch: 4 , batch: 379 , training loss: 4.948394\n",
      "[INFO] Epoch: 4 , batch: 380 , training loss: 5.033923\n",
      "[INFO] Epoch: 4 , batch: 381 , training loss: 4.805496\n",
      "[INFO] Epoch: 4 , batch: 382 , training loss: 5.014919\n",
      "[INFO] Epoch: 4 , batch: 383 , training loss: 5.131516\n",
      "[INFO] Epoch: 4 , batch: 384 , training loss: 5.247823\n",
      "[INFO] Epoch: 4 , batch: 385 , training loss: 5.022855\n",
      "[INFO] Epoch: 4 , batch: 386 , training loss: 5.142317\n",
      "[INFO] Epoch: 4 , batch: 387 , training loss: 5.084178\n",
      "[INFO] Epoch: 4 , batch: 388 , training loss: 4.832124\n",
      "[INFO] Epoch: 4 , batch: 389 , training loss: 4.628446\n",
      "[INFO] Epoch: 4 , batch: 390 , training loss: 4.604979\n",
      "[INFO] Epoch: 4 , batch: 391 , training loss: 4.633791\n",
      "[INFO] Epoch: 4 , batch: 392 , training loss: 4.965498\n",
      "[INFO] Epoch: 4 , batch: 393 , training loss: 4.867406\n",
      "[INFO] Epoch: 4 , batch: 394 , training loss: 4.948637\n",
      "[INFO] Epoch: 4 , batch: 395 , training loss: 4.720419\n",
      "[INFO] Epoch: 4 , batch: 396 , training loss: 4.527426\n",
      "[INFO] Epoch: 4 , batch: 397 , training loss: 4.699216\n",
      "[INFO] Epoch: 4 , batch: 398 , training loss: 4.557528\n",
      "[INFO] Epoch: 4 , batch: 399 , training loss: 4.611018\n",
      "[INFO] Epoch: 4 , batch: 400 , training loss: 4.546617\n",
      "[INFO] Epoch: 4 , batch: 401 , training loss: 4.981053\n",
      "[INFO] Epoch: 4 , batch: 402 , training loss: 4.701808\n",
      "[INFO] Epoch: 4 , batch: 403 , training loss: 4.565619\n",
      "[INFO] Epoch: 4 , batch: 404 , training loss: 4.704151\n",
      "[INFO] Epoch: 4 , batch: 405 , training loss: 4.811071\n",
      "[INFO] Epoch: 4 , batch: 406 , training loss: 4.656667\n",
      "[INFO] Epoch: 4 , batch: 407 , training loss: 4.731827\n",
      "[INFO] Epoch: 4 , batch: 408 , training loss: 4.650885\n",
      "[INFO] Epoch: 4 , batch: 409 , training loss: 4.671624\n",
      "[INFO] Epoch: 4 , batch: 410 , training loss: 4.709232\n",
      "[INFO] Epoch: 4 , batch: 411 , training loss: 4.906481\n",
      "[INFO] Epoch: 4 , batch: 412 , training loss: 4.743344\n",
      "[INFO] Epoch: 4 , batch: 413 , training loss: 4.588356\n",
      "[INFO] Epoch: 4 , batch: 414 , training loss: 4.640626\n",
      "[INFO] Epoch: 4 , batch: 415 , training loss: 4.671865\n",
      "[INFO] Epoch: 4 , batch: 416 , training loss: 4.741686\n",
      "[INFO] Epoch: 4 , batch: 417 , training loss: 4.646402\n",
      "[INFO] Epoch: 4 , batch: 418 , training loss: 4.692800\n",
      "[INFO] Epoch: 4 , batch: 419 , training loss: 4.634553\n",
      "[INFO] Epoch: 4 , batch: 420 , training loss: 4.601441\n",
      "[INFO] Epoch: 4 , batch: 421 , training loss: 4.608524\n",
      "[INFO] Epoch: 4 , batch: 422 , training loss: 4.486689\n",
      "[INFO] Epoch: 4 , batch: 423 , training loss: 4.744694\n",
      "[INFO] Epoch: 4 , batch: 424 , training loss: 4.863835\n",
      "[INFO] Epoch: 4 , batch: 425 , training loss: 4.778188\n",
      "[INFO] Epoch: 4 , batch: 426 , training loss: 4.409770\n",
      "[INFO] Epoch: 4 , batch: 427 , training loss: 4.715430\n",
      "[INFO] Epoch: 4 , batch: 428 , training loss: 4.610871\n",
      "[INFO] Epoch: 4 , batch: 429 , training loss: 4.425038\n",
      "[INFO] Epoch: 4 , batch: 430 , training loss: 4.732318\n",
      "[INFO] Epoch: 4 , batch: 431 , training loss: 4.273366\n",
      "[INFO] Epoch: 4 , batch: 432 , training loss: 4.360135\n",
      "[INFO] Epoch: 4 , batch: 433 , training loss: 4.351422\n",
      "[INFO] Epoch: 4 , batch: 434 , training loss: 4.253981\n",
      "[INFO] Epoch: 4 , batch: 435 , training loss: 4.612465\n",
      "[INFO] Epoch: 4 , batch: 436 , training loss: 4.707378\n",
      "[INFO] Epoch: 4 , batch: 437 , training loss: 4.504774\n",
      "[INFO] Epoch: 4 , batch: 438 , training loss: 4.269378\n",
      "[INFO] Epoch: 4 , batch: 439 , training loss: 4.513945\n",
      "[INFO] Epoch: 4 , batch: 440 , training loss: 4.693656\n",
      "[INFO] Epoch: 4 , batch: 441 , training loss: 4.705922\n",
      "[INFO] Epoch: 4 , batch: 442 , training loss: 4.503234\n",
      "[INFO] Epoch: 4 , batch: 443 , training loss: 4.743641\n",
      "[INFO] Epoch: 4 , batch: 444 , training loss: 4.329557\n",
      "[INFO] Epoch: 4 , batch: 445 , training loss: 4.189628\n",
      "[INFO] Epoch: 4 , batch: 446 , training loss: 4.097171\n",
      "[INFO] Epoch: 4 , batch: 447 , training loss: 4.348990\n",
      "[INFO] Epoch: 4 , batch: 448 , training loss: 4.508160\n",
      "[INFO] Epoch: 4 , batch: 449 , training loss: 4.929463\n",
      "[INFO] Epoch: 4 , batch: 450 , training loss: 4.998650\n",
      "[INFO] Epoch: 4 , batch: 451 , training loss: 4.869237\n",
      "[INFO] Epoch: 4 , batch: 452 , training loss: 4.606520\n",
      "[INFO] Epoch: 4 , batch: 453 , training loss: 4.385780\n",
      "[INFO] Epoch: 4 , batch: 454 , training loss: 4.542271\n",
      "[INFO] Epoch: 4 , batch: 455 , training loss: 4.577241\n",
      "[INFO] Epoch: 4 , batch: 456 , training loss: 4.556445\n",
      "[INFO] Epoch: 4 , batch: 457 , training loss: 4.665750\n",
      "[INFO] Epoch: 4 , batch: 458 , training loss: 4.383043\n",
      "[INFO] Epoch: 4 , batch: 459 , training loss: 4.362863\n",
      "[INFO] Epoch: 4 , batch: 460 , training loss: 4.498242\n",
      "[INFO] Epoch: 4 , batch: 461 , training loss: 4.504377\n",
      "[INFO] Epoch: 4 , batch: 462 , training loss: 4.543068\n",
      "[INFO] Epoch: 4 , batch: 463 , training loss: 4.432566\n",
      "[INFO] Epoch: 4 , batch: 464 , training loss: 4.597060\n",
      "[INFO] Epoch: 4 , batch: 465 , training loss: 4.534047\n",
      "[INFO] Epoch: 4 , batch: 466 , training loss: 4.627019\n",
      "[INFO] Epoch: 4 , batch: 467 , training loss: 4.683467\n",
      "[INFO] Epoch: 4 , batch: 468 , training loss: 4.616489\n",
      "[INFO] Epoch: 4 , batch: 469 , training loss: 4.615907\n",
      "[INFO] Epoch: 4 , batch: 470 , training loss: 4.368557\n",
      "[INFO] Epoch: 4 , batch: 471 , training loss: 4.507898\n",
      "[INFO] Epoch: 4 , batch: 472 , training loss: 4.577685\n",
      "[INFO] Epoch: 4 , batch: 473 , training loss: 4.479907\n",
      "[INFO] Epoch: 4 , batch: 474 , training loss: 4.299763\n",
      "[INFO] Epoch: 4 , batch: 475 , training loss: 4.143547\n",
      "[INFO] Epoch: 4 , batch: 476 , training loss: 4.539862\n",
      "[INFO] Epoch: 4 , batch: 477 , training loss: 4.647719\n",
      "[INFO] Epoch: 4 , batch: 478 , training loss: 4.726523\n",
      "[INFO] Epoch: 4 , batch: 479 , training loss: 4.671931\n",
      "[INFO] Epoch: 4 , batch: 480 , training loss: 4.806393\n",
      "[INFO] Epoch: 4 , batch: 481 , training loss: 4.633895\n",
      "[INFO] Epoch: 4 , batch: 482 , training loss: 4.773025\n",
      "[INFO] Epoch: 4 , batch: 483 , training loss: 4.588024\n",
      "[INFO] Epoch: 4 , batch: 484 , training loss: 4.393529\n",
      "[INFO] Epoch: 4 , batch: 485 , training loss: 4.560530\n",
      "[INFO] Epoch: 4 , batch: 486 , training loss: 4.393982\n",
      "[INFO] Epoch: 4 , batch: 487 , training loss: 4.407598\n",
      "[INFO] Epoch: 4 , batch: 488 , training loss: 4.586026\n",
      "[INFO] Epoch: 4 , batch: 489 , training loss: 4.471574\n",
      "[INFO] Epoch: 4 , batch: 490 , training loss: 4.548448\n",
      "[INFO] Epoch: 4 , batch: 491 , training loss: 4.517919\n",
      "[INFO] Epoch: 4 , batch: 492 , training loss: 4.408607\n",
      "[INFO] Epoch: 4 , batch: 493 , training loss: 4.599031\n",
      "[INFO] Epoch: 4 , batch: 494 , training loss: 4.504381\n",
      "[INFO] Epoch: 4 , batch: 495 , training loss: 4.638832\n",
      "[INFO] Epoch: 4 , batch: 496 , training loss: 4.527481\n",
      "[INFO] Epoch: 4 , batch: 497 , training loss: 4.534781\n",
      "[INFO] Epoch: 4 , batch: 498 , training loss: 4.587703\n",
      "[INFO] Epoch: 4 , batch: 499 , training loss: 4.634132\n",
      "[INFO] Epoch: 4 , batch: 500 , training loss: 4.878277\n",
      "[INFO] Epoch: 4 , batch: 501 , training loss: 5.259032\n",
      "[INFO] Epoch: 4 , batch: 502 , training loss: 5.405100\n",
      "[INFO] Epoch: 4 , batch: 503 , training loss: 5.155195\n",
      "[INFO] Epoch: 4 , batch: 504 , training loss: 5.246829\n",
      "[INFO] Epoch: 4 , batch: 505 , training loss: 5.136366\n",
      "[INFO] Epoch: 4 , batch: 506 , training loss: 5.031375\n",
      "[INFO] Epoch: 4 , batch: 507 , training loss: 5.063409\n",
      "[INFO] Epoch: 4 , batch: 508 , training loss: 4.995286\n",
      "[INFO] Epoch: 4 , batch: 509 , training loss: 4.740616\n",
      "[INFO] Epoch: 4 , batch: 510 , training loss: 4.812819\n",
      "[INFO] Epoch: 4 , batch: 511 , training loss: 4.715058\n",
      "[INFO] Epoch: 4 , batch: 512 , training loss: 4.790685\n",
      "[INFO] Epoch: 4 , batch: 513 , training loss: 5.053642\n",
      "[INFO] Epoch: 4 , batch: 514 , training loss: 4.704859\n",
      "[INFO] Epoch: 4 , batch: 515 , training loss: 4.958660\n",
      "[INFO] Epoch: 4 , batch: 516 , training loss: 4.740674\n",
      "[INFO] Epoch: 4 , batch: 517 , training loss: 4.766789\n",
      "[INFO] Epoch: 4 , batch: 518 , training loss: 4.711194\n",
      "[INFO] Epoch: 4 , batch: 519 , training loss: 4.592759\n",
      "[INFO] Epoch: 4 , batch: 520 , training loss: 4.804172\n",
      "[INFO] Epoch: 4 , batch: 521 , training loss: 4.817876\n",
      "[INFO] Epoch: 4 , batch: 522 , training loss: 4.890911\n",
      "[INFO] Epoch: 4 , batch: 523 , training loss: 4.752428\n",
      "[INFO] Epoch: 4 , batch: 524 , training loss: 5.021680\n",
      "[INFO] Epoch: 4 , batch: 525 , training loss: 4.950017\n",
      "[INFO] Epoch: 4 , batch: 526 , training loss: 4.683317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 4 , batch: 527 , training loss: 4.713213\n",
      "[INFO] Epoch: 4 , batch: 528 , training loss: 4.793269\n",
      "[INFO] Epoch: 4 , batch: 529 , training loss: 4.738179\n",
      "[INFO] Epoch: 4 , batch: 530 , training loss: 4.573019\n",
      "[INFO] Epoch: 4 , batch: 531 , training loss: 4.745100\n",
      "[INFO] Epoch: 4 , batch: 532 , training loss: 4.579354\n",
      "[INFO] Epoch: 4 , batch: 533 , training loss: 4.750011\n",
      "[INFO] Epoch: 4 , batch: 534 , training loss: 4.747084\n",
      "[INFO] Epoch: 4 , batch: 535 , training loss: 4.752322\n",
      "[INFO] Epoch: 4 , batch: 536 , training loss: 4.641160\n",
      "[INFO] Epoch: 4 , batch: 537 , training loss: 4.587095\n",
      "[INFO] Epoch: 4 , batch: 538 , training loss: 4.669525\n",
      "[INFO] Epoch: 4 , batch: 539 , training loss: 4.838786\n",
      "[INFO] Epoch: 4 , batch: 540 , training loss: 5.480739\n",
      "[INFO] Epoch: 4 , batch: 541 , training loss: 5.433294\n",
      "[INFO] Epoch: 4 , batch: 542 , training loss: 5.202365\n",
      "[INFO] Epoch: 5 , batch: 0 , training loss: 4.707055\n",
      "[INFO] Epoch: 5 , batch: 1 , training loss: 4.579248\n",
      "[INFO] Epoch: 5 , batch: 2 , training loss: 4.410646\n",
      "[INFO] Epoch: 5 , batch: 3 , training loss: 4.270956\n",
      "[INFO] Epoch: 5 , batch: 4 , training loss: 4.605016\n",
      "[INFO] Epoch: 5 , batch: 5 , training loss: 4.266565\n",
      "[INFO] Epoch: 5 , batch: 6 , training loss: 4.893374\n",
      "[INFO] Epoch: 5 , batch: 7 , training loss: 4.470065\n",
      "[INFO] Epoch: 5 , batch: 8 , training loss: 4.165505\n",
      "[INFO] Epoch: 5 , batch: 9 , training loss: 4.340828\n",
      "[INFO] Epoch: 5 , batch: 10 , training loss: 4.221011\n",
      "[INFO] Epoch: 5 , batch: 11 , training loss: 4.125659\n",
      "[INFO] Epoch: 5 , batch: 12 , training loss: 4.152890\n",
      "[INFO] Epoch: 5 , batch: 13 , training loss: 4.085927\n",
      "[INFO] Epoch: 5 , batch: 14 , training loss: 3.891485\n",
      "[INFO] Epoch: 5 , batch: 15 , training loss: 4.145543\n",
      "[INFO] Epoch: 5 , batch: 16 , training loss: 4.042780\n",
      "[INFO] Epoch: 5 , batch: 17 , training loss: 4.179991\n",
      "[INFO] Epoch: 5 , batch: 18 , training loss: 4.137828\n",
      "[INFO] Epoch: 5 , batch: 19 , training loss: 3.870368\n",
      "[INFO] Epoch: 5 , batch: 20 , training loss: 3.838382\n",
      "[INFO] Epoch: 5 , batch: 21 , training loss: 3.960432\n",
      "[INFO] Epoch: 5 , batch: 22 , training loss: 4.050785\n",
      "[INFO] Epoch: 5 , batch: 23 , training loss: 4.162570\n",
      "[INFO] Epoch: 5 , batch: 24 , training loss: 4.038215\n",
      "[INFO] Epoch: 5 , batch: 25 , training loss: 4.146502\n",
      "[INFO] Epoch: 5 , batch: 26 , training loss: 3.989515\n",
      "[INFO] Epoch: 5 , batch: 27 , training loss: 3.982043\n",
      "[INFO] Epoch: 5 , batch: 28 , training loss: 4.211090\n",
      "[INFO] Epoch: 5 , batch: 29 , training loss: 3.943241\n",
      "[INFO] Epoch: 5 , batch: 30 , training loss: 3.944948\n",
      "[INFO] Epoch: 5 , batch: 31 , training loss: 4.051618\n",
      "[INFO] Epoch: 5 , batch: 32 , training loss: 4.033058\n",
      "[INFO] Epoch: 5 , batch: 33 , training loss: 4.144286\n",
      "[INFO] Epoch: 5 , batch: 34 , training loss: 4.161059\n",
      "[INFO] Epoch: 5 , batch: 35 , training loss: 4.057189\n",
      "[INFO] Epoch: 5 , batch: 36 , training loss: 4.046592\n",
      "[INFO] Epoch: 5 , batch: 37 , training loss: 3.957377\n",
      "[INFO] Epoch: 5 , batch: 38 , training loss: 4.097284\n",
      "[INFO] Epoch: 5 , batch: 39 , training loss: 3.911716\n",
      "[INFO] Epoch: 5 , batch: 40 , training loss: 4.035256\n",
      "[INFO] Epoch: 5 , batch: 41 , training loss: 4.237212\n",
      "[INFO] Epoch: 5 , batch: 42 , training loss: 4.862654\n",
      "[INFO] Epoch: 5 , batch: 43 , training loss: 4.573634\n",
      "[INFO] Epoch: 5 , batch: 44 , training loss: 4.650701\n",
      "[INFO] Epoch: 5 , batch: 45 , training loss: 4.822285\n",
      "[INFO] Epoch: 5 , batch: 46 , training loss: 5.101663\n",
      "[INFO] Epoch: 5 , batch: 47 , training loss: 4.410124\n",
      "[INFO] Epoch: 5 , batch: 48 , training loss: 4.523024\n",
      "[INFO] Epoch: 5 , batch: 49 , training loss: 4.514523\n",
      "[INFO] Epoch: 5 , batch: 50 , training loss: 4.284122\n",
      "[INFO] Epoch: 5 , batch: 51 , training loss: 4.324845\n",
      "[INFO] Epoch: 5 , batch: 52 , training loss: 4.134585\n",
      "[INFO] Epoch: 5 , batch: 53 , training loss: 4.310615\n",
      "[INFO] Epoch: 5 , batch: 54 , training loss: 4.270634\n",
      "[INFO] Epoch: 5 , batch: 55 , training loss: 4.364086\n",
      "[INFO] Epoch: 5 , batch: 56 , training loss: 4.180129\n",
      "[INFO] Epoch: 5 , batch: 57 , training loss: 4.067358\n",
      "[INFO] Epoch: 5 , batch: 58 , training loss: 4.101744\n",
      "[INFO] Epoch: 5 , batch: 59 , training loss: 4.208536\n",
      "[INFO] Epoch: 5 , batch: 60 , training loss: 4.016220\n",
      "[INFO] Epoch: 5 , batch: 61 , training loss: 4.135624\n",
      "[INFO] Epoch: 5 , batch: 62 , training loss: 4.012424\n",
      "[INFO] Epoch: 5 , batch: 63 , training loss: 4.204368\n",
      "[INFO] Epoch: 5 , batch: 64 , training loss: 4.372915\n",
      "[INFO] Epoch: 5 , batch: 65 , training loss: 4.072744\n",
      "[INFO] Epoch: 5 , batch: 66 , training loss: 3.907331\n",
      "[INFO] Epoch: 5 , batch: 67 , training loss: 3.942515\n",
      "[INFO] Epoch: 5 , batch: 68 , training loss: 4.260120\n",
      "[INFO] Epoch: 5 , batch: 69 , training loss: 4.030753\n",
      "[INFO] Epoch: 5 , batch: 70 , training loss: 4.326852\n",
      "[INFO] Epoch: 5 , batch: 71 , training loss: 4.123531\n",
      "[INFO] Epoch: 5 , batch: 72 , training loss: 4.222573\n",
      "[INFO] Epoch: 5 , batch: 73 , training loss: 4.125243\n",
      "[INFO] Epoch: 5 , batch: 74 , training loss: 4.284655\n",
      "[INFO] Epoch: 5 , batch: 75 , training loss: 4.015777\n",
      "[INFO] Epoch: 5 , batch: 76 , training loss: 4.187855\n",
      "[INFO] Epoch: 5 , batch: 77 , training loss: 4.067811\n",
      "[INFO] Epoch: 5 , batch: 78 , training loss: 4.207157\n",
      "[INFO] Epoch: 5 , batch: 79 , training loss: 4.007988\n",
      "[INFO] Epoch: 5 , batch: 80 , training loss: 4.213839\n",
      "[INFO] Epoch: 5 , batch: 81 , training loss: 4.192251\n",
      "[INFO] Epoch: 5 , batch: 82 , training loss: 4.172704\n",
      "[INFO] Epoch: 5 , batch: 83 , training loss: 4.273585\n",
      "[INFO] Epoch: 5 , batch: 84 , training loss: 4.231205\n",
      "[INFO] Epoch: 5 , batch: 85 , training loss: 4.284880\n",
      "[INFO] Epoch: 5 , batch: 86 , training loss: 4.272114\n",
      "[INFO] Epoch: 5 , batch: 87 , training loss: 4.235024\n",
      "[INFO] Epoch: 5 , batch: 88 , training loss: 4.384943\n",
      "[INFO] Epoch: 5 , batch: 89 , training loss: 4.138149\n",
      "[INFO] Epoch: 5 , batch: 90 , training loss: 4.216021\n",
      "[INFO] Epoch: 5 , batch: 91 , training loss: 4.151673\n",
      "[INFO] Epoch: 5 , batch: 92 , training loss: 4.129258\n",
      "[INFO] Epoch: 5 , batch: 93 , training loss: 4.230065\n",
      "[INFO] Epoch: 5 , batch: 94 , training loss: 4.437610\n",
      "[INFO] Epoch: 5 , batch: 95 , training loss: 4.200460\n",
      "[INFO] Epoch: 5 , batch: 96 , training loss: 4.153930\n",
      "[INFO] Epoch: 5 , batch: 97 , training loss: 4.145270\n",
      "[INFO] Epoch: 5 , batch: 98 , training loss: 4.122495\n",
      "[INFO] Epoch: 5 , batch: 99 , training loss: 4.193185\n",
      "[INFO] Epoch: 5 , batch: 100 , training loss: 4.021668\n",
      "[INFO] Epoch: 5 , batch: 101 , training loss: 4.035234\n",
      "[INFO] Epoch: 5 , batch: 102 , training loss: 4.276974\n",
      "[INFO] Epoch: 5 , batch: 103 , training loss: 3.980222\n",
      "[INFO] Epoch: 5 , batch: 104 , training loss: 3.952568\n",
      "[INFO] Epoch: 5 , batch: 105 , training loss: 4.256650\n",
      "[INFO] Epoch: 5 , batch: 106 , training loss: 4.251390\n",
      "[INFO] Epoch: 5 , batch: 107 , training loss: 4.117554\n",
      "[INFO] Epoch: 5 , batch: 108 , training loss: 3.991596\n",
      "[INFO] Epoch: 5 , batch: 109 , training loss: 3.884554\n",
      "[INFO] Epoch: 5 , batch: 110 , training loss: 4.191889\n",
      "[INFO] Epoch: 5 , batch: 111 , training loss: 4.207812\n",
      "[INFO] Epoch: 5 , batch: 112 , training loss: 4.192168\n",
      "[INFO] Epoch: 5 , batch: 113 , training loss: 4.108492\n",
      "[INFO] Epoch: 5 , batch: 114 , training loss: 4.217045\n",
      "[INFO] Epoch: 5 , batch: 115 , training loss: 4.126928\n",
      "[INFO] Epoch: 5 , batch: 116 , training loss: 4.140556\n",
      "[INFO] Epoch: 5 , batch: 117 , training loss: 4.381339\n",
      "[INFO] Epoch: 5 , batch: 118 , training loss: 4.328945\n",
      "[INFO] Epoch: 5 , batch: 119 , training loss: 4.438487\n",
      "[INFO] Epoch: 5 , batch: 120 , training loss: 4.387533\n",
      "[INFO] Epoch: 5 , batch: 121 , training loss: 4.195176\n",
      "[INFO] Epoch: 5 , batch: 122 , training loss: 4.101353\n",
      "[INFO] Epoch: 5 , batch: 123 , training loss: 4.218392\n",
      "[INFO] Epoch: 5 , batch: 124 , training loss: 4.387284\n",
      "[INFO] Epoch: 5 , batch: 125 , training loss: 4.017400\n",
      "[INFO] Epoch: 5 , batch: 126 , training loss: 4.039124\n",
      "[INFO] Epoch: 5 , batch: 127 , training loss: 4.096394\n",
      "[INFO] Epoch: 5 , batch: 128 , training loss: 4.280097\n",
      "[INFO] Epoch: 5 , batch: 129 , training loss: 4.185596\n",
      "[INFO] Epoch: 5 , batch: 130 , training loss: 4.213515\n",
      "[INFO] Epoch: 5 , batch: 131 , training loss: 4.179490\n",
      "[INFO] Epoch: 5 , batch: 132 , training loss: 4.243838\n",
      "[INFO] Epoch: 5 , batch: 133 , training loss: 4.153361\n",
      "[INFO] Epoch: 5 , batch: 134 , training loss: 3.907403\n",
      "[INFO] Epoch: 5 , batch: 135 , training loss: 3.943207\n",
      "[INFO] Epoch: 5 , batch: 136 , training loss: 4.279802\n",
      "[INFO] Epoch: 5 , batch: 137 , training loss: 4.187803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 5 , batch: 138 , training loss: 4.320770\n",
      "[INFO] Epoch: 5 , batch: 139 , training loss: 4.977879\n",
      "[INFO] Epoch: 5 , batch: 140 , training loss: 4.880589\n",
      "[INFO] Epoch: 5 , batch: 141 , training loss: 4.552136\n",
      "[INFO] Epoch: 5 , batch: 142 , training loss: 4.146137\n",
      "[INFO] Epoch: 5 , batch: 143 , training loss: 4.267188\n",
      "[INFO] Epoch: 5 , batch: 144 , training loss: 4.108490\n",
      "[INFO] Epoch: 5 , batch: 145 , training loss: 4.238589\n",
      "[INFO] Epoch: 5 , batch: 146 , training loss: 4.445482\n",
      "[INFO] Epoch: 5 , batch: 147 , training loss: 4.030544\n",
      "[INFO] Epoch: 5 , batch: 148 , training loss: 4.031121\n",
      "[INFO] Epoch: 5 , batch: 149 , training loss: 4.117268\n",
      "[INFO] Epoch: 5 , batch: 150 , training loss: 4.402950\n",
      "[INFO] Epoch: 5 , batch: 151 , training loss: 4.127120\n",
      "[INFO] Epoch: 5 , batch: 152 , training loss: 4.130159\n",
      "[INFO] Epoch: 5 , batch: 153 , training loss: 4.277923\n",
      "[INFO] Epoch: 5 , batch: 154 , training loss: 4.304842\n",
      "[INFO] Epoch: 5 , batch: 155 , training loss: 4.545557\n",
      "[INFO] Epoch: 5 , batch: 156 , training loss: 4.241887\n",
      "[INFO] Epoch: 5 , batch: 157 , training loss: 4.201145\n",
      "[INFO] Epoch: 5 , batch: 158 , training loss: 4.588914\n",
      "[INFO] Epoch: 5 , batch: 159 , training loss: 4.471365\n",
      "[INFO] Epoch: 5 , batch: 160 , training loss: 4.907747\n",
      "[INFO] Epoch: 5 , batch: 161 , training loss: 4.785666\n",
      "[INFO] Epoch: 5 , batch: 162 , training loss: 4.661800\n",
      "[INFO] Epoch: 5 , batch: 163 , training loss: 4.834687\n",
      "[INFO] Epoch: 5 , batch: 164 , training loss: 4.690499\n",
      "[INFO] Epoch: 5 , batch: 165 , training loss: 4.641547\n",
      "[INFO] Epoch: 5 , batch: 166 , training loss: 4.824017\n",
      "[INFO] Epoch: 5 , batch: 167 , training loss: 5.338262\n",
      "[INFO] Epoch: 5 , batch: 168 , training loss: 5.055158\n",
      "[INFO] Epoch: 5 , batch: 169 , training loss: 4.850001\n",
      "[INFO] Epoch: 5 , batch: 170 , training loss: 4.877649\n",
      "[INFO] Epoch: 5 , batch: 171 , training loss: 4.451090\n",
      "[INFO] Epoch: 5 , batch: 172 , training loss: 4.606973\n",
      "[INFO] Epoch: 5 , batch: 173 , training loss: 4.851565\n",
      "[INFO] Epoch: 5 , batch: 174 , training loss: 5.179435\n",
      "[INFO] Epoch: 5 , batch: 175 , training loss: 5.269732\n",
      "[INFO] Epoch: 5 , batch: 176 , training loss: 5.078743\n",
      "[INFO] Epoch: 5 , batch: 177 , training loss: 4.658087\n",
      "[INFO] Epoch: 5 , batch: 178 , training loss: 4.582602\n",
      "[INFO] Epoch: 5 , batch: 179 , training loss: 4.643637\n",
      "[INFO] Epoch: 5 , batch: 180 , training loss: 4.551489\n",
      "[INFO] Epoch: 5 , batch: 181 , training loss: 4.794141\n",
      "[INFO] Epoch: 5 , batch: 182 , training loss: 4.685044\n",
      "[INFO] Epoch: 5 , batch: 183 , training loss: 4.647778\n",
      "[INFO] Epoch: 5 , batch: 184 , training loss: 4.539218\n",
      "[INFO] Epoch: 5 , batch: 185 , training loss: 4.517436\n",
      "[INFO] Epoch: 5 , batch: 186 , training loss: 4.644332\n",
      "[INFO] Epoch: 5 , batch: 187 , training loss: 4.774249\n",
      "[INFO] Epoch: 5 , batch: 188 , training loss: 4.733679\n",
      "[INFO] Epoch: 5 , batch: 189 , training loss: 4.625129\n",
      "[INFO] Epoch: 5 , batch: 190 , training loss: 4.600958\n",
      "[INFO] Epoch: 5 , batch: 191 , training loss: 4.745730\n",
      "[INFO] Epoch: 5 , batch: 192 , training loss: 4.516326\n",
      "[INFO] Epoch: 5 , batch: 193 , training loss: 4.644506\n",
      "[INFO] Epoch: 5 , batch: 194 , training loss: 4.539326\n",
      "[INFO] Epoch: 5 , batch: 195 , training loss: 4.642011\n",
      "[INFO] Epoch: 5 , batch: 196 , training loss: 4.398414\n",
      "[INFO] Epoch: 5 , batch: 197 , training loss: 4.573725\n",
      "[INFO] Epoch: 5 , batch: 198 , training loss: 4.380916\n",
      "[INFO] Epoch: 5 , batch: 199 , training loss: 4.506720\n",
      "[INFO] Epoch: 5 , batch: 200 , training loss: 4.422632\n",
      "[INFO] Epoch: 5 , batch: 201 , training loss: 4.337170\n",
      "[INFO] Epoch: 5 , batch: 202 , training loss: 4.313972\n",
      "[INFO] Epoch: 5 , batch: 203 , training loss: 4.328713\n",
      "[INFO] Epoch: 5 , batch: 204 , training loss: 4.481344\n",
      "[INFO] Epoch: 5 , batch: 205 , training loss: 4.060437\n",
      "[INFO] Epoch: 5 , batch: 206 , training loss: 3.972767\n",
      "[INFO] Epoch: 5 , batch: 207 , training loss: 3.990407\n",
      "[INFO] Epoch: 5 , batch: 208 , training loss: 4.367499\n",
      "[INFO] Epoch: 5 , batch: 209 , training loss: 4.287666\n",
      "[INFO] Epoch: 5 , batch: 210 , training loss: 4.354608\n",
      "[INFO] Epoch: 5 , batch: 211 , training loss: 4.335830\n",
      "[INFO] Epoch: 5 , batch: 212 , training loss: 4.426441\n",
      "[INFO] Epoch: 5 , batch: 213 , training loss: 4.400904\n",
      "[INFO] Epoch: 5 , batch: 214 , training loss: 4.426054\n",
      "[INFO] Epoch: 5 , batch: 215 , training loss: 4.678225\n",
      "[INFO] Epoch: 5 , batch: 216 , training loss: 4.374212\n",
      "[INFO] Epoch: 5 , batch: 217 , training loss: 4.304743\n",
      "[INFO] Epoch: 5 , batch: 218 , training loss: 4.312680\n",
      "[INFO] Epoch: 5 , batch: 219 , training loss: 4.417106\n",
      "[INFO] Epoch: 5 , batch: 220 , training loss: 4.217745\n",
      "[INFO] Epoch: 5 , batch: 221 , training loss: 4.242111\n",
      "[INFO] Epoch: 5 , batch: 222 , training loss: 4.400670\n",
      "[INFO] Epoch: 5 , batch: 223 , training loss: 4.472435\n",
      "[INFO] Epoch: 5 , batch: 224 , training loss: 4.514485\n",
      "[INFO] Epoch: 5 , batch: 225 , training loss: 4.356947\n",
      "[INFO] Epoch: 5 , batch: 226 , training loss: 4.528086\n",
      "[INFO] Epoch: 5 , batch: 227 , training loss: 4.487443\n",
      "[INFO] Epoch: 5 , batch: 228 , training loss: 4.516762\n",
      "[INFO] Epoch: 5 , batch: 229 , training loss: 4.396029\n",
      "[INFO] Epoch: 5 , batch: 230 , training loss: 4.236936\n",
      "[INFO] Epoch: 5 , batch: 231 , training loss: 4.060876\n",
      "[INFO] Epoch: 5 , batch: 232 , training loss: 4.224097\n",
      "[INFO] Epoch: 5 , batch: 233 , training loss: 4.270364\n",
      "[INFO] Epoch: 5 , batch: 234 , training loss: 3.913378\n",
      "[INFO] Epoch: 5 , batch: 235 , training loss: 4.053125\n",
      "[INFO] Epoch: 5 , batch: 236 , training loss: 4.233681\n",
      "[INFO] Epoch: 5 , batch: 237 , training loss: 4.399922\n",
      "[INFO] Epoch: 5 , batch: 238 , training loss: 4.127708\n",
      "[INFO] Epoch: 5 , batch: 239 , training loss: 4.206564\n",
      "[INFO] Epoch: 5 , batch: 240 , training loss: 4.261533\n",
      "[INFO] Epoch: 5 , batch: 241 , training loss: 4.035898\n",
      "[INFO] Epoch: 5 , batch: 242 , training loss: 4.048067\n",
      "[INFO] Epoch: 5 , batch: 243 , training loss: 4.397408\n",
      "[INFO] Epoch: 5 , batch: 244 , training loss: 4.299465\n",
      "[INFO] Epoch: 5 , batch: 245 , training loss: 4.328990\n",
      "[INFO] Epoch: 5 , batch: 246 , training loss: 3.954826\n",
      "[INFO] Epoch: 5 , batch: 247 , training loss: 4.129347\n",
      "[INFO] Epoch: 5 , batch: 248 , training loss: 4.238955\n",
      "[INFO] Epoch: 5 , batch: 249 , training loss: 4.201681\n",
      "[INFO] Epoch: 5 , batch: 250 , training loss: 3.970625\n",
      "[INFO] Epoch: 5 , batch: 251 , training loss: 4.505029\n",
      "[INFO] Epoch: 5 , batch: 252 , training loss: 4.147905\n",
      "[INFO] Epoch: 5 , batch: 253 , training loss: 4.085095\n",
      "[INFO] Epoch: 5 , batch: 254 , training loss: 4.412410\n",
      "[INFO] Epoch: 5 , batch: 255 , training loss: 4.365311\n",
      "[INFO] Epoch: 5 , batch: 256 , training loss: 4.321849\n",
      "[INFO] Epoch: 5 , batch: 257 , training loss: 4.538023\n",
      "[INFO] Epoch: 5 , batch: 258 , training loss: 4.564749\n",
      "[INFO] Epoch: 5 , batch: 259 , training loss: 4.615734\n",
      "[INFO] Epoch: 5 , batch: 260 , training loss: 4.298936\n",
      "[INFO] Epoch: 5 , batch: 261 , training loss: 4.496705\n",
      "[INFO] Epoch: 5 , batch: 262 , training loss: 4.710512\n",
      "[INFO] Epoch: 5 , batch: 263 , training loss: 4.836032\n",
      "[INFO] Epoch: 5 , batch: 264 , training loss: 4.151913\n",
      "[INFO] Epoch: 5 , batch: 265 , training loss: 4.286253\n",
      "[INFO] Epoch: 5 , batch: 266 , training loss: 4.809477\n",
      "[INFO] Epoch: 5 , batch: 267 , training loss: 4.445749\n",
      "[INFO] Epoch: 5 , batch: 268 , training loss: 4.340792\n",
      "[INFO] Epoch: 5 , batch: 269 , training loss: 4.384047\n",
      "[INFO] Epoch: 5 , batch: 270 , training loss: 4.372329\n",
      "[INFO] Epoch: 5 , batch: 271 , training loss: 4.412290\n",
      "[INFO] Epoch: 5 , batch: 272 , training loss: 4.382033\n",
      "[INFO] Epoch: 5 , batch: 273 , training loss: 4.382548\n",
      "[INFO] Epoch: 5 , batch: 274 , training loss: 4.520080\n",
      "[INFO] Epoch: 5 , batch: 275 , training loss: 4.389560\n",
      "[INFO] Epoch: 5 , batch: 276 , training loss: 4.428275\n",
      "[INFO] Epoch: 5 , batch: 277 , training loss: 4.589233\n",
      "[INFO] Epoch: 5 , batch: 278 , training loss: 4.170178\n",
      "[INFO] Epoch: 5 , batch: 279 , training loss: 4.216334\n",
      "[INFO] Epoch: 5 , batch: 280 , training loss: 4.140675\n",
      "[INFO] Epoch: 5 , batch: 281 , training loss: 4.312780\n",
      "[INFO] Epoch: 5 , batch: 282 , training loss: 4.203975\n",
      "[INFO] Epoch: 5 , batch: 283 , training loss: 4.274842\n",
      "[INFO] Epoch: 5 , batch: 284 , training loss: 4.301417\n",
      "[INFO] Epoch: 5 , batch: 285 , training loss: 4.311066\n",
      "[INFO] Epoch: 5 , batch: 286 , training loss: 4.267225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 5 , batch: 287 , training loss: 4.123357\n",
      "[INFO] Epoch: 5 , batch: 288 , training loss: 4.162844\n",
      "[INFO] Epoch: 5 , batch: 289 , training loss: 4.213649\n",
      "[INFO] Epoch: 5 , batch: 290 , training loss: 3.997197\n",
      "[INFO] Epoch: 5 , batch: 291 , training loss: 3.996387\n",
      "[INFO] Epoch: 5 , batch: 292 , training loss: 4.084719\n",
      "[INFO] Epoch: 5 , batch: 293 , training loss: 4.031305\n",
      "[INFO] Epoch: 5 , batch: 294 , training loss: 4.731465\n",
      "[INFO] Epoch: 5 , batch: 295 , training loss: 4.482431\n",
      "[INFO] Epoch: 5 , batch: 296 , training loss: 4.407100\n",
      "[INFO] Epoch: 5 , batch: 297 , training loss: 4.327910\n",
      "[INFO] Epoch: 5 , batch: 298 , training loss: 4.182263\n",
      "[INFO] Epoch: 5 , batch: 299 , training loss: 4.177586\n",
      "[INFO] Epoch: 5 , batch: 300 , training loss: 4.154388\n",
      "[INFO] Epoch: 5 , batch: 301 , training loss: 4.092125\n",
      "[INFO] Epoch: 5 , batch: 302 , training loss: 4.296305\n",
      "[INFO] Epoch: 5 , batch: 303 , training loss: 4.291140\n",
      "[INFO] Epoch: 5 , batch: 304 , training loss: 4.479740\n",
      "[INFO] Epoch: 5 , batch: 305 , training loss: 4.213749\n",
      "[INFO] Epoch: 5 , batch: 306 , training loss: 4.361139\n",
      "[INFO] Epoch: 5 , batch: 307 , training loss: 4.345184\n",
      "[INFO] Epoch: 5 , batch: 308 , training loss: 4.234425\n",
      "[INFO] Epoch: 5 , batch: 309 , training loss: 4.238494\n",
      "[INFO] Epoch: 5 , batch: 310 , training loss: 4.059859\n",
      "[INFO] Epoch: 5 , batch: 311 , training loss: 4.107251\n",
      "[INFO] Epoch: 5 , batch: 312 , training loss: 3.979679\n",
      "[INFO] Epoch: 5 , batch: 313 , training loss: 4.145483\n",
      "[INFO] Epoch: 5 , batch: 314 , training loss: 4.218029\n",
      "[INFO] Epoch: 5 , batch: 315 , training loss: 4.249580\n",
      "[INFO] Epoch: 5 , batch: 316 , training loss: 4.587833\n",
      "[INFO] Epoch: 5 , batch: 317 , training loss: 5.113679\n",
      "[INFO] Epoch: 5 , batch: 318 , training loss: 5.220247\n",
      "[INFO] Epoch: 5 , batch: 319 , training loss: 4.791512\n",
      "[INFO] Epoch: 5 , batch: 320 , training loss: 4.256892\n",
      "[INFO] Epoch: 5 , batch: 321 , training loss: 4.049313\n",
      "[INFO] Epoch: 5 , batch: 322 , training loss: 4.187480\n",
      "[INFO] Epoch: 5 , batch: 323 , training loss: 4.179589\n",
      "[INFO] Epoch: 5 , batch: 324 , training loss: 4.178522\n",
      "[INFO] Epoch: 5 , batch: 325 , training loss: 4.344688\n",
      "[INFO] Epoch: 5 , batch: 326 , training loss: 4.379011\n",
      "[INFO] Epoch: 5 , batch: 327 , training loss: 4.294318\n",
      "[INFO] Epoch: 5 , batch: 328 , training loss: 4.277528\n",
      "[INFO] Epoch: 5 , batch: 329 , training loss: 4.164336\n",
      "[INFO] Epoch: 5 , batch: 330 , training loss: 4.157968\n",
      "[INFO] Epoch: 5 , batch: 331 , training loss: 4.364043\n",
      "[INFO] Epoch: 5 , batch: 332 , training loss: 4.125833\n",
      "[INFO] Epoch: 5 , batch: 333 , training loss: 4.111702\n",
      "[INFO] Epoch: 5 , batch: 334 , training loss: 4.210228\n",
      "[INFO] Epoch: 5 , batch: 335 , training loss: 4.317044\n",
      "[INFO] Epoch: 5 , batch: 336 , training loss: 4.306646\n",
      "[INFO] Epoch: 5 , batch: 337 , training loss: 4.369918\n",
      "[INFO] Epoch: 5 , batch: 338 , training loss: 4.569040\n",
      "[INFO] Epoch: 5 , batch: 339 , training loss: 4.384065\n",
      "[INFO] Epoch: 5 , batch: 340 , training loss: 4.582460\n",
      "[INFO] Epoch: 5 , batch: 341 , training loss: 4.300310\n",
      "[INFO] Epoch: 5 , batch: 342 , training loss: 4.132778\n",
      "[INFO] Epoch: 5 , batch: 343 , training loss: 4.210961\n",
      "[INFO] Epoch: 5 , batch: 344 , training loss: 4.045783\n",
      "[INFO] Epoch: 5 , batch: 345 , training loss: 4.189037\n",
      "[INFO] Epoch: 5 , batch: 346 , training loss: 4.231045\n",
      "[INFO] Epoch: 5 , batch: 347 , training loss: 4.145395\n",
      "[INFO] Epoch: 5 , batch: 348 , training loss: 4.336790\n",
      "[INFO] Epoch: 5 , batch: 349 , training loss: 4.414139\n",
      "[INFO] Epoch: 5 , batch: 350 , training loss: 4.165986\n",
      "[INFO] Epoch: 5 , batch: 351 , training loss: 4.256952\n",
      "[INFO] Epoch: 5 , batch: 352 , training loss: 4.270024\n",
      "[INFO] Epoch: 5 , batch: 353 , training loss: 4.226385\n",
      "[INFO] Epoch: 5 , batch: 354 , training loss: 4.336375\n",
      "[INFO] Epoch: 5 , batch: 355 , training loss: 4.379086\n",
      "[INFO] Epoch: 5 , batch: 356 , training loss: 4.242574\n",
      "[INFO] Epoch: 5 , batch: 357 , training loss: 4.305368\n",
      "[INFO] Epoch: 5 , batch: 358 , training loss: 4.238590\n",
      "[INFO] Epoch: 5 , batch: 359 , training loss: 4.232797\n",
      "[INFO] Epoch: 5 , batch: 360 , training loss: 4.256278\n",
      "[INFO] Epoch: 5 , batch: 361 , training loss: 4.236476\n",
      "[INFO] Epoch: 5 , batch: 362 , training loss: 4.334313\n",
      "[INFO] Epoch: 5 , batch: 363 , training loss: 4.243512\n",
      "[INFO] Epoch: 5 , batch: 364 , training loss: 4.275546\n",
      "[INFO] Epoch: 5 , batch: 365 , training loss: 4.176290\n",
      "[INFO] Epoch: 5 , batch: 366 , training loss: 4.328714\n",
      "[INFO] Epoch: 5 , batch: 367 , training loss: 4.386164\n",
      "[INFO] Epoch: 5 , batch: 368 , training loss: 4.935250\n",
      "[INFO] Epoch: 5 , batch: 369 , training loss: 4.557935\n",
      "[INFO] Epoch: 5 , batch: 370 , training loss: 4.289041\n",
      "[INFO] Epoch: 5 , batch: 371 , training loss: 4.819886\n",
      "[INFO] Epoch: 5 , batch: 372 , training loss: 5.093232\n",
      "[INFO] Epoch: 5 , batch: 373 , training loss: 5.101004\n",
      "[INFO] Epoch: 5 , batch: 374 , training loss: 5.199024\n",
      "[INFO] Epoch: 5 , batch: 375 , training loss: 5.120764\n",
      "[INFO] Epoch: 5 , batch: 376 , training loss: 5.042437\n",
      "[INFO] Epoch: 5 , batch: 377 , training loss: 4.735881\n",
      "[INFO] Epoch: 5 , batch: 378 , training loss: 4.844738\n",
      "[INFO] Epoch: 5 , batch: 379 , training loss: 4.884520\n",
      "[INFO] Epoch: 5 , batch: 380 , training loss: 4.985775\n",
      "[INFO] Epoch: 5 , batch: 381 , training loss: 4.755034\n",
      "[INFO] Epoch: 5 , batch: 382 , training loss: 4.984577\n",
      "[INFO] Epoch: 5 , batch: 383 , training loss: 5.060989\n",
      "[INFO] Epoch: 5 , batch: 384 , training loss: 5.135399\n",
      "[INFO] Epoch: 5 , batch: 385 , training loss: 4.897101\n",
      "[INFO] Epoch: 5 , batch: 386 , training loss: 5.046021\n",
      "[INFO] Epoch: 5 , batch: 387 , training loss: 4.973032\n",
      "[INFO] Epoch: 5 , batch: 388 , training loss: 4.749817\n",
      "[INFO] Epoch: 5 , batch: 389 , training loss: 4.556279\n",
      "[INFO] Epoch: 5 , batch: 390 , training loss: 4.531724\n",
      "[INFO] Epoch: 5 , batch: 391 , training loss: 4.550688\n",
      "[INFO] Epoch: 5 , batch: 392 , training loss: 4.907180\n",
      "[INFO] Epoch: 5 , batch: 393 , training loss: 4.799426\n",
      "[INFO] Epoch: 5 , batch: 394 , training loss: 4.882468\n",
      "[INFO] Epoch: 5 , batch: 395 , training loss: 4.667632\n",
      "[INFO] Epoch: 5 , batch: 396 , training loss: 4.448153\n",
      "[INFO] Epoch: 5 , batch: 397 , training loss: 4.632743\n",
      "[INFO] Epoch: 5 , batch: 398 , training loss: 4.482366\n",
      "[INFO] Epoch: 5 , batch: 399 , training loss: 4.537990\n",
      "[INFO] Epoch: 5 , batch: 400 , training loss: 4.490994\n",
      "[INFO] Epoch: 5 , batch: 401 , training loss: 4.911673\n",
      "[INFO] Epoch: 5 , batch: 402 , training loss: 4.648496\n",
      "[INFO] Epoch: 5 , batch: 403 , training loss: 4.498597\n",
      "[INFO] Epoch: 5 , batch: 404 , training loss: 4.651314\n",
      "[INFO] Epoch: 5 , batch: 405 , training loss: 4.748212\n",
      "[INFO] Epoch: 5 , batch: 406 , training loss: 4.602630\n",
      "[INFO] Epoch: 5 , batch: 407 , training loss: 4.682485\n",
      "[INFO] Epoch: 5 , batch: 408 , training loss: 4.588720\n",
      "[INFO] Epoch: 5 , batch: 409 , training loss: 4.617935\n",
      "[INFO] Epoch: 5 , batch: 410 , training loss: 4.656432\n",
      "[INFO] Epoch: 5 , batch: 411 , training loss: 4.850945\n",
      "[INFO] Epoch: 5 , batch: 412 , training loss: 4.681106\n",
      "[INFO] Epoch: 5 , batch: 413 , training loss: 4.534135\n",
      "[INFO] Epoch: 5 , batch: 414 , training loss: 4.581066\n",
      "[INFO] Epoch: 5 , batch: 415 , training loss: 4.625376\n",
      "[INFO] Epoch: 5 , batch: 416 , training loss: 4.692040\n",
      "[INFO] Epoch: 5 , batch: 417 , training loss: 4.596979\n",
      "[INFO] Epoch: 5 , batch: 418 , training loss: 4.628368\n",
      "[INFO] Epoch: 5 , batch: 419 , training loss: 4.573065\n",
      "[INFO] Epoch: 5 , batch: 420 , training loss: 4.551591\n",
      "[INFO] Epoch: 5 , batch: 421 , training loss: 4.559560\n",
      "[INFO] Epoch: 5 , batch: 422 , training loss: 4.436574\n",
      "[INFO] Epoch: 5 , batch: 423 , training loss: 4.698092\n",
      "[INFO] Epoch: 5 , batch: 424 , training loss: 4.810947\n",
      "[INFO] Epoch: 5 , batch: 425 , training loss: 4.716634\n",
      "[INFO] Epoch: 5 , batch: 426 , training loss: 4.366303\n",
      "[INFO] Epoch: 5 , batch: 427 , training loss: 4.664460\n",
      "[INFO] Epoch: 5 , batch: 428 , training loss: 4.552221\n",
      "[INFO] Epoch: 5 , batch: 429 , training loss: 4.379929\n",
      "[INFO] Epoch: 5 , batch: 430 , training loss: 4.686919\n",
      "[INFO] Epoch: 5 , batch: 431 , training loss: 4.235198\n",
      "[INFO] Epoch: 5 , batch: 432 , training loss: 4.313889\n",
      "[INFO] Epoch: 5 , batch: 433 , training loss: 4.307012\n",
      "[INFO] Epoch: 5 , batch: 434 , training loss: 4.209735\n",
      "[INFO] Epoch: 5 , batch: 435 , training loss: 4.558683\n",
      "[INFO] Epoch: 5 , batch: 436 , training loss: 4.653248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 5 , batch: 437 , training loss: 4.445970\n",
      "[INFO] Epoch: 5 , batch: 438 , training loss: 4.227425\n",
      "[INFO] Epoch: 5 , batch: 439 , training loss: 4.463109\n",
      "[INFO] Epoch: 5 , batch: 440 , training loss: 4.649335\n",
      "[INFO] Epoch: 5 , batch: 441 , training loss: 4.670240\n",
      "[INFO] Epoch: 5 , batch: 442 , training loss: 4.458758\n",
      "[INFO] Epoch: 5 , batch: 443 , training loss: 4.691903\n",
      "[INFO] Epoch: 5 , batch: 444 , training loss: 4.273821\n",
      "[INFO] Epoch: 5 , batch: 445 , training loss: 4.143386\n",
      "[INFO] Epoch: 5 , batch: 446 , training loss: 4.056189\n",
      "[INFO] Epoch: 5 , batch: 447 , training loss: 4.296750\n",
      "[INFO] Epoch: 5 , batch: 448 , training loss: 4.465584\n",
      "[INFO] Epoch: 5 , batch: 449 , training loss: 4.877165\n",
      "[INFO] Epoch: 5 , batch: 450 , training loss: 4.946037\n",
      "[INFO] Epoch: 5 , batch: 451 , training loss: 4.816056\n",
      "[INFO] Epoch: 5 , batch: 452 , training loss: 4.569448\n",
      "[INFO] Epoch: 5 , batch: 453 , training loss: 4.348049\n",
      "[INFO] Epoch: 5 , batch: 454 , training loss: 4.505045\n",
      "[INFO] Epoch: 5 , batch: 455 , training loss: 4.530818\n",
      "[INFO] Epoch: 5 , batch: 456 , training loss: 4.510321\n",
      "[INFO] Epoch: 5 , batch: 457 , training loss: 4.618196\n",
      "[INFO] Epoch: 5 , batch: 458 , training loss: 4.337047\n",
      "[INFO] Epoch: 5 , batch: 459 , training loss: 4.322535\n",
      "[INFO] Epoch: 5 , batch: 460 , training loss: 4.456316\n",
      "[INFO] Epoch: 5 , batch: 461 , training loss: 4.441456\n",
      "[INFO] Epoch: 5 , batch: 462 , training loss: 4.495283\n",
      "[INFO] Epoch: 5 , batch: 463 , training loss: 4.388271\n",
      "[INFO] Epoch: 5 , batch: 464 , training loss: 4.557186\n",
      "[INFO] Epoch: 5 , batch: 465 , training loss: 4.489842\n",
      "[INFO] Epoch: 5 , batch: 466 , training loss: 4.585422\n",
      "[INFO] Epoch: 5 , batch: 467 , training loss: 4.630882\n",
      "[INFO] Epoch: 5 , batch: 468 , training loss: 4.571608\n",
      "[INFO] Epoch: 5 , batch: 469 , training loss: 4.577960\n",
      "[INFO] Epoch: 5 , batch: 470 , training loss: 4.332023\n",
      "[INFO] Epoch: 5 , batch: 471 , training loss: 4.469437\n",
      "[INFO] Epoch: 5 , batch: 472 , training loss: 4.536884\n",
      "[INFO] Epoch: 5 , batch: 473 , training loss: 4.445876\n",
      "[INFO] Epoch: 5 , batch: 474 , training loss: 4.258298\n",
      "[INFO] Epoch: 5 , batch: 475 , training loss: 4.103324\n",
      "[INFO] Epoch: 5 , batch: 476 , training loss: 4.511764\n",
      "[INFO] Epoch: 5 , batch: 477 , training loss: 4.615836\n",
      "[INFO] Epoch: 5 , batch: 478 , training loss: 4.679410\n",
      "[INFO] Epoch: 5 , batch: 479 , training loss: 4.618468\n",
      "[INFO] Epoch: 5 , batch: 480 , training loss: 4.751342\n",
      "[INFO] Epoch: 5 , batch: 481 , training loss: 4.589362\n",
      "[INFO] Epoch: 5 , batch: 482 , training loss: 4.726832\n",
      "[INFO] Epoch: 5 , batch: 483 , training loss: 4.554760\n",
      "[INFO] Epoch: 5 , batch: 484 , training loss: 4.348654\n",
      "[INFO] Epoch: 5 , batch: 485 , training loss: 4.504169\n",
      "[INFO] Epoch: 5 , batch: 486 , training loss: 4.345760\n",
      "[INFO] Epoch: 5 , batch: 487 , training loss: 4.358227\n",
      "[INFO] Epoch: 5 , batch: 488 , training loss: 4.529520\n",
      "[INFO] Epoch: 5 , batch: 489 , training loss: 4.425492\n",
      "[INFO] Epoch: 5 , batch: 490 , training loss: 4.502850\n",
      "[INFO] Epoch: 5 , batch: 491 , training loss: 4.465819\n",
      "[INFO] Epoch: 5 , batch: 492 , training loss: 4.368660\n",
      "[INFO] Epoch: 5 , batch: 493 , training loss: 4.554302\n",
      "[INFO] Epoch: 5 , batch: 494 , training loss: 4.474635\n",
      "[INFO] Epoch: 5 , batch: 495 , training loss: 4.588448\n",
      "[INFO] Epoch: 5 , batch: 496 , training loss: 4.484097\n",
      "[INFO] Epoch: 5 , batch: 497 , training loss: 4.496359\n",
      "[INFO] Epoch: 5 , batch: 498 , training loss: 4.534292\n",
      "[INFO] Epoch: 5 , batch: 499 , training loss: 4.577135\n",
      "[INFO] Epoch: 5 , batch: 500 , training loss: 4.791090\n",
      "[INFO] Epoch: 5 , batch: 501 , training loss: 5.200794\n",
      "[INFO] Epoch: 5 , batch: 502 , training loss: 5.357163\n",
      "[INFO] Epoch: 5 , batch: 503 , training loss: 5.135686\n",
      "[INFO] Epoch: 5 , batch: 504 , training loss: 5.185476\n",
      "[INFO] Epoch: 5 , batch: 505 , training loss: 5.042617\n",
      "[INFO] Epoch: 5 , batch: 506 , training loss: 4.984914\n",
      "[INFO] Epoch: 5 , batch: 507 , training loss: 5.043775\n",
      "[INFO] Epoch: 5 , batch: 508 , training loss: 4.982531\n",
      "[INFO] Epoch: 5 , batch: 509 , training loss: 4.721653\n",
      "[INFO] Epoch: 5 , batch: 510 , training loss: 4.781178\n",
      "[INFO] Epoch: 5 , batch: 511 , training loss: 4.661907\n",
      "[INFO] Epoch: 5 , batch: 512 , training loss: 4.728617\n",
      "[INFO] Epoch: 5 , batch: 513 , training loss: 5.002224\n",
      "[INFO] Epoch: 5 , batch: 514 , training loss: 4.683191\n",
      "[INFO] Epoch: 5 , batch: 515 , training loss: 4.937086\n",
      "[INFO] Epoch: 5 , batch: 516 , training loss: 4.724867\n",
      "[INFO] Epoch: 5 , batch: 517 , training loss: 4.721590\n",
      "[INFO] Epoch: 5 , batch: 518 , training loss: 4.667959\n",
      "[INFO] Epoch: 5 , batch: 519 , training loss: 4.538306\n",
      "[INFO] Epoch: 5 , batch: 520 , training loss: 4.758895\n",
      "[INFO] Epoch: 5 , batch: 521 , training loss: 4.766140\n",
      "[INFO] Epoch: 5 , batch: 522 , training loss: 4.853096\n",
      "[INFO] Epoch: 5 , batch: 523 , training loss: 4.713599\n",
      "[INFO] Epoch: 5 , batch: 524 , training loss: 4.992674\n",
      "[INFO] Epoch: 5 , batch: 525 , training loss: 4.899243\n",
      "[INFO] Epoch: 5 , batch: 526 , training loss: 4.637355\n",
      "[INFO] Epoch: 5 , batch: 527 , training loss: 4.670723\n",
      "[INFO] Epoch: 5 , batch: 528 , training loss: 4.742229\n",
      "[INFO] Epoch: 5 , batch: 529 , training loss: 4.707345\n",
      "[INFO] Epoch: 5 , batch: 530 , training loss: 4.548292\n",
      "[INFO] Epoch: 5 , batch: 531 , training loss: 4.714815\n",
      "[INFO] Epoch: 5 , batch: 532 , training loss: 4.542682\n",
      "[INFO] Epoch: 5 , batch: 533 , training loss: 4.709306\n",
      "[INFO] Epoch: 5 , batch: 534 , training loss: 4.709617\n",
      "[INFO] Epoch: 5 , batch: 535 , training loss: 4.711576\n",
      "[INFO] Epoch: 5 , batch: 536 , training loss: 4.593252\n",
      "[INFO] Epoch: 5 , batch: 537 , training loss: 4.533120\n",
      "[INFO] Epoch: 5 , batch: 538 , training loss: 4.606348\n",
      "[INFO] Epoch: 5 , batch: 539 , training loss: 4.787180\n",
      "[INFO] Epoch: 5 , batch: 540 , training loss: 5.436314\n",
      "[INFO] Epoch: 5 , batch: 541 , training loss: 5.371375\n",
      "[INFO] Epoch: 5 , batch: 542 , training loss: 5.160655\n",
      "[INFO] Epoch: 6 , batch: 0 , training loss: 4.540893\n",
      "[INFO] Epoch: 6 , batch: 1 , training loss: 4.469151\n",
      "[INFO] Epoch: 6 , batch: 2 , training loss: 4.262893\n",
      "[INFO] Epoch: 6 , batch: 3 , training loss: 4.166887\n",
      "[INFO] Epoch: 6 , batch: 4 , training loss: 4.493572\n",
      "[INFO] Epoch: 6 , batch: 5 , training loss: 4.181205\n",
      "[INFO] Epoch: 6 , batch: 6 , training loss: 4.746441\n",
      "[INFO] Epoch: 6 , batch: 7 , training loss: 4.339093\n",
      "[INFO] Epoch: 6 , batch: 8 , training loss: 4.025721\n",
      "[INFO] Epoch: 6 , batch: 9 , training loss: 4.225460\n",
      "[INFO] Epoch: 6 , batch: 10 , training loss: 4.114005\n",
      "[INFO] Epoch: 6 , batch: 11 , training loss: 4.049587\n",
      "[INFO] Epoch: 6 , batch: 12 , training loss: 4.069722\n",
      "[INFO] Epoch: 6 , batch: 13 , training loss: 4.044446\n",
      "[INFO] Epoch: 6 , batch: 14 , training loss: 3.828084\n",
      "[INFO] Epoch: 6 , batch: 15 , training loss: 4.080384\n",
      "[INFO] Epoch: 6 , batch: 16 , training loss: 3.956720\n",
      "[INFO] Epoch: 6 , batch: 17 , training loss: 4.105222\n",
      "[INFO] Epoch: 6 , batch: 18 , training loss: 4.068734\n",
      "[INFO] Epoch: 6 , batch: 19 , training loss: 3.802151\n",
      "[INFO] Epoch: 6 , batch: 20 , training loss: 3.770458\n",
      "[INFO] Epoch: 6 , batch: 21 , training loss: 3.920008\n",
      "[INFO] Epoch: 6 , batch: 22 , training loss: 3.946030\n",
      "[INFO] Epoch: 6 , batch: 23 , training loss: 4.086580\n",
      "[INFO] Epoch: 6 , batch: 24 , training loss: 3.960529\n",
      "[INFO] Epoch: 6 , batch: 25 , training loss: 4.073155\n",
      "[INFO] Epoch: 6 , batch: 26 , training loss: 3.938575\n",
      "[INFO] Epoch: 6 , batch: 27 , training loss: 3.909655\n",
      "[INFO] Epoch: 6 , batch: 28 , training loss: 4.131302\n",
      "[INFO] Epoch: 6 , batch: 29 , training loss: 3.880111\n",
      "[INFO] Epoch: 6 , batch: 30 , training loss: 3.890322\n",
      "[INFO] Epoch: 6 , batch: 31 , training loss: 3.984983\n",
      "[INFO] Epoch: 6 , batch: 32 , training loss: 3.981051\n",
      "[INFO] Epoch: 6 , batch: 33 , training loss: 4.076530\n",
      "[INFO] Epoch: 6 , batch: 34 , training loss: 4.086295\n",
      "[INFO] Epoch: 6 , batch: 35 , training loss: 3.981579\n",
      "[INFO] Epoch: 6 , batch: 36 , training loss: 3.984833\n",
      "[INFO] Epoch: 6 , batch: 37 , training loss: 3.884467\n",
      "[INFO] Epoch: 6 , batch: 38 , training loss: 4.037958\n",
      "[INFO] Epoch: 6 , batch: 39 , training loss: 3.829966\n",
      "[INFO] Epoch: 6 , batch: 40 , training loss: 3.965528\n",
      "[INFO] Epoch: 6 , batch: 41 , training loss: 4.176816\n",
      "[INFO] Epoch: 6 , batch: 42 , training loss: 4.716497\n",
      "[INFO] Epoch: 6 , batch: 43 , training loss: 4.405327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 6 , batch: 44 , training loss: 4.598636\n",
      "[INFO] Epoch: 6 , batch: 45 , training loss: 4.702172\n",
      "[INFO] Epoch: 6 , batch: 46 , training loss: 4.910319\n",
      "[INFO] Epoch: 6 , batch: 47 , training loss: 4.340682\n",
      "[INFO] Epoch: 6 , batch: 48 , training loss: 4.379837\n",
      "[INFO] Epoch: 6 , batch: 49 , training loss: 4.420000\n",
      "[INFO] Epoch: 6 , batch: 50 , training loss: 4.185549\n",
      "[INFO] Epoch: 6 , batch: 51 , training loss: 4.255052\n",
      "[INFO] Epoch: 6 , batch: 52 , training loss: 4.059002\n",
      "[INFO] Epoch: 6 , batch: 53 , training loss: 4.231379\n",
      "[INFO] Epoch: 6 , batch: 54 , training loss: 4.214304\n",
      "[INFO] Epoch: 6 , batch: 55 , training loss: 4.309915\n",
      "[INFO] Epoch: 6 , batch: 56 , training loss: 4.130229\n",
      "[INFO] Epoch: 6 , batch: 57 , training loss: 4.011373\n",
      "[INFO] Epoch: 6 , batch: 58 , training loss: 4.051123\n",
      "[INFO] Epoch: 6 , batch: 59 , training loss: 4.158974\n",
      "[INFO] Epoch: 6 , batch: 60 , training loss: 3.992069\n",
      "[INFO] Epoch: 6 , batch: 61 , training loss: 4.093797\n",
      "[INFO] Epoch: 6 , batch: 62 , training loss: 3.979188\n",
      "[INFO] Epoch: 6 , batch: 63 , training loss: 4.173014\n",
      "[INFO] Epoch: 6 , batch: 64 , training loss: 4.324450\n",
      "[INFO] Epoch: 6 , batch: 65 , training loss: 4.043438\n",
      "[INFO] Epoch: 6 , batch: 66 , training loss: 3.897855\n",
      "[INFO] Epoch: 6 , batch: 67 , training loss: 3.900497\n",
      "[INFO] Epoch: 6 , batch: 68 , training loss: 4.204750\n",
      "[INFO] Epoch: 6 , batch: 69 , training loss: 4.002741\n",
      "[INFO] Epoch: 6 , batch: 70 , training loss: 4.275332\n",
      "[INFO] Epoch: 6 , batch: 71 , training loss: 4.076328\n",
      "[INFO] Epoch: 6 , batch: 72 , training loss: 4.157402\n",
      "[INFO] Epoch: 6 , batch: 73 , training loss: 4.084241\n",
      "[INFO] Epoch: 6 , batch: 74 , training loss: 4.237756\n",
      "[INFO] Epoch: 6 , batch: 75 , training loss: 3.991718\n",
      "[INFO] Epoch: 6 , batch: 76 , training loss: 4.151905\n",
      "[INFO] Epoch: 6 , batch: 77 , training loss: 4.047071\n",
      "[INFO] Epoch: 6 , batch: 78 , training loss: 4.170879\n",
      "[INFO] Epoch: 6 , batch: 79 , training loss: 3.973895\n",
      "[INFO] Epoch: 6 , batch: 80 , training loss: 4.183197\n",
      "[INFO] Epoch: 6 , batch: 81 , training loss: 4.154257\n",
      "[INFO] Epoch: 6 , batch: 82 , training loss: 4.120699\n",
      "[INFO] Epoch: 6 , batch: 83 , training loss: 4.225762\n",
      "[INFO] Epoch: 6 , batch: 84 , training loss: 4.177131\n",
      "[INFO] Epoch: 6 , batch: 85 , training loss: 4.247213\n",
      "[INFO] Epoch: 6 , batch: 86 , training loss: 4.225405\n",
      "[INFO] Epoch: 6 , batch: 87 , training loss: 4.168241\n",
      "[INFO] Epoch: 6 , batch: 88 , training loss: 4.334132\n",
      "[INFO] Epoch: 6 , batch: 89 , training loss: 4.098864\n",
      "[INFO] Epoch: 6 , batch: 90 , training loss: 4.168998\n",
      "[INFO] Epoch: 6 , batch: 91 , training loss: 4.102631\n",
      "[INFO] Epoch: 6 , batch: 92 , training loss: 4.109010\n",
      "[INFO] Epoch: 6 , batch: 93 , training loss: 4.200642\n",
      "[INFO] Epoch: 6 , batch: 94 , training loss: 4.376658\n",
      "[INFO] Epoch: 6 , batch: 95 , training loss: 4.141923\n",
      "[INFO] Epoch: 6 , batch: 96 , training loss: 4.100776\n",
      "[INFO] Epoch: 6 , batch: 97 , training loss: 4.102787\n",
      "[INFO] Epoch: 6 , batch: 98 , training loss: 4.054628\n",
      "[INFO] Epoch: 6 , batch: 99 , training loss: 4.156520\n",
      "[INFO] Epoch: 6 , batch: 100 , training loss: 3.984485\n",
      "[INFO] Epoch: 6 , batch: 101 , training loss: 4.007678\n",
      "[INFO] Epoch: 6 , batch: 102 , training loss: 4.228132\n",
      "[INFO] Epoch: 6 , batch: 103 , training loss: 3.941643\n",
      "[INFO] Epoch: 6 , batch: 104 , training loss: 3.912838\n",
      "[INFO] Epoch: 6 , batch: 105 , training loss: 4.210127\n",
      "[INFO] Epoch: 6 , batch: 106 , training loss: 4.209082\n",
      "[INFO] Epoch: 6 , batch: 107 , training loss: 4.068202\n",
      "[INFO] Epoch: 6 , batch: 108 , training loss: 3.973344\n",
      "[INFO] Epoch: 6 , batch: 109 , training loss: 3.841227\n",
      "[INFO] Epoch: 6 , batch: 110 , training loss: 4.115816\n",
      "[INFO] Epoch: 6 , batch: 111 , training loss: 4.167730\n",
      "[INFO] Epoch: 6 , batch: 112 , training loss: 4.116470\n",
      "[INFO] Epoch: 6 , batch: 113 , training loss: 4.064194\n",
      "[INFO] Epoch: 6 , batch: 114 , training loss: 4.184700\n",
      "[INFO] Epoch: 6 , batch: 115 , training loss: 4.071576\n",
      "[INFO] Epoch: 6 , batch: 116 , training loss: 4.072368\n",
      "[INFO] Epoch: 6 , batch: 117 , training loss: 4.312561\n",
      "[INFO] Epoch: 6 , batch: 118 , training loss: 4.262269\n",
      "[INFO] Epoch: 6 , batch: 119 , training loss: 4.372262\n",
      "[INFO] Epoch: 6 , batch: 120 , training loss: 4.326687\n",
      "[INFO] Epoch: 6 , batch: 121 , training loss: 4.152753\n",
      "[INFO] Epoch: 6 , batch: 122 , training loss: 4.034982\n",
      "[INFO] Epoch: 6 , batch: 123 , training loss: 4.137964\n",
      "[INFO] Epoch: 6 , batch: 124 , training loss: 4.323269\n",
      "[INFO] Epoch: 6 , batch: 125 , training loss: 3.978357\n",
      "[INFO] Epoch: 6 , batch: 126 , training loss: 3.996827\n",
      "[INFO] Epoch: 6 , batch: 127 , training loss: 4.043995\n",
      "[INFO] Epoch: 6 , batch: 128 , training loss: 4.218295\n",
      "[INFO] Epoch: 6 , batch: 129 , training loss: 4.120307\n",
      "[INFO] Epoch: 6 , batch: 130 , training loss: 4.161320\n",
      "[INFO] Epoch: 6 , batch: 131 , training loss: 4.133419\n",
      "[INFO] Epoch: 6 , batch: 132 , training loss: 4.171195\n",
      "[INFO] Epoch: 6 , batch: 133 , training loss: 4.114156\n",
      "[INFO] Epoch: 6 , batch: 134 , training loss: 3.837821\n",
      "[INFO] Epoch: 6 , batch: 135 , training loss: 3.894496\n",
      "[INFO] Epoch: 6 , batch: 136 , training loss: 4.227653\n",
      "[INFO] Epoch: 6 , batch: 137 , training loss: 4.144270\n",
      "[INFO] Epoch: 6 , batch: 138 , training loss: 4.252471\n",
      "[INFO] Epoch: 6 , batch: 139 , training loss: 4.892387\n",
      "[INFO] Epoch: 6 , batch: 140 , training loss: 4.791104\n",
      "[INFO] Epoch: 6 , batch: 141 , training loss: 4.477749\n",
      "[INFO] Epoch: 6 , batch: 142 , training loss: 4.073435\n",
      "[INFO] Epoch: 6 , batch: 143 , training loss: 4.201834\n",
      "[INFO] Epoch: 6 , batch: 144 , training loss: 4.061587\n",
      "[INFO] Epoch: 6 , batch: 145 , training loss: 4.184413\n",
      "[INFO] Epoch: 6 , batch: 146 , training loss: 4.387730\n",
      "[INFO] Epoch: 6 , batch: 147 , training loss: 3.995801\n",
      "[INFO] Epoch: 6 , batch: 148 , training loss: 3.999372\n",
      "[INFO] Epoch: 6 , batch: 149 , training loss: 4.056051\n",
      "[INFO] Epoch: 6 , batch: 150 , training loss: 4.334280\n",
      "[INFO] Epoch: 6 , batch: 151 , training loss: 4.087600\n",
      "[INFO] Epoch: 6 , batch: 152 , training loss: 4.097418\n",
      "[INFO] Epoch: 6 , batch: 153 , training loss: 4.203381\n",
      "[INFO] Epoch: 6 , batch: 154 , training loss: 4.245667\n",
      "[INFO] Epoch: 6 , batch: 155 , training loss: 4.489250\n",
      "[INFO] Epoch: 6 , batch: 156 , training loss: 4.157143\n",
      "[INFO] Epoch: 6 , batch: 157 , training loss: 4.145325\n",
      "[INFO] Epoch: 6 , batch: 158 , training loss: 4.500515\n",
      "[INFO] Epoch: 6 , batch: 159 , training loss: 4.387011\n",
      "[INFO] Epoch: 6 , batch: 160 , training loss: 4.826899\n",
      "[INFO] Epoch: 6 , batch: 161 , training loss: 4.737155\n",
      "[INFO] Epoch: 6 , batch: 162 , training loss: 4.620319\n",
      "[INFO] Epoch: 6 , batch: 163 , training loss: 4.788830\n",
      "[INFO] Epoch: 6 , batch: 164 , training loss: 4.675945\n",
      "[INFO] Epoch: 6 , batch: 165 , training loss: 4.595778\n",
      "[INFO] Epoch: 6 , batch: 166 , training loss: 4.720115\n",
      "[INFO] Epoch: 6 , batch: 167 , training loss: 5.158719\n",
      "[INFO] Epoch: 6 , batch: 168 , training loss: 4.890959\n",
      "[INFO] Epoch: 6 , batch: 169 , training loss: 4.714451\n",
      "[INFO] Epoch: 6 , batch: 170 , training loss: 4.756166\n",
      "[INFO] Epoch: 6 , batch: 171 , training loss: 4.304702\n",
      "[INFO] Epoch: 6 , batch: 172 , training loss: 4.437148\n",
      "[INFO] Epoch: 6 , batch: 173 , training loss: 4.715675\n",
      "[INFO] Epoch: 6 , batch: 174 , training loss: 5.061259\n",
      "[INFO] Epoch: 6 , batch: 175 , training loss: 5.225904\n",
      "[INFO] Epoch: 6 , batch: 176 , training loss: 5.051666\n",
      "[INFO] Epoch: 6 , batch: 177 , training loss: 4.583270\n",
      "[INFO] Epoch: 6 , batch: 178 , training loss: 4.504161\n",
      "[INFO] Epoch: 6 , batch: 179 , training loss: 4.570041\n",
      "[INFO] Epoch: 6 , batch: 180 , training loss: 4.492065\n",
      "[INFO] Epoch: 6 , batch: 181 , training loss: 4.722832\n",
      "[INFO] Epoch: 6 , batch: 182 , training loss: 4.631069\n",
      "[INFO] Epoch: 6 , batch: 183 , training loss: 4.580110\n",
      "[INFO] Epoch: 6 , batch: 184 , training loss: 4.465183\n",
      "[INFO] Epoch: 6 , batch: 185 , training loss: 4.442144\n",
      "[INFO] Epoch: 6 , batch: 186 , training loss: 4.570459\n",
      "[INFO] Epoch: 6 , batch: 187 , training loss: 4.706644\n",
      "[INFO] Epoch: 6 , batch: 188 , training loss: 4.663414\n",
      "[INFO] Epoch: 6 , batch: 189 , training loss: 4.553299\n",
      "[INFO] Epoch: 6 , batch: 190 , training loss: 4.537980\n",
      "[INFO] Epoch: 6 , batch: 191 , training loss: 4.678467\n",
      "[INFO] Epoch: 6 , batch: 192 , training loss: 4.458191\n",
      "[INFO] Epoch: 6 , batch: 193 , training loss: 4.605938\n",
      "[INFO] Epoch: 6 , batch: 194 , training loss: 4.500604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 6 , batch: 195 , training loss: 4.588756\n",
      "[INFO] Epoch: 6 , batch: 196 , training loss: 4.343478\n",
      "[INFO] Epoch: 6 , batch: 197 , training loss: 4.500824\n",
      "[INFO] Epoch: 6 , batch: 198 , training loss: 4.333642\n",
      "[INFO] Epoch: 6 , batch: 199 , training loss: 4.448914\n",
      "[INFO] Epoch: 6 , batch: 200 , training loss: 4.375676\n",
      "[INFO] Epoch: 6 , batch: 201 , training loss: 4.290647\n",
      "[INFO] Epoch: 6 , batch: 202 , training loss: 4.278056\n",
      "[INFO] Epoch: 6 , batch: 203 , training loss: 4.304767\n",
      "[INFO] Epoch: 6 , batch: 204 , training loss: 4.433115\n",
      "[INFO] Epoch: 6 , batch: 205 , training loss: 4.020610\n",
      "[INFO] Epoch: 6 , batch: 206 , training loss: 3.948359\n",
      "[INFO] Epoch: 6 , batch: 207 , training loss: 3.964713\n",
      "[INFO] Epoch: 6 , batch: 208 , training loss: 4.323410\n",
      "[INFO] Epoch: 6 , batch: 209 , training loss: 4.240492\n",
      "[INFO] Epoch: 6 , batch: 210 , training loss: 4.301030\n",
      "[INFO] Epoch: 6 , batch: 211 , training loss: 4.286086\n",
      "[INFO] Epoch: 6 , batch: 212 , training loss: 4.389877\n",
      "[INFO] Epoch: 6 , batch: 213 , training loss: 4.365616\n",
      "[INFO] Epoch: 6 , batch: 214 , training loss: 4.388204\n",
      "[INFO] Epoch: 6 , batch: 215 , training loss: 4.635911\n",
      "[INFO] Epoch: 6 , batch: 216 , training loss: 4.337249\n",
      "[INFO] Epoch: 6 , batch: 217 , training loss: 4.268481\n",
      "[INFO] Epoch: 6 , batch: 218 , training loss: 4.278770\n",
      "[INFO] Epoch: 6 , batch: 219 , training loss: 4.382538\n",
      "[INFO] Epoch: 6 , batch: 220 , training loss: 4.186786\n",
      "[INFO] Epoch: 6 , batch: 221 , training loss: 4.202172\n",
      "[INFO] Epoch: 6 , batch: 222 , training loss: 4.359940\n",
      "[INFO] Epoch: 6 , batch: 223 , training loss: 4.444002\n",
      "[INFO] Epoch: 6 , batch: 224 , training loss: 4.487253\n",
      "[INFO] Epoch: 6 , batch: 225 , training loss: 4.323110\n",
      "[INFO] Epoch: 6 , batch: 226 , training loss: 4.480342\n",
      "[INFO] Epoch: 6 , batch: 227 , training loss: 4.446561\n",
      "[INFO] Epoch: 6 , batch: 228 , training loss: 4.481191\n",
      "[INFO] Epoch: 6 , batch: 229 , training loss: 4.351275\n",
      "[INFO] Epoch: 6 , batch: 230 , training loss: 4.201577\n",
      "[INFO] Epoch: 6 , batch: 231 , training loss: 4.025132\n",
      "[INFO] Epoch: 6 , batch: 232 , training loss: 4.194862\n",
      "[INFO] Epoch: 6 , batch: 233 , training loss: 4.220805\n",
      "[INFO] Epoch: 6 , batch: 234 , training loss: 3.887447\n",
      "[INFO] Epoch: 6 , batch: 235 , training loss: 4.032394\n",
      "[INFO] Epoch: 6 , batch: 236 , training loss: 4.188947\n",
      "[INFO] Epoch: 6 , batch: 237 , training loss: 4.362215\n",
      "[INFO] Epoch: 6 , batch: 238 , training loss: 4.096021\n",
      "[INFO] Epoch: 6 , batch: 239 , training loss: 4.167540\n",
      "[INFO] Epoch: 6 , batch: 240 , training loss: 4.236156\n",
      "[INFO] Epoch: 6 , batch: 241 , training loss: 3.986420\n",
      "[INFO] Epoch: 6 , batch: 242 , training loss: 4.015936\n",
      "[INFO] Epoch: 6 , batch: 243 , training loss: 4.347466\n",
      "[INFO] Epoch: 6 , batch: 244 , training loss: 4.259550\n",
      "[INFO] Epoch: 6 , batch: 245 , training loss: 4.285604\n",
      "[INFO] Epoch: 6 , batch: 246 , training loss: 3.933361\n",
      "[INFO] Epoch: 6 , batch: 247 , training loss: 4.105643\n",
      "[INFO] Epoch: 6 , batch: 248 , training loss: 4.195108\n",
      "[INFO] Epoch: 6 , batch: 249 , training loss: 4.168824\n",
      "[INFO] Epoch: 6 , batch: 250 , training loss: 3.949113\n",
      "[INFO] Epoch: 6 , batch: 251 , training loss: 4.466362\n",
      "[INFO] Epoch: 6 , batch: 252 , training loss: 4.114501\n",
      "[INFO] Epoch: 6 , batch: 253 , training loss: 4.055320\n",
      "[INFO] Epoch: 6 , batch: 254 , training loss: 4.368263\n",
      "[INFO] Epoch: 6 , batch: 255 , training loss: 4.308371\n",
      "[INFO] Epoch: 6 , batch: 256 , training loss: 4.288839\n",
      "[INFO] Epoch: 6 , batch: 257 , training loss: 4.505320\n",
      "[INFO] Epoch: 6 , batch: 258 , training loss: 4.522287\n",
      "[INFO] Epoch: 6 , batch: 259 , training loss: 4.570765\n",
      "[INFO] Epoch: 6 , batch: 260 , training loss: 4.261916\n",
      "[INFO] Epoch: 6 , batch: 261 , training loss: 4.459231\n",
      "[INFO] Epoch: 6 , batch: 262 , training loss: 4.664587\n",
      "[INFO] Epoch: 6 , batch: 263 , training loss: 4.788546\n",
      "[INFO] Epoch: 6 , batch: 264 , training loss: 4.107043\n",
      "[INFO] Epoch: 6 , batch: 265 , training loss: 4.237187\n",
      "[INFO] Epoch: 6 , batch: 266 , training loss: 4.748012\n",
      "[INFO] Epoch: 6 , batch: 267 , training loss: 4.399629\n",
      "[INFO] Epoch: 6 , batch: 268 , training loss: 4.313435\n",
      "[INFO] Epoch: 6 , batch: 269 , training loss: 4.353618\n",
      "[INFO] Epoch: 6 , batch: 270 , training loss: 4.334838\n",
      "[INFO] Epoch: 6 , batch: 271 , training loss: 4.384503\n",
      "[INFO] Epoch: 6 , batch: 272 , training loss: 4.348169\n",
      "[INFO] Epoch: 6 , batch: 273 , training loss: 4.337628\n",
      "[INFO] Epoch: 6 , batch: 274 , training loss: 4.489336\n",
      "[INFO] Epoch: 6 , batch: 275 , training loss: 4.352337\n",
      "[INFO] Epoch: 6 , batch: 276 , training loss: 4.392752\n",
      "[INFO] Epoch: 6 , batch: 277 , training loss: 4.546098\n",
      "[INFO] Epoch: 6 , batch: 278 , training loss: 4.147005\n",
      "[INFO] Epoch: 6 , batch: 279 , training loss: 4.188193\n",
      "[INFO] Epoch: 6 , batch: 280 , training loss: 4.115624\n",
      "[INFO] Epoch: 6 , batch: 281 , training loss: 4.271331\n",
      "[INFO] Epoch: 6 , batch: 282 , training loss: 4.174369\n",
      "[INFO] Epoch: 6 , batch: 283 , training loss: 4.226284\n",
      "[INFO] Epoch: 6 , batch: 284 , training loss: 4.265938\n",
      "[INFO] Epoch: 6 , batch: 285 , training loss: 4.265994\n",
      "[INFO] Epoch: 6 , batch: 286 , training loss: 4.214518\n",
      "[INFO] Epoch: 6 , batch: 287 , training loss: 4.081336\n",
      "[INFO] Epoch: 6 , batch: 288 , training loss: 4.130320\n",
      "[INFO] Epoch: 6 , batch: 289 , training loss: 4.180126\n",
      "[INFO] Epoch: 6 , batch: 290 , training loss: 3.957741\n",
      "[INFO] Epoch: 6 , batch: 291 , training loss: 3.958160\n",
      "[INFO] Epoch: 6 , batch: 292 , training loss: 4.050913\n",
      "[INFO] Epoch: 6 , batch: 293 , training loss: 3.998152\n",
      "[INFO] Epoch: 6 , batch: 294 , training loss: 4.691560\n",
      "[INFO] Epoch: 6 , batch: 295 , training loss: 4.445471\n",
      "[INFO] Epoch: 6 , batch: 296 , training loss: 4.356623\n",
      "[INFO] Epoch: 6 , batch: 297 , training loss: 4.295342\n",
      "[INFO] Epoch: 6 , batch: 298 , training loss: 4.142214\n",
      "[INFO] Epoch: 6 , batch: 299 , training loss: 4.157727\n",
      "[INFO] Epoch: 6 , batch: 300 , training loss: 4.132062\n",
      "[INFO] Epoch: 6 , batch: 301 , training loss: 4.064250\n",
      "[INFO] Epoch: 6 , batch: 302 , training loss: 4.253359\n",
      "[INFO] Epoch: 6 , batch: 303 , training loss: 4.253049\n",
      "[INFO] Epoch: 6 , batch: 304 , training loss: 4.443172\n",
      "[INFO] Epoch: 6 , batch: 305 , training loss: 4.186199\n",
      "[INFO] Epoch: 6 , batch: 306 , training loss: 4.334126\n",
      "[INFO] Epoch: 6 , batch: 307 , training loss: 4.310608\n",
      "[INFO] Epoch: 6 , batch: 308 , training loss: 4.201289\n",
      "[INFO] Epoch: 6 , batch: 309 , training loss: 4.197762\n",
      "[INFO] Epoch: 6 , batch: 310 , training loss: 4.023338\n",
      "[INFO] Epoch: 6 , batch: 311 , training loss: 4.074442\n",
      "[INFO] Epoch: 6 , batch: 312 , training loss: 3.953350\n",
      "[INFO] Epoch: 6 , batch: 313 , training loss: 4.116343\n",
      "[INFO] Epoch: 6 , batch: 314 , training loss: 4.179522\n",
      "[INFO] Epoch: 6 , batch: 315 , training loss: 4.223888\n",
      "[INFO] Epoch: 6 , batch: 316 , training loss: 4.549016\n",
      "[INFO] Epoch: 6 , batch: 317 , training loss: 5.065187\n",
      "[INFO] Epoch: 6 , batch: 318 , training loss: 5.165494\n",
      "[INFO] Epoch: 6 , batch: 319 , training loss: 4.754412\n",
      "[INFO] Epoch: 6 , batch: 320 , training loss: 4.224576\n",
      "[INFO] Epoch: 6 , batch: 321 , training loss: 4.018950\n",
      "[INFO] Epoch: 6 , batch: 322 , training loss: 4.157779\n",
      "[INFO] Epoch: 6 , batch: 323 , training loss: 4.161832\n",
      "[INFO] Epoch: 6 , batch: 324 , training loss: 4.159142\n",
      "[INFO] Epoch: 6 , batch: 325 , training loss: 4.302953\n",
      "[INFO] Epoch: 6 , batch: 326 , training loss: 4.336787\n",
      "[INFO] Epoch: 6 , batch: 327 , training loss: 4.257685\n",
      "[INFO] Epoch: 6 , batch: 328 , training loss: 4.243668\n",
      "[INFO] Epoch: 6 , batch: 329 , training loss: 4.142550\n",
      "[INFO] Epoch: 6 , batch: 330 , training loss: 4.126505\n",
      "[INFO] Epoch: 6 , batch: 331 , training loss: 4.335138\n",
      "[INFO] Epoch: 6 , batch: 332 , training loss: 4.097791\n",
      "[INFO] Epoch: 6 , batch: 333 , training loss: 4.083297\n",
      "[INFO] Epoch: 6 , batch: 334 , training loss: 4.166843\n",
      "[INFO] Epoch: 6 , batch: 335 , training loss: 4.295269\n",
      "[INFO] Epoch: 6 , batch: 336 , training loss: 4.276740\n",
      "[INFO] Epoch: 6 , batch: 337 , training loss: 4.349452\n",
      "[INFO] Epoch: 6 , batch: 338 , training loss: 4.538488\n",
      "[INFO] Epoch: 6 , batch: 339 , training loss: 4.347030\n",
      "[INFO] Epoch: 6 , batch: 340 , training loss: 4.548279\n",
      "[INFO] Epoch: 6 , batch: 341 , training loss: 4.270741\n",
      "[INFO] Epoch: 6 , batch: 342 , training loss: 4.106864\n",
      "[INFO] Epoch: 6 , batch: 343 , training loss: 4.170450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 6 , batch: 344 , training loss: 4.007492\n",
      "[INFO] Epoch: 6 , batch: 345 , training loss: 4.151242\n",
      "[INFO] Epoch: 6 , batch: 346 , training loss: 4.197357\n",
      "[INFO] Epoch: 6 , batch: 347 , training loss: 4.112768\n",
      "[INFO] Epoch: 6 , batch: 348 , training loss: 4.279962\n",
      "[INFO] Epoch: 6 , batch: 349 , training loss: 4.359607\n",
      "[INFO] Epoch: 6 , batch: 350 , training loss: 4.132475\n",
      "[INFO] Epoch: 6 , batch: 351 , training loss: 4.225561\n",
      "[INFO] Epoch: 6 , batch: 352 , training loss: 4.234293\n",
      "[INFO] Epoch: 6 , batch: 353 , training loss: 4.187037\n",
      "[INFO] Epoch: 6 , batch: 354 , training loss: 4.307005\n",
      "[INFO] Epoch: 6 , batch: 355 , training loss: 4.351105\n",
      "[INFO] Epoch: 6 , batch: 356 , training loss: 4.207219\n",
      "[INFO] Epoch: 6 , batch: 357 , training loss: 4.266009\n",
      "[INFO] Epoch: 6 , batch: 358 , training loss: 4.200161\n",
      "[INFO] Epoch: 6 , batch: 359 , training loss: 4.195029\n",
      "[INFO] Epoch: 6 , batch: 360 , training loss: 4.236523\n",
      "[INFO] Epoch: 6 , batch: 361 , training loss: 4.204577\n",
      "[INFO] Epoch: 6 , batch: 362 , training loss: 4.301344\n",
      "[INFO] Epoch: 6 , batch: 363 , training loss: 4.220972\n",
      "[INFO] Epoch: 6 , batch: 364 , training loss: 4.242988\n",
      "[INFO] Epoch: 6 , batch: 365 , training loss: 4.139893\n",
      "[INFO] Epoch: 6 , batch: 366 , training loss: 4.300714\n",
      "[INFO] Epoch: 6 , batch: 367 , training loss: 4.365772\n",
      "[INFO] Epoch: 6 , batch: 368 , training loss: 4.896791\n",
      "[INFO] Epoch: 6 , batch: 369 , training loss: 4.512473\n",
      "[INFO] Epoch: 6 , batch: 370 , training loss: 4.244745\n",
      "[INFO] Epoch: 6 , batch: 371 , training loss: 4.743169\n",
      "[INFO] Epoch: 6 , batch: 372 , training loss: 5.040549\n",
      "[INFO] Epoch: 6 , batch: 373 , training loss: 5.052860\n",
      "[INFO] Epoch: 6 , batch: 374 , training loss: 5.170820\n",
      "[INFO] Epoch: 6 , batch: 375 , training loss: 5.081015\n",
      "[INFO] Epoch: 6 , batch: 376 , training loss: 4.998477\n",
      "[INFO] Epoch: 6 , batch: 377 , training loss: 4.705643\n",
      "[INFO] Epoch: 6 , batch: 378 , training loss: 4.794023\n",
      "[INFO] Epoch: 6 , batch: 379 , training loss: 4.822212\n",
      "[INFO] Epoch: 6 , batch: 380 , training loss: 4.954966\n",
      "[INFO] Epoch: 6 , batch: 381 , training loss: 4.729482\n",
      "[INFO] Epoch: 6 , batch: 382 , training loss: 4.977876\n",
      "[INFO] Epoch: 6 , batch: 383 , training loss: 5.019632\n",
      "[INFO] Epoch: 6 , batch: 384 , training loss: 5.054350\n",
      "[INFO] Epoch: 6 , batch: 385 , training loss: 4.809639\n",
      "[INFO] Epoch: 6 , batch: 386 , training loss: 4.958952\n",
      "[INFO] Epoch: 6 , batch: 387 , training loss: 4.913377\n",
      "[INFO] Epoch: 6 , batch: 388 , training loss: 4.688731\n",
      "[INFO] Epoch: 6 , batch: 389 , training loss: 4.510361\n",
      "[INFO] Epoch: 6 , batch: 390 , training loss: 4.489190\n",
      "[INFO] Epoch: 6 , batch: 391 , training loss: 4.509717\n",
      "[INFO] Epoch: 6 , batch: 392 , training loss: 4.863511\n",
      "[INFO] Epoch: 6 , batch: 393 , training loss: 4.766519\n",
      "[INFO] Epoch: 6 , batch: 394 , training loss: 4.836895\n",
      "[INFO] Epoch: 6 , batch: 395 , training loss: 4.631871\n",
      "[INFO] Epoch: 6 , batch: 396 , training loss: 4.427796\n",
      "[INFO] Epoch: 6 , batch: 397 , training loss: 4.595223\n",
      "[INFO] Epoch: 6 , batch: 398 , training loss: 4.455874\n",
      "[INFO] Epoch: 6 , batch: 399 , training loss: 4.511514\n",
      "[INFO] Epoch: 6 , batch: 400 , training loss: 4.473096\n",
      "[INFO] Epoch: 6 , batch: 401 , training loss: 4.897564\n",
      "[INFO] Epoch: 6 , batch: 402 , training loss: 4.629676\n",
      "[INFO] Epoch: 6 , batch: 403 , training loss: 4.466979\n",
      "[INFO] Epoch: 6 , batch: 404 , training loss: 4.609851\n",
      "[INFO] Epoch: 6 , batch: 405 , training loss: 4.706959\n",
      "[INFO] Epoch: 6 , batch: 406 , training loss: 4.576126\n",
      "[INFO] Epoch: 6 , batch: 407 , training loss: 4.637487\n",
      "[INFO] Epoch: 6 , batch: 408 , training loss: 4.547810\n",
      "[INFO] Epoch: 6 , batch: 409 , training loss: 4.578979\n",
      "[INFO] Epoch: 6 , batch: 410 , training loss: 4.627709\n",
      "[INFO] Epoch: 6 , batch: 411 , training loss: 4.818821\n",
      "[INFO] Epoch: 6 , batch: 412 , training loss: 4.633858\n",
      "[INFO] Epoch: 6 , batch: 413 , training loss: 4.495373\n",
      "[INFO] Epoch: 6 , batch: 414 , training loss: 4.548670\n",
      "[INFO] Epoch: 6 , batch: 415 , training loss: 4.587338\n",
      "[INFO] Epoch: 6 , batch: 416 , training loss: 4.658099\n",
      "[INFO] Epoch: 6 , batch: 417 , training loss: 4.563915\n",
      "[INFO] Epoch: 6 , batch: 418 , training loss: 4.600081\n",
      "[INFO] Epoch: 6 , batch: 419 , training loss: 4.542059\n",
      "[INFO] Epoch: 6 , batch: 420 , training loss: 4.518079\n",
      "[INFO] Epoch: 6 , batch: 421 , training loss: 4.527565\n",
      "[INFO] Epoch: 6 , batch: 422 , training loss: 4.397330\n",
      "[INFO] Epoch: 6 , batch: 423 , training loss: 4.647305\n",
      "[INFO] Epoch: 6 , batch: 424 , training loss: 4.768260\n",
      "[INFO] Epoch: 6 , batch: 425 , training loss: 4.675331\n",
      "[INFO] Epoch: 6 , batch: 426 , training loss: 4.328192\n",
      "[INFO] Epoch: 6 , batch: 427 , training loss: 4.635159\n",
      "[INFO] Epoch: 6 , batch: 428 , training loss: 4.515784\n",
      "[INFO] Epoch: 6 , batch: 429 , training loss: 4.354015\n",
      "[INFO] Epoch: 6 , batch: 430 , training loss: 4.652802\n",
      "[INFO] Epoch: 6 , batch: 431 , training loss: 4.202809\n",
      "[INFO] Epoch: 6 , batch: 432 , training loss: 4.278890\n",
      "[INFO] Epoch: 6 , batch: 433 , training loss: 4.280762\n",
      "[INFO] Epoch: 6 , batch: 434 , training loss: 4.185980\n",
      "[INFO] Epoch: 6 , batch: 435 , training loss: 4.529309\n",
      "[INFO] Epoch: 6 , batch: 436 , training loss: 4.612801\n",
      "[INFO] Epoch: 6 , batch: 437 , training loss: 4.401852\n",
      "[INFO] Epoch: 6 , batch: 438 , training loss: 4.204197\n",
      "[INFO] Epoch: 6 , batch: 439 , training loss: 4.428953\n",
      "[INFO] Epoch: 6 , batch: 440 , training loss: 4.613000\n",
      "[INFO] Epoch: 6 , batch: 441 , training loss: 4.639892\n",
      "[INFO] Epoch: 6 , batch: 442 , training loss: 4.427034\n",
      "[INFO] Epoch: 6 , batch: 443 , training loss: 4.645734\n",
      "[INFO] Epoch: 6 , batch: 444 , training loss: 4.243083\n",
      "[INFO] Epoch: 6 , batch: 445 , training loss: 4.121488\n",
      "[INFO] Epoch: 6 , batch: 446 , training loss: 4.041535\n",
      "[INFO] Epoch: 6 , batch: 447 , training loss: 4.262427\n",
      "[INFO] Epoch: 6 , batch: 448 , training loss: 4.419292\n",
      "[INFO] Epoch: 6 , batch: 449 , training loss: 4.836967\n",
      "[INFO] Epoch: 6 , batch: 450 , training loss: 4.906427\n",
      "[INFO] Epoch: 6 , batch: 451 , training loss: 4.773284\n",
      "[INFO] Epoch: 6 , batch: 452 , training loss: 4.533031\n",
      "[INFO] Epoch: 6 , batch: 453 , training loss: 4.319832\n",
      "[INFO] Epoch: 6 , batch: 454 , training loss: 4.470139\n",
      "[INFO] Epoch: 6 , batch: 455 , training loss: 4.502323\n",
      "[INFO] Epoch: 6 , batch: 456 , training loss: 4.478425\n",
      "[INFO] Epoch: 6 , batch: 457 , training loss: 4.578576\n",
      "[INFO] Epoch: 6 , batch: 458 , training loss: 4.306451\n",
      "[INFO] Epoch: 6 , batch: 459 , training loss: 4.292510\n",
      "[INFO] Epoch: 6 , batch: 460 , training loss: 4.426245\n",
      "[INFO] Epoch: 6 , batch: 461 , training loss: 4.399450\n",
      "[INFO] Epoch: 6 , batch: 462 , training loss: 4.461182\n",
      "[INFO] Epoch: 6 , batch: 463 , training loss: 4.349338\n",
      "[INFO] Epoch: 6 , batch: 464 , training loss: 4.527509\n",
      "[INFO] Epoch: 6 , batch: 465 , training loss: 4.465775\n",
      "[INFO] Epoch: 6 , batch: 466 , training loss: 4.561405\n",
      "[INFO] Epoch: 6 , batch: 467 , training loss: 4.585078\n",
      "[INFO] Epoch: 6 , batch: 468 , training loss: 4.529384\n",
      "[INFO] Epoch: 6 , batch: 469 , training loss: 4.536723\n",
      "[INFO] Epoch: 6 , batch: 470 , training loss: 4.307576\n",
      "[INFO] Epoch: 6 , batch: 471 , training loss: 4.442290\n",
      "[INFO] Epoch: 6 , batch: 472 , training loss: 4.510957\n",
      "[INFO] Epoch: 6 , batch: 473 , training loss: 4.419898\n",
      "[INFO] Epoch: 6 , batch: 474 , training loss: 4.226523\n",
      "[INFO] Epoch: 6 , batch: 475 , training loss: 4.076952\n",
      "[INFO] Epoch: 6 , batch: 476 , training loss: 4.483565\n",
      "[INFO] Epoch: 6 , batch: 477 , training loss: 4.591937\n",
      "[INFO] Epoch: 6 , batch: 478 , training loss: 4.637609\n",
      "[INFO] Epoch: 6 , batch: 479 , training loss: 4.573502\n",
      "[INFO] Epoch: 6 , batch: 480 , training loss: 4.714115\n",
      "[INFO] Epoch: 6 , batch: 481 , training loss: 4.568264\n",
      "[INFO] Epoch: 6 , batch: 482 , training loss: 4.695951\n",
      "[INFO] Epoch: 6 , batch: 483 , training loss: 4.522100\n",
      "[INFO] Epoch: 6 , batch: 484 , training loss: 4.314167\n",
      "[INFO] Epoch: 6 , batch: 485 , training loss: 4.471796\n",
      "[INFO] Epoch: 6 , batch: 486 , training loss: 4.313519\n",
      "[INFO] Epoch: 6 , batch: 487 , training loss: 4.328050\n",
      "[INFO] Epoch: 6 , batch: 488 , training loss: 4.488284\n",
      "[INFO] Epoch: 6 , batch: 489 , training loss: 4.392753\n",
      "[INFO] Epoch: 6 , batch: 490 , training loss: 4.476712\n",
      "[INFO] Epoch: 6 , batch: 491 , training loss: 4.424115\n",
      "[INFO] Epoch: 6 , batch: 492 , training loss: 4.341495\n",
      "[INFO] Epoch: 6 , batch: 493 , training loss: 4.522362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 6 , batch: 494 , training loss: 4.432868\n",
      "[INFO] Epoch: 6 , batch: 495 , training loss: 4.548522\n",
      "[INFO] Epoch: 6 , batch: 496 , training loss: 4.449282\n",
      "[INFO] Epoch: 6 , batch: 497 , training loss: 4.470452\n",
      "[INFO] Epoch: 6 , batch: 498 , training loss: 4.498553\n",
      "[INFO] Epoch: 6 , batch: 499 , training loss: 4.543126\n",
      "[INFO] Epoch: 6 , batch: 500 , training loss: 4.746325\n",
      "[INFO] Epoch: 6 , batch: 501 , training loss: 5.161888\n",
      "[INFO] Epoch: 6 , batch: 502 , training loss: 5.309732\n",
      "[INFO] Epoch: 6 , batch: 503 , training loss: 5.038728\n",
      "[INFO] Epoch: 6 , batch: 504 , training loss: 5.134815\n",
      "[INFO] Epoch: 6 , batch: 505 , training loss: 5.025667\n",
      "[INFO] Epoch: 6 , batch: 506 , training loss: 4.966291\n",
      "[INFO] Epoch: 6 , batch: 507 , training loss: 5.002390\n",
      "[INFO] Epoch: 6 , batch: 508 , training loss: 4.951136\n",
      "[INFO] Epoch: 6 , batch: 509 , training loss: 4.707098\n",
      "[INFO] Epoch: 6 , batch: 510 , training loss: 4.782787\n",
      "[INFO] Epoch: 6 , batch: 511 , training loss: 4.663652\n",
      "[INFO] Epoch: 6 , batch: 512 , training loss: 4.722735\n",
      "[INFO] Epoch: 6 , batch: 513 , training loss: 4.980556\n",
      "[INFO] Epoch: 6 , batch: 514 , training loss: 4.627532\n",
      "[INFO] Epoch: 6 , batch: 515 , training loss: 4.884230\n",
      "[INFO] Epoch: 6 , batch: 516 , training loss: 4.681319\n",
      "[INFO] Epoch: 6 , batch: 517 , training loss: 4.687768\n",
      "[INFO] Epoch: 6 , batch: 518 , training loss: 4.634285\n",
      "[INFO] Epoch: 6 , batch: 519 , training loss: 4.504350\n",
      "[INFO] Epoch: 6 , batch: 520 , training loss: 4.729991\n",
      "[INFO] Epoch: 6 , batch: 521 , training loss: 4.709846\n",
      "[INFO] Epoch: 6 , batch: 522 , training loss: 4.805579\n",
      "[INFO] Epoch: 6 , batch: 523 , training loss: 4.668731\n",
      "[INFO] Epoch: 6 , batch: 524 , training loss: 4.956325\n",
      "[INFO] Epoch: 6 , batch: 525 , training loss: 4.855968\n",
      "[INFO] Epoch: 6 , batch: 526 , training loss: 4.594046\n",
      "[INFO] Epoch: 6 , batch: 527 , training loss: 4.630756\n",
      "[INFO] Epoch: 6 , batch: 528 , training loss: 4.697933\n",
      "[INFO] Epoch: 6 , batch: 529 , training loss: 4.669963\n",
      "[INFO] Epoch: 6 , batch: 530 , training loss: 4.511262\n",
      "[INFO] Epoch: 6 , batch: 531 , training loss: 4.671744\n",
      "[INFO] Epoch: 6 , batch: 532 , training loss: 4.515300\n",
      "[INFO] Epoch: 6 , batch: 533 , training loss: 4.673573\n",
      "[INFO] Epoch: 6 , batch: 534 , training loss: 4.667456\n",
      "[INFO] Epoch: 6 , batch: 535 , training loss: 4.670820\n",
      "[INFO] Epoch: 6 , batch: 536 , training loss: 4.551979\n",
      "[INFO] Epoch: 6 , batch: 537 , training loss: 4.500613\n",
      "[INFO] Epoch: 6 , batch: 538 , training loss: 4.581914\n",
      "[INFO] Epoch: 6 , batch: 539 , training loss: 4.757096\n",
      "[INFO] Epoch: 6 , batch: 540 , training loss: 5.373518\n",
      "[INFO] Epoch: 6 , batch: 541 , training loss: 5.315114\n",
      "[INFO] Epoch: 6 , batch: 542 , training loss: 5.117454\n",
      "[INFO] Epoch: 7 , batch: 0 , training loss: 4.462996\n",
      "[INFO] Epoch: 7 , batch: 1 , training loss: 4.326893\n",
      "[INFO] Epoch: 7 , batch: 2 , training loss: 4.190454\n",
      "[INFO] Epoch: 7 , batch: 3 , training loss: 4.097365\n",
      "[INFO] Epoch: 7 , batch: 4 , training loss: 4.408688\n",
      "[INFO] Epoch: 7 , batch: 5 , training loss: 4.097728\n",
      "[INFO] Epoch: 7 , batch: 6 , training loss: 4.619953\n",
      "[INFO] Epoch: 7 , batch: 7 , training loss: 4.257300\n",
      "[INFO] Epoch: 7 , batch: 8 , training loss: 3.924993\n",
      "[INFO] Epoch: 7 , batch: 9 , training loss: 4.151206\n",
      "[INFO] Epoch: 7 , batch: 10 , training loss: 4.036794\n",
      "[INFO] Epoch: 7 , batch: 11 , training loss: 3.975166\n",
      "[INFO] Epoch: 7 , batch: 12 , training loss: 4.002979\n",
      "[INFO] Epoch: 7 , batch: 13 , training loss: 3.992604\n",
      "[INFO] Epoch: 7 , batch: 14 , training loss: 3.776954\n",
      "[INFO] Epoch: 7 , batch: 15 , training loss: 4.001692\n",
      "[INFO] Epoch: 7 , batch: 16 , training loss: 3.892582\n",
      "[INFO] Epoch: 7 , batch: 17 , training loss: 4.041176\n",
      "[INFO] Epoch: 7 , batch: 18 , training loss: 3.985308\n",
      "[INFO] Epoch: 7 , batch: 19 , training loss: 3.740403\n",
      "[INFO] Epoch: 7 , batch: 20 , training loss: 3.728133\n",
      "[INFO] Epoch: 7 , batch: 21 , training loss: 3.852003\n",
      "[INFO] Epoch: 7 , batch: 22 , training loss: 3.874449\n",
      "[INFO] Epoch: 7 , batch: 23 , training loss: 4.021835\n",
      "[INFO] Epoch: 7 , batch: 24 , training loss: 3.922310\n",
      "[INFO] Epoch: 7 , batch: 25 , training loss: 3.990984\n",
      "[INFO] Epoch: 7 , batch: 26 , training loss: 3.868072\n",
      "[INFO] Epoch: 7 , batch: 27 , training loss: 3.848576\n",
      "[INFO] Epoch: 7 , batch: 28 , training loss: 4.041722\n",
      "[INFO] Epoch: 7 , batch: 29 , training loss: 3.819383\n",
      "[INFO] Epoch: 7 , batch: 30 , training loss: 3.831822\n",
      "[INFO] Epoch: 7 , batch: 31 , training loss: 3.928643\n",
      "[INFO] Epoch: 7 , batch: 32 , training loss: 3.930537\n",
      "[INFO] Epoch: 7 , batch: 33 , training loss: 4.032495\n",
      "[INFO] Epoch: 7 , batch: 34 , training loss: 4.013228\n",
      "[INFO] Epoch: 7 , batch: 35 , training loss: 3.901072\n",
      "[INFO] Epoch: 7 , batch: 36 , training loss: 3.931902\n",
      "[INFO] Epoch: 7 , batch: 37 , training loss: 3.835962\n",
      "[INFO] Epoch: 7 , batch: 38 , training loss: 3.959019\n",
      "[INFO] Epoch: 7 , batch: 39 , training loss: 3.763540\n",
      "[INFO] Epoch: 7 , batch: 40 , training loss: 3.931688\n",
      "[INFO] Epoch: 7 , batch: 41 , training loss: 4.069556\n",
      "[INFO] Epoch: 7 , batch: 42 , training loss: 4.626209\n",
      "[INFO] Epoch: 7 , batch: 43 , training loss: 4.281251\n",
      "[INFO] Epoch: 7 , batch: 44 , training loss: 4.508639\n",
      "[INFO] Epoch: 7 , batch: 45 , training loss: 4.623359\n",
      "[INFO] Epoch: 7 , batch: 46 , training loss: 4.782202\n",
      "[INFO] Epoch: 7 , batch: 47 , training loss: 4.274207\n",
      "[INFO] Epoch: 7 , batch: 48 , training loss: 4.255823\n",
      "[INFO] Epoch: 7 , batch: 49 , training loss: 4.354051\n",
      "[INFO] Epoch: 7 , batch: 50 , training loss: 4.117873\n",
      "[INFO] Epoch: 7 , batch: 51 , training loss: 4.194685\n",
      "[INFO] Epoch: 7 , batch: 52 , training loss: 3.997249\n",
      "[INFO] Epoch: 7 , batch: 53 , training loss: 4.136595\n",
      "[INFO] Epoch: 7 , batch: 54 , training loss: 4.162178\n",
      "[INFO] Epoch: 7 , batch: 55 , training loss: 4.221375\n",
      "[INFO] Epoch: 7 , batch: 56 , training loss: 4.062354\n",
      "[INFO] Epoch: 7 , batch: 57 , training loss: 3.962604\n",
      "[INFO] Epoch: 7 , batch: 58 , training loss: 3.985914\n",
      "[INFO] Epoch: 7 , batch: 59 , training loss: 4.078407\n",
      "[INFO] Epoch: 7 , batch: 60 , training loss: 3.935534\n",
      "[INFO] Epoch: 7 , batch: 61 , training loss: 4.033229\n",
      "[INFO] Epoch: 7 , batch: 62 , training loss: 3.911855\n",
      "[INFO] Epoch: 7 , batch: 63 , training loss: 4.110130\n",
      "[INFO] Epoch: 7 , batch: 64 , training loss: 4.267936\n",
      "[INFO] Epoch: 7 , batch: 65 , training loss: 3.990622\n",
      "[INFO] Epoch: 7 , batch: 66 , training loss: 3.836653\n",
      "[INFO] Epoch: 7 , batch: 67 , training loss: 3.843763\n",
      "[INFO] Epoch: 7 , batch: 68 , training loss: 4.151403\n",
      "[INFO] Epoch: 7 , batch: 69 , training loss: 3.957989\n",
      "[INFO] Epoch: 7 , batch: 70 , training loss: 4.229033\n",
      "[INFO] Epoch: 7 , batch: 71 , training loss: 4.049956\n",
      "[INFO] Epoch: 7 , batch: 72 , training loss: 4.118686\n",
      "[INFO] Epoch: 7 , batch: 73 , training loss: 4.049121\n",
      "[INFO] Epoch: 7 , batch: 74 , training loss: 4.182023\n",
      "[INFO] Epoch: 7 , batch: 75 , training loss: 3.949299\n",
      "[INFO] Epoch: 7 , batch: 76 , training loss: 4.112917\n",
      "[INFO] Epoch: 7 , batch: 77 , training loss: 4.003499\n",
      "[INFO] Epoch: 7 , batch: 78 , training loss: 4.136250\n",
      "[INFO] Epoch: 7 , batch: 79 , training loss: 3.918789\n",
      "[INFO] Epoch: 7 , batch: 80 , training loss: 4.154250\n",
      "[INFO] Epoch: 7 , batch: 81 , training loss: 4.101277\n",
      "[INFO] Epoch: 7 , batch: 82 , training loss: 4.091796\n",
      "[INFO] Epoch: 7 , batch: 83 , training loss: 4.190568\n",
      "[INFO] Epoch: 7 , batch: 84 , training loss: 4.138026\n",
      "[INFO] Epoch: 7 , batch: 85 , training loss: 4.224173\n",
      "[INFO] Epoch: 7 , batch: 86 , training loss: 4.163990\n",
      "[INFO] Epoch: 7 , batch: 87 , training loss: 4.129457\n",
      "[INFO] Epoch: 7 , batch: 88 , training loss: 4.300647\n",
      "[INFO] Epoch: 7 , batch: 89 , training loss: 4.047169\n",
      "[INFO] Epoch: 7 , batch: 90 , training loss: 4.142478\n",
      "[INFO] Epoch: 7 , batch: 91 , training loss: 4.062798\n",
      "[INFO] Epoch: 7 , batch: 92 , training loss: 4.080020\n",
      "[INFO] Epoch: 7 , batch: 93 , training loss: 4.174469\n",
      "[INFO] Epoch: 7 , batch: 94 , training loss: 4.334429\n",
      "[INFO] Epoch: 7 , batch: 95 , training loss: 4.117250\n",
      "[INFO] Epoch: 7 , batch: 96 , training loss: 4.070292\n",
      "[INFO] Epoch: 7 , batch: 97 , training loss: 4.066454\n",
      "[INFO] Epoch: 7 , batch: 98 , training loss: 4.014853\n",
      "[INFO] Epoch: 7 , batch: 99 , training loss: 4.120935\n",
      "[INFO] Epoch: 7 , batch: 100 , training loss: 3.955699\n",
      "[INFO] Epoch: 7 , batch: 101 , training loss: 3.997340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 7 , batch: 102 , training loss: 4.189031\n",
      "[INFO] Epoch: 7 , batch: 103 , training loss: 3.906574\n",
      "[INFO] Epoch: 7 , batch: 104 , training loss: 3.856691\n",
      "[INFO] Epoch: 7 , batch: 105 , training loss: 4.171866\n",
      "[INFO] Epoch: 7 , batch: 106 , training loss: 4.182535\n",
      "[INFO] Epoch: 7 , batch: 107 , training loss: 4.025460\n",
      "[INFO] Epoch: 7 , batch: 108 , training loss: 3.936785\n",
      "[INFO] Epoch: 7 , batch: 109 , training loss: 3.809929\n",
      "[INFO] Epoch: 7 , batch: 110 , training loss: 4.095433\n",
      "[INFO] Epoch: 7 , batch: 111 , training loss: 4.123893\n",
      "[INFO] Epoch: 7 , batch: 112 , training loss: 4.079118\n",
      "[INFO] Epoch: 7 , batch: 113 , training loss: 4.024203\n",
      "[INFO] Epoch: 7 , batch: 114 , training loss: 4.121686\n",
      "[INFO] Epoch: 7 , batch: 115 , training loss: 4.040012\n",
      "[INFO] Epoch: 7 , batch: 116 , training loss: 4.027074\n",
      "[INFO] Epoch: 7 , batch: 117 , training loss: 4.267090\n",
      "[INFO] Epoch: 7 , batch: 118 , training loss: 4.216416\n",
      "[INFO] Epoch: 7 , batch: 119 , training loss: 4.337456\n",
      "[INFO] Epoch: 7 , batch: 120 , training loss: 4.296447\n",
      "[INFO] Epoch: 7 , batch: 121 , training loss: 4.108157\n",
      "[INFO] Epoch: 7 , batch: 122 , training loss: 4.016417\n",
      "[INFO] Epoch: 7 , batch: 123 , training loss: 4.101043\n",
      "[INFO] Epoch: 7 , batch: 124 , training loss: 4.241547\n",
      "[INFO] Epoch: 7 , batch: 125 , training loss: 3.938303\n",
      "[INFO] Epoch: 7 , batch: 126 , training loss: 3.965318\n",
      "[INFO] Epoch: 7 , batch: 127 , training loss: 4.007691\n",
      "[INFO] Epoch: 7 , batch: 128 , training loss: 4.179983\n",
      "[INFO] Epoch: 7 , batch: 129 , training loss: 4.068585\n",
      "[INFO] Epoch: 7 , batch: 130 , training loss: 4.121861\n",
      "[INFO] Epoch: 7 , batch: 131 , training loss: 4.091434\n",
      "[INFO] Epoch: 7 , batch: 132 , training loss: 4.123093\n",
      "[INFO] Epoch: 7 , batch: 133 , training loss: 4.081649\n",
      "[INFO] Epoch: 7 , batch: 134 , training loss: 3.802974\n",
      "[INFO] Epoch: 7 , batch: 135 , training loss: 3.872591\n",
      "[INFO] Epoch: 7 , batch: 136 , training loss: 4.181966\n",
      "[INFO] Epoch: 7 , batch: 137 , training loss: 4.093905\n",
      "[INFO] Epoch: 7 , batch: 138 , training loss: 4.195022\n",
      "[INFO] Epoch: 7 , batch: 139 , training loss: 4.837231\n",
      "[INFO] Epoch: 7 , batch: 140 , training loss: 4.715784\n",
      "[INFO] Epoch: 7 , batch: 141 , training loss: 4.406357\n",
      "[INFO] Epoch: 7 , batch: 142 , training loss: 4.022307\n",
      "[INFO] Epoch: 7 , batch: 143 , training loss: 4.158398\n",
      "[INFO] Epoch: 7 , batch: 144 , training loss: 4.001693\n",
      "[INFO] Epoch: 7 , batch: 145 , training loss: 4.123981\n",
      "[INFO] Epoch: 7 , batch: 146 , training loss: 4.331974\n",
      "[INFO] Epoch: 7 , batch: 147 , training loss: 3.939442\n",
      "[INFO] Epoch: 7 , batch: 148 , training loss: 3.941041\n",
      "[INFO] Epoch: 7 , batch: 149 , training loss: 4.016140\n",
      "[INFO] Epoch: 7 , batch: 150 , training loss: 4.293326\n",
      "[INFO] Epoch: 7 , batch: 151 , training loss: 4.045513\n",
      "[INFO] Epoch: 7 , batch: 152 , training loss: 4.065608\n",
      "[INFO] Epoch: 7 , batch: 153 , training loss: 4.157512\n",
      "[INFO] Epoch: 7 , batch: 154 , training loss: 4.205105\n",
      "[INFO] Epoch: 7 , batch: 155 , training loss: 4.419236\n",
      "[INFO] Epoch: 7 , batch: 156 , training loss: 4.124048\n",
      "[INFO] Epoch: 7 , batch: 157 , training loss: 4.119450\n",
      "[INFO] Epoch: 7 , batch: 158 , training loss: 4.410601\n",
      "[INFO] Epoch: 7 , batch: 159 , training loss: 4.303725\n",
      "[INFO] Epoch: 7 , batch: 160 , training loss: 4.739938\n",
      "[INFO] Epoch: 7 , batch: 161 , training loss: 4.657759\n",
      "[INFO] Epoch: 7 , batch: 162 , training loss: 4.563912\n",
      "[INFO] Epoch: 7 , batch: 163 , training loss: 4.753291\n",
      "[INFO] Epoch: 7 , batch: 164 , training loss: 4.619320\n",
      "[INFO] Epoch: 7 , batch: 165 , training loss: 4.572672\n",
      "[INFO] Epoch: 7 , batch: 166 , training loss: 4.651492\n",
      "[INFO] Epoch: 7 , batch: 167 , training loss: 5.010478\n",
      "[INFO] Epoch: 7 , batch: 168 , training loss: 4.738728\n",
      "[INFO] Epoch: 7 , batch: 169 , training loss: 4.612174\n",
      "[INFO] Epoch: 7 , batch: 170 , training loss: 4.642672\n",
      "[INFO] Epoch: 7 , batch: 171 , training loss: 4.169012\n",
      "[INFO] Epoch: 7 , batch: 172 , training loss: 4.313447\n",
      "[INFO] Epoch: 7 , batch: 173 , training loss: 4.557959\n",
      "[INFO] Epoch: 7 , batch: 174 , training loss: 4.995459\n",
      "[INFO] Epoch: 7 , batch: 175 , training loss: 5.189737\n",
      "[INFO] Epoch: 7 , batch: 176 , training loss: 4.988597\n",
      "[INFO] Epoch: 7 , batch: 177 , training loss: 4.521216\n",
      "[INFO] Epoch: 7 , batch: 178 , training loss: 4.433347\n",
      "[INFO] Epoch: 7 , batch: 179 , training loss: 4.520003\n",
      "[INFO] Epoch: 7 , batch: 180 , training loss: 4.434698\n",
      "[INFO] Epoch: 7 , batch: 181 , training loss: 4.683990\n",
      "[INFO] Epoch: 7 , batch: 182 , training loss: 4.570143\n",
      "[INFO] Epoch: 7 , batch: 183 , training loss: 4.528028\n",
      "[INFO] Epoch: 7 , batch: 184 , training loss: 4.441658\n",
      "[INFO] Epoch: 7 , batch: 185 , training loss: 4.398588\n",
      "[INFO] Epoch: 7 , batch: 186 , training loss: 4.543997\n",
      "[INFO] Epoch: 7 , batch: 187 , training loss: 4.630374\n",
      "[INFO] Epoch: 7 , batch: 188 , training loss: 4.595134\n",
      "[INFO] Epoch: 7 , batch: 189 , training loss: 4.509097\n",
      "[INFO] Epoch: 7 , batch: 190 , training loss: 4.526524\n",
      "[INFO] Epoch: 7 , batch: 191 , training loss: 4.658611\n",
      "[INFO] Epoch: 7 , batch: 192 , training loss: 4.429783\n",
      "[INFO] Epoch: 7 , batch: 193 , training loss: 4.566372\n",
      "[INFO] Epoch: 7 , batch: 194 , training loss: 4.475055\n",
      "[INFO] Epoch: 7 , batch: 195 , training loss: 4.535534\n",
      "[INFO] Epoch: 7 , batch: 196 , training loss: 4.285892\n",
      "[INFO] Epoch: 7 , batch: 197 , training loss: 4.434672\n",
      "[INFO] Epoch: 7 , batch: 198 , training loss: 4.287125\n",
      "[INFO] Epoch: 7 , batch: 199 , training loss: 4.410790\n",
      "[INFO] Epoch: 7 , batch: 200 , training loss: 4.341002\n",
      "[INFO] Epoch: 7 , batch: 201 , training loss: 4.243737\n",
      "[INFO] Epoch: 7 , batch: 202 , training loss: 4.230128\n",
      "[INFO] Epoch: 7 , batch: 203 , training loss: 4.288141\n",
      "[INFO] Epoch: 7 , batch: 204 , training loss: 4.390965\n",
      "[INFO] Epoch: 7 , batch: 205 , training loss: 3.998849\n",
      "[INFO] Epoch: 7 , batch: 206 , training loss: 3.917048\n",
      "[INFO] Epoch: 7 , batch: 207 , training loss: 3.918987\n",
      "[INFO] Epoch: 7 , batch: 208 , training loss: 4.286654\n",
      "[INFO] Epoch: 7 , batch: 209 , training loss: 4.210005\n",
      "[INFO] Epoch: 7 , batch: 210 , training loss: 4.264852\n",
      "[INFO] Epoch: 7 , batch: 211 , training loss: 4.245358\n",
      "[INFO] Epoch: 7 , batch: 212 , training loss: 4.357653\n",
      "[INFO] Epoch: 7 , batch: 213 , training loss: 4.319876\n",
      "[INFO] Epoch: 7 , batch: 214 , training loss: 4.376221\n",
      "[INFO] Epoch: 7 , batch: 215 , training loss: 4.597477\n",
      "[INFO] Epoch: 7 , batch: 216 , training loss: 4.310832\n",
      "[INFO] Epoch: 7 , batch: 217 , training loss: 4.266985\n",
      "[INFO] Epoch: 7 , batch: 218 , training loss: 4.236708\n",
      "[INFO] Epoch: 7 , batch: 219 , training loss: 4.349017\n",
      "[INFO] Epoch: 7 , batch: 220 , training loss: 4.157079\n",
      "[INFO] Epoch: 7 , batch: 221 , training loss: 4.164974\n",
      "[INFO] Epoch: 7 , batch: 222 , training loss: 4.331884\n",
      "[INFO] Epoch: 7 , batch: 223 , training loss: 4.396238\n",
      "[INFO] Epoch: 7 , batch: 224 , training loss: 4.457711\n",
      "[INFO] Epoch: 7 , batch: 225 , training loss: 4.310747\n",
      "[INFO] Epoch: 7 , batch: 226 , training loss: 4.448571\n",
      "[INFO] Epoch: 7 , batch: 227 , training loss: 4.426474\n",
      "[INFO] Epoch: 7 , batch: 228 , training loss: 4.465178\n",
      "[INFO] Epoch: 7 , batch: 229 , training loss: 4.320801\n",
      "[INFO] Epoch: 7 , batch: 230 , training loss: 4.173049\n",
      "[INFO] Epoch: 7 , batch: 231 , training loss: 3.991434\n",
      "[INFO] Epoch: 7 , batch: 232 , training loss: 4.167663\n",
      "[INFO] Epoch: 7 , batch: 233 , training loss: 4.212823\n",
      "[INFO] Epoch: 7 , batch: 234 , training loss: 3.863197\n",
      "[INFO] Epoch: 7 , batch: 235 , training loss: 4.000071\n",
      "[INFO] Epoch: 7 , batch: 236 , training loss: 4.156092\n",
      "[INFO] Epoch: 7 , batch: 237 , training loss: 4.337243\n",
      "[INFO] Epoch: 7 , batch: 238 , training loss: 4.083105\n",
      "[INFO] Epoch: 7 , batch: 239 , training loss: 4.137272\n",
      "[INFO] Epoch: 7 , batch: 240 , training loss: 4.205933\n",
      "[INFO] Epoch: 7 , batch: 241 , training loss: 3.968398\n",
      "[INFO] Epoch: 7 , batch: 242 , training loss: 3.996228\n",
      "[INFO] Epoch: 7 , batch: 243 , training loss: 4.334247\n",
      "[INFO] Epoch: 7 , batch: 244 , training loss: 4.244534\n",
      "[INFO] Epoch: 7 , batch: 245 , training loss: 4.255876\n",
      "[INFO] Epoch: 7 , batch: 246 , training loss: 3.897928\n",
      "[INFO] Epoch: 7 , batch: 247 , training loss: 4.066493\n",
      "[INFO] Epoch: 7 , batch: 248 , training loss: 4.166624\n",
      "[INFO] Epoch: 7 , batch: 249 , training loss: 4.138683\n",
      "[INFO] Epoch: 7 , batch: 250 , training loss: 3.918869\n",
      "[INFO] Epoch: 7 , batch: 251 , training loss: 4.436092\n",
      "[INFO] Epoch: 7 , batch: 252 , training loss: 4.069582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 7 , batch: 253 , training loss: 4.025575\n",
      "[INFO] Epoch: 7 , batch: 254 , training loss: 4.328507\n",
      "[INFO] Epoch: 7 , batch: 255 , training loss: 4.289217\n",
      "[INFO] Epoch: 7 , batch: 256 , training loss: 4.261568\n",
      "[INFO] Epoch: 7 , batch: 257 , training loss: 4.466117\n",
      "[INFO] Epoch: 7 , batch: 258 , training loss: 4.489979\n",
      "[INFO] Epoch: 7 , batch: 259 , training loss: 4.541776\n",
      "[INFO] Epoch: 7 , batch: 260 , training loss: 4.231857\n",
      "[INFO] Epoch: 7 , batch: 261 , training loss: 4.422221\n",
      "[INFO] Epoch: 7 , batch: 262 , training loss: 4.625586\n",
      "[INFO] Epoch: 7 , batch: 263 , training loss: 4.742576\n",
      "[INFO] Epoch: 7 , batch: 264 , training loss: 4.085682\n",
      "[INFO] Epoch: 7 , batch: 265 , training loss: 4.196269\n",
      "[INFO] Epoch: 7 , batch: 266 , training loss: 4.703182\n",
      "[INFO] Epoch: 7 , batch: 267 , training loss: 4.384506\n",
      "[INFO] Epoch: 7 , batch: 268 , training loss: 4.282007\n",
      "[INFO] Epoch: 7 , batch: 269 , training loss: 4.324600\n",
      "[INFO] Epoch: 7 , batch: 270 , training loss: 4.291266\n",
      "[INFO] Epoch: 7 , batch: 271 , training loss: 4.347827\n",
      "[INFO] Epoch: 7 , batch: 272 , training loss: 4.319684\n",
      "[INFO] Epoch: 7 , batch: 273 , training loss: 4.326713\n",
      "[INFO] Epoch: 7 , batch: 274 , training loss: 4.446872\n",
      "[INFO] Epoch: 7 , batch: 275 , training loss: 4.316517\n",
      "[INFO] Epoch: 7 , batch: 276 , training loss: 4.366390\n",
      "[INFO] Epoch: 7 , batch: 277 , training loss: 4.519391\n",
      "[INFO] Epoch: 7 , batch: 278 , training loss: 4.127779\n",
      "[INFO] Epoch: 7 , batch: 279 , training loss: 4.159703\n",
      "[INFO] Epoch: 7 , batch: 280 , training loss: 4.097025\n",
      "[INFO] Epoch: 7 , batch: 281 , training loss: 4.250197\n",
      "[INFO] Epoch: 7 , batch: 282 , training loss: 4.156607\n",
      "[INFO] Epoch: 7 , batch: 283 , training loss: 4.199138\n",
      "[INFO] Epoch: 7 , batch: 284 , training loss: 4.239464\n",
      "[INFO] Epoch: 7 , batch: 285 , training loss: 4.223829\n",
      "[INFO] Epoch: 7 , batch: 286 , training loss: 4.186060\n",
      "[INFO] Epoch: 7 , batch: 287 , training loss: 4.067084\n",
      "[INFO] Epoch: 7 , batch: 288 , training loss: 4.102938\n",
      "[INFO] Epoch: 7 , batch: 289 , training loss: 4.147817\n",
      "[INFO] Epoch: 7 , batch: 290 , training loss: 3.938506\n",
      "[INFO] Epoch: 7 , batch: 291 , training loss: 3.923288\n",
      "[INFO] Epoch: 7 , batch: 292 , training loss: 4.027680\n",
      "[INFO] Epoch: 7 , batch: 293 , training loss: 3.967916\n",
      "[INFO] Epoch: 7 , batch: 294 , training loss: 4.678179\n",
      "[INFO] Epoch: 7 , batch: 295 , training loss: 4.408748\n",
      "[INFO] Epoch: 7 , batch: 296 , training loss: 4.325799\n",
      "[INFO] Epoch: 7 , batch: 297 , training loss: 4.278807\n",
      "[INFO] Epoch: 7 , batch: 298 , training loss: 4.107424\n",
      "[INFO] Epoch: 7 , batch: 299 , training loss: 4.131262\n",
      "[INFO] Epoch: 7 , batch: 300 , training loss: 4.104271\n",
      "[INFO] Epoch: 7 , batch: 301 , training loss: 4.042705\n",
      "[INFO] Epoch: 7 , batch: 302 , training loss: 4.218682\n",
      "[INFO] Epoch: 7 , batch: 303 , training loss: 4.226683\n",
      "[INFO] Epoch: 7 , batch: 304 , training loss: 4.419294\n",
      "[INFO] Epoch: 7 , batch: 305 , training loss: 4.170636\n",
      "[INFO] Epoch: 7 , batch: 306 , training loss: 4.300276\n",
      "[INFO] Epoch: 7 , batch: 307 , training loss: 4.276897\n",
      "[INFO] Epoch: 7 , batch: 308 , training loss: 4.167612\n",
      "[INFO] Epoch: 7 , batch: 309 , training loss: 4.162176\n",
      "[INFO] Epoch: 7 , batch: 310 , training loss: 4.008926\n",
      "[INFO] Epoch: 7 , batch: 311 , training loss: 4.045485\n",
      "[INFO] Epoch: 7 , batch: 312 , training loss: 3.926747\n",
      "[INFO] Epoch: 7 , batch: 313 , training loss: 4.078258\n",
      "[INFO] Epoch: 7 , batch: 314 , training loss: 4.149596\n",
      "[INFO] Epoch: 7 , batch: 315 , training loss: 4.199936\n",
      "[INFO] Epoch: 7 , batch: 316 , training loss: 4.511927\n",
      "[INFO] Epoch: 7 , batch: 317 , training loss: 5.022818\n",
      "[INFO] Epoch: 7 , batch: 318 , training loss: 5.112012\n",
      "[INFO] Epoch: 7 , batch: 319 , training loss: 4.709362\n",
      "[INFO] Epoch: 7 , batch: 320 , training loss: 4.180535\n",
      "[INFO] Epoch: 7 , batch: 321 , training loss: 3.988561\n",
      "[INFO] Epoch: 7 , batch: 322 , training loss: 4.126048\n",
      "[INFO] Epoch: 7 , batch: 323 , training loss: 4.134420\n",
      "[INFO] Epoch: 7 , batch: 324 , training loss: 4.127561\n",
      "[INFO] Epoch: 7 , batch: 325 , training loss: 4.271981\n",
      "[INFO] Epoch: 7 , batch: 326 , training loss: 4.307800\n",
      "[INFO] Epoch: 7 , batch: 327 , training loss: 4.232069\n",
      "[INFO] Epoch: 7 , batch: 328 , training loss: 4.218208\n",
      "[INFO] Epoch: 7 , batch: 329 , training loss: 4.113107\n",
      "[INFO] Epoch: 7 , batch: 330 , training loss: 4.112259\n",
      "[INFO] Epoch: 7 , batch: 331 , training loss: 4.308832\n",
      "[INFO] Epoch: 7 , batch: 332 , training loss: 4.082578\n",
      "[INFO] Epoch: 7 , batch: 333 , training loss: 4.070745\n",
      "[INFO] Epoch: 7 , batch: 334 , training loss: 4.142587\n",
      "[INFO] Epoch: 7 , batch: 335 , training loss: 4.262516\n",
      "[INFO] Epoch: 7 , batch: 336 , training loss: 4.248608\n",
      "[INFO] Epoch: 7 , batch: 337 , training loss: 4.324724\n",
      "[INFO] Epoch: 7 , batch: 338 , training loss: 4.508361\n",
      "[INFO] Epoch: 7 , batch: 339 , training loss: 4.318926\n",
      "[INFO] Epoch: 7 , batch: 340 , training loss: 4.521461\n",
      "[INFO] Epoch: 7 , batch: 341 , training loss: 4.246479\n",
      "[INFO] Epoch: 7 , batch: 342 , training loss: 4.056106\n",
      "[INFO] Epoch: 7 , batch: 343 , training loss: 4.136021\n",
      "[INFO] Epoch: 7 , batch: 344 , training loss: 3.982712\n",
      "[INFO] Epoch: 7 , batch: 345 , training loss: 4.126529\n",
      "[INFO] Epoch: 7 , batch: 346 , training loss: 4.159509\n",
      "[INFO] Epoch: 7 , batch: 347 , training loss: 4.080340\n",
      "[INFO] Epoch: 7 , batch: 348 , training loss: 4.240675\n",
      "[INFO] Epoch: 7 , batch: 349 , training loss: 4.327226\n",
      "[INFO] Epoch: 7 , batch: 350 , training loss: 4.118597\n",
      "[INFO] Epoch: 7 , batch: 351 , training loss: 4.198862\n",
      "[INFO] Epoch: 7 , batch: 352 , training loss: 4.213544\n",
      "[INFO] Epoch: 7 , batch: 353 , training loss: 4.156924\n",
      "[INFO] Epoch: 7 , batch: 354 , training loss: 4.273477\n",
      "[INFO] Epoch: 7 , batch: 355 , training loss: 4.315169\n",
      "[INFO] Epoch: 7 , batch: 356 , training loss: 4.166265\n",
      "[INFO] Epoch: 7 , batch: 357 , training loss: 4.239070\n",
      "[INFO] Epoch: 7 , batch: 358 , training loss: 4.172738\n",
      "[INFO] Epoch: 7 , batch: 359 , training loss: 4.152306\n",
      "[INFO] Epoch: 7 , batch: 360 , training loss: 4.222794\n",
      "[INFO] Epoch: 7 , batch: 361 , training loss: 4.194387\n",
      "[INFO] Epoch: 7 , batch: 362 , training loss: 4.281415\n",
      "[INFO] Epoch: 7 , batch: 363 , training loss: 4.190410\n",
      "[INFO] Epoch: 7 , batch: 364 , training loss: 4.223781\n",
      "[INFO] Epoch: 7 , batch: 365 , training loss: 4.121609\n",
      "[INFO] Epoch: 7 , batch: 366 , training loss: 4.285057\n",
      "[INFO] Epoch: 7 , batch: 367 , training loss: 4.330274\n",
      "[INFO] Epoch: 7 , batch: 368 , training loss: 4.844133\n",
      "[INFO] Epoch: 7 , batch: 369 , training loss: 4.472450\n",
      "[INFO] Epoch: 7 , batch: 370 , training loss: 4.228813\n",
      "[INFO] Epoch: 7 , batch: 371 , training loss: 4.714997\n",
      "[INFO] Epoch: 7 , batch: 372 , training loss: 4.980742\n",
      "[INFO] Epoch: 7 , batch: 373 , training loss: 5.007219\n",
      "[INFO] Epoch: 7 , batch: 374 , training loss: 5.130210\n",
      "[INFO] Epoch: 7 , batch: 375 , training loss: 5.046453\n",
      "[INFO] Epoch: 7 , batch: 376 , training loss: 4.983100\n",
      "[INFO] Epoch: 7 , batch: 377 , training loss: 4.695884\n",
      "[INFO] Epoch: 7 , batch: 378 , training loss: 4.760923\n",
      "[INFO] Epoch: 7 , batch: 379 , training loss: 4.779507\n",
      "[INFO] Epoch: 7 , batch: 380 , training loss: 4.898847\n",
      "[INFO] Epoch: 7 , batch: 381 , training loss: 4.688732\n",
      "[INFO] Epoch: 7 , batch: 382 , training loss: 4.968504\n",
      "[INFO] Epoch: 7 , batch: 383 , training loss: 4.981951\n",
      "[INFO] Epoch: 7 , batch: 384 , training loss: 5.036275\n",
      "[INFO] Epoch: 7 , batch: 385 , training loss: 4.760889\n",
      "[INFO] Epoch: 7 , batch: 386 , training loss: 4.916127\n",
      "[INFO] Epoch: 7 , batch: 387 , training loss: 4.860398\n",
      "[INFO] Epoch: 7 , batch: 388 , training loss: 4.635872\n",
      "[INFO] Epoch: 7 , batch: 389 , training loss: 4.459808\n",
      "[INFO] Epoch: 7 , batch: 390 , training loss: 4.442347\n",
      "[INFO] Epoch: 7 , batch: 391 , training loss: 4.467861\n",
      "[INFO] Epoch: 7 , batch: 392 , training loss: 4.830457\n",
      "[INFO] Epoch: 7 , batch: 393 , training loss: 4.713928\n",
      "[INFO] Epoch: 7 , batch: 394 , training loss: 4.799557\n",
      "[INFO] Epoch: 7 , batch: 395 , training loss: 4.588551\n",
      "[INFO] Epoch: 7 , batch: 396 , training loss: 4.383584\n",
      "[INFO] Epoch: 7 , batch: 397 , training loss: 4.547602\n",
      "[INFO] Epoch: 7 , batch: 398 , training loss: 4.418985\n",
      "[INFO] Epoch: 7 , batch: 399 , training loss: 4.464911\n",
      "[INFO] Epoch: 7 , batch: 400 , training loss: 4.439808\n",
      "[INFO] Epoch: 7 , batch: 401 , training loss: 4.866449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 7 , batch: 402 , training loss: 4.590739\n",
      "[INFO] Epoch: 7 , batch: 403 , training loss: 4.436814\n",
      "[INFO] Epoch: 7 , batch: 404 , training loss: 4.588539\n",
      "[INFO] Epoch: 7 , batch: 405 , training loss: 4.664824\n",
      "[INFO] Epoch: 7 , batch: 406 , training loss: 4.543680\n",
      "[INFO] Epoch: 7 , batch: 407 , training loss: 4.602206\n",
      "[INFO] Epoch: 7 , batch: 408 , training loss: 4.515717\n",
      "[INFO] Epoch: 7 , batch: 409 , training loss: 4.544214\n",
      "[INFO] Epoch: 7 , batch: 410 , training loss: 4.604670\n",
      "[INFO] Epoch: 7 , batch: 411 , training loss: 4.789505\n",
      "[INFO] Epoch: 7 , batch: 412 , training loss: 4.610002\n",
      "[INFO] Epoch: 7 , batch: 413 , training loss: 4.481308\n",
      "[INFO] Epoch: 7 , batch: 414 , training loss: 4.515618\n",
      "[INFO] Epoch: 7 , batch: 415 , training loss: 4.562349\n",
      "[INFO] Epoch: 7 , batch: 416 , training loss: 4.633106\n",
      "[INFO] Epoch: 7 , batch: 417 , training loss: 4.535630\n",
      "[INFO] Epoch: 7 , batch: 418 , training loss: 4.573911\n",
      "[INFO] Epoch: 7 , batch: 419 , training loss: 4.519059\n",
      "[INFO] Epoch: 7 , batch: 420 , training loss: 4.490431\n",
      "[INFO] Epoch: 7 , batch: 421 , training loss: 4.495912\n",
      "[INFO] Epoch: 7 , batch: 422 , training loss: 4.372168\n",
      "[INFO] Epoch: 7 , batch: 423 , training loss: 4.611686\n",
      "[INFO] Epoch: 7 , batch: 424 , training loss: 4.747663\n",
      "[INFO] Epoch: 7 , batch: 425 , training loss: 4.646979\n",
      "[INFO] Epoch: 7 , batch: 426 , training loss: 4.311643\n",
      "[INFO] Epoch: 7 , batch: 427 , training loss: 4.595975\n",
      "[INFO] Epoch: 7 , batch: 428 , training loss: 4.475721\n",
      "[INFO] Epoch: 7 , batch: 429 , training loss: 4.336390\n",
      "[INFO] Epoch: 7 , batch: 430 , training loss: 4.624156\n",
      "[INFO] Epoch: 7 , batch: 431 , training loss: 4.186963\n",
      "[INFO] Epoch: 7 , batch: 432 , training loss: 4.255930\n",
      "[INFO] Epoch: 7 , batch: 433 , training loss: 4.265627\n",
      "[INFO] Epoch: 7 , batch: 434 , training loss: 4.167045\n",
      "[INFO] Epoch: 7 , batch: 435 , training loss: 4.500522\n",
      "[INFO] Epoch: 7 , batch: 436 , training loss: 4.586120\n",
      "[INFO] Epoch: 7 , batch: 437 , training loss: 4.380388\n",
      "[INFO] Epoch: 7 , batch: 438 , training loss: 4.191689\n",
      "[INFO] Epoch: 7 , batch: 439 , training loss: 4.409654\n",
      "[INFO] Epoch: 7 , batch: 440 , training loss: 4.586140\n",
      "[INFO] Epoch: 7 , batch: 441 , training loss: 4.614412\n",
      "[INFO] Epoch: 7 , batch: 442 , training loss: 4.405125\n",
      "[INFO] Epoch: 7 , batch: 443 , training loss: 4.614169\n",
      "[INFO] Epoch: 7 , batch: 444 , training loss: 4.217723\n",
      "[INFO] Epoch: 7 , batch: 445 , training loss: 4.109765\n",
      "[INFO] Epoch: 7 , batch: 446 , training loss: 4.031115\n",
      "[INFO] Epoch: 7 , batch: 447 , training loss: 4.235158\n",
      "[INFO] Epoch: 7 , batch: 448 , training loss: 4.387084\n",
      "[INFO] Epoch: 7 , batch: 449 , training loss: 4.802222\n",
      "[INFO] Epoch: 7 , batch: 450 , training loss: 4.865574\n",
      "[INFO] Epoch: 7 , batch: 451 , training loss: 4.741597\n",
      "[INFO] Epoch: 7 , batch: 452 , training loss: 4.509583\n",
      "[INFO] Epoch: 7 , batch: 453 , training loss: 4.304955\n",
      "[INFO] Epoch: 7 , batch: 454 , training loss: 4.444481\n",
      "[INFO] Epoch: 7 , batch: 455 , training loss: 4.477612\n",
      "[INFO] Epoch: 7 , batch: 456 , training loss: 4.462501\n",
      "[INFO] Epoch: 7 , batch: 457 , training loss: 4.553225\n",
      "[INFO] Epoch: 7 , batch: 458 , training loss: 4.283684\n",
      "[INFO] Epoch: 7 , batch: 459 , training loss: 4.267204\n",
      "[INFO] Epoch: 7 , batch: 460 , training loss: 4.413387\n",
      "[INFO] Epoch: 7 , batch: 461 , training loss: 4.364904\n",
      "[INFO] Epoch: 7 , batch: 462 , training loss: 4.433520\n",
      "[INFO] Epoch: 7 , batch: 463 , training loss: 4.325622\n",
      "[INFO] Epoch: 7 , batch: 464 , training loss: 4.509992\n",
      "[INFO] Epoch: 7 , batch: 465 , training loss: 4.435107\n",
      "[INFO] Epoch: 7 , batch: 466 , training loss: 4.538445\n",
      "[INFO] Epoch: 7 , batch: 467 , training loss: 4.546581\n",
      "[INFO] Epoch: 7 , batch: 468 , training loss: 4.500833\n",
      "[INFO] Epoch: 7 , batch: 469 , training loss: 4.515561\n",
      "[INFO] Epoch: 7 , batch: 470 , training loss: 4.284554\n",
      "[INFO] Epoch: 7 , batch: 471 , training loss: 4.420134\n",
      "[INFO] Epoch: 7 , batch: 472 , training loss: 4.476444\n",
      "[INFO] Epoch: 7 , batch: 473 , training loss: 4.382356\n",
      "[INFO] Epoch: 7 , batch: 474 , training loss: 4.194583\n",
      "[INFO] Epoch: 7 , batch: 475 , training loss: 4.056529\n",
      "[INFO] Epoch: 7 , batch: 476 , training loss: 4.458827\n",
      "[INFO] Epoch: 7 , batch: 477 , training loss: 4.569066\n",
      "[INFO] Epoch: 7 , batch: 478 , training loss: 4.616325\n",
      "[INFO] Epoch: 7 , batch: 479 , training loss: 4.548801\n",
      "[INFO] Epoch: 7 , batch: 480 , training loss: 4.683280\n",
      "[INFO] Epoch: 7 , batch: 481 , training loss: 4.548237\n",
      "[INFO] Epoch: 7 , batch: 482 , training loss: 4.680474\n",
      "[INFO] Epoch: 7 , batch: 483 , training loss: 4.505545\n",
      "[INFO] Epoch: 7 , batch: 484 , training loss: 4.286132\n",
      "[INFO] Epoch: 7 , batch: 485 , training loss: 4.435707\n",
      "[INFO] Epoch: 7 , batch: 486 , training loss: 4.301842\n",
      "[INFO] Epoch: 7 , batch: 487 , training loss: 4.304513\n",
      "[INFO] Epoch: 7 , batch: 488 , training loss: 4.463890\n",
      "[INFO] Epoch: 7 , batch: 489 , training loss: 4.363197\n",
      "[INFO] Epoch: 7 , batch: 490 , training loss: 4.454135\n",
      "[INFO] Epoch: 7 , batch: 491 , training loss: 4.397539\n",
      "[INFO] Epoch: 7 , batch: 492 , training loss: 4.315812\n",
      "[INFO] Epoch: 7 , batch: 493 , training loss: 4.500003\n",
      "[INFO] Epoch: 7 , batch: 494 , training loss: 4.407568\n",
      "[INFO] Epoch: 7 , batch: 495 , training loss: 4.525570\n",
      "[INFO] Epoch: 7 , batch: 496 , training loss: 4.420326\n",
      "[INFO] Epoch: 7 , batch: 497 , training loss: 4.447160\n",
      "[INFO] Epoch: 7 , batch: 498 , training loss: 4.470277\n",
      "[INFO] Epoch: 7 , batch: 499 , training loss: 4.527276\n",
      "[INFO] Epoch: 7 , batch: 500 , training loss: 4.701208\n",
      "[INFO] Epoch: 7 , batch: 501 , training loss: 5.106082\n",
      "[INFO] Epoch: 7 , batch: 502 , training loss: 5.249193\n",
      "[INFO] Epoch: 7 , batch: 503 , training loss: 5.005344\n",
      "[INFO] Epoch: 7 , batch: 504 , training loss: 5.096797\n",
      "[INFO] Epoch: 7 , batch: 505 , training loss: 5.000912\n",
      "[INFO] Epoch: 7 , batch: 506 , training loss: 4.925086\n",
      "[INFO] Epoch: 7 , batch: 507 , training loss: 4.971251\n",
      "[INFO] Epoch: 7 , batch: 508 , training loss: 4.899516\n",
      "[INFO] Epoch: 7 , batch: 509 , training loss: 4.669956\n",
      "[INFO] Epoch: 7 , batch: 510 , training loss: 4.756232\n",
      "[INFO] Epoch: 7 , batch: 511 , training loss: 4.642360\n",
      "[INFO] Epoch: 7 , batch: 512 , training loss: 4.712873\n",
      "[INFO] Epoch: 7 , batch: 513 , training loss: 4.966661\n",
      "[INFO] Epoch: 7 , batch: 514 , training loss: 4.592952\n",
      "[INFO] Epoch: 7 , batch: 515 , training loss: 4.850471\n",
      "[INFO] Epoch: 7 , batch: 516 , training loss: 4.627812\n",
      "[INFO] Epoch: 7 , batch: 517 , training loss: 4.642535\n",
      "[INFO] Epoch: 7 , batch: 518 , training loss: 4.590292\n",
      "[INFO] Epoch: 7 , batch: 519 , training loss: 4.453725\n",
      "[INFO] Epoch: 7 , batch: 520 , training loss: 4.695728\n",
      "[INFO] Epoch: 7 , batch: 521 , training loss: 4.668098\n",
      "[INFO] Epoch: 7 , batch: 522 , training loss: 4.753360\n",
      "[INFO] Epoch: 7 , batch: 523 , training loss: 4.638159\n",
      "[INFO] Epoch: 7 , batch: 524 , training loss: 4.939286\n",
      "[INFO] Epoch: 7 , batch: 525 , training loss: 4.818780\n",
      "[INFO] Epoch: 7 , batch: 526 , training loss: 4.568905\n",
      "[INFO] Epoch: 7 , batch: 527 , training loss: 4.600966\n",
      "[INFO] Epoch: 7 , batch: 528 , training loss: 4.657750\n",
      "[INFO] Epoch: 7 , batch: 529 , training loss: 4.636626\n",
      "[INFO] Epoch: 7 , batch: 530 , training loss: 4.477527\n",
      "[INFO] Epoch: 7 , batch: 531 , training loss: 4.640095\n",
      "[INFO] Epoch: 7 , batch: 532 , training loss: 4.490983\n",
      "[INFO] Epoch: 7 , batch: 533 , training loss: 4.638504\n",
      "[INFO] Epoch: 7 , batch: 534 , training loss: 4.638912\n",
      "[INFO] Epoch: 7 , batch: 535 , training loss: 4.646818\n",
      "[INFO] Epoch: 7 , batch: 536 , training loss: 4.518116\n",
      "[INFO] Epoch: 7 , batch: 537 , training loss: 4.469930\n",
      "[INFO] Epoch: 7 , batch: 538 , training loss: 4.551284\n",
      "[INFO] Epoch: 7 , batch: 539 , training loss: 4.724200\n",
      "[INFO] Epoch: 7 , batch: 540 , training loss: 5.322508\n",
      "[INFO] Epoch: 7 , batch: 541 , training loss: 5.242692\n",
      "[INFO] Epoch: 7 , batch: 542 , training loss: 5.059190\n",
      "[INFO] Epoch: 8 , batch: 0 , training loss: 4.358438\n",
      "[INFO] Epoch: 8 , batch: 1 , training loss: 4.258826\n",
      "[INFO] Epoch: 8 , batch: 2 , training loss: 4.131310\n",
      "[INFO] Epoch: 8 , batch: 3 , training loss: 4.041521\n",
      "[INFO] Epoch: 8 , batch: 4 , training loss: 4.351729\n",
      "[INFO] Epoch: 8 , batch: 5 , training loss: 4.046654\n",
      "[INFO] Epoch: 8 , batch: 6 , training loss: 4.523284\n",
      "[INFO] Epoch: 8 , batch: 7 , training loss: 4.176138\n",
      "[INFO] Epoch: 8 , batch: 8 , training loss: 3.850096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 8 , batch: 9 , training loss: 4.057925\n",
      "[INFO] Epoch: 8 , batch: 10 , training loss: 3.975214\n",
      "[INFO] Epoch: 8 , batch: 11 , training loss: 3.903705\n",
      "[INFO] Epoch: 8 , batch: 12 , training loss: 3.919376\n",
      "[INFO] Epoch: 8 , batch: 13 , training loss: 3.922712\n",
      "[INFO] Epoch: 8 , batch: 14 , training loss: 3.751937\n",
      "[INFO] Epoch: 8 , batch: 15 , training loss: 3.949752\n",
      "[INFO] Epoch: 8 , batch: 16 , training loss: 3.846560\n",
      "[INFO] Epoch: 8 , batch: 17 , training loss: 4.008368\n",
      "[INFO] Epoch: 8 , batch: 18 , training loss: 3.931633\n",
      "[INFO] Epoch: 8 , batch: 19 , training loss: 3.679619\n",
      "[INFO] Epoch: 8 , batch: 20 , training loss: 3.694080\n",
      "[INFO] Epoch: 8 , batch: 21 , training loss: 3.834494\n",
      "[INFO] Epoch: 8 , batch: 22 , training loss: 3.797732\n",
      "[INFO] Epoch: 8 , batch: 23 , training loss: 3.982249\n",
      "[INFO] Epoch: 8 , batch: 24 , training loss: 3.861725\n",
      "[INFO] Epoch: 8 , batch: 25 , training loss: 3.960574\n",
      "[INFO] Epoch: 8 , batch: 26 , training loss: 3.827826\n",
      "[INFO] Epoch: 8 , batch: 27 , training loss: 3.795257\n",
      "[INFO] Epoch: 8 , batch: 28 , training loss: 3.987916\n",
      "[INFO] Epoch: 8 , batch: 29 , training loss: 3.758161\n",
      "[INFO] Epoch: 8 , batch: 30 , training loss: 3.782930\n",
      "[INFO] Epoch: 8 , batch: 31 , training loss: 3.905940\n",
      "[INFO] Epoch: 8 , batch: 32 , training loss: 3.888401\n",
      "[INFO] Epoch: 8 , batch: 33 , training loss: 3.959539\n",
      "[INFO] Epoch: 8 , batch: 34 , training loss: 3.939609\n",
      "[INFO] Epoch: 8 , batch: 35 , training loss: 3.855553\n",
      "[INFO] Epoch: 8 , batch: 36 , training loss: 3.888226\n",
      "[INFO] Epoch: 8 , batch: 37 , training loss: 3.790359\n",
      "[INFO] Epoch: 8 , batch: 38 , training loss: 3.938373\n",
      "[INFO] Epoch: 8 , batch: 39 , training loss: 3.711074\n",
      "[INFO] Epoch: 8 , batch: 40 , training loss: 3.894349\n",
      "[INFO] Epoch: 8 , batch: 41 , training loss: 4.013923\n",
      "[INFO] Epoch: 8 , batch: 42 , training loss: 4.525028\n",
      "[INFO] Epoch: 8 , batch: 43 , training loss: 4.201049\n",
      "[INFO] Epoch: 8 , batch: 44 , training loss: 4.449100\n",
      "[INFO] Epoch: 8 , batch: 45 , training loss: 4.552985\n",
      "[INFO] Epoch: 8 , batch: 46 , training loss: 4.671572\n",
      "[INFO] Epoch: 8 , batch: 47 , training loss: 4.214137\n",
      "[INFO] Epoch: 8 , batch: 48 , training loss: 4.162014\n",
      "[INFO] Epoch: 8 , batch: 49 , training loss: 4.292166\n",
      "[INFO] Epoch: 8 , batch: 50 , training loss: 4.057582\n",
      "[INFO] Epoch: 8 , batch: 51 , training loss: 4.162009\n",
      "[INFO] Epoch: 8 , batch: 52 , training loss: 3.944303\n",
      "[INFO] Epoch: 8 , batch: 53 , training loss: 4.087018\n",
      "[INFO] Epoch: 8 , batch: 54 , training loss: 4.088982\n",
      "[INFO] Epoch: 8 , batch: 55 , training loss: 4.178137\n",
      "[INFO] Epoch: 8 , batch: 56 , training loss: 4.020265\n",
      "[INFO] Epoch: 8 , batch: 57 , training loss: 3.906294\n",
      "[INFO] Epoch: 8 , batch: 58 , training loss: 3.951413\n",
      "[INFO] Epoch: 8 , batch: 59 , training loss: 4.039309\n",
      "[INFO] Epoch: 8 , batch: 60 , training loss: 3.922467\n",
      "[INFO] Epoch: 8 , batch: 61 , training loss: 3.997662\n",
      "[INFO] Epoch: 8 , batch: 62 , training loss: 3.903534\n",
      "[INFO] Epoch: 8 , batch: 63 , training loss: 4.083107\n",
      "[INFO] Epoch: 8 , batch: 64 , training loss: 4.263720\n",
      "[INFO] Epoch: 8 , batch: 65 , training loss: 3.943354\n",
      "[INFO] Epoch: 8 , batch: 66 , training loss: 3.809806\n",
      "[INFO] Epoch: 8 , batch: 67 , training loss: 3.827259\n",
      "[INFO] Epoch: 8 , batch: 68 , training loss: 4.118809\n",
      "[INFO] Epoch: 8 , batch: 69 , training loss: 3.940869\n",
      "[INFO] Epoch: 8 , batch: 70 , training loss: 4.194926\n",
      "[INFO] Epoch: 8 , batch: 71 , training loss: 4.049354\n",
      "[INFO] Epoch: 8 , batch: 72 , training loss: 4.094750\n",
      "[INFO] Epoch: 8 , batch: 73 , training loss: 4.038893\n",
      "[INFO] Epoch: 8 , batch: 74 , training loss: 4.172774\n",
      "[INFO] Epoch: 8 , batch: 75 , training loss: 3.932106\n",
      "[INFO] Epoch: 8 , batch: 76 , training loss: 4.061890\n",
      "[INFO] Epoch: 8 , batch: 77 , training loss: 4.017923\n",
      "[INFO] Epoch: 8 , batch: 78 , training loss: 4.114611\n",
      "[INFO] Epoch: 8 , batch: 79 , training loss: 3.912553\n",
      "[INFO] Epoch: 8 , batch: 80 , training loss: 4.119592\n",
      "[INFO] Epoch: 8 , batch: 81 , training loss: 4.082824\n",
      "[INFO] Epoch: 8 , batch: 82 , training loss: 4.067773\n",
      "[INFO] Epoch: 8 , batch: 83 , training loss: 4.174314\n",
      "[INFO] Epoch: 8 , batch: 84 , training loss: 4.101551\n",
      "[INFO] Epoch: 8 , batch: 85 , training loss: 4.177210\n",
      "[INFO] Epoch: 8 , batch: 86 , training loss: 4.148735\n",
      "[INFO] Epoch: 8 , batch: 87 , training loss: 4.099039\n",
      "[INFO] Epoch: 8 , batch: 88 , training loss: 4.259294\n",
      "[INFO] Epoch: 8 , batch: 89 , training loss: 4.042861\n",
      "[INFO] Epoch: 8 , batch: 90 , training loss: 4.114743\n",
      "[INFO] Epoch: 8 , batch: 91 , training loss: 4.047270\n",
      "[INFO] Epoch: 8 , batch: 92 , training loss: 4.062387\n",
      "[INFO] Epoch: 8 , batch: 93 , training loss: 4.142744\n",
      "[INFO] Epoch: 8 , batch: 94 , training loss: 4.325991\n",
      "[INFO] Epoch: 8 , batch: 95 , training loss: 4.086469\n",
      "[INFO] Epoch: 8 , batch: 96 , training loss: 4.061822\n",
      "[INFO] Epoch: 8 , batch: 97 , training loss: 4.041949\n",
      "[INFO] Epoch: 8 , batch: 98 , training loss: 3.962857\n",
      "[INFO] Epoch: 8 , batch: 99 , training loss: 4.077826\n",
      "[INFO] Epoch: 8 , batch: 100 , training loss: 3.912528\n",
      "[INFO] Epoch: 8 , batch: 101 , training loss: 3.971587\n",
      "[INFO] Epoch: 8 , batch: 102 , training loss: 4.149539\n",
      "[INFO] Epoch: 8 , batch: 103 , training loss: 3.899841\n",
      "[INFO] Epoch: 8 , batch: 104 , training loss: 3.851220\n",
      "[INFO] Epoch: 8 , batch: 105 , training loss: 4.143353\n",
      "[INFO] Epoch: 8 , batch: 106 , training loss: 4.153537\n",
      "[INFO] Epoch: 8 , batch: 107 , training loss: 4.009074\n",
      "[INFO] Epoch: 8 , batch: 108 , training loss: 3.916713\n",
      "[INFO] Epoch: 8 , batch: 109 , training loss: 3.814178\n",
      "[INFO] Epoch: 8 , batch: 110 , training loss: 4.075653\n",
      "[INFO] Epoch: 8 , batch: 111 , training loss: 4.111715\n",
      "[INFO] Epoch: 8 , batch: 112 , training loss: 4.075131\n",
      "[INFO] Epoch: 8 , batch: 113 , training loss: 4.014073\n",
      "[INFO] Epoch: 8 , batch: 114 , training loss: 4.099748\n",
      "[INFO] Epoch: 8 , batch: 115 , training loss: 4.023464\n",
      "[INFO] Epoch: 8 , batch: 116 , training loss: 3.987543\n",
      "[INFO] Epoch: 8 , batch: 117 , training loss: 4.220775\n",
      "[INFO] Epoch: 8 , batch: 118 , training loss: 4.189277\n",
      "[INFO] Epoch: 8 , batch: 119 , training loss: 4.305335\n",
      "[INFO] Epoch: 8 , batch: 120 , training loss: 4.270051\n",
      "[INFO] Epoch: 8 , batch: 121 , training loss: 4.109590\n",
      "[INFO] Epoch: 8 , batch: 122 , training loss: 4.007136\n",
      "[INFO] Epoch: 8 , batch: 123 , training loss: 4.076400\n",
      "[INFO] Epoch: 8 , batch: 124 , training loss: 4.218366\n",
      "[INFO] Epoch: 8 , batch: 125 , training loss: 3.917654\n",
      "[INFO] Epoch: 8 , batch: 126 , training loss: 3.956251\n",
      "[INFO] Epoch: 8 , batch: 127 , training loss: 3.992889\n",
      "[INFO] Epoch: 8 , batch: 128 , training loss: 4.140091\n",
      "[INFO] Epoch: 8 , batch: 129 , training loss: 4.053802\n",
      "[INFO] Epoch: 8 , batch: 130 , training loss: 4.102146\n",
      "[INFO] Epoch: 8 , batch: 131 , training loss: 4.065393\n",
      "[INFO] Epoch: 8 , batch: 132 , training loss: 4.102486\n",
      "[INFO] Epoch: 8 , batch: 133 , training loss: 4.042801\n",
      "[INFO] Epoch: 8 , batch: 134 , training loss: 3.778295\n",
      "[INFO] Epoch: 8 , batch: 135 , training loss: 3.840562\n",
      "[INFO] Epoch: 8 , batch: 136 , training loss: 4.165133\n",
      "[INFO] Epoch: 8 , batch: 137 , training loss: 4.064947\n",
      "[INFO] Epoch: 8 , batch: 138 , training loss: 4.144748\n",
      "[INFO] Epoch: 8 , batch: 139 , training loss: 4.792505\n",
      "[INFO] Epoch: 8 , batch: 140 , training loss: 4.661351\n",
      "[INFO] Epoch: 8 , batch: 141 , training loss: 4.359011\n",
      "[INFO] Epoch: 8 , batch: 142 , training loss: 3.991504\n",
      "[INFO] Epoch: 8 , batch: 143 , training loss: 4.130295\n",
      "[INFO] Epoch: 8 , batch: 144 , training loss: 3.973188\n",
      "[INFO] Epoch: 8 , batch: 145 , training loss: 4.090975\n",
      "[INFO] Epoch: 8 , batch: 146 , training loss: 4.292184\n",
      "[INFO] Epoch: 8 , batch: 147 , training loss: 3.919581\n",
      "[INFO] Epoch: 8 , batch: 148 , training loss: 3.917828\n",
      "[INFO] Epoch: 8 , batch: 149 , training loss: 3.984364\n",
      "[INFO] Epoch: 8 , batch: 150 , training loss: 4.265630\n",
      "[INFO] Epoch: 8 , batch: 151 , training loss: 4.018620\n",
      "[INFO] Epoch: 8 , batch: 152 , training loss: 4.028206\n",
      "[INFO] Epoch: 8 , batch: 153 , training loss: 4.113610\n",
      "[INFO] Epoch: 8 , batch: 154 , training loss: 4.177829\n",
      "[INFO] Epoch: 8 , batch: 155 , training loss: 4.376007\n",
      "[INFO] Epoch: 8 , batch: 156 , training loss: 4.113110\n",
      "[INFO] Epoch: 8 , batch: 157 , training loss: 4.099385\n",
      "[INFO] Epoch: 8 , batch: 158 , training loss: 4.390999\n",
      "[INFO] Epoch: 8 , batch: 159 , training loss: 4.262621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 8 , batch: 160 , training loss: 4.706976\n",
      "[INFO] Epoch: 8 , batch: 161 , training loss: 4.645939\n",
      "[INFO] Epoch: 8 , batch: 162 , training loss: 4.526791\n",
      "[INFO] Epoch: 8 , batch: 163 , training loss: 4.751768\n",
      "[INFO] Epoch: 8 , batch: 164 , training loss: 4.592022\n",
      "[INFO] Epoch: 8 , batch: 165 , training loss: 4.548280\n",
      "[INFO] Epoch: 8 , batch: 166 , training loss: 4.552351\n",
      "[INFO] Epoch: 8 , batch: 167 , training loss: 4.911281\n",
      "[INFO] Epoch: 8 , batch: 168 , training loss: 4.617966\n",
      "[INFO] Epoch: 8 , batch: 169 , training loss: 4.524320\n",
      "[INFO] Epoch: 8 , batch: 170 , training loss: 4.563882\n",
      "[INFO] Epoch: 8 , batch: 171 , training loss: 4.067259\n",
      "[INFO] Epoch: 8 , batch: 172 , training loss: 4.208088\n",
      "[INFO] Epoch: 8 , batch: 173 , training loss: 4.476462\n",
      "[INFO] Epoch: 8 , batch: 174 , training loss: 4.948739\n",
      "[INFO] Epoch: 8 , batch: 175 , training loss: 5.121639\n",
      "[INFO] Epoch: 8 , batch: 176 , training loss: 4.918327\n",
      "[INFO] Epoch: 8 , batch: 177 , training loss: 4.472906\n",
      "[INFO] Epoch: 8 , batch: 178 , training loss: 4.394358\n",
      "[INFO] Epoch: 8 , batch: 179 , training loss: 4.474324\n",
      "[INFO] Epoch: 8 , batch: 180 , training loss: 4.383825\n",
      "[INFO] Epoch: 8 , batch: 181 , training loss: 4.662561\n",
      "[INFO] Epoch: 8 , batch: 182 , training loss: 4.567721\n",
      "[INFO] Epoch: 8 , batch: 183 , training loss: 4.488583\n",
      "[INFO] Epoch: 8 , batch: 184 , training loss: 4.393215\n",
      "[INFO] Epoch: 8 , batch: 185 , training loss: 4.362809\n",
      "[INFO] Epoch: 8 , batch: 186 , training loss: 4.511568\n",
      "[INFO] Epoch: 8 , batch: 187 , training loss: 4.592853\n",
      "[INFO] Epoch: 8 , batch: 188 , training loss: 4.579582\n",
      "[INFO] Epoch: 8 , batch: 189 , training loss: 4.463999\n",
      "[INFO] Epoch: 8 , batch: 190 , training loss: 4.470788\n",
      "[INFO] Epoch: 8 , batch: 191 , training loss: 4.608557\n",
      "[INFO] Epoch: 8 , batch: 192 , training loss: 4.420936\n",
      "[INFO] Epoch: 8 , batch: 193 , training loss: 4.537242\n",
      "[INFO] Epoch: 8 , batch: 194 , training loss: 4.455797\n",
      "[INFO] Epoch: 8 , batch: 195 , training loss: 4.468659\n",
      "[INFO] Epoch: 8 , batch: 196 , training loss: 4.262997\n",
      "[INFO] Epoch: 8 , batch: 197 , training loss: 4.385033\n",
      "[INFO] Epoch: 8 , batch: 198 , training loss: 4.258179\n",
      "[INFO] Epoch: 8 , batch: 199 , training loss: 4.387214\n",
      "[INFO] Epoch: 8 , batch: 200 , training loss: 4.316290\n",
      "[INFO] Epoch: 8 , batch: 201 , training loss: 4.224228\n",
      "[INFO] Epoch: 8 , batch: 202 , training loss: 4.202552\n",
      "[INFO] Epoch: 8 , batch: 203 , training loss: 4.239359\n",
      "[INFO] Epoch: 8 , batch: 204 , training loss: 4.352883\n",
      "[INFO] Epoch: 8 , batch: 205 , training loss: 3.984435\n",
      "[INFO] Epoch: 8 , batch: 206 , training loss: 3.900659\n",
      "[INFO] Epoch: 8 , batch: 207 , training loss: 3.900419\n",
      "[INFO] Epoch: 8 , batch: 208 , training loss: 4.263063\n",
      "[INFO] Epoch: 8 , batch: 209 , training loss: 4.189440\n",
      "[INFO] Epoch: 8 , batch: 210 , training loss: 4.237958\n",
      "[INFO] Epoch: 8 , batch: 211 , training loss: 4.235208\n",
      "[INFO] Epoch: 8 , batch: 212 , training loss: 4.329494\n",
      "[INFO] Epoch: 8 , batch: 213 , training loss: 4.294308\n",
      "[INFO] Epoch: 8 , batch: 214 , training loss: 4.354061\n",
      "[INFO] Epoch: 8 , batch: 215 , training loss: 4.566350\n",
      "[INFO] Epoch: 8 , batch: 216 , training loss: 4.277355\n",
      "[INFO] Epoch: 8 , batch: 217 , training loss: 4.219353\n",
      "[INFO] Epoch: 8 , batch: 218 , training loss: 4.218071\n",
      "[INFO] Epoch: 8 , batch: 219 , training loss: 4.328704\n",
      "[INFO] Epoch: 8 , batch: 220 , training loss: 4.133638\n",
      "[INFO] Epoch: 8 , batch: 221 , training loss: 4.143849\n",
      "[INFO] Epoch: 8 , batch: 222 , training loss: 4.299631\n",
      "[INFO] Epoch: 8 , batch: 223 , training loss: 4.396154\n",
      "[INFO] Epoch: 8 , batch: 224 , training loss: 4.425211\n",
      "[INFO] Epoch: 8 , batch: 225 , training loss: 4.285207\n",
      "[INFO] Epoch: 8 , batch: 226 , training loss: 4.423535\n",
      "[INFO] Epoch: 8 , batch: 227 , training loss: 4.381896\n",
      "[INFO] Epoch: 8 , batch: 228 , training loss: 4.428911\n",
      "[INFO] Epoch: 8 , batch: 229 , training loss: 4.300057\n",
      "[INFO] Epoch: 8 , batch: 230 , training loss: 4.148717\n",
      "[INFO] Epoch: 8 , batch: 231 , training loss: 3.985166\n",
      "[INFO] Epoch: 8 , batch: 232 , training loss: 4.145737\n",
      "[INFO] Epoch: 8 , batch: 233 , training loss: 4.188668\n",
      "[INFO] Epoch: 8 , batch: 234 , training loss: 3.832122\n",
      "[INFO] Epoch: 8 , batch: 235 , training loss: 3.994859\n",
      "[INFO] Epoch: 8 , batch: 236 , training loss: 4.124658\n",
      "[INFO] Epoch: 8 , batch: 237 , training loss: 4.303880\n",
      "[INFO] Epoch: 8 , batch: 238 , training loss: 4.057528\n",
      "[INFO] Epoch: 8 , batch: 239 , training loss: 4.114008\n",
      "[INFO] Epoch: 8 , batch: 240 , training loss: 4.170426\n",
      "[INFO] Epoch: 8 , batch: 241 , training loss: 3.931971\n",
      "[INFO] Epoch: 8 , batch: 242 , training loss: 3.964910\n",
      "[INFO] Epoch: 8 , batch: 243 , training loss: 4.294631\n",
      "[INFO] Epoch: 8 , batch: 244 , training loss: 4.221281\n",
      "[INFO] Epoch: 8 , batch: 245 , training loss: 4.240417\n",
      "[INFO] Epoch: 8 , batch: 246 , training loss: 3.873931\n",
      "[INFO] Epoch: 8 , batch: 247 , training loss: 4.059744\n",
      "[INFO] Epoch: 8 , batch: 248 , training loss: 4.141718\n",
      "[INFO] Epoch: 8 , batch: 249 , training loss: 4.117814\n",
      "[INFO] Epoch: 8 , batch: 250 , training loss: 3.905707\n",
      "[INFO] Epoch: 8 , batch: 251 , training loss: 4.417934\n",
      "[INFO] Epoch: 8 , batch: 252 , training loss: 4.052880\n",
      "[INFO] Epoch: 8 , batch: 253 , training loss: 4.002820\n",
      "[INFO] Epoch: 8 , batch: 254 , training loss: 4.314992\n",
      "[INFO] Epoch: 8 , batch: 255 , training loss: 4.254106\n",
      "[INFO] Epoch: 8 , batch: 256 , training loss: 4.252102\n",
      "[INFO] Epoch: 8 , batch: 257 , training loss: 4.452180\n",
      "[INFO] Epoch: 8 , batch: 258 , training loss: 4.458051\n",
      "[INFO] Epoch: 8 , batch: 259 , training loss: 4.503711\n",
      "[INFO] Epoch: 8 , batch: 260 , training loss: 4.209067\n",
      "[INFO] Epoch: 8 , batch: 261 , training loss: 4.388621\n",
      "[INFO] Epoch: 8 , batch: 262 , training loss: 4.604541\n",
      "[INFO] Epoch: 8 , batch: 263 , training loss: 4.742270\n",
      "[INFO] Epoch: 8 , batch: 264 , training loss: 4.058136\n",
      "[INFO] Epoch: 8 , batch: 265 , training loss: 4.170500\n",
      "[INFO] Epoch: 8 , batch: 266 , training loss: 4.667858\n",
      "[INFO] Epoch: 8 , batch: 267 , training loss: 4.361979\n",
      "[INFO] Epoch: 8 , batch: 268 , training loss: 4.266183\n",
      "[INFO] Epoch: 8 , batch: 269 , training loss: 4.311217\n",
      "[INFO] Epoch: 8 , batch: 270 , training loss: 4.276219\n",
      "[INFO] Epoch: 8 , batch: 271 , training loss: 4.319191\n",
      "[INFO] Epoch: 8 , batch: 272 , training loss: 4.290869\n",
      "[INFO] Epoch: 8 , batch: 273 , training loss: 4.292343\n",
      "[INFO] Epoch: 8 , batch: 274 , training loss: 4.428092\n",
      "[INFO] Epoch: 8 , batch: 275 , training loss: 4.278571\n",
      "[INFO] Epoch: 8 , batch: 276 , training loss: 4.327116\n",
      "[INFO] Epoch: 8 , batch: 277 , training loss: 4.480942\n",
      "[INFO] Epoch: 8 , batch: 278 , training loss: 4.104299\n",
      "[INFO] Epoch: 8 , batch: 279 , training loss: 4.137241\n",
      "[INFO] Epoch: 8 , batch: 280 , training loss: 4.081278\n",
      "[INFO] Epoch: 8 , batch: 281 , training loss: 4.215582\n",
      "[INFO] Epoch: 8 , batch: 282 , training loss: 4.132382\n",
      "[INFO] Epoch: 8 , batch: 283 , training loss: 4.164303\n",
      "[INFO] Epoch: 8 , batch: 284 , training loss: 4.219134\n",
      "[INFO] Epoch: 8 , batch: 285 , training loss: 4.198692\n",
      "[INFO] Epoch: 8 , batch: 286 , training loss: 4.159099\n",
      "[INFO] Epoch: 8 , batch: 287 , training loss: 4.044913\n",
      "[INFO] Epoch: 8 , batch: 288 , training loss: 4.076872\n",
      "[INFO] Epoch: 8 , batch: 289 , training loss: 4.137065\n",
      "[INFO] Epoch: 8 , batch: 290 , training loss: 3.918129\n",
      "[INFO] Epoch: 8 , batch: 291 , training loss: 3.884207\n",
      "[INFO] Epoch: 8 , batch: 292 , training loss: 4.017268\n",
      "[INFO] Epoch: 8 , batch: 293 , training loss: 3.941447\n",
      "[INFO] Epoch: 8 , batch: 294 , training loss: 4.653453\n",
      "[INFO] Epoch: 8 , batch: 295 , training loss: 4.386460\n",
      "[INFO] Epoch: 8 , batch: 296 , training loss: 4.303955\n",
      "[INFO] Epoch: 8 , batch: 297 , training loss: 4.254717\n",
      "[INFO] Epoch: 8 , batch: 298 , training loss: 4.091383\n",
      "[INFO] Epoch: 8 , batch: 299 , training loss: 4.114695\n",
      "[INFO] Epoch: 8 , batch: 300 , training loss: 4.083983\n",
      "[INFO] Epoch: 8 , batch: 301 , training loss: 4.023937\n",
      "[INFO] Epoch: 8 , batch: 302 , training loss: 4.201797\n",
      "[INFO] Epoch: 8 , batch: 303 , training loss: 4.221861\n",
      "[INFO] Epoch: 8 , batch: 304 , training loss: 4.402194\n",
      "[INFO] Epoch: 8 , batch: 305 , training loss: 4.163028\n",
      "[INFO] Epoch: 8 , batch: 306 , training loss: 4.288951\n",
      "[INFO] Epoch: 8 , batch: 307 , training loss: 4.265099\n",
      "[INFO] Epoch: 8 , batch: 308 , training loss: 4.140059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 8 , batch: 309 , training loss: 4.133517\n",
      "[INFO] Epoch: 8 , batch: 310 , training loss: 4.004589\n",
      "[INFO] Epoch: 8 , batch: 311 , training loss: 4.027263\n",
      "[INFO] Epoch: 8 , batch: 312 , training loss: 3.912520\n",
      "[INFO] Epoch: 8 , batch: 313 , training loss: 4.052621\n",
      "[INFO] Epoch: 8 , batch: 314 , training loss: 4.121521\n",
      "[INFO] Epoch: 8 , batch: 315 , training loss: 4.179010\n",
      "[INFO] Epoch: 8 , batch: 316 , training loss: 4.486677\n",
      "[INFO] Epoch: 8 , batch: 317 , training loss: 4.979034\n",
      "[INFO] Epoch: 8 , batch: 318 , training loss: 5.078215\n",
      "[INFO] Epoch: 8 , batch: 319 , training loss: 4.681595\n",
      "[INFO] Epoch: 8 , batch: 320 , training loss: 4.169407\n",
      "[INFO] Epoch: 8 , batch: 321 , training loss: 3.973151\n",
      "[INFO] Epoch: 8 , batch: 322 , training loss: 4.098486\n",
      "[INFO] Epoch: 8 , batch: 323 , training loss: 4.101223\n",
      "[INFO] Epoch: 8 , batch: 324 , training loss: 4.109766\n",
      "[INFO] Epoch: 8 , batch: 325 , training loss: 4.253843\n",
      "[INFO] Epoch: 8 , batch: 326 , training loss: 4.291934\n",
      "[INFO] Epoch: 8 , batch: 327 , training loss: 4.220191\n",
      "[INFO] Epoch: 8 , batch: 328 , training loss: 4.202158\n",
      "[INFO] Epoch: 8 , batch: 329 , training loss: 4.081326\n",
      "[INFO] Epoch: 8 , batch: 330 , training loss: 4.091671\n",
      "[INFO] Epoch: 8 , batch: 331 , training loss: 4.287941\n",
      "[INFO] Epoch: 8 , batch: 332 , training loss: 4.062371\n",
      "[INFO] Epoch: 8 , batch: 333 , training loss: 4.062557\n",
      "[INFO] Epoch: 8 , batch: 334 , training loss: 4.131181\n",
      "[INFO] Epoch: 8 , batch: 335 , training loss: 4.250318\n",
      "[INFO] Epoch: 8 , batch: 336 , training loss: 4.232779\n",
      "[INFO] Epoch: 8 , batch: 337 , training loss: 4.317456\n",
      "[INFO] Epoch: 8 , batch: 338 , training loss: 4.499464\n",
      "[INFO] Epoch: 8 , batch: 339 , training loss: 4.307032\n",
      "[INFO] Epoch: 8 , batch: 340 , training loss: 4.501000\n",
      "[INFO] Epoch: 8 , batch: 341 , training loss: 4.234388\n",
      "[INFO] Epoch: 8 , batch: 342 , training loss: 4.041027\n",
      "[INFO] Epoch: 8 , batch: 343 , training loss: 4.107888\n",
      "[INFO] Epoch: 8 , batch: 344 , training loss: 3.968695\n",
      "[INFO] Epoch: 8 , batch: 345 , training loss: 4.110630\n",
      "[INFO] Epoch: 8 , batch: 346 , training loss: 4.153609\n",
      "[INFO] Epoch: 8 , batch: 347 , training loss: 4.070323\n",
      "[INFO] Epoch: 8 , batch: 348 , training loss: 4.218530\n",
      "[INFO] Epoch: 8 , batch: 349 , training loss: 4.286712\n",
      "[INFO] Epoch: 8 , batch: 350 , training loss: 4.101152\n",
      "[INFO] Epoch: 8 , batch: 351 , training loss: 4.169644\n",
      "[INFO] Epoch: 8 , batch: 352 , training loss: 4.186890\n",
      "[INFO] Epoch: 8 , batch: 353 , training loss: 4.135521\n",
      "[INFO] Epoch: 8 , batch: 354 , training loss: 4.247085\n",
      "[INFO] Epoch: 8 , batch: 355 , training loss: 4.285363\n",
      "[INFO] Epoch: 8 , batch: 356 , training loss: 4.156150\n",
      "[INFO] Epoch: 8 , batch: 357 , training loss: 4.235601\n",
      "[INFO] Epoch: 8 , batch: 358 , training loss: 4.151423\n",
      "[INFO] Epoch: 8 , batch: 359 , training loss: 4.123856\n",
      "[INFO] Epoch: 8 , batch: 360 , training loss: 4.203798\n",
      "[INFO] Epoch: 8 , batch: 361 , training loss: 4.177370\n",
      "[INFO] Epoch: 8 , batch: 362 , training loss: 4.266617\n",
      "[INFO] Epoch: 8 , batch: 363 , training loss: 4.163820\n",
      "[INFO] Epoch: 8 , batch: 364 , training loss: 4.218457\n",
      "[INFO] Epoch: 8 , batch: 365 , training loss: 4.104304\n",
      "[INFO] Epoch: 8 , batch: 366 , training loss: 4.258998\n",
      "[INFO] Epoch: 8 , batch: 367 , training loss: 4.301357\n",
      "[INFO] Epoch: 8 , batch: 368 , training loss: 4.816194\n",
      "[INFO] Epoch: 8 , batch: 369 , training loss: 4.445292\n",
      "[INFO] Epoch: 8 , batch: 370 , training loss: 4.198386\n",
      "[INFO] Epoch: 8 , batch: 371 , training loss: 4.670566\n",
      "[INFO] Epoch: 8 , batch: 372 , training loss: 4.976510\n",
      "[INFO] Epoch: 8 , batch: 373 , training loss: 4.982177\n",
      "[INFO] Epoch: 8 , batch: 374 , training loss: 5.110982\n",
      "[INFO] Epoch: 8 , batch: 375 , training loss: 5.040172\n",
      "[INFO] Epoch: 8 , batch: 376 , training loss: 4.977159\n",
      "[INFO] Epoch: 8 , batch: 377 , training loss: 4.672729\n",
      "[INFO] Epoch: 8 , batch: 378 , training loss: 4.741793\n",
      "[INFO] Epoch: 8 , batch: 379 , training loss: 4.737491\n",
      "[INFO] Epoch: 8 , batch: 380 , training loss: 4.855618\n",
      "[INFO] Epoch: 8 , batch: 381 , training loss: 4.642284\n",
      "[INFO] Epoch: 8 , batch: 382 , training loss: 4.934774\n",
      "[INFO] Epoch: 8 , batch: 383 , training loss: 4.956090\n",
      "[INFO] Epoch: 8 , batch: 384 , training loss: 5.010485\n",
      "[INFO] Epoch: 8 , batch: 385 , training loss: 4.707507\n",
      "[INFO] Epoch: 8 , batch: 386 , training loss: 4.895838\n",
      "[INFO] Epoch: 8 , batch: 387 , training loss: 4.836343\n",
      "[INFO] Epoch: 8 , batch: 388 , training loss: 4.605234\n",
      "[INFO] Epoch: 8 , batch: 389 , training loss: 4.420042\n",
      "[INFO] Epoch: 8 , batch: 390 , training loss: 4.404656\n",
      "[INFO] Epoch: 8 , batch: 391 , training loss: 4.423418\n",
      "[INFO] Epoch: 8 , batch: 392 , training loss: 4.805112\n",
      "[INFO] Epoch: 8 , batch: 393 , training loss: 4.703839\n",
      "[INFO] Epoch: 8 , batch: 394 , training loss: 4.769704\n",
      "[INFO] Epoch: 8 , batch: 395 , training loss: 4.558137\n",
      "[INFO] Epoch: 8 , batch: 396 , training loss: 4.353114\n",
      "[INFO] Epoch: 8 , batch: 397 , training loss: 4.524610\n",
      "[INFO] Epoch: 8 , batch: 398 , training loss: 4.386532\n",
      "[INFO] Epoch: 8 , batch: 399 , training loss: 4.442184\n",
      "[INFO] Epoch: 8 , batch: 400 , training loss: 4.418192\n",
      "[INFO] Epoch: 8 , batch: 401 , training loss: 4.842145\n",
      "[INFO] Epoch: 8 , batch: 402 , training loss: 4.574867\n",
      "[INFO] Epoch: 8 , batch: 403 , training loss: 4.408581\n",
      "[INFO] Epoch: 8 , batch: 404 , training loss: 4.555376\n",
      "[INFO] Epoch: 8 , batch: 405 , training loss: 4.643931\n",
      "[INFO] Epoch: 8 , batch: 406 , training loss: 4.521857\n",
      "[INFO] Epoch: 8 , batch: 407 , training loss: 4.580026\n",
      "[INFO] Epoch: 8 , batch: 408 , training loss: 4.504386\n",
      "[INFO] Epoch: 8 , batch: 409 , training loss: 4.520559\n",
      "[INFO] Epoch: 8 , batch: 410 , training loss: 4.599189\n",
      "[INFO] Epoch: 8 , batch: 411 , training loss: 4.759975\n",
      "[INFO] Epoch: 8 , batch: 412 , training loss: 4.596440\n",
      "[INFO] Epoch: 8 , batch: 413 , training loss: 4.464895\n",
      "[INFO] Epoch: 8 , batch: 414 , training loss: 4.487885\n",
      "[INFO] Epoch: 8 , batch: 415 , training loss: 4.543398\n",
      "[INFO] Epoch: 8 , batch: 416 , training loss: 4.620294\n",
      "[INFO] Epoch: 8 , batch: 417 , training loss: 4.502583\n",
      "[INFO] Epoch: 8 , batch: 418 , training loss: 4.565098\n",
      "[INFO] Epoch: 8 , batch: 419 , training loss: 4.501793\n",
      "[INFO] Epoch: 8 , batch: 420 , training loss: 4.469051\n",
      "[INFO] Epoch: 8 , batch: 421 , training loss: 4.477474\n",
      "[INFO] Epoch: 8 , batch: 422 , training loss: 4.352768\n",
      "[INFO] Epoch: 8 , batch: 423 , training loss: 4.588890\n",
      "[INFO] Epoch: 8 , batch: 424 , training loss: 4.734665\n",
      "[INFO] Epoch: 8 , batch: 425 , training loss: 4.622981\n",
      "[INFO] Epoch: 8 , batch: 426 , training loss: 4.291657\n",
      "[INFO] Epoch: 8 , batch: 427 , training loss: 4.581892\n",
      "[INFO] Epoch: 8 , batch: 428 , training loss: 4.455936\n",
      "[INFO] Epoch: 8 , batch: 429 , training loss: 4.315312\n",
      "[INFO] Epoch: 8 , batch: 430 , training loss: 4.595852\n",
      "[INFO] Epoch: 8 , batch: 431 , training loss: 4.153641\n",
      "[INFO] Epoch: 8 , batch: 432 , training loss: 4.236958\n",
      "[INFO] Epoch: 8 , batch: 433 , training loss: 4.255507\n",
      "[INFO] Epoch: 8 , batch: 434 , training loss: 4.145089\n",
      "[INFO] Epoch: 8 , batch: 435 , training loss: 4.489353\n",
      "[INFO] Epoch: 8 , batch: 436 , training loss: 4.560059\n",
      "[INFO] Epoch: 8 , batch: 437 , training loss: 4.352505\n",
      "[INFO] Epoch: 8 , batch: 438 , training loss: 4.168077\n",
      "[INFO] Epoch: 8 , batch: 439 , training loss: 4.393847\n",
      "[INFO] Epoch: 8 , batch: 440 , training loss: 4.568379\n",
      "[INFO] Epoch: 8 , batch: 441 , training loss: 4.595431\n",
      "[INFO] Epoch: 8 , batch: 442 , training loss: 4.387527\n",
      "[INFO] Epoch: 8 , batch: 443 , training loss: 4.588492\n",
      "[INFO] Epoch: 8 , batch: 444 , training loss: 4.194589\n",
      "[INFO] Epoch: 8 , batch: 445 , training loss: 4.090146\n",
      "[INFO] Epoch: 8 , batch: 446 , training loss: 4.009941\n",
      "[INFO] Epoch: 8 , batch: 447 , training loss: 4.211285\n",
      "[INFO] Epoch: 8 , batch: 448 , training loss: 4.373030\n",
      "[INFO] Epoch: 8 , batch: 449 , training loss: 4.762136\n",
      "[INFO] Epoch: 8 , batch: 450 , training loss: 4.843119\n",
      "[INFO] Epoch: 8 , batch: 451 , training loss: 4.712654\n",
      "[INFO] Epoch: 8 , batch: 452 , training loss: 4.493979\n",
      "[INFO] Epoch: 8 , batch: 453 , training loss: 4.279564\n",
      "[INFO] Epoch: 8 , batch: 454 , training loss: 4.434633\n",
      "[INFO] Epoch: 8 , batch: 455 , training loss: 4.457722\n",
      "[INFO] Epoch: 8 , batch: 456 , training loss: 4.436701\n",
      "[INFO] Epoch: 8 , batch: 457 , training loss: 4.535172\n",
      "[INFO] Epoch: 8 , batch: 458 , training loss: 4.260498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 8 , batch: 459 , training loss: 4.247108\n",
      "[INFO] Epoch: 8 , batch: 460 , training loss: 4.386853\n",
      "[INFO] Epoch: 8 , batch: 461 , training loss: 4.334005\n",
      "[INFO] Epoch: 8 , batch: 462 , training loss: 4.415272\n",
      "[INFO] Epoch: 8 , batch: 463 , training loss: 4.310099\n",
      "[INFO] Epoch: 8 , batch: 464 , training loss: 4.480678\n",
      "[INFO] Epoch: 8 , batch: 465 , training loss: 4.431067\n",
      "[INFO] Epoch: 8 , batch: 466 , training loss: 4.528839\n",
      "[INFO] Epoch: 8 , batch: 467 , training loss: 4.522582\n",
      "[INFO] Epoch: 8 , batch: 468 , training loss: 4.478306\n",
      "[INFO] Epoch: 8 , batch: 469 , training loss: 4.496188\n",
      "[INFO] Epoch: 8 , batch: 470 , training loss: 4.266890\n",
      "[INFO] Epoch: 8 , batch: 471 , training loss: 4.405373\n",
      "[INFO] Epoch: 8 , batch: 472 , training loss: 4.455740\n",
      "[INFO] Epoch: 8 , batch: 473 , training loss: 4.369987\n",
      "[INFO] Epoch: 8 , batch: 474 , training loss: 4.178088\n",
      "[INFO] Epoch: 8 , batch: 475 , training loss: 4.030294\n",
      "[INFO] Epoch: 8 , batch: 476 , training loss: 4.444328\n",
      "[INFO] Epoch: 8 , batch: 477 , training loss: 4.549275\n",
      "[INFO] Epoch: 8 , batch: 478 , training loss: 4.597941\n",
      "[INFO] Epoch: 8 , batch: 479 , training loss: 4.523520\n",
      "[INFO] Epoch: 8 , batch: 480 , training loss: 4.661564\n",
      "[INFO] Epoch: 8 , batch: 481 , training loss: 4.520425\n",
      "[INFO] Epoch: 8 , batch: 482 , training loss: 4.662742\n",
      "[INFO] Epoch: 8 , batch: 483 , training loss: 4.487374\n",
      "[INFO] Epoch: 8 , batch: 484 , training loss: 4.258836\n",
      "[INFO] Epoch: 8 , batch: 485 , training loss: 4.411346\n",
      "[INFO] Epoch: 8 , batch: 486 , training loss: 4.285246\n",
      "[INFO] Epoch: 8 , batch: 487 , training loss: 4.289278\n",
      "[INFO] Epoch: 8 , batch: 488 , training loss: 4.452498\n",
      "[INFO] Epoch: 8 , batch: 489 , training loss: 4.349010\n",
      "[INFO] Epoch: 8 , batch: 490 , training loss: 4.430241\n",
      "[INFO] Epoch: 8 , batch: 491 , training loss: 4.375770\n",
      "[INFO] Epoch: 8 , batch: 492 , training loss: 4.305811\n",
      "[INFO] Epoch: 8 , batch: 493 , training loss: 4.477089\n",
      "[INFO] Epoch: 8 , batch: 494 , training loss: 4.383645\n",
      "[INFO] Epoch: 8 , batch: 495 , training loss: 4.517797\n",
      "[INFO] Epoch: 8 , batch: 496 , training loss: 4.408012\n",
      "[INFO] Epoch: 8 , batch: 497 , training loss: 4.435280\n",
      "[INFO] Epoch: 8 , batch: 498 , training loss: 4.451372\n",
      "[INFO] Epoch: 8 , batch: 499 , training loss: 4.504774\n",
      "[INFO] Epoch: 8 , batch: 500 , training loss: 4.678963\n",
      "[INFO] Epoch: 8 , batch: 501 , training loss: 5.075376\n",
      "[INFO] Epoch: 8 , batch: 502 , training loss: 5.231906\n",
      "[INFO] Epoch: 8 , batch: 503 , training loss: 5.008797\n",
      "[INFO] Epoch: 8 , batch: 504 , training loss: 5.075968\n",
      "[INFO] Epoch: 8 , batch: 505 , training loss: 4.981585\n",
      "[INFO] Epoch: 8 , batch: 506 , training loss: 4.905385\n",
      "[INFO] Epoch: 8 , batch: 507 , training loss: 4.934325\n",
      "[INFO] Epoch: 8 , batch: 508 , training loss: 4.852357\n",
      "[INFO] Epoch: 8 , batch: 509 , training loss: 4.621723\n",
      "[INFO] Epoch: 8 , batch: 510 , training loss: 4.718189\n",
      "[INFO] Epoch: 8 , batch: 511 , training loss: 4.615837\n",
      "[INFO] Epoch: 8 , batch: 512 , training loss: 4.700378\n",
      "[INFO] Epoch: 8 , batch: 513 , training loss: 4.948031\n",
      "[INFO] Epoch: 8 , batch: 514 , training loss: 4.580210\n",
      "[INFO] Epoch: 8 , batch: 515 , training loss: 4.825605\n",
      "[INFO] Epoch: 8 , batch: 516 , training loss: 4.615417\n",
      "[INFO] Epoch: 8 , batch: 517 , training loss: 4.596634\n",
      "[INFO] Epoch: 8 , batch: 518 , training loss: 4.563457\n",
      "[INFO] Epoch: 8 , batch: 519 , training loss: 4.432870\n",
      "[INFO] Epoch: 8 , batch: 520 , training loss: 4.674945\n",
      "[INFO] Epoch: 8 , batch: 521 , training loss: 4.651852\n",
      "[INFO] Epoch: 8 , batch: 522 , training loss: 4.729375\n",
      "[INFO] Epoch: 8 , batch: 523 , training loss: 4.606390\n",
      "[INFO] Epoch: 8 , batch: 524 , training loss: 4.908175\n",
      "[INFO] Epoch: 8 , batch: 525 , training loss: 4.794459\n",
      "[INFO] Epoch: 8 , batch: 526 , training loss: 4.552591\n",
      "[INFO] Epoch: 8 , batch: 527 , training loss: 4.585910\n",
      "[INFO] Epoch: 8 , batch: 528 , training loss: 4.634618\n",
      "[INFO] Epoch: 8 , batch: 529 , training loss: 4.606538\n",
      "[INFO] Epoch: 8 , batch: 530 , training loss: 4.450656\n",
      "[INFO] Epoch: 8 , batch: 531 , training loss: 4.620198\n",
      "[INFO] Epoch: 8 , batch: 532 , training loss: 4.470748\n",
      "[INFO] Epoch: 8 , batch: 533 , training loss: 4.624554\n",
      "[INFO] Epoch: 8 , batch: 534 , training loss: 4.624592\n",
      "[INFO] Epoch: 8 , batch: 535 , training loss: 4.633924\n",
      "[INFO] Epoch: 8 , batch: 536 , training loss: 4.496813\n",
      "[INFO] Epoch: 8 , batch: 537 , training loss: 4.443594\n",
      "[INFO] Epoch: 8 , batch: 538 , training loss: 4.531901\n",
      "[INFO] Epoch: 8 , batch: 539 , training loss: 4.695146\n",
      "[INFO] Epoch: 8 , batch: 540 , training loss: 5.291242\n",
      "[INFO] Epoch: 8 , batch: 541 , training loss: 5.211851\n",
      "[INFO] Epoch: 8 , batch: 542 , training loss: 5.037274\n",
      "[INFO] Epoch: 9 , batch: 0 , training loss: 4.346364\n",
      "[INFO] Epoch: 9 , batch: 1 , training loss: 4.179255\n",
      "[INFO] Epoch: 9 , batch: 2 , training loss: 4.100882\n",
      "[INFO] Epoch: 9 , batch: 3 , training loss: 3.975947\n",
      "[INFO] Epoch: 9 , batch: 4 , training loss: 4.326563\n",
      "[INFO] Epoch: 9 , batch: 5 , training loss: 3.963569\n",
      "[INFO] Epoch: 9 , batch: 6 , training loss: 4.441418\n",
      "[INFO] Epoch: 9 , batch: 7 , training loss: 4.146073\n",
      "[INFO] Epoch: 9 , batch: 8 , training loss: 3.809755\n",
      "[INFO] Epoch: 9 , batch: 9 , training loss: 4.010431\n",
      "[INFO] Epoch: 9 , batch: 10 , training loss: 3.912942\n",
      "[INFO] Epoch: 9 , batch: 11 , training loss: 3.869429\n",
      "[INFO] Epoch: 9 , batch: 12 , training loss: 3.859308\n",
      "[INFO] Epoch: 9 , batch: 13 , training loss: 3.888378\n",
      "[INFO] Epoch: 9 , batch: 14 , training loss: 3.715858\n",
      "[INFO] Epoch: 9 , batch: 15 , training loss: 3.903502\n",
      "[INFO] Epoch: 9 , batch: 16 , training loss: 3.806564\n",
      "[INFO] Epoch: 9 , batch: 17 , training loss: 3.952796\n",
      "[INFO] Epoch: 9 , batch: 18 , training loss: 3.872452\n",
      "[INFO] Epoch: 9 , batch: 19 , training loss: 3.648895\n",
      "[INFO] Epoch: 9 , batch: 20 , training loss: 3.620140\n",
      "[INFO] Epoch: 9 , batch: 21 , training loss: 3.768382\n",
      "[INFO] Epoch: 9 , batch: 22 , training loss: 3.743472\n",
      "[INFO] Epoch: 9 , batch: 23 , training loss: 3.930970\n",
      "[INFO] Epoch: 9 , batch: 24 , training loss: 3.819730\n",
      "[INFO] Epoch: 9 , batch: 25 , training loss: 3.925268\n",
      "[INFO] Epoch: 9 , batch: 26 , training loss: 3.769109\n",
      "[INFO] Epoch: 9 , batch: 27 , training loss: 3.743254\n",
      "[INFO] Epoch: 9 , batch: 28 , training loss: 3.970636\n",
      "[INFO] Epoch: 9 , batch: 29 , training loss: 3.707521\n",
      "[INFO] Epoch: 9 , batch: 30 , training loss: 3.744611\n",
      "[INFO] Epoch: 9 , batch: 31 , training loss: 3.891255\n",
      "[INFO] Epoch: 9 , batch: 32 , training loss: 3.851960\n",
      "[INFO] Epoch: 9 , batch: 33 , training loss: 3.930065\n",
      "[INFO] Epoch: 9 , batch: 34 , training loss: 3.890654\n",
      "[INFO] Epoch: 9 , batch: 35 , training loss: 3.802391\n",
      "[INFO] Epoch: 9 , batch: 36 , training loss: 3.871252\n",
      "[INFO] Epoch: 9 , batch: 37 , training loss: 3.766238\n",
      "[INFO] Epoch: 9 , batch: 38 , training loss: 3.878057\n",
      "[INFO] Epoch: 9 , batch: 39 , training loss: 3.680934\n",
      "[INFO] Epoch: 9 , batch: 40 , training loss: 3.872584\n",
      "[INFO] Epoch: 9 , batch: 41 , training loss: 3.933267\n",
      "[INFO] Epoch: 9 , batch: 42 , training loss: 4.466316\n",
      "[INFO] Epoch: 9 , batch: 43 , training loss: 4.119483\n",
      "[INFO] Epoch: 9 , batch: 44 , training loss: 4.388930\n",
      "[INFO] Epoch: 9 , batch: 45 , training loss: 4.449850\n",
      "[INFO] Epoch: 9 , batch: 46 , training loss: 4.592802\n",
      "[INFO] Epoch: 9 , batch: 47 , training loss: 4.152497\n",
      "[INFO] Epoch: 9 , batch: 48 , training loss: 4.120867\n",
      "[INFO] Epoch: 9 , batch: 49 , training loss: 4.257662\n",
      "[INFO] Epoch: 9 , batch: 50 , training loss: 3.986782\n",
      "[INFO] Epoch: 9 , batch: 51 , training loss: 4.105144\n",
      "[INFO] Epoch: 9 , batch: 52 , training loss: 3.919084\n",
      "[INFO] Epoch: 9 , batch: 53 , training loss: 4.045304\n",
      "[INFO] Epoch: 9 , batch: 54 , training loss: 4.036878\n",
      "[INFO] Epoch: 9 , batch: 55 , training loss: 4.156126\n",
      "[INFO] Epoch: 9 , batch: 56 , training loss: 3.966338\n",
      "[INFO] Epoch: 9 , batch: 57 , training loss: 3.886759\n",
      "[INFO] Epoch: 9 , batch: 58 , training loss: 3.914705\n",
      "[INFO] Epoch: 9 , batch: 59 , training loss: 4.011353\n",
      "[INFO] Epoch: 9 , batch: 60 , training loss: 3.871307\n",
      "[INFO] Epoch: 9 , batch: 61 , training loss: 3.988074\n",
      "[INFO] Epoch: 9 , batch: 62 , training loss: 3.864435\n",
      "[INFO] Epoch: 9 , batch: 63 , training loss: 4.039740\n",
      "[INFO] Epoch: 9 , batch: 64 , training loss: 4.224685\n",
      "[INFO] Epoch: 9 , batch: 65 , training loss: 3.910106\n",
      "[INFO] Epoch: 9 , batch: 66 , training loss: 3.795050\n",
      "[INFO] Epoch: 9 , batch: 67 , training loss: 3.815192\n",
      "[INFO] Epoch: 9 , batch: 68 , training loss: 4.093184\n",
      "[INFO] Epoch: 9 , batch: 69 , training loss: 3.904501\n",
      "[INFO] Epoch: 9 , batch: 70 , training loss: 4.149745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 9 , batch: 71 , training loss: 4.019191\n",
      "[INFO] Epoch: 9 , batch: 72 , training loss: 4.078899\n",
      "[INFO] Epoch: 9 , batch: 73 , training loss: 3.990618\n",
      "[INFO] Epoch: 9 , batch: 74 , training loss: 4.127032\n",
      "[INFO] Epoch: 9 , batch: 75 , training loss: 3.922659\n",
      "[INFO] Epoch: 9 , batch: 76 , training loss: 4.034647\n",
      "[INFO] Epoch: 9 , batch: 77 , training loss: 4.012727\n",
      "[INFO] Epoch: 9 , batch: 78 , training loss: 4.100175\n",
      "[INFO] Epoch: 9 , batch: 79 , training loss: 3.898601\n",
      "[INFO] Epoch: 9 , batch: 80 , training loss: 4.106154\n",
      "[INFO] Epoch: 9 , batch: 81 , training loss: 4.082623\n",
      "[INFO] Epoch: 9 , batch: 82 , training loss: 4.046355\n",
      "[INFO] Epoch: 9 , batch: 83 , training loss: 4.162879\n",
      "[INFO] Epoch: 9 , batch: 84 , training loss: 4.098264\n",
      "[INFO] Epoch: 9 , batch: 85 , training loss: 4.175441\n",
      "[INFO] Epoch: 9 , batch: 86 , training loss: 4.118716\n",
      "[INFO] Epoch: 9 , batch: 87 , training loss: 4.084133\n",
      "[INFO] Epoch: 9 , batch: 88 , training loss: 4.252638\n",
      "[INFO] Epoch: 9 , batch: 89 , training loss: 4.023405\n",
      "[INFO] Epoch: 9 , batch: 90 , training loss: 4.092866\n",
      "[INFO] Epoch: 9 , batch: 91 , training loss: 4.032846\n",
      "[INFO] Epoch: 9 , batch: 92 , training loss: 4.043526\n",
      "[INFO] Epoch: 9 , batch: 93 , training loss: 4.119394\n",
      "[INFO] Epoch: 9 , batch: 94 , training loss: 4.287750\n",
      "[INFO] Epoch: 9 , batch: 95 , training loss: 4.060805\n",
      "[INFO] Epoch: 9 , batch: 96 , training loss: 4.032291\n",
      "[INFO] Epoch: 9 , batch: 97 , training loss: 4.026161\n",
      "[INFO] Epoch: 9 , batch: 98 , training loss: 3.933078\n",
      "[INFO] Epoch: 9 , batch: 99 , training loss: 4.045365\n",
      "[INFO] Epoch: 9 , batch: 100 , training loss: 3.902049\n",
      "[INFO] Epoch: 9 , batch: 101 , training loss: 3.962137\n",
      "[INFO] Epoch: 9 , batch: 102 , training loss: 4.124574\n",
      "[INFO] Epoch: 9 , batch: 103 , training loss: 3.880889\n",
      "[INFO] Epoch: 9 , batch: 104 , training loss: 3.834706\n",
      "[INFO] Epoch: 9 , batch: 105 , training loss: 4.104385\n",
      "[INFO] Epoch: 9 , batch: 106 , training loss: 4.132905\n",
      "[INFO] Epoch: 9 , batch: 107 , training loss: 3.969794\n",
      "[INFO] Epoch: 9 , batch: 108 , training loss: 3.908781\n",
      "[INFO] Epoch: 9 , batch: 109 , training loss: 3.790350\n",
      "[INFO] Epoch: 9 , batch: 110 , training loss: 4.055367\n",
      "[INFO] Epoch: 9 , batch: 111 , training loss: 4.112532\n",
      "[INFO] Epoch: 9 , batch: 112 , training loss: 4.047015\n",
      "[INFO] Epoch: 9 , batch: 113 , training loss: 4.010982\n",
      "[INFO] Epoch: 9 , batch: 114 , training loss: 4.065601\n",
      "[INFO] Epoch: 9 , batch: 115 , training loss: 3.992460\n",
      "[INFO] Epoch: 9 , batch: 116 , training loss: 3.958236\n",
      "[INFO] Epoch: 9 , batch: 117 , training loss: 4.202584\n",
      "[INFO] Epoch: 9 , batch: 118 , training loss: 4.165746\n",
      "[INFO] Epoch: 9 , batch: 119 , training loss: 4.298252\n",
      "[INFO] Epoch: 9 , batch: 120 , training loss: 4.241925\n",
      "[INFO] Epoch: 9 , batch: 121 , training loss: 4.099699\n",
      "[INFO] Epoch: 9 , batch: 122 , training loss: 3.977684\n",
      "[INFO] Epoch: 9 , batch: 123 , training loss: 4.028916\n",
      "[INFO] Epoch: 9 , batch: 124 , training loss: 4.190831\n",
      "[INFO] Epoch: 9 , batch: 125 , training loss: 3.898027\n",
      "[INFO] Epoch: 9 , batch: 126 , training loss: 3.923642\n",
      "[INFO] Epoch: 9 , batch: 127 , training loss: 3.961108\n",
      "[INFO] Epoch: 9 , batch: 128 , training loss: 4.111291\n",
      "[INFO] Epoch: 9 , batch: 129 , training loss: 4.015154\n",
      "[INFO] Epoch: 9 , batch: 130 , training loss: 4.057239\n",
      "[INFO] Epoch: 9 , batch: 131 , training loss: 4.041232\n",
      "[INFO] Epoch: 9 , batch: 132 , training loss: 4.083095\n",
      "[INFO] Epoch: 9 , batch: 133 , training loss: 4.021040\n",
      "[INFO] Epoch: 9 , batch: 134 , training loss: 3.769230\n",
      "[INFO] Epoch: 9 , batch: 135 , training loss: 3.808557\n",
      "[INFO] Epoch: 9 , batch: 136 , training loss: 4.137032\n",
      "[INFO] Epoch: 9 , batch: 137 , training loss: 4.051484\n",
      "[INFO] Epoch: 9 , batch: 138 , training loss: 4.118339\n",
      "[INFO] Epoch: 9 , batch: 139 , training loss: 4.747020\n",
      "[INFO] Epoch: 9 , batch: 140 , training loss: 4.607011\n",
      "[INFO] Epoch: 9 , batch: 141 , training loss: 4.302047\n",
      "[INFO] Epoch: 9 , batch: 142 , training loss: 3.976251\n",
      "[INFO] Epoch: 9 , batch: 143 , training loss: 4.105749\n",
      "[INFO] Epoch: 9 , batch: 144 , training loss: 3.951841\n",
      "[INFO] Epoch: 9 , batch: 145 , training loss: 4.041377\n",
      "[INFO] Epoch: 9 , batch: 146 , training loss: 4.256649\n",
      "[INFO] Epoch: 9 , batch: 147 , training loss: 3.884902\n",
      "[INFO] Epoch: 9 , batch: 148 , training loss: 3.890403\n",
      "[INFO] Epoch: 9 , batch: 149 , training loss: 3.973735\n",
      "[INFO] Epoch: 9 , batch: 150 , training loss: 4.241700\n",
      "[INFO] Epoch: 9 , batch: 151 , training loss: 4.007638\n",
      "[INFO] Epoch: 9 , batch: 152 , training loss: 4.002460\n",
      "[INFO] Epoch: 9 , batch: 153 , training loss: 4.091754\n",
      "[INFO] Epoch: 9 , batch: 154 , training loss: 4.136005\n",
      "[INFO] Epoch: 9 , batch: 155 , training loss: 4.372483\n",
      "[INFO] Epoch: 9 , batch: 156 , training loss: 4.079869\n",
      "[INFO] Epoch: 9 , batch: 157 , training loss: 4.053561\n",
      "[INFO] Epoch: 9 , batch: 158 , training loss: 4.331160\n",
      "[INFO] Epoch: 9 , batch: 159 , training loss: 4.223802\n",
      "[INFO] Epoch: 9 , batch: 160 , training loss: 4.644438\n",
      "[INFO] Epoch: 9 , batch: 161 , training loss: 4.608784\n",
      "[INFO] Epoch: 9 , batch: 162 , training loss: 4.494993\n",
      "[INFO] Epoch: 9 , batch: 163 , training loss: 4.701984\n",
      "[INFO] Epoch: 9 , batch: 164 , training loss: 4.571032\n",
      "[INFO] Epoch: 9 , batch: 165 , training loss: 4.529146\n",
      "[INFO] Epoch: 9 , batch: 166 , training loss: 4.522056\n",
      "[INFO] Epoch: 9 , batch: 167 , training loss: 4.818510\n",
      "[INFO] Epoch: 9 , batch: 168 , training loss: 4.511218\n",
      "[INFO] Epoch: 9 , batch: 169 , training loss: 4.425572\n",
      "[INFO] Epoch: 9 , batch: 170 , training loss: 4.498711\n",
      "[INFO] Epoch: 9 , batch: 171 , training loss: 3.985849\n",
      "[INFO] Epoch: 9 , batch: 172 , training loss: 4.193965\n",
      "[INFO] Epoch: 9 , batch: 173 , training loss: 4.443893\n",
      "[INFO] Epoch: 9 , batch: 174 , training loss: 4.840232\n",
      "[INFO] Epoch: 9 , batch: 175 , training loss: 5.047421\n",
      "[INFO] Epoch: 9 , batch: 176 , training loss: 4.829462\n",
      "[INFO] Epoch: 9 , batch: 177 , training loss: 4.409461\n",
      "[INFO] Epoch: 9 , batch: 178 , training loss: 4.337337\n",
      "[INFO] Epoch: 9 , batch: 179 , training loss: 4.421633\n",
      "[INFO] Epoch: 9 , batch: 180 , training loss: 4.330705\n",
      "[INFO] Epoch: 9 , batch: 181 , training loss: 4.609221\n",
      "[INFO] Epoch: 9 , batch: 182 , training loss: 4.532713\n",
      "[INFO] Epoch: 9 , batch: 183 , training loss: 4.482043\n",
      "[INFO] Epoch: 9 , batch: 184 , training loss: 4.369943\n",
      "[INFO] Epoch: 9 , batch: 185 , training loss: 4.333427\n",
      "[INFO] Epoch: 9 , batch: 186 , training loss: 4.470496\n",
      "[INFO] Epoch: 9 , batch: 187 , training loss: 4.546244\n",
      "[INFO] Epoch: 9 , batch: 188 , training loss: 4.543934\n",
      "[INFO] Epoch: 9 , batch: 189 , training loss: 4.437256\n",
      "[INFO] Epoch: 9 , batch: 190 , training loss: 4.465373\n",
      "[INFO] Epoch: 9 , batch: 191 , training loss: 4.592107\n",
      "[INFO] Epoch: 9 , batch: 192 , training loss: 4.385946\n",
      "[INFO] Epoch: 9 , batch: 193 , training loss: 4.515113\n",
      "[INFO] Epoch: 9 , batch: 194 , training loss: 4.423546\n",
      "[INFO] Epoch: 9 , batch: 195 , training loss: 4.440722\n",
      "[INFO] Epoch: 9 , batch: 196 , training loss: 4.227392\n",
      "[INFO] Epoch: 9 , batch: 197 , training loss: 4.358226\n",
      "[INFO] Epoch: 9 , batch: 198 , training loss: 4.237071\n",
      "[INFO] Epoch: 9 , batch: 199 , training loss: 4.352586\n",
      "[INFO] Epoch: 9 , batch: 200 , training loss: 4.284809\n",
      "[INFO] Epoch: 9 , batch: 201 , training loss: 4.203084\n",
      "[INFO] Epoch: 9 , batch: 202 , training loss: 4.169553\n",
      "[INFO] Epoch: 9 , batch: 203 , training loss: 4.228937\n",
      "[INFO] Epoch: 9 , batch: 204 , training loss: 4.351402\n",
      "[INFO] Epoch: 9 , batch: 205 , training loss: 3.959174\n",
      "[INFO] Epoch: 9 , batch: 206 , training loss: 3.866503\n",
      "[INFO] Epoch: 9 , batch: 207 , training loss: 3.892842\n",
      "[INFO] Epoch: 9 , batch: 208 , training loss: 4.240298\n",
      "[INFO] Epoch: 9 , batch: 209 , training loss: 4.148116\n",
      "[INFO] Epoch: 9 , batch: 210 , training loss: 4.217587\n",
      "[INFO] Epoch: 9 , batch: 211 , training loss: 4.201807\n",
      "[INFO] Epoch: 9 , batch: 212 , training loss: 4.308832\n",
      "[INFO] Epoch: 9 , batch: 213 , training loss: 4.280441\n",
      "[INFO] Epoch: 9 , batch: 214 , training loss: 4.345466\n",
      "[INFO] Epoch: 9 , batch: 215 , training loss: 4.539973\n",
      "[INFO] Epoch: 9 , batch: 216 , training loss: 4.257540\n",
      "[INFO] Epoch: 9 , batch: 217 , training loss: 4.202893\n",
      "[INFO] Epoch: 9 , batch: 218 , training loss: 4.203380\n",
      "[INFO] Epoch: 9 , batch: 219 , training loss: 4.309323\n",
      "[INFO] Epoch: 9 , batch: 220 , training loss: 4.130357\n",
      "[INFO] Epoch: 9 , batch: 221 , training loss: 4.119233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 9 , batch: 222 , training loss: 4.285414\n",
      "[INFO] Epoch: 9 , batch: 223 , training loss: 4.365530\n",
      "[INFO] Epoch: 9 , batch: 224 , training loss: 4.407368\n",
      "[INFO] Epoch: 9 , batch: 225 , training loss: 4.279611\n",
      "[INFO] Epoch: 9 , batch: 226 , training loss: 4.415973\n",
      "[INFO] Epoch: 9 , batch: 227 , training loss: 4.371955\n",
      "[INFO] Epoch: 9 , batch: 228 , training loss: 4.426509\n",
      "[INFO] Epoch: 9 , batch: 229 , training loss: 4.289562\n",
      "[INFO] Epoch: 9 , batch: 230 , training loss: 4.144720\n",
      "[INFO] Epoch: 9 , batch: 231 , training loss: 3.968787\n",
      "[INFO] Epoch: 9 , batch: 232 , training loss: 4.120409\n",
      "[INFO] Epoch: 9 , batch: 233 , training loss: 4.171459\n",
      "[INFO] Epoch: 9 , batch: 234 , training loss: 3.830448\n",
      "[INFO] Epoch: 9 , batch: 235 , training loss: 3.963380\n",
      "[INFO] Epoch: 9 , batch: 236 , training loss: 4.111677\n",
      "[INFO] Epoch: 9 , batch: 237 , training loss: 4.282737\n",
      "[INFO] Epoch: 9 , batch: 238 , training loss: 4.034693\n",
      "[INFO] Epoch: 9 , batch: 239 , training loss: 4.093475\n",
      "[INFO] Epoch: 9 , batch: 240 , training loss: 4.145841\n",
      "[INFO] Epoch: 9 , batch: 241 , training loss: 3.933622\n",
      "[INFO] Epoch: 9 , batch: 242 , training loss: 3.957526\n",
      "[INFO] Epoch: 9 , batch: 243 , training loss: 4.277469\n",
      "[INFO] Epoch: 9 , batch: 244 , training loss: 4.206894\n",
      "[INFO] Epoch: 9 , batch: 245 , training loss: 4.224344\n",
      "[INFO] Epoch: 9 , batch: 246 , training loss: 3.869182\n",
      "[INFO] Epoch: 9 , batch: 247 , training loss: 4.042524\n",
      "[INFO] Epoch: 9 , batch: 248 , training loss: 4.113736\n",
      "[INFO] Epoch: 9 , batch: 249 , training loss: 4.086563\n",
      "[INFO] Epoch: 9 , batch: 250 , training loss: 3.886199\n",
      "[INFO] Epoch: 9 , batch: 251 , training loss: 4.391057\n",
      "[INFO] Epoch: 9 , batch: 252 , training loss: 4.043425\n",
      "[INFO] Epoch: 9 , batch: 253 , training loss: 3.992175\n",
      "[INFO] Epoch: 9 , batch: 254 , training loss: 4.303086\n",
      "[INFO] Epoch: 9 , batch: 255 , training loss: 4.224923\n",
      "[INFO] Epoch: 9 , batch: 256 , training loss: 4.239303\n",
      "[INFO] Epoch: 9 , batch: 257 , training loss: 4.413408\n",
      "[INFO] Epoch: 9 , batch: 258 , training loss: 4.434256\n",
      "[INFO] Epoch: 9 , batch: 259 , training loss: 4.487077\n",
      "[INFO] Epoch: 9 , batch: 260 , training loss: 4.193691\n",
      "[INFO] Epoch: 9 , batch: 261 , training loss: 4.378294\n",
      "[INFO] Epoch: 9 , batch: 262 , training loss: 4.583456\n",
      "[INFO] Epoch: 9 , batch: 263 , training loss: 4.718047\n",
      "[INFO] Epoch: 9 , batch: 264 , training loss: 4.049262\n",
      "[INFO] Epoch: 9 , batch: 265 , training loss: 4.160627\n",
      "[INFO] Epoch: 9 , batch: 266 , training loss: 4.628848\n",
      "[INFO] Epoch: 9 , batch: 267 , training loss: 4.327910\n",
      "[INFO] Epoch: 9 , batch: 268 , training loss: 4.239858\n",
      "[INFO] Epoch: 9 , batch: 269 , training loss: 4.275436\n",
      "[INFO] Epoch: 9 , batch: 270 , training loss: 4.255136\n",
      "[INFO] Epoch: 9 , batch: 271 , training loss: 4.310452\n",
      "[INFO] Epoch: 9 , batch: 272 , training loss: 4.287016\n",
      "[INFO] Epoch: 9 , batch: 273 , training loss: 4.282619\n",
      "[INFO] Epoch: 9 , batch: 274 , training loss: 4.418257\n",
      "[INFO] Epoch: 9 , batch: 275 , training loss: 4.263728\n",
      "[INFO] Epoch: 9 , batch: 276 , training loss: 4.326744\n",
      "[INFO] Epoch: 9 , batch: 277 , training loss: 4.470271\n",
      "[INFO] Epoch: 9 , batch: 278 , training loss: 4.091388\n",
      "[INFO] Epoch: 9 , batch: 279 , training loss: 4.123875\n",
      "[INFO] Epoch: 9 , batch: 280 , training loss: 4.061998\n",
      "[INFO] Epoch: 9 , batch: 281 , training loss: 4.197761\n",
      "[INFO] Epoch: 9 , batch: 282 , training loss: 4.116977\n",
      "[INFO] Epoch: 9 , batch: 283 , training loss: 4.146063\n",
      "[INFO] Epoch: 9 , batch: 284 , training loss: 4.196642\n",
      "[INFO] Epoch: 9 , batch: 285 , training loss: 4.169679\n",
      "[INFO] Epoch: 9 , batch: 286 , training loss: 4.125690\n",
      "[INFO] Epoch: 9 , batch: 287 , training loss: 4.040118\n",
      "[INFO] Epoch: 9 , batch: 288 , training loss: 4.057242\n",
      "[INFO] Epoch: 9 , batch: 289 , training loss: 4.117447\n",
      "[INFO] Epoch: 9 , batch: 290 , training loss: 3.895645\n",
      "[INFO] Epoch: 9 , batch: 291 , training loss: 3.877229\n",
      "[INFO] Epoch: 9 , batch: 292 , training loss: 3.988904\n",
      "[INFO] Epoch: 9 , batch: 293 , training loss: 3.920306\n",
      "[INFO] Epoch: 9 , batch: 294 , training loss: 4.623700\n",
      "[INFO] Epoch: 9 , batch: 295 , training loss: 4.370812\n",
      "[INFO] Epoch: 9 , batch: 296 , training loss: 4.294648\n",
      "[INFO] Epoch: 9 , batch: 297 , training loss: 4.226987\n",
      "[INFO] Epoch: 9 , batch: 298 , training loss: 4.061815\n",
      "[INFO] Epoch: 9 , batch: 299 , training loss: 4.084457\n",
      "[INFO] Epoch: 9 , batch: 300 , training loss: 4.076811\n",
      "[INFO] Epoch: 9 , batch: 301 , training loss: 4.008807\n",
      "[INFO] Epoch: 9 , batch: 302 , training loss: 4.191640\n",
      "[INFO] Epoch: 9 , batch: 303 , training loss: 4.207542\n",
      "[INFO] Epoch: 9 , batch: 304 , training loss: 4.386400\n",
      "[INFO] Epoch: 9 , batch: 305 , training loss: 4.138834\n",
      "[INFO] Epoch: 9 , batch: 306 , training loss: 4.283580\n",
      "[INFO] Epoch: 9 , batch: 307 , training loss: 4.257980\n",
      "[INFO] Epoch: 9 , batch: 308 , training loss: 4.124078\n",
      "[INFO] Epoch: 9 , batch: 309 , training loss: 4.127672\n",
      "[INFO] Epoch: 9 , batch: 310 , training loss: 3.985066\n",
      "[INFO] Epoch: 9 , batch: 311 , training loss: 4.001639\n",
      "[INFO] Epoch: 9 , batch: 312 , training loss: 3.899931\n",
      "[INFO] Epoch: 9 , batch: 313 , training loss: 4.026649\n",
      "[INFO] Epoch: 9 , batch: 314 , training loss: 4.108067\n",
      "[INFO] Epoch: 9 , batch: 315 , training loss: 4.161530\n",
      "[INFO] Epoch: 9 , batch: 316 , training loss: 4.466286\n",
      "[INFO] Epoch: 9 , batch: 317 , training loss: 4.924327\n",
      "[INFO] Epoch: 9 , batch: 318 , training loss: 5.049240\n",
      "[INFO] Epoch: 9 , batch: 319 , training loss: 4.668502\n",
      "[INFO] Epoch: 9 , batch: 320 , training loss: 4.155974\n",
      "[INFO] Epoch: 9 , batch: 321 , training loss: 3.944329\n",
      "[INFO] Epoch: 9 , batch: 322 , training loss: 4.073211\n",
      "[INFO] Epoch: 9 , batch: 323 , training loss: 4.094349\n",
      "[INFO] Epoch: 9 , batch: 324 , training loss: 4.097884\n",
      "[INFO] Epoch: 9 , batch: 325 , training loss: 4.230234\n",
      "[INFO] Epoch: 9 , batch: 326 , training loss: 4.265963\n",
      "[INFO] Epoch: 9 , batch: 327 , training loss: 4.188625\n",
      "[INFO] Epoch: 9 , batch: 328 , training loss: 4.185736\n",
      "[INFO] Epoch: 9 , batch: 329 , training loss: 4.070839\n",
      "[INFO] Epoch: 9 , batch: 330 , training loss: 4.085690\n",
      "[INFO] Epoch: 9 , batch: 331 , training loss: 4.266358\n",
      "[INFO] Epoch: 9 , batch: 332 , training loss: 4.054937\n",
      "[INFO] Epoch: 9 , batch: 333 , training loss: 4.038203\n",
      "[INFO] Epoch: 9 , batch: 334 , training loss: 4.101091\n",
      "[INFO] Epoch: 9 , batch: 335 , training loss: 4.216891\n",
      "[INFO] Epoch: 9 , batch: 336 , training loss: 4.204265\n",
      "[INFO] Epoch: 9 , batch: 337 , training loss: 4.290552\n",
      "[INFO] Epoch: 9 , batch: 338 , training loss: 4.477195\n",
      "[INFO] Epoch: 9 , batch: 339 , training loss: 4.286275\n",
      "[INFO] Epoch: 9 , batch: 340 , training loss: 4.479602\n",
      "[INFO] Epoch: 9 , batch: 341 , training loss: 4.204540\n",
      "[INFO] Epoch: 9 , batch: 342 , training loss: 4.019718\n",
      "[INFO] Epoch: 9 , batch: 343 , training loss: 4.087007\n",
      "[INFO] Epoch: 9 , batch: 344 , training loss: 3.949292\n",
      "[INFO] Epoch: 9 , batch: 345 , training loss: 4.089730\n",
      "[INFO] Epoch: 9 , batch: 346 , training loss: 4.137057\n",
      "[INFO] Epoch: 9 , batch: 347 , training loss: 4.049776\n",
      "[INFO] Epoch: 9 , batch: 348 , training loss: 4.188358\n",
      "[INFO] Epoch: 9 , batch: 349 , training loss: 4.269016\n",
      "[INFO] Epoch: 9 , batch: 350 , training loss: 4.085996\n",
      "[INFO] Epoch: 9 , batch: 351 , training loss: 4.167214\n",
      "[INFO] Epoch: 9 , batch: 352 , training loss: 4.175220\n",
      "[INFO] Epoch: 9 , batch: 353 , training loss: 4.127180\n",
      "[INFO] Epoch: 9 , batch: 354 , training loss: 4.233281\n",
      "[INFO] Epoch: 9 , batch: 355 , training loss: 4.275532\n",
      "[INFO] Epoch: 9 , batch: 356 , training loss: 4.137001\n",
      "[INFO] Epoch: 9 , batch: 357 , training loss: 4.206613\n",
      "[INFO] Epoch: 9 , batch: 358 , training loss: 4.128534\n",
      "[INFO] Epoch: 9 , batch: 359 , training loss: 4.119233\n",
      "[INFO] Epoch: 9 , batch: 360 , training loss: 4.177563\n",
      "[INFO] Epoch: 9 , batch: 361 , training loss: 4.159760\n",
      "[INFO] Epoch: 9 , batch: 362 , training loss: 4.249522\n",
      "[INFO] Epoch: 9 , batch: 363 , training loss: 4.153390\n",
      "[INFO] Epoch: 9 , batch: 364 , training loss: 4.205779\n",
      "[INFO] Epoch: 9 , batch: 365 , training loss: 4.089374\n",
      "[INFO] Epoch: 9 , batch: 366 , training loss: 4.259835\n",
      "[INFO] Epoch: 9 , batch: 367 , training loss: 4.273252\n",
      "[INFO] Epoch: 9 , batch: 368 , training loss: 4.785773\n",
      "[INFO] Epoch: 9 , batch: 369 , training loss: 4.417185\n",
      "[INFO] Epoch: 9 , batch: 370 , training loss: 4.167651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 9 , batch: 371 , training loss: 4.641292\n",
      "[INFO] Epoch: 9 , batch: 372 , training loss: 4.938234\n",
      "[INFO] Epoch: 9 , batch: 373 , training loss: 4.959558\n",
      "[INFO] Epoch: 9 , batch: 374 , training loss: 5.074240\n",
      "[INFO] Epoch: 9 , batch: 375 , training loss: 5.000963\n",
      "[INFO] Epoch: 9 , batch: 376 , training loss: 4.948653\n",
      "[INFO] Epoch: 9 , batch: 377 , training loss: 4.657973\n",
      "[INFO] Epoch: 9 , batch: 378 , training loss: 4.732767\n",
      "[INFO] Epoch: 9 , batch: 379 , training loss: 4.717575\n",
      "[INFO] Epoch: 9 , batch: 380 , training loss: 4.824220\n",
      "[INFO] Epoch: 9 , batch: 381 , training loss: 4.612788\n",
      "[INFO] Epoch: 9 , batch: 382 , training loss: 4.879972\n",
      "[INFO] Epoch: 9 , batch: 383 , training loss: 4.930329\n",
      "[INFO] Epoch: 9 , batch: 384 , training loss: 4.954720\n",
      "[INFO] Epoch: 9 , batch: 385 , training loss: 4.674753\n",
      "[INFO] Epoch: 9 , batch: 386 , training loss: 4.855825\n",
      "[INFO] Epoch: 9 , batch: 387 , training loss: 4.790163\n",
      "[INFO] Epoch: 9 , batch: 388 , training loss: 4.587307\n",
      "[INFO] Epoch: 9 , batch: 389 , training loss: 4.414517\n",
      "[INFO] Epoch: 9 , batch: 390 , training loss: 4.383454\n",
      "[INFO] Epoch: 9 , batch: 391 , training loss: 4.408762\n",
      "[INFO] Epoch: 9 , batch: 392 , training loss: 4.765931\n",
      "[INFO] Epoch: 9 , batch: 393 , training loss: 4.662574\n",
      "[INFO] Epoch: 9 , batch: 394 , training loss: 4.747532\n",
      "[INFO] Epoch: 9 , batch: 395 , training loss: 4.553587\n",
      "[INFO] Epoch: 9 , batch: 396 , training loss: 4.344625\n",
      "[INFO] Epoch: 9 , batch: 397 , training loss: 4.511023\n",
      "[INFO] Epoch: 9 , batch: 398 , training loss: 4.382851\n",
      "[INFO] Epoch: 9 , batch: 399 , training loss: 4.425950\n",
      "[INFO] Epoch: 9 , batch: 400 , training loss: 4.396329\n",
      "[INFO] Epoch: 9 , batch: 401 , training loss: 4.830554\n",
      "[INFO] Epoch: 9 , batch: 402 , training loss: 4.553450\n",
      "[INFO] Epoch: 9 , batch: 403 , training loss: 4.385315\n",
      "[INFO] Epoch: 9 , batch: 404 , training loss: 4.531139\n",
      "[INFO] Epoch: 9 , batch: 405 , training loss: 4.620104\n",
      "[INFO] Epoch: 9 , batch: 406 , training loss: 4.505863\n",
      "[INFO] Epoch: 9 , batch: 407 , training loss: 4.577472\n",
      "[INFO] Epoch: 9 , batch: 408 , training loss: 4.490504\n",
      "[INFO] Epoch: 9 , batch: 409 , training loss: 4.517214\n",
      "[INFO] Epoch: 9 , batch: 410 , training loss: 4.580451\n",
      "[INFO] Epoch: 9 , batch: 411 , training loss: 4.756993\n",
      "[INFO] Epoch: 9 , batch: 412 , training loss: 4.578687\n",
      "[INFO] Epoch: 9 , batch: 413 , training loss: 4.445583\n",
      "[INFO] Epoch: 9 , batch: 414 , training loss: 4.475504\n",
      "[INFO] Epoch: 9 , batch: 415 , training loss: 4.528194\n",
      "[INFO] Epoch: 9 , batch: 416 , training loss: 4.596180\n",
      "[INFO] Epoch: 9 , batch: 417 , training loss: 4.492825\n",
      "[INFO] Epoch: 9 , batch: 418 , training loss: 4.540135\n",
      "[INFO] Epoch: 9 , batch: 419 , training loss: 4.493796\n",
      "[INFO] Epoch: 9 , batch: 420 , training loss: 4.457349\n",
      "[INFO] Epoch: 9 , batch: 421 , training loss: 4.466949\n",
      "[INFO] Epoch: 9 , batch: 422 , training loss: 4.331512\n",
      "[INFO] Epoch: 9 , batch: 423 , training loss: 4.574492\n",
      "[INFO] Epoch: 9 , batch: 424 , training loss: 4.719177\n",
      "[INFO] Epoch: 9 , batch: 425 , training loss: 4.600090\n",
      "[INFO] Epoch: 9 , batch: 426 , training loss: 4.286747\n",
      "[INFO] Epoch: 9 , batch: 427 , training loss: 4.544205\n",
      "[INFO] Epoch: 9 , batch: 428 , training loss: 4.436691\n",
      "[INFO] Epoch: 9 , batch: 429 , training loss: 4.312248\n",
      "[INFO] Epoch: 9 , batch: 430 , training loss: 4.584772\n",
      "[INFO] Epoch: 9 , batch: 431 , training loss: 4.135192\n",
      "[INFO] Epoch: 9 , batch: 432 , training loss: 4.224717\n",
      "[INFO] Epoch: 9 , batch: 433 , training loss: 4.232517\n",
      "[INFO] Epoch: 9 , batch: 434 , training loss: 4.131721\n",
      "[INFO] Epoch: 9 , batch: 435 , training loss: 4.478283\n",
      "[INFO] Epoch: 9 , batch: 436 , training loss: 4.542728\n",
      "[INFO] Epoch: 9 , batch: 437 , training loss: 4.335382\n",
      "[INFO] Epoch: 9 , batch: 438 , training loss: 4.158340\n",
      "[INFO] Epoch: 9 , batch: 439 , training loss: 4.391133\n",
      "[INFO] Epoch: 9 , batch: 440 , training loss: 4.543322\n",
      "[INFO] Epoch: 9 , batch: 441 , training loss: 4.584557\n",
      "[INFO] Epoch: 9 , batch: 442 , training loss: 4.354944\n",
      "[INFO] Epoch: 9 , batch: 443 , training loss: 4.565003\n",
      "[INFO] Epoch: 9 , batch: 444 , training loss: 4.183067\n",
      "[INFO] Epoch: 9 , batch: 445 , training loss: 4.097368\n",
      "[INFO] Epoch: 9 , batch: 446 , training loss: 4.015969\n",
      "[INFO] Epoch: 9 , batch: 447 , training loss: 4.193958\n",
      "[INFO] Epoch: 9 , batch: 448 , training loss: 4.335164\n",
      "[INFO] Epoch: 9 , batch: 449 , training loss: 4.740135\n",
      "[INFO] Epoch: 9 , batch: 450 , training loss: 4.823736\n",
      "[INFO] Epoch: 9 , batch: 451 , training loss: 4.699030\n",
      "[INFO] Epoch: 9 , batch: 452 , training loss: 4.484943\n",
      "[INFO] Epoch: 9 , batch: 453 , training loss: 4.268016\n",
      "[INFO] Epoch: 9 , batch: 454 , training loss: 4.411036\n",
      "[INFO] Epoch: 9 , batch: 455 , training loss: 4.440994\n",
      "[INFO] Epoch: 9 , batch: 456 , training loss: 4.426789\n",
      "[INFO] Epoch: 9 , batch: 457 , training loss: 4.512489\n",
      "[INFO] Epoch: 9 , batch: 458 , training loss: 4.256268\n",
      "[INFO] Epoch: 9 , batch: 459 , training loss: 4.234864\n",
      "[INFO] Epoch: 9 , batch: 460 , training loss: 4.370265\n",
      "[INFO] Epoch: 9 , batch: 461 , training loss: 4.317023\n",
      "[INFO] Epoch: 9 , batch: 462 , training loss: 4.389696\n",
      "[INFO] Epoch: 9 , batch: 463 , training loss: 4.275208\n",
      "[INFO] Epoch: 9 , batch: 464 , training loss: 4.461311\n",
      "[INFO] Epoch: 9 , batch: 465 , training loss: 4.415590\n",
      "[INFO] Epoch: 9 , batch: 466 , training loss: 4.514021\n",
      "[INFO] Epoch: 9 , batch: 467 , training loss: 4.511303\n",
      "[INFO] Epoch: 9 , batch: 468 , training loss: 4.460020\n",
      "[INFO] Epoch: 9 , batch: 469 , training loss: 4.482447\n",
      "[INFO] Epoch: 9 , batch: 470 , training loss: 4.259485\n",
      "[INFO] Epoch: 9 , batch: 471 , training loss: 4.391009\n",
      "[INFO] Epoch: 9 , batch: 472 , training loss: 4.438304\n",
      "[INFO] Epoch: 9 , batch: 473 , training loss: 4.349060\n",
      "[INFO] Epoch: 9 , batch: 474 , training loss: 4.164452\n",
      "[INFO] Epoch: 9 , batch: 475 , training loss: 4.028141\n",
      "[INFO] Epoch: 9 , batch: 476 , training loss: 4.434512\n",
      "[INFO] Epoch: 9 , batch: 477 , training loss: 4.535410\n",
      "[INFO] Epoch: 9 , batch: 478 , training loss: 4.567402\n",
      "[INFO] Epoch: 9 , batch: 479 , training loss: 4.511743\n",
      "[INFO] Epoch: 9 , batch: 480 , training loss: 4.637940\n",
      "[INFO] Epoch: 9 , batch: 481 , training loss: 4.512335\n",
      "[INFO] Epoch: 9 , batch: 482 , training loss: 4.643197\n",
      "[INFO] Epoch: 9 , batch: 483 , training loss: 4.467491\n",
      "[INFO] Epoch: 9 , batch: 484 , training loss: 4.244174\n",
      "[INFO] Epoch: 9 , batch: 485 , training loss: 4.402977\n",
      "[INFO] Epoch: 9 , batch: 486 , training loss: 4.263969\n",
      "[INFO] Epoch: 9 , batch: 487 , training loss: 4.265886\n",
      "[INFO] Epoch: 9 , batch: 488 , training loss: 4.439272\n",
      "[INFO] Epoch: 9 , batch: 489 , training loss: 4.331267\n",
      "[INFO] Epoch: 9 , batch: 490 , training loss: 4.407105\n",
      "[INFO] Epoch: 9 , batch: 491 , training loss: 4.354317\n",
      "[INFO] Epoch: 9 , batch: 492 , training loss: 4.286970\n",
      "[INFO] Epoch: 9 , batch: 493 , training loss: 4.461251\n",
      "[INFO] Epoch: 9 , batch: 494 , training loss: 4.364444\n",
      "[INFO] Epoch: 9 , batch: 495 , training loss: 4.501746\n",
      "[INFO] Epoch: 9 , batch: 496 , training loss: 4.392097\n",
      "[INFO] Epoch: 9 , batch: 497 , training loss: 4.429994\n",
      "[INFO] Epoch: 9 , batch: 498 , training loss: 4.437641\n",
      "[INFO] Epoch: 9 , batch: 499 , training loss: 4.495232\n",
      "[INFO] Epoch: 9 , batch: 500 , training loss: 4.646202\n",
      "[INFO] Epoch: 9 , batch: 501 , training loss: 5.040170\n",
      "[INFO] Epoch: 9 , batch: 502 , training loss: 5.171437\n",
      "[INFO] Epoch: 9 , batch: 503 , training loss: 4.936980\n",
      "[INFO] Epoch: 9 , batch: 504 , training loss: 5.017091\n",
      "[INFO] Epoch: 9 , batch: 505 , training loss: 4.936872\n",
      "[INFO] Epoch: 9 , batch: 506 , training loss: 4.877873\n",
      "[INFO] Epoch: 9 , batch: 507 , training loss: 4.931267\n",
      "[INFO] Epoch: 9 , batch: 508 , training loss: 4.826016\n",
      "[INFO] Epoch: 9 , batch: 509 , training loss: 4.608916\n",
      "[INFO] Epoch: 9 , batch: 510 , training loss: 4.701842\n",
      "[INFO] Epoch: 9 , batch: 511 , training loss: 4.581983\n",
      "[INFO] Epoch: 9 , batch: 512 , training loss: 4.660710\n",
      "[INFO] Epoch: 9 , batch: 513 , training loss: 4.916802\n",
      "[INFO] Epoch: 9 , batch: 514 , training loss: 4.544847\n",
      "[INFO] Epoch: 9 , batch: 515 , training loss: 4.796515\n",
      "[INFO] Epoch: 9 , batch: 516 , training loss: 4.584920\n",
      "[INFO] Epoch: 9 , batch: 517 , training loss: 4.576086\n",
      "[INFO] Epoch: 9 , batch: 518 , training loss: 4.542934\n",
      "[INFO] Epoch: 9 , batch: 519 , training loss: 4.400963\n",
      "[INFO] Epoch: 9 , batch: 520 , training loss: 4.655731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 9 , batch: 521 , training loss: 4.625320\n",
      "[INFO] Epoch: 9 , batch: 522 , training loss: 4.700429\n",
      "[INFO] Epoch: 9 , batch: 523 , training loss: 4.578334\n",
      "[INFO] Epoch: 9 , batch: 524 , training loss: 4.881629\n",
      "[INFO] Epoch: 9 , batch: 525 , training loss: 4.763741\n",
      "[INFO] Epoch: 9 , batch: 526 , training loss: 4.524094\n",
      "[INFO] Epoch: 9 , batch: 527 , training loss: 4.566601\n",
      "[INFO] Epoch: 9 , batch: 528 , training loss: 4.604346\n",
      "[INFO] Epoch: 9 , batch: 529 , training loss: 4.571621\n",
      "[INFO] Epoch: 9 , batch: 530 , training loss: 4.415977\n",
      "[INFO] Epoch: 9 , batch: 531 , training loss: 4.585540\n",
      "[INFO] Epoch: 9 , batch: 532 , training loss: 4.451429\n",
      "[INFO] Epoch: 9 , batch: 533 , training loss: 4.592753\n",
      "[INFO] Epoch: 9 , batch: 534 , training loss: 4.592787\n",
      "[INFO] Epoch: 9 , batch: 535 , training loss: 4.606688\n",
      "[INFO] Epoch: 9 , batch: 536 , training loss: 4.468498\n",
      "[INFO] Epoch: 9 , batch: 537 , training loss: 4.417320\n",
      "[INFO] Epoch: 9 , batch: 538 , training loss: 4.504987\n",
      "[INFO] Epoch: 9 , batch: 539 , training loss: 4.665723\n",
      "[INFO] Epoch: 9 , batch: 540 , training loss: 5.240833\n",
      "[INFO] Epoch: 9 , batch: 541 , training loss: 5.134195\n",
      "[INFO] Epoch: 9 , batch: 542 , training loss: 4.978281\n",
      "[INFO] Epoch: 10 , batch: 0 , training loss: 4.279437\n",
      "[INFO] Epoch: 10 , batch: 1 , training loss: 4.111004\n",
      "[INFO] Epoch: 10 , batch: 2 , training loss: 4.048027\n",
      "[INFO] Epoch: 10 , batch: 3 , training loss: 3.946879\n",
      "[INFO] Epoch: 10 , batch: 4 , training loss: 4.258030\n",
      "[INFO] Epoch: 10 , batch: 5 , training loss: 3.915852\n",
      "[INFO] Epoch: 10 , batch: 6 , training loss: 4.355180\n",
      "[INFO] Epoch: 10 , batch: 7 , training loss: 4.108600\n",
      "[INFO] Epoch: 10 , batch: 8 , training loss: 3.761787\n",
      "[INFO] Epoch: 10 , batch: 9 , training loss: 3.955764\n",
      "[INFO] Epoch: 10 , batch: 10 , training loss: 3.874639\n",
      "[INFO] Epoch: 10 , batch: 11 , training loss: 3.816823\n",
      "[INFO] Epoch: 10 , batch: 12 , training loss: 3.815092\n",
      "[INFO] Epoch: 10 , batch: 13 , training loss: 3.831310\n",
      "[INFO] Epoch: 10 , batch: 14 , training loss: 3.682308\n",
      "[INFO] Epoch: 10 , batch: 15 , training loss: 3.877208\n",
      "[INFO] Epoch: 10 , batch: 16 , training loss: 3.782777\n",
      "[INFO] Epoch: 10 , batch: 17 , training loss: 3.922915\n",
      "[INFO] Epoch: 10 , batch: 18 , training loss: 3.837737\n",
      "[INFO] Epoch: 10 , batch: 19 , training loss: 3.596277\n",
      "[INFO] Epoch: 10 , batch: 20 , training loss: 3.582946\n",
      "[INFO] Epoch: 10 , batch: 21 , training loss: 3.739256\n",
      "[INFO] Epoch: 10 , batch: 22 , training loss: 3.697957\n",
      "[INFO] Epoch: 10 , batch: 23 , training loss: 3.879457\n",
      "[INFO] Epoch: 10 , batch: 24 , training loss: 3.752307\n",
      "[INFO] Epoch: 10 , batch: 25 , training loss: 3.886441\n",
      "[INFO] Epoch: 10 , batch: 26 , training loss: 3.725061\n",
      "[INFO] Epoch: 10 , batch: 27 , training loss: 3.692465\n",
      "[INFO] Epoch: 10 , batch: 28 , training loss: 3.928663\n",
      "[INFO] Epoch: 10 , batch: 29 , training loss: 3.698086\n",
      "[INFO] Epoch: 10 , batch: 30 , training loss: 3.704944\n",
      "[INFO] Epoch: 10 , batch: 31 , training loss: 3.823973\n",
      "[INFO] Epoch: 10 , batch: 32 , training loss: 3.803497\n",
      "[INFO] Epoch: 10 , batch: 33 , training loss: 3.888658\n",
      "[INFO] Epoch: 10 , batch: 34 , training loss: 3.844527\n",
      "[INFO] Epoch: 10 , batch: 35 , training loss: 3.760868\n",
      "[INFO] Epoch: 10 , batch: 36 , training loss: 3.824622\n",
      "[INFO] Epoch: 10 , batch: 37 , training loss: 3.719429\n",
      "[INFO] Epoch: 10 , batch: 38 , training loss: 3.824547\n",
      "[INFO] Epoch: 10 , batch: 39 , training loss: 3.624485\n",
      "[INFO] Epoch: 10 , batch: 40 , training loss: 3.837646\n",
      "[INFO] Epoch: 10 , batch: 41 , training loss: 3.891877\n",
      "[INFO] Epoch: 10 , batch: 42 , training loss: 4.414547\n",
      "[INFO] Epoch: 10 , batch: 43 , training loss: 4.072634\n",
      "[INFO] Epoch: 10 , batch: 44 , training loss: 4.348106\n",
      "[INFO] Epoch: 10 , batch: 45 , training loss: 4.363435\n",
      "[INFO] Epoch: 10 , batch: 46 , training loss: 4.500883\n",
      "[INFO] Epoch: 10 , batch: 47 , training loss: 4.085161\n",
      "[INFO] Epoch: 10 , batch: 48 , training loss: 4.035511\n",
      "[INFO] Epoch: 10 , batch: 49 , training loss: 4.206410\n",
      "[INFO] Epoch: 10 , batch: 50 , training loss: 3.931084\n",
      "[INFO] Epoch: 10 , batch: 51 , training loss: 4.053928\n",
      "[INFO] Epoch: 10 , batch: 52 , training loss: 3.882141\n",
      "[INFO] Epoch: 10 , batch: 53 , training loss: 4.001112\n",
      "[INFO] Epoch: 10 , batch: 54 , training loss: 4.015279\n",
      "[INFO] Epoch: 10 , batch: 55 , training loss: 4.106982\n",
      "[INFO] Epoch: 10 , batch: 56 , training loss: 3.921565\n",
      "[INFO] Epoch: 10 , batch: 57 , training loss: 3.835832\n",
      "[INFO] Epoch: 10 , batch: 58 , training loss: 3.855395\n",
      "[INFO] Epoch: 10 , batch: 59 , training loss: 3.989122\n",
      "[INFO] Epoch: 10 , batch: 60 , training loss: 3.849205\n",
      "[INFO] Epoch: 10 , batch: 61 , training loss: 3.970113\n",
      "[INFO] Epoch: 10 , batch: 62 , training loss: 3.841920\n",
      "[INFO] Epoch: 10 , batch: 63 , training loss: 4.013089\n",
      "[INFO] Epoch: 10 , batch: 64 , training loss: 4.213330\n",
      "[INFO] Epoch: 10 , batch: 65 , training loss: 3.883647\n",
      "[INFO] Epoch: 10 , batch: 66 , training loss: 3.777511\n",
      "[INFO] Epoch: 10 , batch: 67 , training loss: 3.793565\n",
      "[INFO] Epoch: 10 , batch: 68 , training loss: 4.047131\n",
      "[INFO] Epoch: 10 , batch: 69 , training loss: 3.873331\n",
      "[INFO] Epoch: 10 , batch: 70 , training loss: 4.119114\n",
      "[INFO] Epoch: 10 , batch: 71 , training loss: 3.986297\n",
      "[INFO] Epoch: 10 , batch: 72 , training loss: 4.061872\n",
      "[INFO] Epoch: 10 , batch: 73 , training loss: 3.966647\n",
      "[INFO] Epoch: 10 , batch: 74 , training loss: 4.105183\n",
      "[INFO] Epoch: 10 , batch: 75 , training loss: 3.892219\n",
      "[INFO] Epoch: 10 , batch: 76 , training loss: 4.006392\n",
      "[INFO] Epoch: 10 , batch: 77 , training loss: 3.971725\n",
      "[INFO] Epoch: 10 , batch: 78 , training loss: 4.067046\n",
      "[INFO] Epoch: 10 , batch: 79 , training loss: 3.882386\n",
      "[INFO] Epoch: 10 , batch: 80 , training loss: 4.076420\n",
      "[INFO] Epoch: 10 , batch: 81 , training loss: 4.032632\n",
      "[INFO] Epoch: 10 , batch: 82 , training loss: 4.026989\n",
      "[INFO] Epoch: 10 , batch: 83 , training loss: 4.128924\n",
      "[INFO] Epoch: 10 , batch: 84 , training loss: 4.062156\n",
      "[INFO] Epoch: 10 , batch: 85 , training loss: 4.142332\n",
      "[INFO] Epoch: 10 , batch: 86 , training loss: 4.086453\n",
      "[INFO] Epoch: 10 , batch: 87 , training loss: 4.048604\n",
      "[INFO] Epoch: 10 , batch: 88 , training loss: 4.196080\n",
      "[INFO] Epoch: 10 , batch: 89 , training loss: 4.003840\n",
      "[INFO] Epoch: 10 , batch: 90 , training loss: 4.052082\n",
      "[INFO] Epoch: 10 , batch: 91 , training loss: 3.998614\n",
      "[INFO] Epoch: 10 , batch: 92 , training loss: 4.026594\n",
      "[INFO] Epoch: 10 , batch: 93 , training loss: 4.103624\n",
      "[INFO] Epoch: 10 , batch: 94 , training loss: 4.259710\n",
      "[INFO] Epoch: 10 , batch: 95 , training loss: 4.053424\n",
      "[INFO] Epoch: 10 , batch: 96 , training loss: 4.017768\n",
      "[INFO] Epoch: 10 , batch: 97 , training loss: 3.994751\n",
      "[INFO] Epoch: 10 , batch: 98 , training loss: 3.915863\n",
      "[INFO] Epoch: 10 , batch: 99 , training loss: 4.036694\n",
      "[INFO] Epoch: 10 , batch: 100 , training loss: 3.877666\n",
      "[INFO] Epoch: 10 , batch: 101 , training loss: 3.937386\n",
      "[INFO] Epoch: 10 , batch: 102 , training loss: 4.102822\n",
      "[INFO] Epoch: 10 , batch: 103 , training loss: 3.854124\n",
      "[INFO] Epoch: 10 , batch: 104 , training loss: 3.816056\n",
      "[INFO] Epoch: 10 , batch: 105 , training loss: 4.077081\n",
      "[INFO] Epoch: 10 , batch: 106 , training loss: 4.108025\n",
      "[INFO] Epoch: 10 , batch: 107 , training loss: 3.950711\n",
      "[INFO] Epoch: 10 , batch: 108 , training loss: 3.865382\n",
      "[INFO] Epoch: 10 , batch: 109 , training loss: 3.771300\n",
      "[INFO] Epoch: 10 , batch: 110 , training loss: 4.017940\n",
      "[INFO] Epoch: 10 , batch: 111 , training loss: 4.069806\n",
      "[INFO] Epoch: 10 , batch: 112 , training loss: 4.023147\n",
      "[INFO] Epoch: 10 , batch: 113 , training loss: 3.966017\n",
      "[INFO] Epoch: 10 , batch: 114 , training loss: 4.025503\n",
      "[INFO] Epoch: 10 , batch: 115 , training loss: 3.977259\n",
      "[INFO] Epoch: 10 , batch: 116 , training loss: 3.933442\n",
      "[INFO] Epoch: 10 , batch: 117 , training loss: 4.169055\n",
      "[INFO] Epoch: 10 , batch: 118 , training loss: 4.122457\n",
      "[INFO] Epoch: 10 , batch: 119 , training loss: 4.287806\n",
      "[INFO] Epoch: 10 , batch: 120 , training loss: 4.229692\n",
      "[INFO] Epoch: 10 , batch: 121 , training loss: 4.071934\n",
      "[INFO] Epoch: 10 , batch: 122 , training loss: 3.962835\n",
      "[INFO] Epoch: 10 , batch: 123 , training loss: 4.009221\n",
      "[INFO] Epoch: 10 , batch: 124 , training loss: 4.172901\n",
      "[INFO] Epoch: 10 , batch: 125 , training loss: 3.872114\n",
      "[INFO] Epoch: 10 , batch: 126 , training loss: 3.899886\n",
      "[INFO] Epoch: 10 , batch: 127 , training loss: 3.936671\n",
      "[INFO] Epoch: 10 , batch: 128 , training loss: 4.066536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 10 , batch: 129 , training loss: 4.001260\n",
      "[INFO] Epoch: 10 , batch: 130 , training loss: 4.031801\n",
      "[INFO] Epoch: 10 , batch: 131 , training loss: 4.030621\n",
      "[INFO] Epoch: 10 , batch: 132 , training loss: 4.068963\n",
      "[INFO] Epoch: 10 , batch: 133 , training loss: 4.002753\n",
      "[INFO] Epoch: 10 , batch: 134 , training loss: 3.731458\n",
      "[INFO] Epoch: 10 , batch: 135 , training loss: 3.792051\n",
      "[INFO] Epoch: 10 , batch: 136 , training loss: 4.113279\n",
      "[INFO] Epoch: 10 , batch: 137 , training loss: 4.033787\n",
      "[INFO] Epoch: 10 , batch: 138 , training loss: 4.086899\n",
      "[INFO] Epoch: 10 , batch: 139 , training loss: 4.705314\n",
      "[INFO] Epoch: 10 , batch: 140 , training loss: 4.560800\n",
      "[INFO] Epoch: 10 , batch: 141 , training loss: 4.291562\n",
      "[INFO] Epoch: 10 , batch: 142 , training loss: 3.955394\n",
      "[INFO] Epoch: 10 , batch: 143 , training loss: 4.077787\n",
      "[INFO] Epoch: 10 , batch: 144 , training loss: 3.926537\n",
      "[INFO] Epoch: 10 , batch: 145 , training loss: 4.030387\n",
      "[INFO] Epoch: 10 , batch: 146 , training loss: 4.223653\n",
      "[INFO] Epoch: 10 , batch: 147 , training loss: 3.871508\n",
      "[INFO] Epoch: 10 , batch: 148 , training loss: 3.854360\n",
      "[INFO] Epoch: 10 , batch: 149 , training loss: 3.955776\n",
      "[INFO] Epoch: 10 , batch: 150 , training loss: 4.210982\n",
      "[INFO] Epoch: 10 , batch: 151 , training loss: 4.004185\n",
      "[INFO] Epoch: 10 , batch: 152 , training loss: 3.964262\n",
      "[INFO] Epoch: 10 , batch: 153 , training loss: 4.056417\n",
      "[INFO] Epoch: 10 , batch: 154 , training loss: 4.121871\n",
      "[INFO] Epoch: 10 , batch: 155 , training loss: 4.342509\n",
      "[INFO] Epoch: 10 , batch: 156 , training loss: 4.077419\n",
      "[INFO] Epoch: 10 , batch: 157 , training loss: 4.041091\n",
      "[INFO] Epoch: 10 , batch: 158 , training loss: 4.287832\n",
      "[INFO] Epoch: 10 , batch: 159 , training loss: 4.164246\n",
      "[INFO] Epoch: 10 , batch: 160 , training loss: 4.611928\n",
      "[INFO] Epoch: 10 , batch: 161 , training loss: 4.585043\n",
      "[INFO] Epoch: 10 , batch: 162 , training loss: 4.466345\n",
      "[INFO] Epoch: 10 , batch: 163 , training loss: 4.677553\n",
      "[INFO] Epoch: 10 , batch: 164 , training loss: 4.553220\n",
      "[INFO] Epoch: 10 , batch: 165 , training loss: 4.510593\n",
      "[INFO] Epoch: 10 , batch: 166 , training loss: 4.490441\n",
      "[INFO] Epoch: 10 , batch: 167 , training loss: 4.743081\n",
      "[INFO] Epoch: 10 , batch: 168 , training loss: 4.477826\n",
      "[INFO] Epoch: 10 , batch: 169 , training loss: 4.393085\n",
      "[INFO] Epoch: 10 , batch: 170 , training loss: 4.464346\n",
      "[INFO] Epoch: 10 , batch: 171 , training loss: 3.947201\n",
      "[INFO] Epoch: 10 , batch: 172 , training loss: 4.112329\n",
      "[INFO] Epoch: 10 , batch: 173 , training loss: 4.403412\n",
      "[INFO] Epoch: 10 , batch: 174 , training loss: 4.767795\n",
      "[INFO] Epoch: 10 , batch: 175 , training loss: 5.045102\n",
      "[INFO] Epoch: 10 , batch: 176 , training loss: 4.781140\n",
      "[INFO] Epoch: 10 , batch: 177 , training loss: 4.373309\n",
      "[INFO] Epoch: 10 , batch: 178 , training loss: 4.287510\n",
      "[INFO] Epoch: 10 , batch: 179 , training loss: 4.391199\n",
      "[INFO] Epoch: 10 , batch: 180 , training loss: 4.297506\n",
      "[INFO] Epoch: 10 , batch: 181 , training loss: 4.572893\n",
      "[INFO] Epoch: 10 , batch: 182 , training loss: 4.491914\n",
      "[INFO] Epoch: 10 , batch: 183 , training loss: 4.441692\n",
      "[INFO] Epoch: 10 , batch: 184 , training loss: 4.341202\n",
      "[INFO] Epoch: 10 , batch: 185 , training loss: 4.300913\n",
      "[INFO] Epoch: 10 , batch: 186 , training loss: 4.431448\n",
      "[INFO] Epoch: 10 , batch: 187 , training loss: 4.520969\n",
      "[INFO] Epoch: 10 , batch: 188 , training loss: 4.537506\n",
      "[INFO] Epoch: 10 , batch: 189 , training loss: 4.412083\n",
      "[INFO] Epoch: 10 , batch: 190 , training loss: 4.443316\n",
      "[INFO] Epoch: 10 , batch: 191 , training loss: 4.563322\n",
      "[INFO] Epoch: 10 , batch: 192 , training loss: 4.376818\n",
      "[INFO] Epoch: 10 , batch: 193 , training loss: 4.487968\n",
      "[INFO] Epoch: 10 , batch: 194 , training loss: 4.411292\n",
      "[INFO] Epoch: 10 , batch: 195 , training loss: 4.423957\n",
      "[INFO] Epoch: 10 , batch: 196 , training loss: 4.219143\n",
      "[INFO] Epoch: 10 , batch: 197 , training loss: 4.344736\n",
      "[INFO] Epoch: 10 , batch: 198 , training loss: 4.211643\n",
      "[INFO] Epoch: 10 , batch: 199 , training loss: 4.341269\n",
      "[INFO] Epoch: 10 , batch: 200 , training loss: 4.274299\n",
      "[INFO] Epoch: 10 , batch: 201 , training loss: 4.178981\n",
      "[INFO] Epoch: 10 , batch: 202 , training loss: 4.176960\n",
      "[INFO] Epoch: 10 , batch: 203 , training loss: 4.224906\n",
      "[INFO] Epoch: 10 , batch: 204 , training loss: 4.330432\n",
      "[INFO] Epoch: 10 , batch: 205 , training loss: 3.960506\n",
      "[INFO] Epoch: 10 , batch: 206 , training loss: 3.881496\n",
      "[INFO] Epoch: 10 , batch: 207 , training loss: 3.874625\n",
      "[INFO] Epoch: 10 , batch: 208 , training loss: 4.216267\n",
      "[INFO] Epoch: 10 , batch: 209 , training loss: 4.150423\n",
      "[INFO] Epoch: 10 , batch: 210 , training loss: 4.216174\n",
      "[INFO] Epoch: 10 , batch: 211 , training loss: 4.190733\n",
      "[INFO] Epoch: 10 , batch: 212 , training loss: 4.292204\n",
      "[INFO] Epoch: 10 , batch: 213 , training loss: 4.253767\n",
      "[INFO] Epoch: 10 , batch: 214 , training loss: 4.319394\n",
      "[INFO] Epoch: 10 , batch: 215 , training loss: 4.520141\n",
      "[INFO] Epoch: 10 , batch: 216 , training loss: 4.245941\n",
      "[INFO] Epoch: 10 , batch: 217 , training loss: 4.181928\n",
      "[INFO] Epoch: 10 , batch: 218 , training loss: 4.184181\n",
      "[INFO] Epoch: 10 , batch: 219 , training loss: 4.300361\n",
      "[INFO] Epoch: 10 , batch: 220 , training loss: 4.101655\n",
      "[INFO] Epoch: 10 , batch: 221 , training loss: 4.111207\n",
      "[INFO] Epoch: 10 , batch: 222 , training loss: 4.268912\n",
      "[INFO] Epoch: 10 , batch: 223 , training loss: 4.348167\n",
      "[INFO] Epoch: 10 , batch: 224 , training loss: 4.385365\n",
      "[INFO] Epoch: 10 , batch: 225 , training loss: 4.264450\n",
      "[INFO] Epoch: 10 , batch: 226 , training loss: 4.407439\n",
      "[INFO] Epoch: 10 , batch: 227 , training loss: 4.356608\n",
      "[INFO] Epoch: 10 , batch: 228 , training loss: 4.400169\n",
      "[INFO] Epoch: 10 , batch: 229 , training loss: 4.265517\n",
      "[INFO] Epoch: 10 , batch: 230 , training loss: 4.130824\n",
      "[INFO] Epoch: 10 , batch: 231 , training loss: 3.945966\n",
      "[INFO] Epoch: 10 , batch: 232 , training loss: 4.114682\n",
      "[INFO] Epoch: 10 , batch: 233 , training loss: 4.144529\n",
      "[INFO] Epoch: 10 , batch: 234 , training loss: 3.817779\n",
      "[INFO] Epoch: 10 , batch: 235 , training loss: 3.949831\n",
      "[INFO] Epoch: 10 , batch: 236 , training loss: 4.075883\n",
      "[INFO] Epoch: 10 , batch: 237 , training loss: 4.283540\n",
      "[INFO] Epoch: 10 , batch: 238 , training loss: 4.022537\n",
      "[INFO] Epoch: 10 , batch: 239 , training loss: 4.074585\n",
      "[INFO] Epoch: 10 , batch: 240 , training loss: 4.127021\n",
      "[INFO] Epoch: 10 , batch: 241 , training loss: 3.922004\n",
      "[INFO] Epoch: 10 , batch: 242 , training loss: 3.946924\n",
      "[INFO] Epoch: 10 , batch: 243 , training loss: 4.268500\n",
      "[INFO] Epoch: 10 , batch: 244 , training loss: 4.192031\n",
      "[INFO] Epoch: 10 , batch: 245 , training loss: 4.209637\n",
      "[INFO] Epoch: 10 , batch: 246 , training loss: 3.849785\n",
      "[INFO] Epoch: 10 , batch: 247 , training loss: 4.024232\n",
      "[INFO] Epoch: 10 , batch: 248 , training loss: 4.094507\n",
      "[INFO] Epoch: 10 , batch: 249 , training loss: 4.079981\n",
      "[INFO] Epoch: 10 , batch: 250 , training loss: 3.855619\n",
      "[INFO] Epoch: 10 , batch: 251 , training loss: 4.359370\n",
      "[INFO] Epoch: 10 , batch: 252 , training loss: 4.032728\n",
      "[INFO] Epoch: 10 , batch: 253 , training loss: 3.984677\n",
      "[INFO] Epoch: 10 , batch: 254 , training loss: 4.268847\n",
      "[INFO] Epoch: 10 , batch: 255 , training loss: 4.217969\n",
      "[INFO] Epoch: 10 , batch: 256 , training loss: 4.218749\n",
      "[INFO] Epoch: 10 , batch: 257 , training loss: 4.397749\n",
      "[INFO] Epoch: 10 , batch: 258 , training loss: 4.417046\n",
      "[INFO] Epoch: 10 , batch: 259 , training loss: 4.466777\n",
      "[INFO] Epoch: 10 , batch: 260 , training loss: 4.176648\n",
      "[INFO] Epoch: 10 , batch: 261 , training loss: 4.352856\n",
      "[INFO] Epoch: 10 , batch: 262 , training loss: 4.557675\n",
      "[INFO] Epoch: 10 , batch: 263 , training loss: 4.694378\n",
      "[INFO] Epoch: 10 , batch: 264 , training loss: 4.015174\n",
      "[INFO] Epoch: 10 , batch: 265 , training loss: 4.140940\n",
      "[INFO] Epoch: 10 , batch: 266 , training loss: 4.620932\n",
      "[INFO] Epoch: 10 , batch: 267 , training loss: 4.319522\n",
      "[INFO] Epoch: 10 , batch: 268 , training loss: 4.218967\n",
      "[INFO] Epoch: 10 , batch: 269 , training loss: 4.247725\n",
      "[INFO] Epoch: 10 , batch: 270 , training loss: 4.249482\n",
      "[INFO] Epoch: 10 , batch: 271 , training loss: 4.304037\n",
      "[INFO] Epoch: 10 , batch: 272 , training loss: 4.261788\n",
      "[INFO] Epoch: 10 , batch: 273 , training loss: 4.260474\n",
      "[INFO] Epoch: 10 , batch: 274 , training loss: 4.385971\n",
      "[INFO] Epoch: 10 , batch: 275 , training loss: 4.244350\n",
      "[INFO] Epoch: 10 , batch: 276 , training loss: 4.297599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 10 , batch: 277 , training loss: 4.452087\n",
      "[INFO] Epoch: 10 , batch: 278 , training loss: 4.076372\n",
      "[INFO] Epoch: 10 , batch: 279 , training loss: 4.106620\n",
      "[INFO] Epoch: 10 , batch: 280 , training loss: 4.047116\n",
      "[INFO] Epoch: 10 , batch: 281 , training loss: 4.195744\n",
      "[INFO] Epoch: 10 , batch: 282 , training loss: 4.095879\n",
      "[INFO] Epoch: 10 , batch: 283 , training loss: 4.133827\n",
      "[INFO] Epoch: 10 , batch: 284 , training loss: 4.167083\n",
      "[INFO] Epoch: 10 , batch: 285 , training loss: 4.143351\n",
      "[INFO] Epoch: 10 , batch: 286 , training loss: 4.113954\n",
      "[INFO] Epoch: 10 , batch: 287 , training loss: 4.028555\n",
      "[INFO] Epoch: 10 , batch: 288 , training loss: 4.045636\n",
      "[INFO] Epoch: 10 , batch: 289 , training loss: 4.086012\n",
      "[INFO] Epoch: 10 , batch: 290 , training loss: 3.871996\n",
      "[INFO] Epoch: 10 , batch: 291 , training loss: 3.861439\n",
      "[INFO] Epoch: 10 , batch: 292 , training loss: 3.976678\n",
      "[INFO] Epoch: 10 , batch: 293 , training loss: 3.897419\n",
      "[INFO] Epoch: 10 , batch: 294 , training loss: 4.598915\n",
      "[INFO] Epoch: 10 , batch: 295 , training loss: 4.338473\n",
      "[INFO] Epoch: 10 , batch: 296 , training loss: 4.258313\n",
      "[INFO] Epoch: 10 , batch: 297 , training loss: 4.221471\n",
      "[INFO] Epoch: 10 , batch: 298 , training loss: 4.051863\n",
      "[INFO] Epoch: 10 , batch: 299 , training loss: 4.079298\n",
      "[INFO] Epoch: 10 , batch: 300 , training loss: 4.059535\n",
      "[INFO] Epoch: 10 , batch: 301 , training loss: 4.004035\n",
      "[INFO] Epoch: 10 , batch: 302 , training loss: 4.172268\n",
      "[INFO] Epoch: 10 , batch: 303 , training loss: 4.183221\n",
      "[INFO] Epoch: 10 , batch: 304 , training loss: 4.371191\n",
      "[INFO] Epoch: 10 , batch: 305 , training loss: 4.117967\n",
      "[INFO] Epoch: 10 , batch: 306 , training loss: 4.252914\n",
      "[INFO] Epoch: 10 , batch: 307 , training loss: 4.231943\n",
      "[INFO] Epoch: 10 , batch: 308 , training loss: 4.093970\n",
      "[INFO] Epoch: 10 , batch: 309 , training loss: 4.099509\n",
      "[INFO] Epoch: 10 , batch: 310 , training loss: 3.974618\n",
      "[INFO] Epoch: 10 , batch: 311 , training loss: 3.995476\n",
      "[INFO] Epoch: 10 , batch: 312 , training loss: 3.884459\n",
      "[INFO] Epoch: 10 , batch: 313 , training loss: 4.012281\n",
      "[INFO] Epoch: 10 , batch: 314 , training loss: 4.103925\n",
      "[INFO] Epoch: 10 , batch: 315 , training loss: 4.140611\n",
      "[INFO] Epoch: 10 , batch: 316 , training loss: 4.452181\n",
      "[INFO] Epoch: 10 , batch: 317 , training loss: 4.888326\n",
      "[INFO] Epoch: 10 , batch: 318 , training loss: 5.012030\n",
      "[INFO] Epoch: 10 , batch: 319 , training loss: 4.649420\n",
      "[INFO] Epoch: 10 , batch: 320 , training loss: 4.128374\n",
      "[INFO] Epoch: 10 , batch: 321 , training loss: 3.930780\n",
      "[INFO] Epoch: 10 , batch: 322 , training loss: 4.055676\n",
      "[INFO] Epoch: 10 , batch: 323 , training loss: 4.072284\n",
      "[INFO] Epoch: 10 , batch: 324 , training loss: 4.075212\n",
      "[INFO] Epoch: 10 , batch: 325 , training loss: 4.203769\n",
      "[INFO] Epoch: 10 , batch: 326 , training loss: 4.238342\n",
      "[INFO] Epoch: 10 , batch: 327 , training loss: 4.174438\n",
      "[INFO] Epoch: 10 , batch: 328 , training loss: 4.161724\n",
      "[INFO] Epoch: 10 , batch: 329 , training loss: 4.045230\n",
      "[INFO] Epoch: 10 , batch: 330 , training loss: 4.064668\n",
      "[INFO] Epoch: 10 , batch: 331 , training loss: 4.256916\n",
      "[INFO] Epoch: 10 , batch: 332 , training loss: 4.026588\n",
      "[INFO] Epoch: 10 , batch: 333 , training loss: 4.025357\n",
      "[INFO] Epoch: 10 , batch: 334 , training loss: 4.085162\n",
      "[INFO] Epoch: 10 , batch: 335 , training loss: 4.187259\n",
      "[INFO] Epoch: 10 , batch: 336 , training loss: 4.199335\n",
      "[INFO] Epoch: 10 , batch: 337 , training loss: 4.272466\n",
      "[INFO] Epoch: 10 , batch: 338 , training loss: 4.464389\n",
      "[INFO] Epoch: 10 , batch: 339 , training loss: 4.266784\n",
      "[INFO] Epoch: 10 , batch: 340 , training loss: 4.461342\n",
      "[INFO] Epoch: 10 , batch: 341 , training loss: 4.206326\n",
      "[INFO] Epoch: 10 , batch: 342 , training loss: 3.994565\n",
      "[INFO] Epoch: 10 , batch: 343 , training loss: 4.063696\n",
      "[INFO] Epoch: 10 , batch: 344 , training loss: 3.926527\n",
      "[INFO] Epoch: 10 , batch: 345 , training loss: 4.070906\n",
      "[INFO] Epoch: 10 , batch: 346 , training loss: 4.109523\n",
      "[INFO] Epoch: 10 , batch: 347 , training loss: 4.042018\n",
      "[INFO] Epoch: 10 , batch: 348 , training loss: 4.185357\n",
      "[INFO] Epoch: 10 , batch: 349 , training loss: 4.246756\n",
      "[INFO] Epoch: 10 , batch: 350 , training loss: 4.066271\n",
      "[INFO] Epoch: 10 , batch: 351 , training loss: 4.145702\n",
      "[INFO] Epoch: 10 , batch: 352 , training loss: 4.154570\n",
      "[INFO] Epoch: 10 , batch: 353 , training loss: 4.106248\n",
      "[INFO] Epoch: 10 , batch: 354 , training loss: 4.230705\n",
      "[INFO] Epoch: 10 , batch: 355 , training loss: 4.240830\n",
      "[INFO] Epoch: 10 , batch: 356 , training loss: 4.117473\n",
      "[INFO] Epoch: 10 , batch: 357 , training loss: 4.188598\n",
      "[INFO] Epoch: 10 , batch: 358 , training loss: 4.109929\n",
      "[INFO] Epoch: 10 , batch: 359 , training loss: 4.093517\n",
      "[INFO] Epoch: 10 , batch: 360 , training loss: 4.165566\n",
      "[INFO] Epoch: 10 , batch: 361 , training loss: 4.146268\n",
      "[INFO] Epoch: 10 , batch: 362 , training loss: 4.236591\n",
      "[INFO] Epoch: 10 , batch: 363 , training loss: 4.137987\n",
      "[INFO] Epoch: 10 , batch: 364 , training loss: 4.184122\n",
      "[INFO] Epoch: 10 , batch: 365 , training loss: 4.086531\n",
      "[INFO] Epoch: 10 , batch: 366 , training loss: 4.231236\n",
      "[INFO] Epoch: 10 , batch: 367 , training loss: 4.261822\n",
      "[INFO] Epoch: 10 , batch: 368 , training loss: 4.760262\n",
      "[INFO] Epoch: 10 , batch: 369 , training loss: 4.381388\n",
      "[INFO] Epoch: 10 , batch: 370 , training loss: 4.132348\n",
      "[INFO] Epoch: 10 , batch: 371 , training loss: 4.599341\n",
      "[INFO] Epoch: 10 , batch: 372 , training loss: 4.911014\n",
      "[INFO] Epoch: 10 , batch: 373 , training loss: 4.916744\n",
      "[INFO] Epoch: 10 , batch: 374 , training loss: 5.009212\n",
      "[INFO] Epoch: 10 , batch: 375 , training loss: 4.982032\n",
      "[INFO] Epoch: 10 , batch: 376 , training loss: 4.925832\n",
      "[INFO] Epoch: 10 , batch: 377 , training loss: 4.641498\n",
      "[INFO] Epoch: 10 , batch: 378 , training loss: 4.718251\n",
      "[INFO] Epoch: 10 , batch: 379 , training loss: 4.701919\n",
      "[INFO] Epoch: 10 , batch: 380 , training loss: 4.811076\n",
      "[INFO] Epoch: 10 , batch: 381 , training loss: 4.586435\n",
      "[INFO] Epoch: 10 , batch: 382 , training loss: 4.871405\n",
      "[INFO] Epoch: 10 , batch: 383 , training loss: 4.911259\n",
      "[INFO] Epoch: 10 , batch: 384 , training loss: 4.902791\n",
      "[INFO] Epoch: 10 , batch: 385 , training loss: 4.638962\n",
      "[INFO] Epoch: 10 , batch: 386 , training loss: 4.863086\n",
      "[INFO] Epoch: 10 , batch: 387 , training loss: 4.779279\n",
      "[INFO] Epoch: 10 , batch: 388 , training loss: 4.559081\n",
      "[INFO] Epoch: 10 , batch: 389 , training loss: 4.383449\n",
      "[INFO] Epoch: 10 , batch: 390 , training loss: 4.368741\n",
      "[INFO] Epoch: 10 , batch: 391 , training loss: 4.394644\n",
      "[INFO] Epoch: 10 , batch: 392 , training loss: 4.754101\n",
      "[INFO] Epoch: 10 , batch: 393 , training loss: 4.651186\n",
      "[INFO] Epoch: 10 , batch: 394 , training loss: 4.726253\n",
      "[INFO] Epoch: 10 , batch: 395 , training loss: 4.521787\n",
      "[INFO] Epoch: 10 , batch: 396 , training loss: 4.339472\n",
      "[INFO] Epoch: 10 , batch: 397 , training loss: 4.495047\n",
      "[INFO] Epoch: 10 , batch: 398 , training loss: 4.357202\n",
      "[INFO] Epoch: 10 , batch: 399 , training loss: 4.421456\n",
      "[INFO] Epoch: 10 , batch: 400 , training loss: 4.390435\n",
      "[INFO] Epoch: 10 , batch: 401 , training loss: 4.812942\n",
      "[INFO] Epoch: 10 , batch: 402 , training loss: 4.532200\n",
      "[INFO] Epoch: 10 , batch: 403 , training loss: 4.368546\n",
      "[INFO] Epoch: 10 , batch: 404 , training loss: 4.523128\n",
      "[INFO] Epoch: 10 , batch: 405 , training loss: 4.595253\n",
      "[INFO] Epoch: 10 , batch: 406 , training loss: 4.490129\n",
      "[INFO] Epoch: 10 , batch: 407 , training loss: 4.549597\n",
      "[INFO] Epoch: 10 , batch: 408 , training loss: 4.473023\n",
      "[INFO] Epoch: 10 , batch: 409 , training loss: 4.492648\n",
      "[INFO] Epoch: 10 , batch: 410 , training loss: 4.570618\n",
      "[INFO] Epoch: 10 , batch: 411 , training loss: 4.738073\n",
      "[INFO] Epoch: 10 , batch: 412 , training loss: 4.573883\n",
      "[INFO] Epoch: 10 , batch: 413 , training loss: 4.443499\n",
      "[INFO] Epoch: 10 , batch: 414 , training loss: 4.476774\n",
      "[INFO] Epoch: 10 , batch: 415 , training loss: 4.516675\n",
      "[INFO] Epoch: 10 , batch: 416 , training loss: 4.590116\n",
      "[INFO] Epoch: 10 , batch: 417 , training loss: 4.480586\n",
      "[INFO] Epoch: 10 , batch: 418 , training loss: 4.528055\n",
      "[INFO] Epoch: 10 , batch: 419 , training loss: 4.469303\n",
      "[INFO] Epoch: 10 , batch: 420 , training loss: 4.453602\n",
      "[INFO] Epoch: 10 , batch: 421 , training loss: 4.451550\n",
      "[INFO] Epoch: 10 , batch: 422 , training loss: 4.321810\n",
      "[INFO] Epoch: 10 , batch: 423 , training loss: 4.541680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 10 , batch: 424 , training loss: 4.694019\n",
      "[INFO] Epoch: 10 , batch: 425 , training loss: 4.587776\n",
      "[INFO] Epoch: 10 , batch: 426 , training loss: 4.287522\n",
      "[INFO] Epoch: 10 , batch: 427 , training loss: 4.538501\n",
      "[INFO] Epoch: 10 , batch: 428 , training loss: 4.424311\n",
      "[INFO] Epoch: 10 , batch: 429 , training loss: 4.286613\n",
      "[INFO] Epoch: 10 , batch: 430 , training loss: 4.562868\n",
      "[INFO] Epoch: 10 , batch: 431 , training loss: 4.120381\n",
      "[INFO] Epoch: 10 , batch: 432 , training loss: 4.193088\n",
      "[INFO] Epoch: 10 , batch: 433 , training loss: 4.216028\n",
      "[INFO] Epoch: 10 , batch: 434 , training loss: 4.114288\n",
      "[INFO] Epoch: 10 , batch: 435 , training loss: 4.456573\n",
      "[INFO] Epoch: 10 , batch: 436 , training loss: 4.529157\n",
      "[INFO] Epoch: 10 , batch: 437 , training loss: 4.315980\n",
      "[INFO] Epoch: 10 , batch: 438 , training loss: 4.139173\n",
      "[INFO] Epoch: 10 , batch: 439 , training loss: 4.375789\n",
      "[INFO] Epoch: 10 , batch: 440 , training loss: 4.535594\n",
      "[INFO] Epoch: 10 , batch: 441 , training loss: 4.564660\n",
      "[INFO] Epoch: 10 , batch: 442 , training loss: 4.347063\n",
      "[INFO] Epoch: 10 , batch: 443 , training loss: 4.555165\n",
      "[INFO] Epoch: 10 , batch: 444 , training loss: 4.174420\n",
      "[INFO] Epoch: 10 , batch: 445 , training loss: 4.072400\n",
      "[INFO] Epoch: 10 , batch: 446 , training loss: 3.996012\n",
      "[INFO] Epoch: 10 , batch: 447 , training loss: 4.178264\n",
      "[INFO] Epoch: 10 , batch: 448 , training loss: 4.332122\n",
      "[INFO] Epoch: 10 , batch: 449 , training loss: 4.721078\n",
      "[INFO] Epoch: 10 , batch: 450 , training loss: 4.801267\n",
      "[INFO] Epoch: 10 , batch: 451 , training loss: 4.668637\n",
      "[INFO] Epoch: 10 , batch: 452 , training loss: 4.468106\n",
      "[INFO] Epoch: 10 , batch: 453 , training loss: 4.247971\n",
      "[INFO] Epoch: 10 , batch: 454 , training loss: 4.401639\n",
      "[INFO] Epoch: 10 , batch: 455 , training loss: 4.439383\n",
      "[INFO] Epoch: 10 , batch: 456 , training loss: 4.399063\n",
      "[INFO] Epoch: 10 , batch: 457 , training loss: 4.516665\n",
      "[INFO] Epoch: 10 , batch: 458 , training loss: 4.248708\n",
      "[INFO] Epoch: 10 , batch: 459 , training loss: 4.210406\n",
      "[INFO] Epoch: 10 , batch: 460 , training loss: 4.361504\n",
      "[INFO] Epoch: 10 , batch: 461 , training loss: 4.292261\n",
      "[INFO] Epoch: 10 , batch: 462 , training loss: 4.379114\n",
      "[INFO] Epoch: 10 , batch: 463 , training loss: 4.277388\n",
      "[INFO] Epoch: 10 , batch: 464 , training loss: 4.445406\n",
      "[INFO] Epoch: 10 , batch: 465 , training loss: 4.401208\n",
      "[INFO] Epoch: 10 , batch: 466 , training loss: 4.503092\n",
      "[INFO] Epoch: 10 , batch: 467 , training loss: 4.486173\n",
      "[INFO] Epoch: 10 , batch: 468 , training loss: 4.447517\n",
      "[INFO] Epoch: 10 , batch: 469 , training loss: 4.465998\n",
      "[INFO] Epoch: 10 , batch: 470 , training loss: 4.261808\n",
      "[INFO] Epoch: 10 , batch: 471 , training loss: 4.375929\n",
      "[INFO] Epoch: 10 , batch: 472 , training loss: 4.422133\n",
      "[INFO] Epoch: 10 , batch: 473 , training loss: 4.341667\n",
      "[INFO] Epoch: 10 , batch: 474 , training loss: 4.143190\n",
      "[INFO] Epoch: 10 , batch: 475 , training loss: 4.012071\n",
      "[INFO] Epoch: 10 , batch: 476 , training loss: 4.426050\n",
      "[INFO] Epoch: 10 , batch: 477 , training loss: 4.522748\n",
      "[INFO] Epoch: 10 , batch: 478 , training loss: 4.557741\n",
      "[INFO] Epoch: 10 , batch: 479 , training loss: 4.505390\n",
      "[INFO] Epoch: 10 , batch: 480 , training loss: 4.621178\n",
      "[INFO] Epoch: 10 , batch: 481 , training loss: 4.487551\n",
      "[INFO] Epoch: 10 , batch: 482 , training loss: 4.629203\n",
      "[INFO] Epoch: 10 , batch: 483 , training loss: 4.453155\n",
      "[INFO] Epoch: 10 , batch: 484 , training loss: 4.232005\n",
      "[INFO] Epoch: 10 , batch: 485 , training loss: 4.366561\n",
      "[INFO] Epoch: 10 , batch: 486 , training loss: 4.246510\n",
      "[INFO] Epoch: 10 , batch: 487 , training loss: 4.249580\n",
      "[INFO] Epoch: 10 , batch: 488 , training loss: 4.416620\n",
      "[INFO] Epoch: 10 , batch: 489 , training loss: 4.316757\n",
      "[INFO] Epoch: 10 , batch: 490 , training loss: 4.388765\n",
      "[INFO] Epoch: 10 , batch: 491 , training loss: 4.338869\n",
      "[INFO] Epoch: 10 , batch: 492 , training loss: 4.266043\n",
      "[INFO] Epoch: 10 , batch: 493 , training loss: 4.434572\n",
      "[INFO] Epoch: 10 , batch: 494 , training loss: 4.341776\n",
      "[INFO] Epoch: 10 , batch: 495 , training loss: 4.489387\n",
      "[INFO] Epoch: 10 , batch: 496 , training loss: 4.376810\n",
      "[INFO] Epoch: 10 , batch: 497 , training loss: 4.402460\n",
      "[INFO] Epoch: 10 , batch: 498 , training loss: 4.416471\n",
      "[INFO] Epoch: 10 , batch: 499 , training loss: 4.487772\n",
      "[INFO] Epoch: 10 , batch: 500 , training loss: 4.640754\n",
      "[INFO] Epoch: 10 , batch: 501 , training loss: 5.003098\n",
      "[INFO] Epoch: 10 , batch: 502 , training loss: 5.122028\n",
      "[INFO] Epoch: 10 , batch: 503 , training loss: 4.901752\n",
      "[INFO] Epoch: 10 , batch: 504 , training loss: 4.992476\n",
      "[INFO] Epoch: 10 , batch: 505 , training loss: 4.922390\n",
      "[INFO] Epoch: 10 , batch: 506 , training loss: 4.846166\n",
      "[INFO] Epoch: 10 , batch: 507 , training loss: 4.902007\n",
      "[INFO] Epoch: 10 , batch: 508 , training loss: 4.793587\n",
      "[INFO] Epoch: 10 , batch: 509 , training loss: 4.585248\n",
      "[INFO] Epoch: 10 , batch: 510 , training loss: 4.673729\n",
      "[INFO] Epoch: 10 , batch: 511 , training loss: 4.562919\n",
      "[INFO] Epoch: 10 , batch: 512 , training loss: 4.648225\n",
      "[INFO] Epoch: 10 , batch: 513 , training loss: 4.887835\n",
      "[INFO] Epoch: 10 , batch: 514 , training loss: 4.537850\n",
      "[INFO] Epoch: 10 , batch: 515 , training loss: 4.768176\n",
      "[INFO] Epoch: 10 , batch: 516 , training loss: 4.569961\n",
      "[INFO] Epoch: 10 , batch: 517 , training loss: 4.557762\n",
      "[INFO] Epoch: 10 , batch: 518 , training loss: 4.524011\n",
      "[INFO] Epoch: 10 , batch: 519 , training loss: 4.378378\n",
      "[INFO] Epoch: 10 , batch: 520 , training loss: 4.626977\n",
      "[INFO] Epoch: 10 , batch: 521 , training loss: 4.601854\n",
      "[INFO] Epoch: 10 , batch: 522 , training loss: 4.668777\n",
      "[INFO] Epoch: 10 , batch: 523 , training loss: 4.552992\n",
      "[INFO] Epoch: 10 , batch: 524 , training loss: 4.854949\n",
      "[INFO] Epoch: 10 , batch: 525 , training loss: 4.746452\n",
      "[INFO] Epoch: 10 , batch: 526 , training loss: 4.502098\n",
      "[INFO] Epoch: 10 , batch: 527 , training loss: 4.554664\n",
      "[INFO] Epoch: 10 , batch: 528 , training loss: 4.582888\n",
      "[INFO] Epoch: 10 , batch: 529 , training loss: 4.541902\n",
      "[INFO] Epoch: 10 , batch: 530 , training loss: 4.407448\n",
      "[INFO] Epoch: 10 , batch: 531 , training loss: 4.564347\n",
      "[INFO] Epoch: 10 , batch: 532 , training loss: 4.434134\n",
      "[INFO] Epoch: 10 , batch: 533 , training loss: 4.583129\n",
      "[INFO] Epoch: 10 , batch: 534 , training loss: 4.572318\n",
      "[INFO] Epoch: 10 , batch: 535 , training loss: 4.590685\n",
      "[INFO] Epoch: 10 , batch: 536 , training loss: 4.442781\n",
      "[INFO] Epoch: 10 , batch: 537 , training loss: 4.398571\n",
      "[INFO] Epoch: 10 , batch: 538 , training loss: 4.495951\n",
      "[INFO] Epoch: 10 , batch: 539 , training loss: 4.645456\n",
      "[INFO] Epoch: 10 , batch: 540 , training loss: 5.222026\n",
      "[INFO] Epoch: 10 , batch: 541 , training loss: 5.111248\n",
      "[INFO] Epoch: 10 , batch: 542 , training loss: 4.948045\n",
      "[INFO] Epoch: 11 , batch: 0 , training loss: 4.252202\n",
      "[INFO] Epoch: 11 , batch: 1 , training loss: 4.039586\n",
      "[INFO] Epoch: 11 , batch: 2 , training loss: 4.024845\n",
      "[INFO] Epoch: 11 , batch: 3 , training loss: 3.904158\n",
      "[INFO] Epoch: 11 , batch: 4 , training loss: 4.223646\n",
      "[INFO] Epoch: 11 , batch: 5 , training loss: 3.900130\n",
      "[INFO] Epoch: 11 , batch: 6 , training loss: 4.266654\n",
      "[INFO] Epoch: 11 , batch: 7 , training loss: 4.068069\n",
      "[INFO] Epoch: 11 , batch: 8 , training loss: 3.753214\n",
      "[INFO] Epoch: 11 , batch: 9 , training loss: 3.934275\n",
      "[INFO] Epoch: 11 , batch: 10 , training loss: 3.845463\n",
      "[INFO] Epoch: 11 , batch: 11 , training loss: 3.795340\n",
      "[INFO] Epoch: 11 , batch: 12 , training loss: 3.784130\n",
      "[INFO] Epoch: 11 , batch: 13 , training loss: 3.805430\n",
      "[INFO] Epoch: 11 , batch: 14 , training loss: 3.662184\n",
      "[INFO] Epoch: 11 , batch: 15 , training loss: 3.845634\n",
      "[INFO] Epoch: 11 , batch: 16 , training loss: 3.739339\n",
      "[INFO] Epoch: 11 , batch: 17 , training loss: 3.872125\n",
      "[INFO] Epoch: 11 , batch: 18 , training loss: 3.779096\n",
      "[INFO] Epoch: 11 , batch: 19 , training loss: 3.575343\n",
      "[INFO] Epoch: 11 , batch: 20 , training loss: 3.544627\n",
      "[INFO] Epoch: 11 , batch: 21 , training loss: 3.707785\n",
      "[INFO] Epoch: 11 , batch: 22 , training loss: 3.657246\n",
      "[INFO] Epoch: 11 , batch: 23 , training loss: 3.845548\n",
      "[INFO] Epoch: 11 , batch: 24 , training loss: 3.746601\n",
      "[INFO] Epoch: 11 , batch: 25 , training loss: 3.847833\n",
      "[INFO] Epoch: 11 , batch: 26 , training loss: 3.666156\n",
      "[INFO] Epoch: 11 , batch: 27 , training loss: 3.658555\n",
      "[INFO] Epoch: 11 , batch: 28 , training loss: 3.876942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 11 , batch: 29 , training loss: 3.672043\n",
      "[INFO] Epoch: 11 , batch: 30 , training loss: 3.650406\n",
      "[INFO] Epoch: 11 , batch: 31 , training loss: 3.784697\n",
      "[INFO] Epoch: 11 , batch: 32 , training loss: 3.758676\n",
      "[INFO] Epoch: 11 , batch: 33 , training loss: 3.854188\n",
      "[INFO] Epoch: 11 , batch: 34 , training loss: 3.805438\n",
      "[INFO] Epoch: 11 , batch: 35 , training loss: 3.727814\n",
      "[INFO] Epoch: 11 , batch: 36 , training loss: 3.822709\n",
      "[INFO] Epoch: 11 , batch: 37 , training loss: 3.663559\n",
      "[INFO] Epoch: 11 , batch: 38 , training loss: 3.809815\n",
      "[INFO] Epoch: 11 , batch: 39 , training loss: 3.582437\n",
      "[INFO] Epoch: 11 , batch: 40 , training loss: 3.802994\n",
      "[INFO] Epoch: 11 , batch: 41 , training loss: 3.859680\n",
      "[INFO] Epoch: 11 , batch: 42 , training loss: 4.373768\n",
      "[INFO] Epoch: 11 , batch: 43 , training loss: 4.012698\n",
      "[INFO] Epoch: 11 , batch: 44 , training loss: 4.339028\n",
      "[INFO] Epoch: 11 , batch: 45 , training loss: 4.328929\n",
      "[INFO] Epoch: 11 , batch: 46 , training loss: 4.433072\n",
      "[INFO] Epoch: 11 , batch: 47 , training loss: 4.046008\n",
      "[INFO] Epoch: 11 , batch: 48 , training loss: 4.016379\n",
      "[INFO] Epoch: 11 , batch: 49 , training loss: 4.174778\n",
      "[INFO] Epoch: 11 , batch: 50 , training loss: 3.896703\n",
      "[INFO] Epoch: 11 , batch: 51 , training loss: 4.037000\n",
      "[INFO] Epoch: 11 , batch: 52 , training loss: 3.877578\n",
      "[INFO] Epoch: 11 , batch: 53 , training loss: 3.972549\n",
      "[INFO] Epoch: 11 , batch: 54 , training loss: 3.991674\n",
      "[INFO] Epoch: 11 , batch: 55 , training loss: 4.085707\n",
      "[INFO] Epoch: 11 , batch: 56 , training loss: 3.901533\n",
      "[INFO] Epoch: 11 , batch: 57 , training loss: 3.825674\n",
      "[INFO] Epoch: 11 , batch: 58 , training loss: 3.857465\n",
      "[INFO] Epoch: 11 , batch: 59 , training loss: 3.975667\n",
      "[INFO] Epoch: 11 , batch: 60 , training loss: 3.864603\n",
      "[INFO] Epoch: 11 , batch: 61 , training loss: 3.950005\n",
      "[INFO] Epoch: 11 , batch: 62 , training loss: 3.829029\n",
      "[INFO] Epoch: 11 , batch: 63 , training loss: 3.998238\n",
      "[INFO] Epoch: 11 , batch: 64 , training loss: 4.234591\n",
      "[INFO] Epoch: 11 , batch: 65 , training loss: 3.879344\n",
      "[INFO] Epoch: 11 , batch: 66 , training loss: 3.751416\n",
      "[INFO] Epoch: 11 , batch: 67 , training loss: 3.750845\n",
      "[INFO] Epoch: 11 , batch: 68 , training loss: 4.020929\n",
      "[INFO] Epoch: 11 , batch: 69 , training loss: 3.861105\n",
      "[INFO] Epoch: 11 , batch: 70 , training loss: 4.092166\n",
      "[INFO] Epoch: 11 , batch: 71 , training loss: 3.964582\n",
      "[INFO] Epoch: 11 , batch: 72 , training loss: 4.037075\n",
      "[INFO] Epoch: 11 , batch: 73 , training loss: 3.940276\n",
      "[INFO] Epoch: 11 , batch: 74 , training loss: 4.089318\n",
      "[INFO] Epoch: 11 , batch: 75 , training loss: 3.856306\n",
      "[INFO] Epoch: 11 , batch: 76 , training loss: 3.999381\n",
      "[INFO] Epoch: 11 , batch: 77 , training loss: 3.943966\n",
      "[INFO] Epoch: 11 , batch: 78 , training loss: 4.054705\n",
      "[INFO] Epoch: 11 , batch: 79 , training loss: 3.861516\n",
      "[INFO] Epoch: 11 , batch: 80 , training loss: 4.062721\n",
      "[INFO] Epoch: 11 , batch: 81 , training loss: 4.017335\n",
      "[INFO] Epoch: 11 , batch: 82 , training loss: 3.992991\n",
      "[INFO] Epoch: 11 , batch: 83 , training loss: 4.119287\n",
      "[INFO] Epoch: 11 , batch: 84 , training loss: 4.052930\n",
      "[INFO] Epoch: 11 , batch: 85 , training loss: 4.126172\n",
      "[INFO] Epoch: 11 , batch: 86 , training loss: 4.083899\n",
      "[INFO] Epoch: 11 , batch: 87 , training loss: 4.054237\n",
      "[INFO] Epoch: 11 , batch: 88 , training loss: 4.178474\n",
      "[INFO] Epoch: 11 , batch: 89 , training loss: 4.000823\n",
      "[INFO] Epoch: 11 , batch: 90 , training loss: 4.052195\n",
      "[INFO] Epoch: 11 , batch: 91 , training loss: 3.983296\n",
      "[INFO] Epoch: 11 , batch: 92 , training loss: 3.987855\n",
      "[INFO] Epoch: 11 , batch: 93 , training loss: 4.075154\n",
      "[INFO] Epoch: 11 , batch: 94 , training loss: 4.238746\n",
      "[INFO] Epoch: 11 , batch: 95 , training loss: 4.047821\n",
      "[INFO] Epoch: 11 , batch: 96 , training loss: 4.001694\n",
      "[INFO] Epoch: 11 , batch: 97 , training loss: 3.966922\n",
      "[INFO] Epoch: 11 , batch: 98 , training loss: 3.898066\n",
      "[INFO] Epoch: 11 , batch: 99 , training loss: 4.027045\n",
      "[INFO] Epoch: 11 , batch: 100 , training loss: 3.872429\n",
      "[INFO] Epoch: 11 , batch: 101 , training loss: 3.929654\n",
      "[INFO] Epoch: 11 , batch: 102 , training loss: 4.088252\n",
      "[INFO] Epoch: 11 , batch: 103 , training loss: 3.852910\n",
      "[INFO] Epoch: 11 , batch: 104 , training loss: 3.801027\n",
      "[INFO] Epoch: 11 , batch: 105 , training loss: 4.067405\n",
      "[INFO] Epoch: 11 , batch: 106 , training loss: 4.113535\n",
      "[INFO] Epoch: 11 , batch: 107 , training loss: 3.931381\n",
      "[INFO] Epoch: 11 , batch: 108 , training loss: 3.844651\n",
      "[INFO] Epoch: 11 , batch: 109 , training loss: 3.769465\n",
      "[INFO] Epoch: 11 , batch: 110 , training loss: 3.995697\n",
      "[INFO] Epoch: 11 , batch: 111 , training loss: 4.065231\n",
      "[INFO] Epoch: 11 , batch: 112 , training loss: 3.999906\n",
      "[INFO] Epoch: 11 , batch: 113 , training loss: 3.961546\n",
      "[INFO] Epoch: 11 , batch: 114 , training loss: 4.008207\n",
      "[INFO] Epoch: 11 , batch: 115 , training loss: 3.978955\n",
      "[INFO] Epoch: 11 , batch: 116 , training loss: 3.871073\n",
      "[INFO] Epoch: 11 , batch: 117 , training loss: 4.152637\n",
      "[INFO] Epoch: 11 , batch: 118 , training loss: 4.117901\n",
      "[INFO] Epoch: 11 , batch: 119 , training loss: 4.264636\n",
      "[INFO] Epoch: 11 , batch: 120 , training loss: 4.211092\n",
      "[INFO] Epoch: 11 , batch: 121 , training loss: 4.051683\n",
      "[INFO] Epoch: 11 , batch: 122 , training loss: 3.953266\n",
      "[INFO] Epoch: 11 , batch: 123 , training loss: 3.990457\n",
      "[INFO] Epoch: 11 , batch: 124 , training loss: 4.123353\n",
      "[INFO] Epoch: 11 , batch: 125 , training loss: 3.856486\n",
      "[INFO] Epoch: 11 , batch: 126 , training loss: 3.903687\n",
      "[INFO] Epoch: 11 , batch: 127 , training loss: 3.913752\n",
      "[INFO] Epoch: 11 , batch: 128 , training loss: 4.070753\n",
      "[INFO] Epoch: 11 , batch: 129 , training loss: 3.972924\n",
      "[INFO] Epoch: 11 , batch: 130 , training loss: 4.016133\n",
      "[INFO] Epoch: 11 , batch: 131 , training loss: 4.004505\n",
      "[INFO] Epoch: 11 , batch: 132 , training loss: 4.057876\n",
      "[INFO] Epoch: 11 , batch: 133 , training loss: 3.978618\n",
      "[INFO] Epoch: 11 , batch: 134 , training loss: 3.742260\n",
      "[INFO] Epoch: 11 , batch: 135 , training loss: 3.769125\n",
      "[INFO] Epoch: 11 , batch: 136 , training loss: 4.109041\n",
      "[INFO] Epoch: 11 , batch: 137 , training loss: 4.016403\n",
      "[INFO] Epoch: 11 , batch: 138 , training loss: 4.079509\n",
      "[INFO] Epoch: 11 , batch: 139 , training loss: 4.682883\n",
      "[INFO] Epoch: 11 , batch: 140 , training loss: 4.520569\n",
      "[INFO] Epoch: 11 , batch: 141 , training loss: 4.248489\n",
      "[INFO] Epoch: 11 , batch: 142 , training loss: 3.924720\n",
      "[INFO] Epoch: 11 , batch: 143 , training loss: 4.049428\n",
      "[INFO] Epoch: 11 , batch: 144 , training loss: 3.901085\n",
      "[INFO] Epoch: 11 , batch: 145 , training loss: 3.998744\n",
      "[INFO] Epoch: 11 , batch: 146 , training loss: 4.169094\n",
      "[INFO] Epoch: 11 , batch: 147 , training loss: 3.840925\n",
      "[INFO] Epoch: 11 , batch: 148 , training loss: 3.831551\n",
      "[INFO] Epoch: 11 , batch: 149 , training loss: 3.912893\n",
      "[INFO] Epoch: 11 , batch: 150 , training loss: 4.195880\n",
      "[INFO] Epoch: 11 , batch: 151 , training loss: 3.987780\n",
      "[INFO] Epoch: 11 , batch: 152 , training loss: 3.960672\n",
      "[INFO] Epoch: 11 , batch: 153 , training loss: 4.040249\n",
      "[INFO] Epoch: 11 , batch: 154 , training loss: 4.110559\n",
      "[INFO] Epoch: 11 , batch: 155 , training loss: 4.310267\n",
      "[INFO] Epoch: 11 , batch: 156 , training loss: 4.051860\n",
      "[INFO] Epoch: 11 , batch: 157 , training loss: 4.025029\n",
      "[INFO] Epoch: 11 , batch: 158 , training loss: 4.236917\n",
      "[INFO] Epoch: 11 , batch: 159 , training loss: 4.159856\n",
      "[INFO] Epoch: 11 , batch: 160 , training loss: 4.529097\n",
      "[INFO] Epoch: 11 , batch: 161 , training loss: 4.556044\n",
      "[INFO] Epoch: 11 , batch: 162 , training loss: 4.448193\n",
      "[INFO] Epoch: 11 , batch: 163 , training loss: 4.655679\n",
      "[INFO] Epoch: 11 , batch: 164 , training loss: 4.530853\n",
      "[INFO] Epoch: 11 , batch: 165 , training loss: 4.490046\n",
      "[INFO] Epoch: 11 , batch: 166 , training loss: 4.422946\n",
      "[INFO] Epoch: 11 , batch: 167 , training loss: 4.701360\n",
      "[INFO] Epoch: 11 , batch: 168 , training loss: 4.403197\n",
      "[INFO] Epoch: 11 , batch: 169 , training loss: 4.351218\n",
      "[INFO] Epoch: 11 , batch: 170 , training loss: 4.429612\n",
      "[INFO] Epoch: 11 , batch: 171 , training loss: 3.901094\n",
      "[INFO] Epoch: 11 , batch: 172 , training loss: 4.077033\n",
      "[INFO] Epoch: 11 , batch: 173 , training loss: 4.388472\n",
      "[INFO] Epoch: 11 , batch: 174 , training loss: 4.776069\n",
      "[INFO] Epoch: 11 , batch: 175 , training loss: 5.021077\n",
      "[INFO] Epoch: 11 , batch: 176 , training loss: 4.765166\n",
      "[INFO] Epoch: 11 , batch: 177 , training loss: 4.355103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 11 , batch: 178 , training loss: 4.284648\n",
      "[INFO] Epoch: 11 , batch: 179 , training loss: 4.348002\n",
      "[INFO] Epoch: 11 , batch: 180 , training loss: 4.277561\n",
      "[INFO] Epoch: 11 , batch: 181 , training loss: 4.539659\n",
      "[INFO] Epoch: 11 , batch: 182 , training loss: 4.475884\n",
      "[INFO] Epoch: 11 , batch: 183 , training loss: 4.428103\n",
      "[INFO] Epoch: 11 , batch: 184 , training loss: 4.328281\n",
      "[INFO] Epoch: 11 , batch: 185 , training loss: 4.291950\n",
      "[INFO] Epoch: 11 , batch: 186 , training loss: 4.416493\n",
      "[INFO] Epoch: 11 , batch: 187 , training loss: 4.517644\n",
      "[INFO] Epoch: 11 , batch: 188 , training loss: 4.509438\n",
      "[INFO] Epoch: 11 , batch: 189 , training loss: 4.399512\n",
      "[INFO] Epoch: 11 , batch: 190 , training loss: 4.435869\n",
      "[INFO] Epoch: 11 , batch: 191 , training loss: 4.563133\n",
      "[INFO] Epoch: 11 , batch: 192 , training loss: 4.355486\n",
      "[INFO] Epoch: 11 , batch: 193 , training loss: 4.489182\n",
      "[INFO] Epoch: 11 , batch: 194 , training loss: 4.420442\n",
      "[INFO] Epoch: 11 , batch: 195 , training loss: 4.390799\n",
      "[INFO] Epoch: 11 , batch: 196 , training loss: 4.192686\n",
      "[INFO] Epoch: 11 , batch: 197 , training loss: 4.323437\n",
      "[INFO] Epoch: 11 , batch: 198 , training loss: 4.201781\n",
      "[INFO] Epoch: 11 , batch: 199 , training loss: 4.327696\n",
      "[INFO] Epoch: 11 , batch: 200 , training loss: 4.234667\n",
      "[INFO] Epoch: 11 , batch: 201 , training loss: 4.165471\n",
      "[INFO] Epoch: 11 , batch: 202 , training loss: 4.157315\n",
      "[INFO] Epoch: 11 , batch: 203 , training loss: 4.227741\n",
      "[INFO] Epoch: 11 , batch: 204 , training loss: 4.321264\n",
      "[INFO] Epoch: 11 , batch: 205 , training loss: 3.941894\n",
      "[INFO] Epoch: 11 , batch: 206 , training loss: 3.861072\n",
      "[INFO] Epoch: 11 , batch: 207 , training loss: 3.866615\n",
      "[INFO] Epoch: 11 , batch: 208 , training loss: 4.206273\n",
      "[INFO] Epoch: 11 , batch: 209 , training loss: 4.115452\n",
      "[INFO] Epoch: 11 , batch: 210 , training loss: 4.181414\n",
      "[INFO] Epoch: 11 , batch: 211 , training loss: 4.194051\n",
      "[INFO] Epoch: 11 , batch: 212 , training loss: 4.298732\n",
      "[INFO] Epoch: 11 , batch: 213 , training loss: 4.235356\n",
      "[INFO] Epoch: 11 , batch: 214 , training loss: 4.294399\n",
      "[INFO] Epoch: 11 , batch: 215 , training loss: 4.536586\n",
      "[INFO] Epoch: 11 , batch: 216 , training loss: 4.237117\n",
      "[INFO] Epoch: 11 , batch: 217 , training loss: 4.176235\n",
      "[INFO] Epoch: 11 , batch: 218 , training loss: 4.180011\n",
      "[INFO] Epoch: 11 , batch: 219 , training loss: 4.264893\n",
      "[INFO] Epoch: 11 , batch: 220 , training loss: 4.090707\n",
      "[INFO] Epoch: 11 , batch: 221 , training loss: 4.099390\n",
      "[INFO] Epoch: 11 , batch: 222 , training loss: 4.265552\n",
      "[INFO] Epoch: 11 , batch: 223 , training loss: 4.357974\n",
      "[INFO] Epoch: 11 , batch: 224 , training loss: 4.376177\n",
      "[INFO] Epoch: 11 , batch: 225 , training loss: 4.253828\n",
      "[INFO] Epoch: 11 , batch: 226 , training loss: 4.392410\n",
      "[INFO] Epoch: 11 , batch: 227 , training loss: 4.338471\n",
      "[INFO] Epoch: 11 , batch: 228 , training loss: 4.382017\n",
      "[INFO] Epoch: 11 , batch: 229 , training loss: 4.237625\n",
      "[INFO] Epoch: 11 , batch: 230 , training loss: 4.120914\n",
      "[INFO] Epoch: 11 , batch: 231 , training loss: 3.937834\n",
      "[INFO] Epoch: 11 , batch: 232 , training loss: 4.104853\n",
      "[INFO] Epoch: 11 , batch: 233 , training loss: 4.131207\n",
      "[INFO] Epoch: 11 , batch: 234 , training loss: 3.821967\n",
      "[INFO] Epoch: 11 , batch: 235 , training loss: 3.945211\n",
      "[INFO] Epoch: 11 , batch: 236 , training loss: 4.073117\n",
      "[INFO] Epoch: 11 , batch: 237 , training loss: 4.262691\n",
      "[INFO] Epoch: 11 , batch: 238 , training loss: 4.026333\n",
      "[INFO] Epoch: 11 , batch: 239 , training loss: 4.068811\n",
      "[INFO] Epoch: 11 , batch: 240 , training loss: 4.119018\n",
      "[INFO] Epoch: 11 , batch: 241 , training loss: 3.912342\n",
      "[INFO] Epoch: 11 , batch: 242 , training loss: 3.921476\n",
      "[INFO] Epoch: 11 , batch: 243 , training loss: 4.247939\n",
      "[INFO] Epoch: 11 , batch: 244 , training loss: 4.176332\n",
      "[INFO] Epoch: 11 , batch: 245 , training loss: 4.178915\n",
      "[INFO] Epoch: 11 , batch: 246 , training loss: 3.835225\n",
      "[INFO] Epoch: 11 , batch: 247 , training loss: 3.998836\n",
      "[INFO] Epoch: 11 , batch: 248 , training loss: 4.083205\n",
      "[INFO] Epoch: 11 , batch: 249 , training loss: 4.068865\n",
      "[INFO] Epoch: 11 , batch: 250 , training loss: 3.851094\n",
      "[INFO] Epoch: 11 , batch: 251 , training loss: 4.346327\n",
      "[INFO] Epoch: 11 , batch: 252 , training loss: 4.012887\n",
      "[INFO] Epoch: 11 , batch: 253 , training loss: 3.955598\n",
      "[INFO] Epoch: 11 , batch: 254 , training loss: 4.263229\n",
      "[INFO] Epoch: 11 , batch: 255 , training loss: 4.204730\n",
      "[INFO] Epoch: 11 , batch: 256 , training loss: 4.188138\n",
      "[INFO] Epoch: 11 , batch: 257 , training loss: 4.374306\n",
      "[INFO] Epoch: 11 , batch: 258 , training loss: 4.409338\n",
      "[INFO] Epoch: 11 , batch: 259 , training loss: 4.446999\n",
      "[INFO] Epoch: 11 , batch: 260 , training loss: 4.167101\n",
      "[INFO] Epoch: 11 , batch: 261 , training loss: 4.342832\n",
      "[INFO] Epoch: 11 , batch: 262 , training loss: 4.544446\n",
      "[INFO] Epoch: 11 , batch: 263 , training loss: 4.664167\n",
      "[INFO] Epoch: 11 , batch: 264 , training loss: 4.019871\n",
      "[INFO] Epoch: 11 , batch: 265 , training loss: 4.127956\n",
      "[INFO] Epoch: 11 , batch: 266 , training loss: 4.596236\n",
      "[INFO] Epoch: 11 , batch: 267 , training loss: 4.302547\n",
      "[INFO] Epoch: 11 , batch: 268 , training loss: 4.214027\n",
      "[INFO] Epoch: 11 , batch: 269 , training loss: 4.243714\n",
      "[INFO] Epoch: 11 , batch: 270 , training loss: 4.238944\n",
      "[INFO] Epoch: 11 , batch: 271 , training loss: 4.282792\n",
      "[INFO] Epoch: 11 , batch: 272 , training loss: 4.246982\n",
      "[INFO] Epoch: 11 , batch: 273 , training loss: 4.251364\n",
      "[INFO] Epoch: 11 , batch: 274 , training loss: 4.376465\n",
      "[INFO] Epoch: 11 , batch: 275 , training loss: 4.226507\n",
      "[INFO] Epoch: 11 , batch: 276 , training loss: 4.277095\n",
      "[INFO] Epoch: 11 , batch: 277 , training loss: 4.441613\n",
      "[INFO] Epoch: 11 , batch: 278 , training loss: 4.068299\n",
      "[INFO] Epoch: 11 , batch: 279 , training loss: 4.088592\n",
      "[INFO] Epoch: 11 , batch: 280 , training loss: 4.041662\n",
      "[INFO] Epoch: 11 , batch: 281 , training loss: 4.182481\n",
      "[INFO] Epoch: 11 , batch: 282 , training loss: 4.095555\n",
      "[INFO] Epoch: 11 , batch: 283 , training loss: 4.131180\n",
      "[INFO] Epoch: 11 , batch: 284 , training loss: 4.153130\n",
      "[INFO] Epoch: 11 , batch: 285 , training loss: 4.125893\n",
      "[INFO] Epoch: 11 , batch: 286 , training loss: 4.095091\n",
      "[INFO] Epoch: 11 , batch: 287 , training loss: 4.021167\n",
      "[INFO] Epoch: 11 , batch: 288 , training loss: 4.024930\n",
      "[INFO] Epoch: 11 , batch: 289 , training loss: 4.086837\n",
      "[INFO] Epoch: 11 , batch: 290 , training loss: 3.856717\n",
      "[INFO] Epoch: 11 , batch: 291 , training loss: 3.849702\n",
      "[INFO] Epoch: 11 , batch: 292 , training loss: 3.963335\n",
      "[INFO] Epoch: 11 , batch: 293 , training loss: 3.896405\n",
      "[INFO] Epoch: 11 , batch: 294 , training loss: 4.567246\n",
      "[INFO] Epoch: 11 , batch: 295 , training loss: 4.322408\n",
      "[INFO] Epoch: 11 , batch: 296 , training loss: 4.251053\n",
      "[INFO] Epoch: 11 , batch: 297 , training loss: 4.201900\n",
      "[INFO] Epoch: 11 , batch: 298 , training loss: 4.031992\n",
      "[INFO] Epoch: 11 , batch: 299 , training loss: 4.070030\n",
      "[INFO] Epoch: 11 , batch: 300 , training loss: 4.056850\n",
      "[INFO] Epoch: 11 , batch: 301 , training loss: 3.980591\n",
      "[INFO] Epoch: 11 , batch: 302 , training loss: 4.157074\n",
      "[INFO] Epoch: 11 , batch: 303 , training loss: 4.174989\n",
      "[INFO] Epoch: 11 , batch: 304 , training loss: 4.358666\n",
      "[INFO] Epoch: 11 , batch: 305 , training loss: 4.111785\n",
      "[INFO] Epoch: 11 , batch: 306 , training loss: 4.247821\n",
      "[INFO] Epoch: 11 , batch: 307 , training loss: 4.234143\n",
      "[INFO] Epoch: 11 , batch: 308 , training loss: 4.063850\n",
      "[INFO] Epoch: 11 , batch: 309 , training loss: 4.082633\n",
      "[INFO] Epoch: 11 , batch: 310 , training loss: 3.969593\n",
      "[INFO] Epoch: 11 , batch: 311 , training loss: 3.980931\n",
      "[INFO] Epoch: 11 , batch: 312 , training loss: 3.883591\n",
      "[INFO] Epoch: 11 , batch: 313 , training loss: 4.000998\n",
      "[INFO] Epoch: 11 , batch: 314 , training loss: 4.092310\n",
      "[INFO] Epoch: 11 , batch: 315 , training loss: 4.134233\n",
      "[INFO] Epoch: 11 , batch: 316 , training loss: 4.432660\n",
      "[INFO] Epoch: 11 , batch: 317 , training loss: 4.858389\n",
      "[INFO] Epoch: 11 , batch: 318 , training loss: 4.999235\n",
      "[INFO] Epoch: 11 , batch: 319 , training loss: 4.627284\n",
      "[INFO] Epoch: 11 , batch: 320 , training loss: 4.144213\n",
      "[INFO] Epoch: 11 , batch: 321 , training loss: 3.916592\n",
      "[INFO] Epoch: 11 , batch: 322 , training loss: 4.046275\n",
      "[INFO] Epoch: 11 , batch: 323 , training loss: 4.072368\n",
      "[INFO] Epoch: 11 , batch: 324 , training loss: 4.047850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 11 , batch: 325 , training loss: 4.197760\n",
      "[INFO] Epoch: 11 , batch: 326 , training loss: 4.241002\n",
      "[INFO] Epoch: 11 , batch: 327 , training loss: 4.169722\n",
      "[INFO] Epoch: 11 , batch: 328 , training loss: 4.150289\n",
      "[INFO] Epoch: 11 , batch: 329 , training loss: 4.033653\n",
      "[INFO] Epoch: 11 , batch: 330 , training loss: 4.060200\n",
      "[INFO] Epoch: 11 , batch: 331 , training loss: 4.243261\n",
      "[INFO] Epoch: 11 , batch: 332 , training loss: 4.024876\n",
      "[INFO] Epoch: 11 , batch: 333 , training loss: 4.022202\n",
      "[INFO] Epoch: 11 , batch: 334 , training loss: 4.064959\n",
      "[INFO] Epoch: 11 , batch: 335 , training loss: 4.177979\n",
      "[INFO] Epoch: 11 , batch: 336 , training loss: 4.187516\n",
      "[INFO] Epoch: 11 , batch: 337 , training loss: 4.256587\n",
      "[INFO] Epoch: 11 , batch: 338 , training loss: 4.438931\n",
      "[INFO] Epoch: 11 , batch: 339 , training loss: 4.259306\n",
      "[INFO] Epoch: 11 , batch: 340 , training loss: 4.458341\n",
      "[INFO] Epoch: 11 , batch: 341 , training loss: 4.186147\n",
      "[INFO] Epoch: 11 , batch: 342 , training loss: 3.981844\n",
      "[INFO] Epoch: 11 , batch: 343 , training loss: 4.042535\n",
      "[INFO] Epoch: 11 , batch: 344 , training loss: 3.911102\n",
      "[INFO] Epoch: 11 , batch: 345 , training loss: 4.068411\n",
      "[INFO] Epoch: 11 , batch: 346 , training loss: 4.102992\n",
      "[INFO] Epoch: 11 , batch: 347 , training loss: 4.035477\n",
      "[INFO] Epoch: 11 , batch: 348 , training loss: 4.151544\n",
      "[INFO] Epoch: 11 , batch: 349 , training loss: 4.236062\n",
      "[INFO] Epoch: 11 , batch: 350 , training loss: 4.056646\n",
      "[INFO] Epoch: 11 , batch: 351 , training loss: 4.134919\n",
      "[INFO] Epoch: 11 , batch: 352 , training loss: 4.142645\n",
      "[INFO] Epoch: 11 , batch: 353 , training loss: 4.102327\n",
      "[INFO] Epoch: 11 , batch: 354 , training loss: 4.226402\n",
      "[INFO] Epoch: 11 , batch: 355 , training loss: 4.238521\n",
      "[INFO] Epoch: 11 , batch: 356 , training loss: 4.097297\n",
      "[INFO] Epoch: 11 , batch: 357 , training loss: 4.182819\n",
      "[INFO] Epoch: 11 , batch: 358 , training loss: 4.098629\n",
      "[INFO] Epoch: 11 , batch: 359 , training loss: 4.079526\n",
      "[INFO] Epoch: 11 , batch: 360 , training loss: 4.156469\n",
      "[INFO] Epoch: 11 , batch: 361 , training loss: 4.134124\n",
      "[INFO] Epoch: 11 , batch: 362 , training loss: 4.226439\n",
      "[INFO] Epoch: 11 , batch: 363 , training loss: 4.125086\n",
      "[INFO] Epoch: 11 , batch: 364 , training loss: 4.182530\n",
      "[INFO] Epoch: 11 , batch: 365 , training loss: 4.062386\n",
      "[INFO] Epoch: 11 , batch: 366 , training loss: 4.208107\n",
      "[INFO] Epoch: 11 , batch: 367 , training loss: 4.249795\n",
      "[INFO] Epoch: 11 , batch: 368 , training loss: 4.735151\n",
      "[INFO] Epoch: 11 , batch: 369 , training loss: 4.380489\n",
      "[INFO] Epoch: 11 , batch: 370 , training loss: 4.121085\n",
      "[INFO] Epoch: 11 , batch: 371 , training loss: 4.597750\n",
      "[INFO] Epoch: 11 , batch: 372 , training loss: 4.876187\n",
      "[INFO] Epoch: 11 , batch: 373 , training loss: 4.903863\n",
      "[INFO] Epoch: 11 , batch: 374 , training loss: 4.998694\n",
      "[INFO] Epoch: 11 , batch: 375 , training loss: 4.960051\n",
      "[INFO] Epoch: 11 , batch: 376 , training loss: 4.892975\n",
      "[INFO] Epoch: 11 , batch: 377 , training loss: 4.609328\n",
      "[INFO] Epoch: 11 , batch: 378 , training loss: 4.686629\n",
      "[INFO] Epoch: 11 , batch: 379 , training loss: 4.673663\n",
      "[INFO] Epoch: 11 , batch: 380 , training loss: 4.790116\n",
      "[INFO] Epoch: 11 , batch: 381 , training loss: 4.567669\n",
      "[INFO] Epoch: 11 , batch: 382 , training loss: 4.852280\n",
      "[INFO] Epoch: 11 , batch: 383 , training loss: 4.891303\n",
      "[INFO] Epoch: 11 , batch: 384 , training loss: 4.914291\n",
      "[INFO] Epoch: 11 , batch: 385 , training loss: 4.591194\n",
      "[INFO] Epoch: 11 , batch: 386 , training loss: 4.798238\n",
      "[INFO] Epoch: 11 , batch: 387 , training loss: 4.742450\n",
      "[INFO] Epoch: 11 , batch: 388 , training loss: 4.520070\n",
      "[INFO] Epoch: 11 , batch: 389 , training loss: 4.353752\n",
      "[INFO] Epoch: 11 , batch: 390 , training loss: 4.338299\n",
      "[INFO] Epoch: 11 , batch: 391 , training loss: 4.356005\n",
      "[INFO] Epoch: 11 , batch: 392 , training loss: 4.735633\n",
      "[INFO] Epoch: 11 , batch: 393 , training loss: 4.633775\n",
      "[INFO] Epoch: 11 , batch: 394 , training loss: 4.707552\n",
      "[INFO] Epoch: 11 , batch: 395 , training loss: 4.499144\n",
      "[INFO] Epoch: 11 , batch: 396 , training loss: 4.318711\n",
      "[INFO] Epoch: 11 , batch: 397 , training loss: 4.469424\n",
      "[INFO] Epoch: 11 , batch: 398 , training loss: 4.333441\n",
      "[INFO] Epoch: 11 , batch: 399 , training loss: 4.409148\n",
      "[INFO] Epoch: 11 , batch: 400 , training loss: 4.374738\n",
      "[INFO] Epoch: 11 , batch: 401 , training loss: 4.818571\n",
      "[INFO] Epoch: 11 , batch: 402 , training loss: 4.526036\n",
      "[INFO] Epoch: 11 , batch: 403 , training loss: 4.348564\n",
      "[INFO] Epoch: 11 , batch: 404 , training loss: 4.519360\n",
      "[INFO] Epoch: 11 , batch: 405 , training loss: 4.587074\n",
      "[INFO] Epoch: 11 , batch: 406 , training loss: 4.480478\n",
      "[INFO] Epoch: 11 , batch: 407 , training loss: 4.542095\n",
      "[INFO] Epoch: 11 , batch: 408 , training loss: 4.465300\n",
      "[INFO] Epoch: 11 , batch: 409 , training loss: 4.489768\n",
      "[INFO] Epoch: 11 , batch: 410 , training loss: 4.557154\n",
      "[INFO] Epoch: 11 , batch: 411 , training loss: 4.735240\n",
      "[INFO] Epoch: 11 , batch: 412 , training loss: 4.550578\n",
      "[INFO] Epoch: 11 , batch: 413 , training loss: 4.432073\n",
      "[INFO] Epoch: 11 , batch: 414 , training loss: 4.463532\n",
      "[INFO] Epoch: 11 , batch: 415 , training loss: 4.505205\n",
      "[INFO] Epoch: 11 , batch: 416 , training loss: 4.575129\n",
      "[INFO] Epoch: 11 , batch: 417 , training loss: 4.463376\n",
      "[INFO] Epoch: 11 , batch: 418 , training loss: 4.531817\n",
      "[INFO] Epoch: 11 , batch: 419 , training loss: 4.472496\n",
      "[INFO] Epoch: 11 , batch: 420 , training loss: 4.437103\n",
      "[INFO] Epoch: 11 , batch: 421 , training loss: 4.445213\n",
      "[INFO] Epoch: 11 , batch: 422 , training loss: 4.308307\n",
      "[INFO] Epoch: 11 , batch: 423 , training loss: 4.532125\n",
      "[INFO] Epoch: 11 , batch: 424 , training loss: 4.690578\n",
      "[INFO] Epoch: 11 , batch: 425 , training loss: 4.579294\n",
      "[INFO] Epoch: 11 , batch: 426 , training loss: 4.261862\n",
      "[INFO] Epoch: 11 , batch: 427 , training loss: 4.540547\n",
      "[INFO] Epoch: 11 , batch: 428 , training loss: 4.400574\n",
      "[INFO] Epoch: 11 , batch: 429 , training loss: 4.296382\n",
      "[INFO] Epoch: 11 , batch: 430 , training loss: 4.552841\n",
      "[INFO] Epoch: 11 , batch: 431 , training loss: 4.124722\n",
      "[INFO] Epoch: 11 , batch: 432 , training loss: 4.172449\n",
      "[INFO] Epoch: 11 , batch: 433 , training loss: 4.199740\n",
      "[INFO] Epoch: 11 , batch: 434 , training loss: 4.104143\n",
      "[INFO] Epoch: 11 , batch: 435 , training loss: 4.457608\n",
      "[INFO] Epoch: 11 , batch: 436 , training loss: 4.519584\n",
      "[INFO] Epoch: 11 , batch: 437 , training loss: 4.312635\n",
      "[INFO] Epoch: 11 , batch: 438 , training loss: 4.125587\n",
      "[INFO] Epoch: 11 , batch: 439 , training loss: 4.373423\n",
      "[INFO] Epoch: 11 , batch: 440 , training loss: 4.524242\n",
      "[INFO] Epoch: 11 , batch: 441 , training loss: 4.559206\n",
      "[INFO] Epoch: 11 , batch: 442 , training loss: 4.341018\n",
      "[INFO] Epoch: 11 , batch: 443 , training loss: 4.549323\n",
      "[INFO] Epoch: 11 , batch: 444 , training loss: 4.163011\n",
      "[INFO] Epoch: 11 , batch: 445 , training loss: 4.073368\n",
      "[INFO] Epoch: 11 , batch: 446 , training loss: 3.989744\n",
      "[INFO] Epoch: 11 , batch: 447 , training loss: 4.172843\n",
      "[INFO] Epoch: 11 , batch: 448 , training loss: 4.324743\n",
      "[INFO] Epoch: 11 , batch: 449 , training loss: 4.712166\n",
      "[INFO] Epoch: 11 , batch: 450 , training loss: 4.797973\n",
      "[INFO] Epoch: 11 , batch: 451 , training loss: 4.662815\n",
      "[INFO] Epoch: 11 , batch: 452 , training loss: 4.457962\n",
      "[INFO] Epoch: 11 , batch: 453 , training loss: 4.232526\n",
      "[INFO] Epoch: 11 , batch: 454 , training loss: 4.396935\n",
      "[INFO] Epoch: 11 , batch: 455 , training loss: 4.437442\n",
      "[INFO] Epoch: 11 , batch: 456 , training loss: 4.402189\n",
      "[INFO] Epoch: 11 , batch: 457 , training loss: 4.501119\n",
      "[INFO] Epoch: 11 , batch: 458 , training loss: 4.243668\n",
      "[INFO] Epoch: 11 , batch: 459 , training loss: 4.210267\n",
      "[INFO] Epoch: 11 , batch: 460 , training loss: 4.350571\n",
      "[INFO] Epoch: 11 , batch: 461 , training loss: 4.281625\n",
      "[INFO] Epoch: 11 , batch: 462 , training loss: 4.359045\n",
      "[INFO] Epoch: 11 , batch: 463 , training loss: 4.263529\n",
      "[INFO] Epoch: 11 , batch: 464 , training loss: 4.444505\n",
      "[INFO] Epoch: 11 , batch: 465 , training loss: 4.392949\n",
      "[INFO] Epoch: 11 , batch: 466 , training loss: 4.486378\n",
      "[INFO] Epoch: 11 , batch: 467 , training loss: 4.468789\n",
      "[INFO] Epoch: 11 , batch: 468 , training loss: 4.431012\n",
      "[INFO] Epoch: 11 , batch: 469 , training loss: 4.457230\n",
      "[INFO] Epoch: 11 , batch: 470 , training loss: 4.237470\n",
      "[INFO] Epoch: 11 , batch: 471 , training loss: 4.355930\n",
      "[INFO] Epoch: 11 , batch: 472 , training loss: 4.413368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 11 , batch: 473 , training loss: 4.332541\n",
      "[INFO] Epoch: 11 , batch: 474 , training loss: 4.139182\n",
      "[INFO] Epoch: 11 , batch: 475 , training loss: 3.996634\n",
      "[INFO] Epoch: 11 , batch: 476 , training loss: 4.411034\n",
      "[INFO] Epoch: 11 , batch: 477 , training loss: 4.502493\n",
      "[INFO] Epoch: 11 , batch: 478 , training loss: 4.534169\n",
      "[INFO] Epoch: 11 , batch: 479 , training loss: 4.488636\n",
      "[INFO] Epoch: 11 , batch: 480 , training loss: 4.608919\n",
      "[INFO] Epoch: 11 , batch: 481 , training loss: 4.487627\n",
      "[INFO] Epoch: 11 , batch: 482 , training loss: 4.621130\n",
      "[INFO] Epoch: 11 , batch: 483 , training loss: 4.448769\n",
      "[INFO] Epoch: 11 , batch: 484 , training loss: 4.207228\n",
      "[INFO] Epoch: 11 , batch: 485 , training loss: 4.357998\n",
      "[INFO] Epoch: 11 , batch: 486 , training loss: 4.247019\n",
      "[INFO] Epoch: 11 , batch: 487 , training loss: 4.238277\n",
      "[INFO] Epoch: 11 , batch: 488 , training loss: 4.404225\n",
      "[INFO] Epoch: 11 , batch: 489 , training loss: 4.307827\n",
      "[INFO] Epoch: 11 , batch: 490 , training loss: 4.378344\n",
      "[INFO] Epoch: 11 , batch: 491 , training loss: 4.327991\n",
      "[INFO] Epoch: 11 , batch: 492 , training loss: 4.263079\n",
      "[INFO] Epoch: 11 , batch: 493 , training loss: 4.422352\n",
      "[INFO] Epoch: 11 , batch: 494 , training loss: 4.340006\n",
      "[INFO] Epoch: 11 , batch: 495 , training loss: 4.479691\n",
      "[INFO] Epoch: 11 , batch: 496 , training loss: 4.364346\n",
      "[INFO] Epoch: 11 , batch: 497 , training loss: 4.397027\n",
      "[INFO] Epoch: 11 , batch: 498 , training loss: 4.406775\n",
      "[INFO] Epoch: 11 , batch: 499 , training loss: 4.481509\n",
      "[INFO] Epoch: 11 , batch: 500 , training loss: 4.610802\n",
      "[INFO] Epoch: 11 , batch: 501 , training loss: 4.993699\n",
      "[INFO] Epoch: 11 , batch: 502 , training loss: 5.143757\n",
      "[INFO] Epoch: 11 , batch: 503 , training loss: 4.880025\n",
      "[INFO] Epoch: 11 , batch: 504 , training loss: 4.970362\n",
      "[INFO] Epoch: 11 , batch: 505 , training loss: 4.893196\n",
      "[INFO] Epoch: 11 , batch: 506 , training loss: 4.844608\n",
      "[INFO] Epoch: 11 , batch: 507 , training loss: 4.894424\n",
      "[INFO] Epoch: 11 , batch: 508 , training loss: 4.792968\n",
      "[INFO] Epoch: 11 , batch: 509 , training loss: 4.568126\n",
      "[INFO] Epoch: 11 , batch: 510 , training loss: 4.652007\n",
      "[INFO] Epoch: 11 , batch: 511 , training loss: 4.569893\n",
      "[INFO] Epoch: 11 , batch: 512 , training loss: 4.622715\n",
      "[INFO] Epoch: 11 , batch: 513 , training loss: 4.894873\n",
      "[INFO] Epoch: 11 , batch: 514 , training loss: 4.531247\n",
      "[INFO] Epoch: 11 , batch: 515 , training loss: 4.769480\n",
      "[INFO] Epoch: 11 , batch: 516 , training loss: 4.564722\n",
      "[INFO] Epoch: 11 , batch: 517 , training loss: 4.536450\n",
      "[INFO] Epoch: 11 , batch: 518 , training loss: 4.517567\n",
      "[INFO] Epoch: 11 , batch: 519 , training loss: 4.368785\n",
      "[INFO] Epoch: 11 , batch: 520 , training loss: 4.626740\n",
      "[INFO] Epoch: 11 , batch: 521 , training loss: 4.593352\n",
      "[INFO] Epoch: 11 , batch: 522 , training loss: 4.658235\n",
      "[INFO] Epoch: 11 , batch: 523 , training loss: 4.553029\n",
      "[INFO] Epoch: 11 , batch: 524 , training loss: 4.853062\n",
      "[INFO] Epoch: 11 , batch: 525 , training loss: 4.723732\n",
      "[INFO] Epoch: 11 , batch: 526 , training loss: 4.502348\n",
      "[INFO] Epoch: 11 , batch: 527 , training loss: 4.536995\n",
      "[INFO] Epoch: 11 , batch: 528 , training loss: 4.563301\n",
      "[INFO] Epoch: 11 , batch: 529 , training loss: 4.540443\n",
      "[INFO] Epoch: 11 , batch: 530 , training loss: 4.389187\n",
      "[INFO] Epoch: 11 , batch: 531 , training loss: 4.556813\n",
      "[INFO] Epoch: 11 , batch: 532 , training loss: 4.415699\n",
      "[INFO] Epoch: 11 , batch: 533 , training loss: 4.576660\n",
      "[INFO] Epoch: 11 , batch: 534 , training loss: 4.568744\n",
      "[INFO] Epoch: 11 , batch: 535 , training loss: 4.567662\n",
      "[INFO] Epoch: 11 , batch: 536 , training loss: 4.437518\n",
      "[INFO] Epoch: 11 , batch: 537 , training loss: 4.377775\n",
      "[INFO] Epoch: 11 , batch: 538 , training loss: 4.473412\n",
      "[INFO] Epoch: 11 , batch: 539 , training loss: 4.628669\n",
      "[INFO] Epoch: 11 , batch: 540 , training loss: 5.203388\n",
      "[INFO] Epoch: 11 , batch: 541 , training loss: 5.083115\n",
      "[INFO] Epoch: 11 , batch: 542 , training loss: 4.932823\n",
      "[INFO] Epoch: 12 , batch: 0 , training loss: 4.176372\n",
      "[INFO] Epoch: 12 , batch: 1 , training loss: 3.982991\n",
      "[INFO] Epoch: 12 , batch: 2 , training loss: 3.984545\n",
      "[INFO] Epoch: 12 , batch: 3 , training loss: 3.836743\n",
      "[INFO] Epoch: 12 , batch: 4 , training loss: 4.202742\n",
      "[INFO] Epoch: 12 , batch: 5 , training loss: 3.824868\n",
      "[INFO] Epoch: 12 , batch: 6 , training loss: 4.214415\n",
      "[INFO] Epoch: 12 , batch: 7 , training loss: 4.033544\n",
      "[INFO] Epoch: 12 , batch: 8 , training loss: 3.702244\n",
      "[INFO] Epoch: 12 , batch: 9 , training loss: 3.916840\n",
      "[INFO] Epoch: 12 , batch: 10 , training loss: 3.841550\n",
      "[INFO] Epoch: 12 , batch: 11 , training loss: 3.776437\n",
      "[INFO] Epoch: 12 , batch: 12 , training loss: 3.755673\n",
      "[INFO] Epoch: 12 , batch: 13 , training loss: 3.784175\n",
      "[INFO] Epoch: 12 , batch: 14 , training loss: 3.621173\n",
      "[INFO] Epoch: 12 , batch: 15 , training loss: 3.823995\n",
      "[INFO] Epoch: 12 , batch: 16 , training loss: 3.721920\n",
      "[INFO] Epoch: 12 , batch: 17 , training loss: 3.825275\n",
      "[INFO] Epoch: 12 , batch: 18 , training loss: 3.757185\n",
      "[INFO] Epoch: 12 , batch: 19 , training loss: 3.553857\n",
      "[INFO] Epoch: 12 , batch: 20 , training loss: 3.516260\n",
      "[INFO] Epoch: 12 , batch: 21 , training loss: 3.681572\n",
      "[INFO] Epoch: 12 , batch: 22 , training loss: 3.608005\n",
      "[INFO] Epoch: 12 , batch: 23 , training loss: 3.789064\n",
      "[INFO] Epoch: 12 , batch: 24 , training loss: 3.696970\n",
      "[INFO] Epoch: 12 , batch: 25 , training loss: 3.822980\n",
      "[INFO] Epoch: 12 , batch: 26 , training loss: 3.637446\n",
      "[INFO] Epoch: 12 , batch: 27 , training loss: 3.609352\n",
      "[INFO] Epoch: 12 , batch: 28 , training loss: 3.839415\n",
      "[INFO] Epoch: 12 , batch: 29 , training loss: 3.620347\n",
      "[INFO] Epoch: 12 , batch: 30 , training loss: 3.618035\n",
      "[INFO] Epoch: 12 , batch: 31 , training loss: 3.766675\n",
      "[INFO] Epoch: 12 , batch: 32 , training loss: 3.743662\n",
      "[INFO] Epoch: 12 , batch: 33 , training loss: 3.782052\n",
      "[INFO] Epoch: 12 , batch: 34 , training loss: 3.766544\n",
      "[INFO] Epoch: 12 , batch: 35 , training loss: 3.686520\n",
      "[INFO] Epoch: 12 , batch: 36 , training loss: 3.779765\n",
      "[INFO] Epoch: 12 , batch: 37 , training loss: 3.653184\n",
      "[INFO] Epoch: 12 , batch: 38 , training loss: 3.778948\n",
      "[INFO] Epoch: 12 , batch: 39 , training loss: 3.552733\n",
      "[INFO] Epoch: 12 , batch: 40 , training loss: 3.776809\n",
      "[INFO] Epoch: 12 , batch: 41 , training loss: 3.818673\n",
      "[INFO] Epoch: 12 , batch: 42 , training loss: 4.297957\n",
      "[INFO] Epoch: 12 , batch: 43 , training loss: 3.981008\n",
      "[INFO] Epoch: 12 , batch: 44 , training loss: 4.309567\n",
      "[INFO] Epoch: 12 , batch: 45 , training loss: 4.292461\n",
      "[INFO] Epoch: 12 , batch: 46 , training loss: 4.322553\n",
      "[INFO] Epoch: 12 , batch: 47 , training loss: 3.982604\n",
      "[INFO] Epoch: 12 , batch: 48 , training loss: 3.937497\n",
      "[INFO] Epoch: 12 , batch: 49 , training loss: 4.102613\n",
      "[INFO] Epoch: 12 , batch: 50 , training loss: 3.828974\n",
      "[INFO] Epoch: 12 , batch: 51 , training loss: 4.005398\n",
      "[INFO] Epoch: 12 , batch: 52 , training loss: 3.833285\n",
      "[INFO] Epoch: 12 , batch: 53 , training loss: 3.963264\n",
      "[INFO] Epoch: 12 , batch: 54 , training loss: 3.965921\n",
      "[INFO] Epoch: 12 , batch: 55 , training loss: 4.051795\n",
      "[INFO] Epoch: 12 , batch: 56 , training loss: 3.890881\n",
      "[INFO] Epoch: 12 , batch: 57 , training loss: 3.790973\n",
      "[INFO] Epoch: 12 , batch: 58 , training loss: 3.821850\n",
      "[INFO] Epoch: 12 , batch: 59 , training loss: 3.926242\n",
      "[INFO] Epoch: 12 , batch: 60 , training loss: 3.826023\n",
      "[INFO] Epoch: 12 , batch: 61 , training loss: 3.932902\n",
      "[INFO] Epoch: 12 , batch: 62 , training loss: 3.804309\n",
      "[INFO] Epoch: 12 , batch: 63 , training loss: 3.978307\n",
      "[INFO] Epoch: 12 , batch: 64 , training loss: 4.186080\n",
      "[INFO] Epoch: 12 , batch: 65 , training loss: 3.859153\n",
      "[INFO] Epoch: 12 , batch: 66 , training loss: 3.773170\n",
      "[INFO] Epoch: 12 , batch: 67 , training loss: 3.727536\n",
      "[INFO] Epoch: 12 , batch: 68 , training loss: 3.964188\n",
      "[INFO] Epoch: 12 , batch: 69 , training loss: 3.850798\n",
      "[INFO] Epoch: 12 , batch: 70 , training loss: 4.070634\n",
      "[INFO] Epoch: 12 , batch: 71 , training loss: 3.949511\n",
      "[INFO] Epoch: 12 , batch: 72 , training loss: 4.005471\n",
      "[INFO] Epoch: 12 , batch: 73 , training loss: 3.905465\n",
      "[INFO] Epoch: 12 , batch: 74 , training loss: 4.032866\n",
      "[INFO] Epoch: 12 , batch: 75 , training loss: 3.847031\n",
      "[INFO] Epoch: 12 , batch: 76 , training loss: 4.002654\n",
      "[INFO] Epoch: 12 , batch: 77 , training loss: 3.915128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 12 , batch: 78 , training loss: 4.053800\n",
      "[INFO] Epoch: 12 , batch: 79 , training loss: 3.859146\n",
      "[INFO] Epoch: 12 , batch: 80 , training loss: 4.050510\n",
      "[INFO] Epoch: 12 , batch: 81 , training loss: 4.009046\n",
      "[INFO] Epoch: 12 , batch: 82 , training loss: 3.976976\n",
      "[INFO] Epoch: 12 , batch: 83 , training loss: 4.090712\n",
      "[INFO] Epoch: 12 , batch: 84 , training loss: 4.032284\n",
      "[INFO] Epoch: 12 , batch: 85 , training loss: 4.119580\n",
      "[INFO] Epoch: 12 , batch: 86 , training loss: 4.073442\n",
      "[INFO] Epoch: 12 , batch: 87 , training loss: 4.030575\n",
      "[INFO] Epoch: 12 , batch: 88 , training loss: 4.160842\n",
      "[INFO] Epoch: 12 , batch: 89 , training loss: 3.970479\n",
      "[INFO] Epoch: 12 , batch: 90 , training loss: 4.028374\n",
      "[INFO] Epoch: 12 , batch: 91 , training loss: 3.963771\n",
      "[INFO] Epoch: 12 , batch: 92 , training loss: 3.990490\n",
      "[INFO] Epoch: 12 , batch: 93 , training loss: 4.093769\n",
      "[INFO] Epoch: 12 , batch: 94 , training loss: 4.211681\n",
      "[INFO] Epoch: 12 , batch: 95 , training loss: 4.019297\n",
      "[INFO] Epoch: 12 , batch: 96 , training loss: 3.973220\n",
      "[INFO] Epoch: 12 , batch: 97 , training loss: 3.968400\n",
      "[INFO] Epoch: 12 , batch: 98 , training loss: 3.861472\n",
      "[INFO] Epoch: 12 , batch: 99 , training loss: 4.023658\n",
      "[INFO] Epoch: 12 , batch: 100 , training loss: 3.868446\n",
      "[INFO] Epoch: 12 , batch: 101 , training loss: 3.898926\n",
      "[INFO] Epoch: 12 , batch: 102 , training loss: 4.089606\n",
      "[INFO] Epoch: 12 , batch: 103 , training loss: 3.848498\n",
      "[INFO] Epoch: 12 , batch: 104 , training loss: 3.795953\n",
      "[INFO] Epoch: 12 , batch: 105 , training loss: 4.054881\n",
      "[INFO] Epoch: 12 , batch: 106 , training loss: 4.078759\n",
      "[INFO] Epoch: 12 , batch: 107 , training loss: 3.925064\n",
      "[INFO] Epoch: 12 , batch: 108 , training loss: 3.860669\n",
      "[INFO] Epoch: 12 , batch: 109 , training loss: 3.749647\n",
      "[INFO] Epoch: 12 , batch: 110 , training loss: 3.980594\n",
      "[INFO] Epoch: 12 , batch: 111 , training loss: 4.018497\n",
      "[INFO] Epoch: 12 , batch: 112 , training loss: 3.972556\n",
      "[INFO] Epoch: 12 , batch: 113 , training loss: 3.939509\n",
      "[INFO] Epoch: 12 , batch: 114 , training loss: 3.997197\n",
      "[INFO] Epoch: 12 , batch: 115 , training loss: 3.967367\n",
      "[INFO] Epoch: 12 , batch: 116 , training loss: 3.883779\n",
      "[INFO] Epoch: 12 , batch: 117 , training loss: 4.121632\n",
      "[INFO] Epoch: 12 , batch: 118 , training loss: 4.080608\n",
      "[INFO] Epoch: 12 , batch: 119 , training loss: 4.221502\n",
      "[INFO] Epoch: 12 , batch: 120 , training loss: 4.203270\n",
      "[INFO] Epoch: 12 , batch: 121 , training loss: 4.060276\n",
      "[INFO] Epoch: 12 , batch: 122 , training loss: 3.956458\n",
      "[INFO] Epoch: 12 , batch: 123 , training loss: 3.974161\n",
      "[INFO] Epoch: 12 , batch: 124 , training loss: 4.088290\n",
      "[INFO] Epoch: 12 , batch: 125 , training loss: 3.847891\n",
      "[INFO] Epoch: 12 , batch: 126 , training loss: 3.871078\n",
      "[INFO] Epoch: 12 , batch: 127 , training loss: 3.910583\n",
      "[INFO] Epoch: 12 , batch: 128 , training loss: 4.055233\n",
      "[INFO] Epoch: 12 , batch: 129 , training loss: 3.985124\n",
      "[INFO] Epoch: 12 , batch: 130 , training loss: 3.993173\n",
      "[INFO] Epoch: 12 , batch: 131 , training loss: 4.003215\n",
      "[INFO] Epoch: 12 , batch: 132 , training loss: 4.026481\n",
      "[INFO] Epoch: 12 , batch: 133 , training loss: 3.955056\n",
      "[INFO] Epoch: 12 , batch: 134 , training loss: 3.725167\n",
      "[INFO] Epoch: 12 , batch: 135 , training loss: 3.774341\n",
      "[INFO] Epoch: 12 , batch: 136 , training loss: 4.098072\n",
      "[INFO] Epoch: 12 , batch: 137 , training loss: 4.002707\n",
      "[INFO] Epoch: 12 , batch: 138 , training loss: 4.074084\n",
      "[INFO] Epoch: 12 , batch: 139 , training loss: 4.666806\n",
      "[INFO] Epoch: 12 , batch: 140 , training loss: 4.480224\n",
      "[INFO] Epoch: 12 , batch: 141 , training loss: 4.202090\n",
      "[INFO] Epoch: 12 , batch: 142 , training loss: 3.910342\n",
      "[INFO] Epoch: 12 , batch: 143 , training loss: 4.050101\n",
      "[INFO] Epoch: 12 , batch: 144 , training loss: 3.887761\n",
      "[INFO] Epoch: 12 , batch: 145 , training loss: 3.997380\n",
      "[INFO] Epoch: 12 , batch: 146 , training loss: 4.157469\n",
      "[INFO] Epoch: 12 , batch: 147 , training loss: 3.833674\n",
      "[INFO] Epoch: 12 , batch: 148 , training loss: 3.815716\n",
      "[INFO] Epoch: 12 , batch: 149 , training loss: 3.924003\n",
      "[INFO] Epoch: 12 , batch: 150 , training loss: 4.171200\n",
      "[INFO] Epoch: 12 , batch: 151 , training loss: 3.982147\n",
      "[INFO] Epoch: 12 , batch: 152 , training loss: 3.946943\n",
      "[INFO] Epoch: 12 , batch: 153 , training loss: 4.014114\n",
      "[INFO] Epoch: 12 , batch: 154 , training loss: 4.079482\n",
      "[INFO] Epoch: 12 , batch: 155 , training loss: 4.267719\n",
      "[INFO] Epoch: 12 , batch: 156 , training loss: 4.045975\n",
      "[INFO] Epoch: 12 , batch: 157 , training loss: 4.012825\n",
      "[INFO] Epoch: 12 , batch: 158 , training loss: 4.215714\n",
      "[INFO] Epoch: 12 , batch: 159 , training loss: 4.111820\n",
      "[INFO] Epoch: 12 , batch: 160 , training loss: 4.498221\n",
      "[INFO] Epoch: 12 , batch: 161 , training loss: 4.515128\n",
      "[INFO] Epoch: 12 , batch: 162 , training loss: 4.435689\n",
      "[INFO] Epoch: 12 , batch: 163 , training loss: 4.635168\n",
      "[INFO] Epoch: 12 , batch: 164 , training loss: 4.515817\n",
      "[INFO] Epoch: 12 , batch: 165 , training loss: 4.444406\n",
      "[INFO] Epoch: 12 , batch: 166 , training loss: 4.395437\n",
      "[INFO] Epoch: 12 , batch: 167 , training loss: 4.657278\n",
      "[INFO] Epoch: 12 , batch: 168 , training loss: 4.364797\n",
      "[INFO] Epoch: 12 , batch: 169 , training loss: 4.264904\n",
      "[INFO] Epoch: 12 , batch: 170 , training loss: 4.378377\n",
      "[INFO] Epoch: 12 , batch: 171 , training loss: 3.877991\n",
      "[INFO] Epoch: 12 , batch: 172 , training loss: 4.036353\n",
      "[INFO] Epoch: 12 , batch: 173 , training loss: 4.351681\n",
      "[INFO] Epoch: 12 , batch: 174 , training loss: 4.723072\n",
      "[INFO] Epoch: 12 , batch: 175 , training loss: 4.980955\n",
      "[INFO] Epoch: 12 , batch: 176 , training loss: 4.746449\n",
      "[INFO] Epoch: 12 , batch: 177 , training loss: 4.310039\n",
      "[INFO] Epoch: 12 , batch: 178 , training loss: 4.239277\n",
      "[INFO] Epoch: 12 , batch: 179 , training loss: 4.321118\n",
      "[INFO] Epoch: 12 , batch: 180 , training loss: 4.260283\n",
      "[INFO] Epoch: 12 , batch: 181 , training loss: 4.524147\n",
      "[INFO] Epoch: 12 , batch: 182 , training loss: 4.450518\n",
      "[INFO] Epoch: 12 , batch: 183 , training loss: 4.410755\n",
      "[INFO] Epoch: 12 , batch: 184 , training loss: 4.313621\n",
      "[INFO] Epoch: 12 , batch: 185 , training loss: 4.272476\n",
      "[INFO] Epoch: 12 , batch: 186 , training loss: 4.402727\n",
      "[INFO] Epoch: 12 , batch: 187 , training loss: 4.489516\n",
      "[INFO] Epoch: 12 , batch: 188 , training loss: 4.485066\n",
      "[INFO] Epoch: 12 , batch: 189 , training loss: 4.384528\n",
      "[INFO] Epoch: 12 , batch: 190 , training loss: 4.415631\n",
      "[INFO] Epoch: 12 , batch: 191 , training loss: 4.542356\n",
      "[INFO] Epoch: 12 , batch: 192 , training loss: 4.359310\n",
      "[INFO] Epoch: 12 , batch: 193 , training loss: 4.448423\n",
      "[INFO] Epoch: 12 , batch: 194 , training loss: 4.391647\n",
      "[INFO] Epoch: 12 , batch: 195 , training loss: 4.359868\n",
      "[INFO] Epoch: 12 , batch: 196 , training loss: 4.171983\n",
      "[INFO] Epoch: 12 , batch: 197 , training loss: 4.299930\n",
      "[INFO] Epoch: 12 , batch: 198 , training loss: 4.188704\n",
      "[INFO] Epoch: 12 , batch: 199 , training loss: 4.321721\n",
      "[INFO] Epoch: 12 , batch: 200 , training loss: 4.242386\n",
      "[INFO] Epoch: 12 , batch: 201 , training loss: 4.126318\n",
      "[INFO] Epoch: 12 , batch: 202 , training loss: 4.138565\n",
      "[INFO] Epoch: 12 , batch: 203 , training loss: 4.213480\n",
      "[INFO] Epoch: 12 , batch: 204 , training loss: 4.317882\n",
      "[INFO] Epoch: 12 , batch: 205 , training loss: 3.919655\n",
      "[INFO] Epoch: 12 , batch: 206 , training loss: 3.847157\n",
      "[INFO] Epoch: 12 , batch: 207 , training loss: 3.852454\n",
      "[INFO] Epoch: 12 , batch: 208 , training loss: 4.195704\n",
      "[INFO] Epoch: 12 , batch: 209 , training loss: 4.115189\n",
      "[INFO] Epoch: 12 , batch: 210 , training loss: 4.159915\n",
      "[INFO] Epoch: 12 , batch: 211 , training loss: 4.165441\n",
      "[INFO] Epoch: 12 , batch: 212 , training loss: 4.266145\n",
      "[INFO] Epoch: 12 , batch: 213 , training loss: 4.229758\n",
      "[INFO] Epoch: 12 , batch: 214 , training loss: 4.302629\n",
      "[INFO] Epoch: 12 , batch: 215 , training loss: 4.501346\n",
      "[INFO] Epoch: 12 , batch: 216 , training loss: 4.221566\n",
      "[INFO] Epoch: 12 , batch: 217 , training loss: 4.163433\n",
      "[INFO] Epoch: 12 , batch: 218 , training loss: 4.158758\n",
      "[INFO] Epoch: 12 , batch: 219 , training loss: 4.246579\n",
      "[INFO] Epoch: 12 , batch: 220 , training loss: 4.087104\n",
      "[INFO] Epoch: 12 , batch: 221 , training loss: 4.082017\n",
      "[INFO] Epoch: 12 , batch: 222 , training loss: 4.221471\n",
      "[INFO] Epoch: 12 , batch: 223 , training loss: 4.339237\n",
      "[INFO] Epoch: 12 , batch: 224 , training loss: 4.350730\n",
      "[INFO] Epoch: 12 , batch: 225 , training loss: 4.247499\n",
      "[INFO] Epoch: 12 , batch: 226 , training loss: 4.385965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 12 , batch: 227 , training loss: 4.325570\n",
      "[INFO] Epoch: 12 , batch: 228 , training loss: 4.376089\n",
      "[INFO] Epoch: 12 , batch: 229 , training loss: 4.225498\n",
      "[INFO] Epoch: 12 , batch: 230 , training loss: 4.089906\n",
      "[INFO] Epoch: 12 , batch: 231 , training loss: 3.926677\n",
      "[INFO] Epoch: 12 , batch: 232 , training loss: 4.104845\n",
      "[INFO] Epoch: 12 , batch: 233 , training loss: 4.132433\n",
      "[INFO] Epoch: 12 , batch: 234 , training loss: 3.807576\n",
      "[INFO] Epoch: 12 , batch: 235 , training loss: 3.941124\n",
      "[INFO] Epoch: 12 , batch: 236 , training loss: 4.051847\n",
      "[INFO] Epoch: 12 , batch: 237 , training loss: 4.255017\n",
      "[INFO] Epoch: 12 , batch: 238 , training loss: 4.009839\n",
      "[INFO] Epoch: 12 , batch: 239 , training loss: 4.063674\n",
      "[INFO] Epoch: 12 , batch: 240 , training loss: 4.099025\n",
      "[INFO] Epoch: 12 , batch: 241 , training loss: 3.890034\n",
      "[INFO] Epoch: 12 , batch: 242 , training loss: 3.910551\n",
      "[INFO] Epoch: 12 , batch: 243 , training loss: 4.239963\n",
      "[INFO] Epoch: 12 , batch: 244 , training loss: 4.171217\n",
      "[INFO] Epoch: 12 , batch: 245 , training loss: 4.161828\n",
      "[INFO] Epoch: 12 , batch: 246 , training loss: 3.825351\n",
      "[INFO] Epoch: 12 , batch: 247 , training loss: 3.981551\n",
      "[INFO] Epoch: 12 , batch: 248 , training loss: 4.096283\n",
      "[INFO] Epoch: 12 , batch: 249 , training loss: 4.065369\n",
      "[INFO] Epoch: 12 , batch: 250 , training loss: 3.850888\n",
      "[INFO] Epoch: 12 , batch: 251 , training loss: 4.330563\n",
      "[INFO] Epoch: 12 , batch: 252 , training loss: 4.000285\n",
      "[INFO] Epoch: 12 , batch: 253 , training loss: 3.946305\n",
      "[INFO] Epoch: 12 , batch: 254 , training loss: 4.242983\n",
      "[INFO] Epoch: 12 , batch: 255 , training loss: 4.188322\n",
      "[INFO] Epoch: 12 , batch: 256 , training loss: 4.168734\n",
      "[INFO] Epoch: 12 , batch: 257 , training loss: 4.357811\n",
      "[INFO] Epoch: 12 , batch: 258 , training loss: 4.385043\n",
      "[INFO] Epoch: 12 , batch: 259 , training loss: 4.422919\n",
      "[INFO] Epoch: 12 , batch: 260 , training loss: 4.154789\n",
      "[INFO] Epoch: 12 , batch: 261 , training loss: 4.330066\n",
      "[INFO] Epoch: 12 , batch: 262 , training loss: 4.524789\n",
      "[INFO] Epoch: 12 , batch: 263 , training loss: 4.657080\n",
      "[INFO] Epoch: 12 , batch: 264 , training loss: 4.009487\n",
      "[INFO] Epoch: 12 , batch: 265 , training loss: 4.116867\n",
      "[INFO] Epoch: 12 , batch: 266 , training loss: 4.582870\n",
      "[INFO] Epoch: 12 , batch: 267 , training loss: 4.283718\n",
      "[INFO] Epoch: 12 , batch: 268 , training loss: 4.199614\n",
      "[INFO] Epoch: 12 , batch: 269 , training loss: 4.212197\n",
      "[INFO] Epoch: 12 , batch: 270 , training loss: 4.214423\n",
      "[INFO] Epoch: 12 , batch: 271 , training loss: 4.265161\n",
      "[INFO] Epoch: 12 , batch: 272 , training loss: 4.247663\n",
      "[INFO] Epoch: 12 , batch: 273 , training loss: 4.245821\n",
      "[INFO] Epoch: 12 , batch: 274 , training loss: 4.351382\n",
      "[INFO] Epoch: 12 , batch: 275 , training loss: 4.197808\n",
      "[INFO] Epoch: 12 , batch: 276 , training loss: 4.277377\n",
      "[INFO] Epoch: 12 , batch: 277 , training loss: 4.430754\n",
      "[INFO] Epoch: 12 , batch: 278 , training loss: 4.064145\n",
      "[INFO] Epoch: 12 , batch: 279 , training loss: 4.082147\n",
      "[INFO] Epoch: 12 , batch: 280 , training loss: 4.028456\n",
      "[INFO] Epoch: 12 , batch: 281 , training loss: 4.179436\n",
      "[INFO] Epoch: 12 , batch: 282 , training loss: 4.069200\n",
      "[INFO] Epoch: 12 , batch: 283 , training loss: 4.105632\n",
      "[INFO] Epoch: 12 , batch: 284 , training loss: 4.152394\n",
      "[INFO] Epoch: 12 , batch: 285 , training loss: 4.111178\n",
      "[INFO] Epoch: 12 , batch: 286 , training loss: 4.091581\n",
      "[INFO] Epoch: 12 , batch: 287 , training loss: 4.016374\n",
      "[INFO] Epoch: 12 , batch: 288 , training loss: 3.996491\n",
      "[INFO] Epoch: 12 , batch: 289 , training loss: 4.068842\n",
      "[INFO] Epoch: 12 , batch: 290 , training loss: 3.844935\n",
      "[INFO] Epoch: 12 , batch: 291 , training loss: 3.838071\n",
      "[INFO] Epoch: 12 , batch: 292 , training loss: 3.954753\n",
      "[INFO] Epoch: 12 , batch: 293 , training loss: 3.880506\n",
      "[INFO] Epoch: 12 , batch: 294 , training loss: 4.545039\n",
      "[INFO] Epoch: 12 , batch: 295 , training loss: 4.315158\n",
      "[INFO] Epoch: 12 , batch: 296 , training loss: 4.238379\n",
      "[INFO] Epoch: 12 , batch: 297 , training loss: 4.189386\n",
      "[INFO] Epoch: 12 , batch: 298 , training loss: 4.022007\n",
      "[INFO] Epoch: 12 , batch: 299 , training loss: 4.062217\n",
      "[INFO] Epoch: 12 , batch: 300 , training loss: 4.046978\n",
      "[INFO] Epoch: 12 , batch: 301 , training loss: 3.969006\n",
      "[INFO] Epoch: 12 , batch: 302 , training loss: 4.144151\n",
      "[INFO] Epoch: 12 , batch: 303 , training loss: 4.154970\n",
      "[INFO] Epoch: 12 , batch: 304 , training loss: 4.330651\n",
      "[INFO] Epoch: 12 , batch: 305 , training loss: 4.102131\n",
      "[INFO] Epoch: 12 , batch: 306 , training loss: 4.227553\n",
      "[INFO] Epoch: 12 , batch: 307 , training loss: 4.215671\n",
      "[INFO] Epoch: 12 , batch: 308 , training loss: 4.061102\n",
      "[INFO] Epoch: 12 , batch: 309 , training loss: 4.080523\n",
      "[INFO] Epoch: 12 , batch: 310 , training loss: 3.963511\n",
      "[INFO] Epoch: 12 , batch: 311 , training loss: 3.972731\n",
      "[INFO] Epoch: 12 , batch: 312 , training loss: 3.865563\n",
      "[INFO] Epoch: 12 , batch: 313 , training loss: 3.994775\n",
      "[INFO] Epoch: 12 , batch: 314 , training loss: 4.064755\n",
      "[INFO] Epoch: 12 , batch: 315 , training loss: 4.125741\n",
      "[INFO] Epoch: 12 , batch: 316 , training loss: 4.411333\n",
      "[INFO] Epoch: 12 , batch: 317 , training loss: 4.821075\n",
      "[INFO] Epoch: 12 , batch: 318 , training loss: 4.964616\n",
      "[INFO] Epoch: 12 , batch: 319 , training loss: 4.594240\n",
      "[INFO] Epoch: 12 , batch: 320 , training loss: 4.126063\n",
      "[INFO] Epoch: 12 , batch: 321 , training loss: 3.920974\n",
      "[INFO] Epoch: 12 , batch: 322 , training loss: 4.033856\n",
      "[INFO] Epoch: 12 , batch: 323 , training loss: 4.066898\n",
      "[INFO] Epoch: 12 , batch: 324 , training loss: 4.054058\n",
      "[INFO] Epoch: 12 , batch: 325 , training loss: 4.183457\n",
      "[INFO] Epoch: 12 , batch: 326 , training loss: 4.226781\n",
      "[INFO] Epoch: 12 , batch: 327 , training loss: 4.155171\n",
      "[INFO] Epoch: 12 , batch: 328 , training loss: 4.142668\n",
      "[INFO] Epoch: 12 , batch: 329 , training loss: 4.041873\n",
      "[INFO] Epoch: 12 , batch: 330 , training loss: 4.051894\n",
      "[INFO] Epoch: 12 , batch: 331 , training loss: 4.214065\n",
      "[INFO] Epoch: 12 , batch: 332 , training loss: 4.028329\n",
      "[INFO] Epoch: 12 , batch: 333 , training loss: 4.003874\n",
      "[INFO] Epoch: 12 , batch: 334 , training loss: 4.050692\n",
      "[INFO] Epoch: 12 , batch: 335 , training loss: 4.174510\n",
      "[INFO] Epoch: 12 , batch: 336 , training loss: 4.168666\n",
      "[INFO] Epoch: 12 , batch: 337 , training loss: 4.243008\n",
      "[INFO] Epoch: 12 , batch: 338 , training loss: 4.425432\n",
      "[INFO] Epoch: 12 , batch: 339 , training loss: 4.238277\n",
      "[INFO] Epoch: 12 , batch: 340 , training loss: 4.450881\n",
      "[INFO] Epoch: 12 , batch: 341 , training loss: 4.172970\n",
      "[INFO] Epoch: 12 , batch: 342 , training loss: 3.968173\n",
      "[INFO] Epoch: 12 , batch: 343 , training loss: 4.045616\n",
      "[INFO] Epoch: 12 , batch: 344 , training loss: 3.906396\n",
      "[INFO] Epoch: 12 , batch: 345 , training loss: 4.062700\n",
      "[INFO] Epoch: 12 , batch: 346 , training loss: 4.073597\n",
      "[INFO] Epoch: 12 , batch: 347 , training loss: 4.016843\n",
      "[INFO] Epoch: 12 , batch: 348 , training loss: 4.147232\n",
      "[INFO] Epoch: 12 , batch: 349 , training loss: 4.222417\n",
      "[INFO] Epoch: 12 , batch: 350 , training loss: 4.056156\n",
      "[INFO] Epoch: 12 , batch: 351 , training loss: 4.124867\n",
      "[INFO] Epoch: 12 , batch: 352 , training loss: 4.137065\n",
      "[INFO] Epoch: 12 , batch: 353 , training loss: 4.090552\n",
      "[INFO] Epoch: 12 , batch: 354 , training loss: 4.204367\n",
      "[INFO] Epoch: 12 , batch: 355 , training loss: 4.218844\n",
      "[INFO] Epoch: 12 , batch: 356 , training loss: 4.091211\n",
      "[INFO] Epoch: 12 , batch: 357 , training loss: 4.156294\n",
      "[INFO] Epoch: 12 , batch: 358 , training loss: 4.085166\n",
      "[INFO] Epoch: 12 , batch: 359 , training loss: 4.077550\n",
      "[INFO] Epoch: 12 , batch: 360 , training loss: 4.143823\n",
      "[INFO] Epoch: 12 , batch: 361 , training loss: 4.116124\n",
      "[INFO] Epoch: 12 , batch: 362 , training loss: 4.214030\n",
      "[INFO] Epoch: 12 , batch: 363 , training loss: 4.108324\n",
      "[INFO] Epoch: 12 , batch: 364 , training loss: 4.171958\n",
      "[INFO] Epoch: 12 , batch: 365 , training loss: 4.053110\n",
      "[INFO] Epoch: 12 , batch: 366 , training loss: 4.192340\n",
      "[INFO] Epoch: 12 , batch: 367 , training loss: 4.236314\n",
      "[INFO] Epoch: 12 , batch: 368 , training loss: 4.712376\n",
      "[INFO] Epoch: 12 , batch: 369 , training loss: 4.360209\n",
      "[INFO] Epoch: 12 , batch: 370 , training loss: 4.114925\n",
      "[INFO] Epoch: 12 , batch: 371 , training loss: 4.591593\n",
      "[INFO] Epoch: 12 , batch: 372 , training loss: 4.854321\n",
      "[INFO] Epoch: 12 , batch: 373 , training loss: 4.900318\n",
      "[INFO] Epoch: 12 , batch: 374 , training loss: 4.974514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 12 , batch: 375 , training loss: 4.948904\n",
      "[INFO] Epoch: 12 , batch: 376 , training loss: 4.886043\n",
      "[INFO] Epoch: 12 , batch: 377 , training loss: 4.605187\n",
      "[INFO] Epoch: 12 , batch: 378 , training loss: 4.679687\n",
      "[INFO] Epoch: 12 , batch: 379 , training loss: 4.668080\n",
      "[INFO] Epoch: 12 , batch: 380 , training loss: 4.771799\n",
      "[INFO] Epoch: 12 , batch: 381 , training loss: 4.533545\n",
      "[INFO] Epoch: 12 , batch: 382 , training loss: 4.835350\n",
      "[INFO] Epoch: 12 , batch: 383 , training loss: 4.866444\n",
      "[INFO] Epoch: 12 , batch: 384 , training loss: 4.877204\n",
      "[INFO] Epoch: 12 , batch: 385 , training loss: 4.573069\n",
      "[INFO] Epoch: 12 , batch: 386 , training loss: 4.784966\n",
      "[INFO] Epoch: 12 , batch: 387 , training loss: 4.715294\n",
      "[INFO] Epoch: 12 , batch: 388 , training loss: 4.501795\n",
      "[INFO] Epoch: 12 , batch: 389 , training loss: 4.325753\n",
      "[INFO] Epoch: 12 , batch: 390 , training loss: 4.330540\n",
      "[INFO] Epoch: 12 , batch: 391 , training loss: 4.352801\n",
      "[INFO] Epoch: 12 , batch: 392 , training loss: 4.706432\n",
      "[INFO] Epoch: 12 , batch: 393 , training loss: 4.630229\n",
      "[INFO] Epoch: 12 , batch: 394 , training loss: 4.707412\n",
      "[INFO] Epoch: 12 , batch: 395 , training loss: 4.499453\n",
      "[INFO] Epoch: 12 , batch: 396 , training loss: 4.310749\n",
      "[INFO] Epoch: 12 , batch: 397 , training loss: 4.472560\n",
      "[INFO] Epoch: 12 , batch: 398 , training loss: 4.330538\n",
      "[INFO] Epoch: 12 , batch: 399 , training loss: 4.397691\n",
      "[INFO] Epoch: 12 , batch: 400 , training loss: 4.371452\n",
      "[INFO] Epoch: 12 , batch: 401 , training loss: 4.808199\n",
      "[INFO] Epoch: 12 , batch: 402 , training loss: 4.515963\n",
      "[INFO] Epoch: 12 , batch: 403 , training loss: 4.344721\n",
      "[INFO] Epoch: 12 , batch: 404 , training loss: 4.503831\n",
      "[INFO] Epoch: 12 , batch: 405 , training loss: 4.585312\n",
      "[INFO] Epoch: 12 , batch: 406 , training loss: 4.464338\n",
      "[INFO] Epoch: 12 , batch: 407 , training loss: 4.532871\n",
      "[INFO] Epoch: 12 , batch: 408 , training loss: 4.455832\n",
      "[INFO] Epoch: 12 , batch: 409 , training loss: 4.476536\n",
      "[INFO] Epoch: 12 , batch: 410 , training loss: 4.553109\n",
      "[INFO] Epoch: 12 , batch: 411 , training loss: 4.722467\n",
      "[INFO] Epoch: 12 , batch: 412 , training loss: 4.545230\n",
      "[INFO] Epoch: 12 , batch: 413 , training loss: 4.430184\n",
      "[INFO] Epoch: 12 , batch: 414 , training loss: 4.458576\n",
      "[INFO] Epoch: 12 , batch: 415 , training loss: 4.502697\n",
      "[INFO] Epoch: 12 , batch: 416 , training loss: 4.567073\n",
      "[INFO] Epoch: 12 , batch: 417 , training loss: 4.470172\n",
      "[INFO] Epoch: 12 , batch: 418 , training loss: 4.516234\n",
      "[INFO] Epoch: 12 , batch: 419 , training loss: 4.457713\n",
      "[INFO] Epoch: 12 , batch: 420 , training loss: 4.436729\n",
      "[INFO] Epoch: 12 , batch: 421 , training loss: 4.436870\n",
      "[INFO] Epoch: 12 , batch: 422 , training loss: 4.297344\n",
      "[INFO] Epoch: 12 , batch: 423 , training loss: 4.523939\n",
      "[INFO] Epoch: 12 , batch: 424 , training loss: 4.688738\n",
      "[INFO] Epoch: 12 , batch: 425 , training loss: 4.568824\n",
      "[INFO] Epoch: 12 , batch: 426 , training loss: 4.259432\n",
      "[INFO] Epoch: 12 , batch: 427 , training loss: 4.527647\n",
      "[INFO] Epoch: 12 , batch: 428 , training loss: 4.403892\n",
      "[INFO] Epoch: 12 , batch: 429 , training loss: 4.279384\n",
      "[INFO] Epoch: 12 , batch: 430 , training loss: 4.543432\n",
      "[INFO] Epoch: 12 , batch: 431 , training loss: 4.115692\n",
      "[INFO] Epoch: 12 , batch: 432 , training loss: 4.172202\n",
      "[INFO] Epoch: 12 , batch: 433 , training loss: 4.204150\n",
      "[INFO] Epoch: 12 , batch: 434 , training loss: 4.097366\n",
      "[INFO] Epoch: 12 , batch: 435 , training loss: 4.445859\n",
      "[INFO] Epoch: 12 , batch: 436 , training loss: 4.503144\n",
      "[INFO] Epoch: 12 , batch: 437 , training loss: 4.292064\n",
      "[INFO] Epoch: 12 , batch: 438 , training loss: 4.110198\n",
      "[INFO] Epoch: 12 , batch: 439 , training loss: 4.358337\n",
      "[INFO] Epoch: 12 , batch: 440 , training loss: 4.512892\n",
      "[INFO] Epoch: 12 , batch: 441 , training loss: 4.554084\n",
      "[INFO] Epoch: 12 , batch: 442 , training loss: 4.324298\n",
      "[INFO] Epoch: 12 , batch: 443 , training loss: 4.541847\n",
      "[INFO] Epoch: 12 , batch: 444 , training loss: 4.140747\n",
      "[INFO] Epoch: 12 , batch: 445 , training loss: 4.064867\n",
      "[INFO] Epoch: 12 , batch: 446 , training loss: 3.983400\n",
      "[INFO] Epoch: 12 , batch: 447 , training loss: 4.161973\n",
      "[INFO] Epoch: 12 , batch: 448 , training loss: 4.297652\n",
      "[INFO] Epoch: 12 , batch: 449 , training loss: 4.688461\n",
      "[INFO] Epoch: 12 , batch: 450 , training loss: 4.777337\n",
      "[INFO] Epoch: 12 , batch: 451 , training loss: 4.647362\n",
      "[INFO] Epoch: 12 , batch: 452 , training loss: 4.461092\n",
      "[INFO] Epoch: 12 , batch: 453 , training loss: 4.226967\n",
      "[INFO] Epoch: 12 , batch: 454 , training loss: 4.387079\n",
      "[INFO] Epoch: 12 , batch: 455 , training loss: 4.421715\n",
      "[INFO] Epoch: 12 , batch: 456 , training loss: 4.395282\n",
      "[INFO] Epoch: 12 , batch: 457 , training loss: 4.492297\n",
      "[INFO] Epoch: 12 , batch: 458 , training loss: 4.235637\n",
      "[INFO] Epoch: 12 , batch: 459 , training loss: 4.208686\n",
      "[INFO] Epoch: 12 , batch: 460 , training loss: 4.338763\n",
      "[INFO] Epoch: 12 , batch: 461 , training loss: 4.286148\n",
      "[INFO] Epoch: 12 , batch: 462 , training loss: 4.360167\n",
      "[INFO] Epoch: 12 , batch: 463 , training loss: 4.251473\n",
      "[INFO] Epoch: 12 , batch: 464 , training loss: 4.427834\n",
      "[INFO] Epoch: 12 , batch: 465 , training loss: 4.380773\n",
      "[INFO] Epoch: 12 , batch: 466 , training loss: 4.470479\n",
      "[INFO] Epoch: 12 , batch: 467 , training loss: 4.462933\n",
      "[INFO] Epoch: 12 , batch: 468 , training loss: 4.417453\n",
      "[INFO] Epoch: 12 , batch: 469 , training loss: 4.461222\n",
      "[INFO] Epoch: 12 , batch: 470 , training loss: 4.233647\n",
      "[INFO] Epoch: 12 , batch: 471 , training loss: 4.350529\n",
      "[INFO] Epoch: 12 , batch: 472 , training loss: 4.410586\n",
      "[INFO] Epoch: 12 , batch: 473 , training loss: 4.334455\n",
      "[INFO] Epoch: 12 , batch: 474 , training loss: 4.128529\n",
      "[INFO] Epoch: 12 , batch: 475 , training loss: 3.985105\n",
      "[INFO] Epoch: 12 , batch: 476 , training loss: 4.407439\n",
      "[INFO] Epoch: 12 , batch: 477 , training loss: 4.487930\n",
      "[INFO] Epoch: 12 , batch: 478 , training loss: 4.521202\n",
      "[INFO] Epoch: 12 , batch: 479 , training loss: 4.482201\n",
      "[INFO] Epoch: 12 , batch: 480 , training loss: 4.600076\n",
      "[INFO] Epoch: 12 , batch: 481 , training loss: 4.476464\n",
      "[INFO] Epoch: 12 , batch: 482 , training loss: 4.609471\n",
      "[INFO] Epoch: 12 , batch: 483 , training loss: 4.437920\n",
      "[INFO] Epoch: 12 , batch: 484 , training loss: 4.208103\n",
      "[INFO] Epoch: 12 , batch: 485 , training loss: 4.345130\n",
      "[INFO] Epoch: 12 , batch: 486 , training loss: 4.235408\n",
      "[INFO] Epoch: 12 , batch: 487 , training loss: 4.223187\n",
      "[INFO] Epoch: 12 , batch: 488 , training loss: 4.408490\n",
      "[INFO] Epoch: 12 , batch: 489 , training loss: 4.300160\n",
      "[INFO] Epoch: 12 , batch: 490 , training loss: 4.362943\n",
      "[INFO] Epoch: 12 , batch: 491 , training loss: 4.315526\n",
      "[INFO] Epoch: 12 , batch: 492 , training loss: 4.253317\n",
      "[INFO] Epoch: 12 , batch: 493 , training loss: 4.418671\n",
      "[INFO] Epoch: 12 , batch: 494 , training loss: 4.332965\n",
      "[INFO] Epoch: 12 , batch: 495 , training loss: 4.469348\n",
      "[INFO] Epoch: 12 , batch: 496 , training loss: 4.362219\n",
      "[INFO] Epoch: 12 , batch: 497 , training loss: 4.381612\n",
      "[INFO] Epoch: 12 , batch: 498 , training loss: 4.389672\n",
      "[INFO] Epoch: 12 , batch: 499 , training loss: 4.468556\n",
      "[INFO] Epoch: 12 , batch: 500 , training loss: 4.601124\n",
      "[INFO] Epoch: 12 , batch: 501 , training loss: 4.979743\n",
      "[INFO] Epoch: 12 , batch: 502 , training loss: 5.081711\n",
      "[INFO] Epoch: 12 , batch: 503 , training loss: 4.829739\n",
      "[INFO] Epoch: 12 , batch: 504 , training loss: 4.951874\n",
      "[INFO] Epoch: 12 , batch: 505 , training loss: 4.882117\n",
      "[INFO] Epoch: 12 , batch: 506 , training loss: 4.828940\n",
      "[INFO] Epoch: 12 , batch: 507 , training loss: 4.871887\n",
      "[INFO] Epoch: 12 , batch: 508 , training loss: 4.765445\n",
      "[INFO] Epoch: 12 , batch: 509 , training loss: 4.541049\n",
      "[INFO] Epoch: 12 , batch: 510 , training loss: 4.641240\n",
      "[INFO] Epoch: 12 , batch: 511 , training loss: 4.540802\n",
      "[INFO] Epoch: 12 , batch: 512 , training loss: 4.620748\n",
      "[INFO] Epoch: 12 , batch: 513 , training loss: 4.865494\n",
      "[INFO] Epoch: 12 , batch: 514 , training loss: 4.514102\n",
      "[INFO] Epoch: 12 , batch: 515 , training loss: 4.750074\n",
      "[INFO] Epoch: 12 , batch: 516 , training loss: 4.542426\n",
      "[INFO] Epoch: 12 , batch: 517 , training loss: 4.522660\n",
      "[INFO] Epoch: 12 , batch: 518 , training loss: 4.486261\n",
      "[INFO] Epoch: 12 , batch: 519 , training loss: 4.337520\n",
      "[INFO] Epoch: 12 , batch: 520 , training loss: 4.592474\n",
      "[INFO] Epoch: 12 , batch: 521 , training loss: 4.572999\n",
      "[INFO] Epoch: 12 , batch: 522 , training loss: 4.650706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 12 , batch: 523 , training loss: 4.538884\n",
      "[INFO] Epoch: 12 , batch: 524 , training loss: 4.825552\n",
      "[INFO] Epoch: 12 , batch: 525 , training loss: 4.702567\n",
      "[INFO] Epoch: 12 , batch: 526 , training loss: 4.480513\n",
      "[INFO] Epoch: 12 , batch: 527 , training loss: 4.523263\n",
      "[INFO] Epoch: 12 , batch: 528 , training loss: 4.553837\n",
      "[INFO] Epoch: 12 , batch: 529 , training loss: 4.517817\n",
      "[INFO] Epoch: 12 , batch: 530 , training loss: 4.364259\n",
      "[INFO] Epoch: 12 , batch: 531 , training loss: 4.534999\n",
      "[INFO] Epoch: 12 , batch: 532 , training loss: 4.407880\n",
      "[INFO] Epoch: 12 , batch: 533 , training loss: 4.558121\n",
      "[INFO] Epoch: 12 , batch: 534 , training loss: 4.549501\n",
      "[INFO] Epoch: 12 , batch: 535 , training loss: 4.554134\n",
      "[INFO] Epoch: 12 , batch: 536 , training loss: 4.413639\n",
      "[INFO] Epoch: 12 , batch: 537 , training loss: 4.366206\n",
      "[INFO] Epoch: 12 , batch: 538 , training loss: 4.459736\n",
      "[INFO] Epoch: 12 , batch: 539 , training loss: 4.611692\n",
      "[INFO] Epoch: 12 , batch: 540 , training loss: 5.170838\n",
      "[INFO] Epoch: 12 , batch: 541 , training loss: 5.045690\n",
      "[INFO] Epoch: 12 , batch: 542 , training loss: 4.882071\n",
      "[INFO] Epoch: 13 , batch: 0 , training loss: 4.205150\n",
      "[INFO] Epoch: 13 , batch: 1 , training loss: 3.935650\n",
      "[INFO] Epoch: 13 , batch: 2 , training loss: 3.961381\n",
      "[INFO] Epoch: 13 , batch: 3 , training loss: 3.815058\n",
      "[INFO] Epoch: 13 , batch: 4 , training loss: 4.133010\n",
      "[INFO] Epoch: 13 , batch: 5 , training loss: 3.800584\n",
      "[INFO] Epoch: 13 , batch: 6 , training loss: 4.194239\n",
      "[INFO] Epoch: 13 , batch: 7 , training loss: 4.016789\n",
      "[INFO] Epoch: 13 , batch: 8 , training loss: 3.663795\n",
      "[INFO] Epoch: 13 , batch: 9 , training loss: 3.886867\n",
      "[INFO] Epoch: 13 , batch: 10 , training loss: 3.829673\n",
      "[INFO] Epoch: 13 , batch: 11 , training loss: 3.769184\n",
      "[INFO] Epoch: 13 , batch: 12 , training loss: 3.736244\n",
      "[INFO] Epoch: 13 , batch: 13 , training loss: 3.764021\n",
      "[INFO] Epoch: 13 , batch: 14 , training loss: 3.606430\n",
      "[INFO] Epoch: 13 , batch: 15 , training loss: 3.809387\n",
      "[INFO] Epoch: 13 , batch: 16 , training loss: 3.693371\n",
      "[INFO] Epoch: 13 , batch: 17 , training loss: 3.810079\n",
      "[INFO] Epoch: 13 , batch: 18 , training loss: 3.743168\n",
      "[INFO] Epoch: 13 , batch: 19 , training loss: 3.544935\n",
      "[INFO] Epoch: 13 , batch: 20 , training loss: 3.517256\n",
      "[INFO] Epoch: 13 , batch: 21 , training loss: 3.656479\n",
      "[INFO] Epoch: 13 , batch: 22 , training loss: 3.567272\n",
      "[INFO] Epoch: 13 , batch: 23 , training loss: 3.766482\n",
      "[INFO] Epoch: 13 , batch: 24 , training loss: 3.654724\n",
      "[INFO] Epoch: 13 , batch: 25 , training loss: 3.770224\n",
      "[INFO] Epoch: 13 , batch: 26 , training loss: 3.604203\n",
      "[INFO] Epoch: 13 , batch: 27 , training loss: 3.598644\n",
      "[INFO] Epoch: 13 , batch: 28 , training loss: 3.792243\n",
      "[INFO] Epoch: 13 , batch: 29 , training loss: 3.635941\n",
      "[INFO] Epoch: 13 , batch: 30 , training loss: 3.589024\n",
      "[INFO] Epoch: 13 , batch: 31 , training loss: 3.724200\n",
      "[INFO] Epoch: 13 , batch: 32 , training loss: 3.705994\n",
      "[INFO] Epoch: 13 , batch: 33 , training loss: 3.785916\n",
      "[INFO] Epoch: 13 , batch: 34 , training loss: 3.787684\n",
      "[INFO] Epoch: 13 , batch: 35 , training loss: 3.685571\n",
      "[INFO] Epoch: 13 , batch: 36 , training loss: 3.758048\n",
      "[INFO] Epoch: 13 , batch: 37 , training loss: 3.614851\n",
      "[INFO] Epoch: 13 , batch: 38 , training loss: 3.743381\n",
      "[INFO] Epoch: 13 , batch: 39 , training loss: 3.530070\n",
      "[INFO] Epoch: 13 , batch: 40 , training loss: 3.735958\n",
      "[INFO] Epoch: 13 , batch: 41 , training loss: 3.768481\n",
      "[INFO] Epoch: 13 , batch: 42 , training loss: 4.241892\n",
      "[INFO] Epoch: 13 , batch: 43 , training loss: 3.919552\n",
      "[INFO] Epoch: 13 , batch: 44 , training loss: 4.268205\n",
      "[INFO] Epoch: 13 , batch: 45 , training loss: 4.213817\n",
      "[INFO] Epoch: 13 , batch: 46 , training loss: 4.277969\n",
      "[INFO] Epoch: 13 , batch: 47 , training loss: 3.920646\n",
      "[INFO] Epoch: 13 , batch: 48 , training loss: 3.876503\n",
      "[INFO] Epoch: 13 , batch: 49 , training loss: 4.066135\n",
      "[INFO] Epoch: 13 , batch: 50 , training loss: 3.810006\n",
      "[INFO] Epoch: 13 , batch: 51 , training loss: 3.962468\n",
      "[INFO] Epoch: 13 , batch: 52 , training loss: 3.827086\n",
      "[INFO] Epoch: 13 , batch: 53 , training loss: 3.924269\n",
      "[INFO] Epoch: 13 , batch: 54 , training loss: 3.965105\n",
      "[INFO] Epoch: 13 , batch: 55 , training loss: 4.026800\n",
      "[INFO] Epoch: 13 , batch: 56 , training loss: 3.861233\n",
      "[INFO] Epoch: 13 , batch: 57 , training loss: 3.787199\n",
      "[INFO] Epoch: 13 , batch: 58 , training loss: 3.806136\n",
      "[INFO] Epoch: 13 , batch: 59 , training loss: 3.925454\n",
      "[INFO] Epoch: 13 , batch: 60 , training loss: 3.811740\n",
      "[INFO] Epoch: 13 , batch: 61 , training loss: 3.932884\n",
      "[INFO] Epoch: 13 , batch: 62 , training loss: 3.795222\n",
      "[INFO] Epoch: 13 , batch: 63 , training loss: 3.967613\n",
      "[INFO] Epoch: 13 , batch: 64 , training loss: 4.198788\n",
      "[INFO] Epoch: 13 , batch: 65 , training loss: 3.853116\n",
      "[INFO] Epoch: 13 , batch: 66 , training loss: 3.733885\n",
      "[INFO] Epoch: 13 , batch: 67 , training loss: 3.718061\n",
      "[INFO] Epoch: 13 , batch: 68 , training loss: 3.950314\n",
      "[INFO] Epoch: 13 , batch: 69 , training loss: 3.849610\n",
      "[INFO] Epoch: 13 , batch: 70 , training loss: 4.061080\n",
      "[INFO] Epoch: 13 , batch: 71 , training loss: 3.919609\n",
      "[INFO] Epoch: 13 , batch: 72 , training loss: 3.981176\n",
      "[INFO] Epoch: 13 , batch: 73 , training loss: 3.916704\n",
      "[INFO] Epoch: 13 , batch: 74 , training loss: 4.051234\n",
      "[INFO] Epoch: 13 , batch: 75 , training loss: 3.833642\n",
      "[INFO] Epoch: 13 , batch: 76 , training loss: 3.971606\n",
      "[INFO] Epoch: 13 , batch: 77 , training loss: 3.915133\n",
      "[INFO] Epoch: 13 , batch: 78 , training loss: 4.015625\n",
      "[INFO] Epoch: 13 , batch: 79 , training loss: 3.830386\n",
      "[INFO] Epoch: 13 , batch: 80 , training loss: 4.060947\n",
      "[INFO] Epoch: 13 , batch: 81 , training loss: 3.989027\n",
      "[INFO] Epoch: 13 , batch: 82 , training loss: 3.949094\n",
      "[INFO] Epoch: 13 , batch: 83 , training loss: 4.089337\n",
      "[INFO] Epoch: 13 , batch: 84 , training loss: 4.021507\n",
      "[INFO] Epoch: 13 , batch: 85 , training loss: 4.113714\n",
      "[INFO] Epoch: 13 , batch: 86 , training loss: 4.025109\n",
      "[INFO] Epoch: 13 , batch: 87 , training loss: 4.001215\n",
      "[INFO] Epoch: 13 , batch: 88 , training loss: 4.160534\n",
      "[INFO] Epoch: 13 , batch: 89 , training loss: 3.947380\n",
      "[INFO] Epoch: 13 , batch: 90 , training loss: 4.027420\n",
      "[INFO] Epoch: 13 , batch: 91 , training loss: 3.963844\n",
      "[INFO] Epoch: 13 , batch: 92 , training loss: 3.983628\n",
      "[INFO] Epoch: 13 , batch: 93 , training loss: 4.085973\n",
      "[INFO] Epoch: 13 , batch: 94 , training loss: 4.206426\n",
      "[INFO] Epoch: 13 , batch: 95 , training loss: 4.011508\n",
      "[INFO] Epoch: 13 , batch: 96 , training loss: 3.957691\n",
      "[INFO] Epoch: 13 , batch: 97 , training loss: 3.951252\n",
      "[INFO] Epoch: 13 , batch: 98 , training loss: 3.849548\n",
      "[INFO] Epoch: 13 , batch: 99 , training loss: 4.001753\n",
      "[INFO] Epoch: 13 , batch: 100 , training loss: 3.844697\n",
      "[INFO] Epoch: 13 , batch: 101 , training loss: 3.872366\n",
      "[INFO] Epoch: 13 , batch: 102 , training loss: 4.064807\n",
      "[INFO] Epoch: 13 , batch: 103 , training loss: 3.848941\n",
      "[INFO] Epoch: 13 , batch: 104 , training loss: 3.764713\n",
      "[INFO] Epoch: 13 , batch: 105 , training loss: 4.034008\n",
      "[INFO] Epoch: 13 , batch: 106 , training loss: 4.054282\n",
      "[INFO] Epoch: 13 , batch: 107 , training loss: 3.886683\n",
      "[INFO] Epoch: 13 , batch: 108 , training loss: 3.850809\n",
      "[INFO] Epoch: 13 , batch: 109 , training loss: 3.750141\n",
      "[INFO] Epoch: 13 , batch: 110 , training loss: 3.946355\n",
      "[INFO] Epoch: 13 , batch: 111 , training loss: 4.012964\n",
      "[INFO] Epoch: 13 , batch: 112 , training loss: 3.973461\n",
      "[INFO] Epoch: 13 , batch: 113 , training loss: 3.920914\n",
      "[INFO] Epoch: 13 , batch: 114 , training loss: 3.964883\n",
      "[INFO] Epoch: 13 , batch: 115 , training loss: 3.947902\n",
      "[INFO] Epoch: 13 , batch: 116 , training loss: 3.850384\n",
      "[INFO] Epoch: 13 , batch: 117 , training loss: 4.100361\n",
      "[INFO] Epoch: 13 , batch: 118 , training loss: 4.047997\n",
      "[INFO] Epoch: 13 , batch: 119 , training loss: 4.219462\n",
      "[INFO] Epoch: 13 , batch: 120 , training loss: 4.183951\n",
      "[INFO] Epoch: 13 , batch: 121 , training loss: 4.039740\n",
      "[INFO] Epoch: 13 , batch: 122 , training loss: 3.925992\n",
      "[INFO] Epoch: 13 , batch: 123 , training loss: 3.961561\n",
      "[INFO] Epoch: 13 , batch: 124 , training loss: 4.095819\n",
      "[INFO] Epoch: 13 , batch: 125 , training loss: 3.824834\n",
      "[INFO] Epoch: 13 , batch: 126 , training loss: 3.865716\n",
      "[INFO] Epoch: 13 , batch: 127 , training loss: 3.880193\n",
      "[INFO] Epoch: 13 , batch: 128 , training loss: 4.032744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 13 , batch: 129 , training loss: 3.966135\n",
      "[INFO] Epoch: 13 , batch: 130 , training loss: 3.983555\n",
      "[INFO] Epoch: 13 , batch: 131 , training loss: 3.967814\n",
      "[INFO] Epoch: 13 , batch: 132 , training loss: 3.998412\n",
      "[INFO] Epoch: 13 , batch: 133 , training loss: 3.948774\n",
      "[INFO] Epoch: 13 , batch: 134 , training loss: 3.697677\n",
      "[INFO] Epoch: 13 , batch: 135 , training loss: 3.758561\n",
      "[INFO] Epoch: 13 , batch: 136 , training loss: 4.063523\n",
      "[INFO] Epoch: 13 , batch: 137 , training loss: 3.991285\n",
      "[INFO] Epoch: 13 , batch: 138 , training loss: 4.026906\n",
      "[INFO] Epoch: 13 , batch: 139 , training loss: 4.635235\n",
      "[INFO] Epoch: 13 , batch: 140 , training loss: 4.440405\n",
      "[INFO] Epoch: 13 , batch: 141 , training loss: 4.202063\n",
      "[INFO] Epoch: 13 , batch: 142 , training loss: 3.895598\n",
      "[INFO] Epoch: 13 , batch: 143 , training loss: 4.024477\n",
      "[INFO] Epoch: 13 , batch: 144 , training loss: 3.877929\n",
      "[INFO] Epoch: 13 , batch: 145 , training loss: 3.961188\n",
      "[INFO] Epoch: 13 , batch: 146 , training loss: 4.137268\n",
      "[INFO] Epoch: 13 , batch: 147 , training loss: 3.799717\n",
      "[INFO] Epoch: 13 , batch: 148 , training loss: 3.808657\n",
      "[INFO] Epoch: 13 , batch: 149 , training loss: 3.906622\n",
      "[INFO] Epoch: 13 , batch: 150 , training loss: 4.158593\n",
      "[INFO] Epoch: 13 , batch: 151 , training loss: 3.967222\n",
      "[INFO] Epoch: 13 , batch: 152 , training loss: 3.939043\n",
      "[INFO] Epoch: 13 , batch: 153 , training loss: 3.987381\n",
      "[INFO] Epoch: 13 , batch: 154 , training loss: 4.058843\n",
      "[INFO] Epoch: 13 , batch: 155 , training loss: 4.321212\n",
      "[INFO] Epoch: 13 , batch: 156 , training loss: 4.019449\n",
      "[INFO] Epoch: 13 , batch: 157 , training loss: 3.998829\n",
      "[INFO] Epoch: 13 , batch: 158 , training loss: 4.219350\n",
      "[INFO] Epoch: 13 , batch: 159 , training loss: 4.107067\n",
      "[INFO] Epoch: 13 , batch: 160 , training loss: 4.530356\n",
      "[INFO] Epoch: 13 , batch: 161 , training loss: 4.504725\n",
      "[INFO] Epoch: 13 , batch: 162 , training loss: 4.446425\n",
      "[INFO] Epoch: 13 , batch: 163 , training loss: 4.622226\n",
      "[INFO] Epoch: 13 , batch: 164 , training loss: 4.505963\n",
      "[INFO] Epoch: 13 , batch: 165 , training loss: 4.420592\n",
      "[INFO] Epoch: 13 , batch: 166 , training loss: 4.378277\n",
      "[INFO] Epoch: 13 , batch: 167 , training loss: 4.629427\n",
      "[INFO] Epoch: 13 , batch: 168 , training loss: 4.313380\n",
      "[INFO] Epoch: 13 , batch: 169 , training loss: 4.234672\n",
      "[INFO] Epoch: 13 , batch: 170 , training loss: 4.377724\n",
      "[INFO] Epoch: 13 , batch: 171 , training loss: 3.817941\n",
      "[INFO] Epoch: 13 , batch: 172 , training loss: 3.989886\n",
      "[INFO] Epoch: 13 , batch: 173 , training loss: 4.302510\n",
      "[INFO] Epoch: 13 , batch: 174 , training loss: 4.719101\n",
      "[INFO] Epoch: 13 , batch: 175 , training loss: 4.953744\n",
      "[INFO] Epoch: 13 , batch: 176 , training loss: 4.708141\n",
      "[INFO] Epoch: 13 , batch: 177 , training loss: 4.287281\n",
      "[INFO] Epoch: 13 , batch: 178 , training loss: 4.234122\n",
      "[INFO] Epoch: 13 , batch: 179 , training loss: 4.310332\n",
      "[INFO] Epoch: 13 , batch: 180 , training loss: 4.235947\n",
      "[INFO] Epoch: 13 , batch: 181 , training loss: 4.505247\n",
      "[INFO] Epoch: 13 , batch: 182 , training loss: 4.443558\n",
      "[INFO] Epoch: 13 , batch: 183 , training loss: 4.401274\n",
      "[INFO] Epoch: 13 , batch: 184 , training loss: 4.293961\n",
      "[INFO] Epoch: 13 , batch: 185 , training loss: 4.258791\n",
      "[INFO] Epoch: 13 , batch: 186 , training loss: 4.375427\n",
      "[INFO] Epoch: 13 , batch: 187 , training loss: 4.489912\n",
      "[INFO] Epoch: 13 , batch: 188 , training loss: 4.473949\n",
      "[INFO] Epoch: 13 , batch: 189 , training loss: 4.367912\n",
      "[INFO] Epoch: 13 , batch: 190 , training loss: 4.404561\n",
      "[INFO] Epoch: 13 , batch: 191 , training loss: 4.528389\n",
      "[INFO] Epoch: 13 , batch: 192 , training loss: 4.352541\n",
      "[INFO] Epoch: 13 , batch: 193 , training loss: 4.449907\n",
      "[INFO] Epoch: 13 , batch: 194 , training loss: 4.381604\n",
      "[INFO] Epoch: 13 , batch: 195 , training loss: 4.345253\n",
      "[INFO] Epoch: 13 , batch: 196 , training loss: 4.173415\n",
      "[INFO] Epoch: 13 , batch: 197 , training loss: 4.303114\n",
      "[INFO] Epoch: 13 , batch: 198 , training loss: 4.194498\n",
      "[INFO] Epoch: 13 , batch: 199 , training loss: 4.295730\n",
      "[INFO] Epoch: 13 , batch: 200 , training loss: 4.213478\n",
      "[INFO] Epoch: 13 , batch: 201 , training loss: 4.117237\n",
      "[INFO] Epoch: 13 , batch: 202 , training loss: 4.111375\n",
      "[INFO] Epoch: 13 , batch: 203 , training loss: 4.203730\n",
      "[INFO] Epoch: 13 , batch: 204 , training loss: 4.305458\n",
      "[INFO] Epoch: 13 , batch: 205 , training loss: 3.914806\n",
      "[INFO] Epoch: 13 , batch: 206 , training loss: 3.823991\n",
      "[INFO] Epoch: 13 , batch: 207 , training loss: 3.844939\n",
      "[INFO] Epoch: 13 , batch: 208 , training loss: 4.186369\n",
      "[INFO] Epoch: 13 , batch: 209 , training loss: 4.100084\n",
      "[INFO] Epoch: 13 , batch: 210 , training loss: 4.160234\n",
      "[INFO] Epoch: 13 , batch: 211 , training loss: 4.134812\n",
      "[INFO] Epoch: 13 , batch: 212 , training loss: 4.253874\n",
      "[INFO] Epoch: 13 , batch: 213 , training loss: 4.210065\n",
      "[INFO] Epoch: 13 , batch: 214 , training loss: 4.288427\n",
      "[INFO] Epoch: 13 , batch: 215 , training loss: 4.489287\n",
      "[INFO] Epoch: 13 , batch: 216 , training loss: 4.199275\n",
      "[INFO] Epoch: 13 , batch: 217 , training loss: 4.147804\n",
      "[INFO] Epoch: 13 , batch: 218 , training loss: 4.152721\n",
      "[INFO] Epoch: 13 , batch: 219 , training loss: 4.244337\n",
      "[INFO] Epoch: 13 , batch: 220 , training loss: 4.063399\n",
      "[INFO] Epoch: 13 , batch: 221 , training loss: 4.088887\n",
      "[INFO] Epoch: 13 , batch: 222 , training loss: 4.208532\n",
      "[INFO] Epoch: 13 , batch: 223 , training loss: 4.312398\n",
      "[INFO] Epoch: 13 , batch: 224 , training loss: 4.348146\n",
      "[INFO] Epoch: 13 , batch: 225 , training loss: 4.240135\n",
      "[INFO] Epoch: 13 , batch: 226 , training loss: 4.378160\n",
      "[INFO] Epoch: 13 , batch: 227 , training loss: 4.334252\n",
      "[INFO] Epoch: 13 , batch: 228 , training loss: 4.374190\n",
      "[INFO] Epoch: 13 , batch: 229 , training loss: 4.207952\n",
      "[INFO] Epoch: 13 , batch: 230 , training loss: 4.092420\n",
      "[INFO] Epoch: 13 , batch: 231 , training loss: 3.934998\n",
      "[INFO] Epoch: 13 , batch: 232 , training loss: 4.081013\n",
      "[INFO] Epoch: 13 , batch: 233 , training loss: 4.110331\n",
      "[INFO] Epoch: 13 , batch: 234 , training loss: 3.792898\n",
      "[INFO] Epoch: 13 , batch: 235 , training loss: 3.919385\n",
      "[INFO] Epoch: 13 , batch: 236 , training loss: 4.045839\n",
      "[INFO] Epoch: 13 , batch: 237 , training loss: 4.260952\n",
      "[INFO] Epoch: 13 , batch: 238 , training loss: 4.009429\n",
      "[INFO] Epoch: 13 , batch: 239 , training loss: 4.047431\n",
      "[INFO] Epoch: 13 , batch: 240 , training loss: 4.087721\n",
      "[INFO] Epoch: 13 , batch: 241 , training loss: 3.901096\n",
      "[INFO] Epoch: 13 , batch: 242 , training loss: 3.903815\n",
      "[INFO] Epoch: 13 , batch: 243 , training loss: 4.232325\n",
      "[INFO] Epoch: 13 , batch: 244 , training loss: 4.164790\n",
      "[INFO] Epoch: 13 , batch: 245 , training loss: 4.165706\n",
      "[INFO] Epoch: 13 , batch: 246 , training loss: 3.827104\n",
      "[INFO] Epoch: 13 , batch: 247 , training loss: 3.982267\n",
      "[INFO] Epoch: 13 , batch: 248 , training loss: 4.066019\n",
      "[INFO] Epoch: 13 , batch: 249 , training loss: 4.061145\n",
      "[INFO] Epoch: 13 , batch: 250 , training loss: 3.841484\n",
      "[INFO] Epoch: 13 , batch: 251 , training loss: 4.325145\n",
      "[INFO] Epoch: 13 , batch: 252 , training loss: 3.998023\n",
      "[INFO] Epoch: 13 , batch: 253 , training loss: 3.937480\n",
      "[INFO] Epoch: 13 , batch: 254 , training loss: 4.222692\n",
      "[INFO] Epoch: 13 , batch: 255 , training loss: 4.173272\n",
      "[INFO] Epoch: 13 , batch: 256 , training loss: 4.153545\n",
      "[INFO] Epoch: 13 , batch: 257 , training loss: 4.352360\n",
      "[INFO] Epoch: 13 , batch: 258 , training loss: 4.371618\n",
      "[INFO] Epoch: 13 , batch: 259 , training loss: 4.411166\n",
      "[INFO] Epoch: 13 , batch: 260 , training loss: 4.143732\n",
      "[INFO] Epoch: 13 , batch: 261 , training loss: 4.310987\n",
      "[INFO] Epoch: 13 , batch: 262 , training loss: 4.503650\n",
      "[INFO] Epoch: 13 , batch: 263 , training loss: 4.636010\n",
      "[INFO] Epoch: 13 , batch: 264 , training loss: 3.995893\n",
      "[INFO] Epoch: 13 , batch: 265 , training loss: 4.112193\n",
      "[INFO] Epoch: 13 , batch: 266 , training loss: 4.568539\n",
      "[INFO] Epoch: 13 , batch: 267 , training loss: 4.277448\n",
      "[INFO] Epoch: 13 , batch: 268 , training loss: 4.179056\n",
      "[INFO] Epoch: 13 , batch: 269 , training loss: 4.195022\n",
      "[INFO] Epoch: 13 , batch: 270 , training loss: 4.213729\n",
      "[INFO] Epoch: 13 , batch: 271 , training loss: 4.254303\n",
      "[INFO] Epoch: 13 , batch: 272 , training loss: 4.238157\n",
      "[INFO] Epoch: 13 , batch: 273 , training loss: 4.238023\n",
      "[INFO] Epoch: 13 , batch: 274 , training loss: 4.331406\n",
      "[INFO] Epoch: 13 , batch: 275 , training loss: 4.183526\n",
      "[INFO] Epoch: 13 , batch: 276 , training loss: 4.255339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 13 , batch: 277 , training loss: 4.412928\n",
      "[INFO] Epoch: 13 , batch: 278 , training loss: 4.040180\n",
      "[INFO] Epoch: 13 , batch: 279 , training loss: 4.070585\n",
      "[INFO] Epoch: 13 , batch: 280 , training loss: 4.033079\n",
      "[INFO] Epoch: 13 , batch: 281 , training loss: 4.165728\n",
      "[INFO] Epoch: 13 , batch: 282 , training loss: 4.069377\n",
      "[INFO] Epoch: 13 , batch: 283 , training loss: 4.082463\n",
      "[INFO] Epoch: 13 , batch: 284 , training loss: 4.146848\n",
      "[INFO] Epoch: 13 , batch: 285 , training loss: 4.091352\n",
      "[INFO] Epoch: 13 , batch: 286 , training loss: 4.071187\n",
      "[INFO] Epoch: 13 , batch: 287 , training loss: 3.993194\n",
      "[INFO] Epoch: 13 , batch: 288 , training loss: 3.983614\n",
      "[INFO] Epoch: 13 , batch: 289 , training loss: 4.067755\n",
      "[INFO] Epoch: 13 , batch: 290 , training loss: 3.838902\n",
      "[INFO] Epoch: 13 , batch: 291 , training loss: 3.826283\n",
      "[INFO] Epoch: 13 , batch: 292 , training loss: 3.944609\n",
      "[INFO] Epoch: 13 , batch: 293 , training loss: 3.873930\n",
      "[INFO] Epoch: 13 , batch: 294 , training loss: 4.536227\n",
      "[INFO] Epoch: 13 , batch: 295 , training loss: 4.293895\n",
      "[INFO] Epoch: 13 , batch: 296 , training loss: 4.232770\n",
      "[INFO] Epoch: 13 , batch: 297 , training loss: 4.190126\n",
      "[INFO] Epoch: 13 , batch: 298 , training loss: 4.013910\n",
      "[INFO] Epoch: 13 , batch: 299 , training loss: 4.057375\n",
      "[INFO] Epoch: 13 , batch: 300 , training loss: 4.027164\n",
      "[INFO] Epoch: 13 , batch: 301 , training loss: 3.976291\n",
      "[INFO] Epoch: 13 , batch: 302 , training loss: 4.140075\n",
      "[INFO] Epoch: 13 , batch: 303 , training loss: 4.149179\n",
      "[INFO] Epoch: 13 , batch: 304 , training loss: 4.313196\n",
      "[INFO] Epoch: 13 , batch: 305 , training loss: 4.096217\n",
      "[INFO] Epoch: 13 , batch: 306 , training loss: 4.219152\n",
      "[INFO] Epoch: 13 , batch: 307 , training loss: 4.227361\n",
      "[INFO] Epoch: 13 , batch: 308 , training loss: 4.049478\n",
      "[INFO] Epoch: 13 , batch: 309 , training loss: 4.066290\n",
      "[INFO] Epoch: 13 , batch: 310 , training loss: 3.948133\n",
      "[INFO] Epoch: 13 , batch: 311 , training loss: 3.963790\n",
      "[INFO] Epoch: 13 , batch: 312 , training loss: 3.859962\n",
      "[INFO] Epoch: 13 , batch: 313 , training loss: 3.967403\n",
      "[INFO] Epoch: 13 , batch: 314 , training loss: 4.052263\n",
      "[INFO] Epoch: 13 , batch: 315 , training loss: 4.103563\n",
      "[INFO] Epoch: 13 , batch: 316 , training loss: 4.406016\n",
      "[INFO] Epoch: 13 , batch: 317 , training loss: 4.816984\n",
      "[INFO] Epoch: 13 , batch: 318 , training loss: 4.959856\n",
      "[INFO] Epoch: 13 , batch: 319 , training loss: 4.590053\n",
      "[INFO] Epoch: 13 , batch: 320 , training loss: 4.105427\n",
      "[INFO] Epoch: 13 , batch: 321 , training loss: 3.906979\n",
      "[INFO] Epoch: 13 , batch: 322 , training loss: 4.025487\n",
      "[INFO] Epoch: 13 , batch: 323 , training loss: 4.056612\n",
      "[INFO] Epoch: 13 , batch: 324 , training loss: 4.035841\n",
      "[INFO] Epoch: 13 , batch: 325 , training loss: 4.172881\n",
      "[INFO] Epoch: 13 , batch: 326 , training loss: 4.210220\n",
      "[INFO] Epoch: 13 , batch: 327 , training loss: 4.141979\n",
      "[INFO] Epoch: 13 , batch: 328 , training loss: 4.141585\n",
      "[INFO] Epoch: 13 , batch: 329 , training loss: 4.030519\n",
      "[INFO] Epoch: 13 , batch: 330 , training loss: 4.028646\n",
      "[INFO] Epoch: 13 , batch: 331 , training loss: 4.203742\n",
      "[INFO] Epoch: 13 , batch: 332 , training loss: 4.014678\n",
      "[INFO] Epoch: 13 , batch: 333 , training loss: 4.014854\n",
      "[INFO] Epoch: 13 , batch: 334 , training loss: 4.050046\n",
      "[INFO] Epoch: 13 , batch: 335 , training loss: 4.147421\n",
      "[INFO] Epoch: 13 , batch: 336 , training loss: 4.161061\n",
      "[INFO] Epoch: 13 , batch: 337 , training loss: 4.242733\n",
      "[INFO] Epoch: 13 , batch: 338 , training loss: 4.419080\n",
      "[INFO] Epoch: 13 , batch: 339 , training loss: 4.221042\n",
      "[INFO] Epoch: 13 , batch: 340 , training loss: 4.446902\n",
      "[INFO] Epoch: 13 , batch: 341 , training loss: 4.169192\n",
      "[INFO] Epoch: 13 , batch: 342 , training loss: 3.961347\n",
      "[INFO] Epoch: 13 , batch: 343 , training loss: 4.027191\n",
      "[INFO] Epoch: 13 , batch: 344 , training loss: 3.888294\n",
      "[INFO] Epoch: 13 , batch: 345 , training loss: 4.049341\n",
      "[INFO] Epoch: 13 , batch: 346 , training loss: 4.074963\n",
      "[INFO] Epoch: 13 , batch: 347 , training loss: 3.997203\n",
      "[INFO] Epoch: 13 , batch: 348 , training loss: 4.122021\n",
      "[INFO] Epoch: 13 , batch: 349 , training loss: 4.214206\n",
      "[INFO] Epoch: 13 , batch: 350 , training loss: 4.039773\n",
      "[INFO] Epoch: 13 , batch: 351 , training loss: 4.116873\n",
      "[INFO] Epoch: 13 , batch: 352 , training loss: 4.117430\n",
      "[INFO] Epoch: 13 , batch: 353 , training loss: 4.090855\n",
      "[INFO] Epoch: 13 , batch: 354 , training loss: 4.195868\n",
      "[INFO] Epoch: 13 , batch: 355 , training loss: 4.212214\n",
      "[INFO] Epoch: 13 , batch: 356 , training loss: 4.078662\n",
      "[INFO] Epoch: 13 , batch: 357 , training loss: 4.163186\n",
      "[INFO] Epoch: 13 , batch: 358 , training loss: 4.078044\n",
      "[INFO] Epoch: 13 , batch: 359 , training loss: 4.055749\n",
      "[INFO] Epoch: 13 , batch: 360 , training loss: 4.128802\n",
      "[INFO] Epoch: 13 , batch: 361 , training loss: 4.117597\n",
      "[INFO] Epoch: 13 , batch: 362 , training loss: 4.217529\n",
      "[INFO] Epoch: 13 , batch: 363 , training loss: 4.107780\n",
      "[INFO] Epoch: 13 , batch: 364 , training loss: 4.166165\n",
      "[INFO] Epoch: 13 , batch: 365 , training loss: 4.062860\n",
      "[INFO] Epoch: 13 , batch: 366 , training loss: 4.184832\n",
      "[INFO] Epoch: 13 , batch: 367 , training loss: 4.233184\n",
      "[INFO] Epoch: 13 , batch: 368 , training loss: 4.696735\n",
      "[INFO] Epoch: 13 , batch: 369 , training loss: 4.358204\n",
      "[INFO] Epoch: 13 , batch: 370 , training loss: 4.096680\n",
      "[INFO] Epoch: 13 , batch: 371 , training loss: 4.561875\n",
      "[INFO] Epoch: 13 , batch: 372 , training loss: 4.822606\n",
      "[INFO] Epoch: 13 , batch: 373 , training loss: 4.880471\n",
      "[INFO] Epoch: 13 , batch: 374 , training loss: 4.983798\n",
      "[INFO] Epoch: 13 , batch: 375 , training loss: 4.965767\n",
      "[INFO] Epoch: 13 , batch: 376 , training loss: 4.862147\n",
      "[INFO] Epoch: 13 , batch: 377 , training loss: 4.585383\n",
      "[INFO] Epoch: 13 , batch: 378 , training loss: 4.659746\n",
      "[INFO] Epoch: 13 , batch: 379 , training loss: 4.652053\n",
      "[INFO] Epoch: 13 , batch: 380 , training loss: 4.770644\n",
      "[INFO] Epoch: 13 , batch: 381 , training loss: 4.513005\n",
      "[INFO] Epoch: 13 , batch: 382 , training loss: 4.825073\n",
      "[INFO] Epoch: 13 , batch: 383 , training loss: 4.821980\n",
      "[INFO] Epoch: 13 , batch: 384 , training loss: 4.839592\n",
      "[INFO] Epoch: 13 , batch: 385 , training loss: 4.541605\n",
      "[INFO] Epoch: 13 , batch: 386 , training loss: 4.764221\n",
      "[INFO] Epoch: 13 , batch: 387 , training loss: 4.706660\n",
      "[INFO] Epoch: 13 , batch: 388 , training loss: 4.478910\n",
      "[INFO] Epoch: 13 , batch: 389 , training loss: 4.321774\n",
      "[INFO] Epoch: 13 , batch: 390 , training loss: 4.311562\n",
      "[INFO] Epoch: 13 , batch: 391 , training loss: 4.322568\n",
      "[INFO] Epoch: 13 , batch: 392 , training loss: 4.694026\n",
      "[INFO] Epoch: 13 , batch: 393 , training loss: 4.608341\n",
      "[INFO] Epoch: 13 , batch: 394 , training loss: 4.676401\n",
      "[INFO] Epoch: 13 , batch: 395 , training loss: 4.495993\n",
      "[INFO] Epoch: 13 , batch: 396 , training loss: 4.290371\n",
      "[INFO] Epoch: 13 , batch: 397 , training loss: 4.461201\n",
      "[INFO] Epoch: 13 , batch: 398 , training loss: 4.320477\n",
      "[INFO] Epoch: 13 , batch: 399 , training loss: 4.392520\n",
      "[INFO] Epoch: 13 , batch: 400 , training loss: 4.353013\n",
      "[INFO] Epoch: 13 , batch: 401 , training loss: 4.806323\n",
      "[INFO] Epoch: 13 , batch: 402 , training loss: 4.507846\n",
      "[INFO] Epoch: 13 , batch: 403 , training loss: 4.343771\n",
      "[INFO] Epoch: 13 , batch: 404 , training loss: 4.497831\n",
      "[INFO] Epoch: 13 , batch: 405 , training loss: 4.572167\n",
      "[INFO] Epoch: 13 , batch: 406 , training loss: 4.461347\n",
      "[INFO] Epoch: 13 , batch: 407 , training loss: 4.524451\n",
      "[INFO] Epoch: 13 , batch: 408 , training loss: 4.437180\n",
      "[INFO] Epoch: 13 , batch: 409 , training loss: 4.468741\n",
      "[INFO] Epoch: 13 , batch: 410 , training loss: 4.532424\n",
      "[INFO] Epoch: 13 , batch: 411 , training loss: 4.715476\n",
      "[INFO] Epoch: 13 , batch: 412 , training loss: 4.549131\n",
      "[INFO] Epoch: 13 , batch: 413 , training loss: 4.409192\n",
      "[INFO] Epoch: 13 , batch: 414 , training loss: 4.446708\n",
      "[INFO] Epoch: 13 , batch: 415 , training loss: 4.480745\n",
      "[INFO] Epoch: 13 , batch: 416 , training loss: 4.551637\n",
      "[INFO] Epoch: 13 , batch: 417 , training loss: 4.448828\n",
      "[INFO] Epoch: 13 , batch: 418 , training loss: 4.514399\n",
      "[INFO] Epoch: 13 , batch: 419 , training loss: 4.437039\n",
      "[INFO] Epoch: 13 , batch: 420 , training loss: 4.423667\n",
      "[INFO] Epoch: 13 , batch: 421 , training loss: 4.421913\n",
      "[INFO] Epoch: 13 , batch: 422 , training loss: 4.290523\n",
      "[INFO] Epoch: 13 , batch: 423 , training loss: 4.495255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 13 , batch: 424 , training loss: 4.680926\n",
      "[INFO] Epoch: 13 , batch: 425 , training loss: 4.543470\n",
      "[INFO] Epoch: 13 , batch: 426 , training loss: 4.255456\n",
      "[INFO] Epoch: 13 , batch: 427 , training loss: 4.521696\n",
      "[INFO] Epoch: 13 , batch: 428 , training loss: 4.383262\n",
      "[INFO] Epoch: 13 , batch: 429 , training loss: 4.269951\n",
      "[INFO] Epoch: 13 , batch: 430 , training loss: 4.524691\n",
      "[INFO] Epoch: 13 , batch: 431 , training loss: 4.102166\n",
      "[INFO] Epoch: 13 , batch: 432 , training loss: 4.162139\n",
      "[INFO] Epoch: 13 , batch: 433 , training loss: 4.185597\n",
      "[INFO] Epoch: 13 , batch: 434 , training loss: 4.088684\n",
      "[INFO] Epoch: 13 , batch: 435 , training loss: 4.431334\n",
      "[INFO] Epoch: 13 , batch: 436 , training loss: 4.488245\n",
      "[INFO] Epoch: 13 , batch: 437 , training loss: 4.272388\n",
      "[INFO] Epoch: 13 , batch: 438 , training loss: 4.104824\n",
      "[INFO] Epoch: 13 , batch: 439 , training loss: 4.360778\n",
      "[INFO] Epoch: 13 , batch: 440 , training loss: 4.498768\n",
      "[INFO] Epoch: 13 , batch: 441 , training loss: 4.548459\n",
      "[INFO] Epoch: 13 , batch: 442 , training loss: 4.318531\n",
      "[INFO] Epoch: 13 , batch: 443 , training loss: 4.538673\n",
      "[INFO] Epoch: 13 , batch: 444 , training loss: 4.122019\n",
      "[INFO] Epoch: 13 , batch: 445 , training loss: 4.056802\n",
      "[INFO] Epoch: 13 , batch: 446 , training loss: 3.970317\n",
      "[INFO] Epoch: 13 , batch: 447 , training loss: 4.154003\n",
      "[INFO] Epoch: 13 , batch: 448 , training loss: 4.300969\n",
      "[INFO] Epoch: 13 , batch: 449 , training loss: 4.678731\n",
      "[INFO] Epoch: 13 , batch: 450 , training loss: 4.761984\n",
      "[INFO] Epoch: 13 , batch: 451 , training loss: 4.630379\n",
      "[INFO] Epoch: 13 , batch: 452 , training loss: 4.437720\n",
      "[INFO] Epoch: 13 , batch: 453 , training loss: 4.211456\n",
      "[INFO] Epoch: 13 , batch: 454 , training loss: 4.381734\n",
      "[INFO] Epoch: 13 , batch: 455 , training loss: 4.409343\n",
      "[INFO] Epoch: 13 , batch: 456 , training loss: 4.390337\n",
      "[INFO] Epoch: 13 , batch: 457 , training loss: 4.486924\n",
      "[INFO] Epoch: 13 , batch: 458 , training loss: 4.234200\n",
      "[INFO] Epoch: 13 , batch: 459 , training loss: 4.198077\n",
      "[INFO] Epoch: 13 , batch: 460 , training loss: 4.329829\n",
      "[INFO] Epoch: 13 , batch: 461 , training loss: 4.287410\n",
      "[INFO] Epoch: 13 , batch: 462 , training loss: 4.346130\n",
      "[INFO] Epoch: 13 , batch: 463 , training loss: 4.242745\n",
      "[INFO] Epoch: 13 , batch: 464 , training loss: 4.425592\n",
      "[INFO] Epoch: 13 , batch: 465 , training loss: 4.365589\n",
      "[INFO] Epoch: 13 , batch: 466 , training loss: 4.469214\n",
      "[INFO] Epoch: 13 , batch: 467 , training loss: 4.444366\n",
      "[INFO] Epoch: 13 , batch: 468 , training loss: 4.406609\n",
      "[INFO] Epoch: 13 , batch: 469 , training loss: 4.449334\n",
      "[INFO] Epoch: 13 , batch: 470 , training loss: 4.221073\n",
      "[INFO] Epoch: 13 , batch: 471 , training loss: 4.342829\n",
      "[INFO] Epoch: 13 , batch: 472 , training loss: 4.400270\n",
      "[INFO] Epoch: 13 , batch: 473 , training loss: 4.335346\n",
      "[INFO] Epoch: 13 , batch: 474 , training loss: 4.117374\n",
      "[INFO] Epoch: 13 , batch: 475 , training loss: 3.978589\n",
      "[INFO] Epoch: 13 , batch: 476 , training loss: 4.400128\n",
      "[INFO] Epoch: 13 , batch: 477 , training loss: 4.486804\n",
      "[INFO] Epoch: 13 , batch: 478 , training loss: 4.522622\n",
      "[INFO] Epoch: 13 , batch: 479 , training loss: 4.472060\n",
      "[INFO] Epoch: 13 , batch: 480 , training loss: 4.594660\n",
      "[INFO] Epoch: 13 , batch: 481 , training loss: 4.471827\n",
      "[INFO] Epoch: 13 , batch: 482 , training loss: 4.589439\n",
      "[INFO] Epoch: 13 , batch: 483 , training loss: 4.432023\n",
      "[INFO] Epoch: 13 , batch: 484 , training loss: 4.202751\n",
      "[INFO] Epoch: 13 , batch: 485 , training loss: 4.337079\n",
      "[INFO] Epoch: 13 , batch: 486 , training loss: 4.231332\n",
      "[INFO] Epoch: 13 , batch: 487 , training loss: 4.209362\n",
      "[INFO] Epoch: 13 , batch: 488 , training loss: 4.398249\n",
      "[INFO] Epoch: 13 , batch: 489 , training loss: 4.285505\n",
      "[INFO] Epoch: 13 , batch: 490 , training loss: 4.368599\n",
      "[INFO] Epoch: 13 , batch: 491 , training loss: 4.302694\n",
      "[INFO] Epoch: 13 , batch: 492 , training loss: 4.246363\n",
      "[INFO] Epoch: 13 , batch: 493 , training loss: 4.411123\n",
      "[INFO] Epoch: 13 , batch: 494 , training loss: 4.319807\n",
      "[INFO] Epoch: 13 , batch: 495 , training loss: 4.465614\n",
      "[INFO] Epoch: 13 , batch: 496 , training loss: 4.350301\n",
      "[INFO] Epoch: 13 , batch: 497 , training loss: 4.377545\n",
      "[INFO] Epoch: 13 , batch: 498 , training loss: 4.384301\n",
      "[INFO] Epoch: 13 , batch: 499 , training loss: 4.456196\n",
      "[INFO] Epoch: 13 , batch: 500 , training loss: 4.595763\n",
      "[INFO] Epoch: 13 , batch: 501 , training loss: 4.949181\n",
      "[INFO] Epoch: 13 , batch: 502 , training loss: 5.039939\n",
      "[INFO] Epoch: 13 , batch: 503 , training loss: 4.772192\n",
      "[INFO] Epoch: 13 , batch: 504 , training loss: 4.890516\n",
      "[INFO] Epoch: 13 , batch: 505 , training loss: 4.835981\n",
      "[INFO] Epoch: 13 , batch: 506 , training loss: 4.785552\n",
      "[INFO] Epoch: 13 , batch: 507 , training loss: 4.841018\n",
      "[INFO] Epoch: 13 , batch: 508 , training loss: 4.754777\n",
      "[INFO] Epoch: 13 , batch: 509 , training loss: 4.531187\n",
      "[INFO] Epoch: 13 , batch: 510 , training loss: 4.629544\n",
      "[INFO] Epoch: 13 , batch: 511 , training loss: 4.517266\n",
      "[INFO] Epoch: 13 , batch: 512 , training loss: 4.583113\n",
      "[INFO] Epoch: 13 , batch: 513 , training loss: 4.834905\n",
      "[INFO] Epoch: 13 , batch: 514 , training loss: 4.503338\n",
      "[INFO] Epoch: 13 , batch: 515 , training loss: 4.754641\n",
      "[INFO] Epoch: 13 , batch: 516 , training loss: 4.533123\n",
      "[INFO] Epoch: 13 , batch: 517 , training loss: 4.512471\n",
      "[INFO] Epoch: 13 , batch: 518 , training loss: 4.474733\n",
      "[INFO] Epoch: 13 , batch: 519 , training loss: 4.331789\n",
      "[INFO] Epoch: 13 , batch: 520 , training loss: 4.572471\n",
      "[INFO] Epoch: 13 , batch: 521 , training loss: 4.561900\n",
      "[INFO] Epoch: 13 , batch: 522 , training loss: 4.642839\n",
      "[INFO] Epoch: 13 , batch: 523 , training loss: 4.522676\n",
      "[INFO] Epoch: 13 , batch: 524 , training loss: 4.808406\n",
      "[INFO] Epoch: 13 , batch: 525 , training loss: 4.684285\n",
      "[INFO] Epoch: 13 , batch: 526 , training loss: 4.472363\n",
      "[INFO] Epoch: 13 , batch: 527 , training loss: 4.520728\n",
      "[INFO] Epoch: 13 , batch: 528 , training loss: 4.541072\n",
      "[INFO] Epoch: 13 , batch: 529 , training loss: 4.515474\n",
      "[INFO] Epoch: 13 , batch: 530 , training loss: 4.366307\n",
      "[INFO] Epoch: 13 , batch: 531 , training loss: 4.526732\n",
      "[INFO] Epoch: 13 , batch: 532 , training loss: 4.396571\n",
      "[INFO] Epoch: 13 , batch: 533 , training loss: 4.547235\n",
      "[INFO] Epoch: 13 , batch: 534 , training loss: 4.530520\n",
      "[INFO] Epoch: 13 , batch: 535 , training loss: 4.545819\n",
      "[INFO] Epoch: 13 , batch: 536 , training loss: 4.396743\n",
      "[INFO] Epoch: 13 , batch: 537 , training loss: 4.350407\n",
      "[INFO] Epoch: 13 , batch: 538 , training loss: 4.455886\n",
      "[INFO] Epoch: 13 , batch: 539 , training loss: 4.595499\n",
      "[INFO] Epoch: 13 , batch: 540 , training loss: 5.159875\n",
      "[INFO] Epoch: 13 , batch: 541 , training loss: 5.016140\n",
      "[INFO] Epoch: 13 , batch: 542 , training loss: 4.848377\n",
      "[INFO] Epoch: 14 , batch: 0 , training loss: 4.116992\n",
      "[INFO] Epoch: 14 , batch: 1 , training loss: 3.874934\n",
      "[INFO] Epoch: 14 , batch: 2 , training loss: 3.934321\n",
      "[INFO] Epoch: 14 , batch: 3 , training loss: 3.803425\n",
      "[INFO] Epoch: 14 , batch: 4 , training loss: 4.154309\n",
      "[INFO] Epoch: 14 , batch: 5 , training loss: 3.801511\n",
      "[INFO] Epoch: 14 , batch: 6 , training loss: 4.132550\n",
      "[INFO] Epoch: 14 , batch: 7 , training loss: 3.986258\n",
      "[INFO] Epoch: 14 , batch: 8 , training loss: 3.644518\n",
      "[INFO] Epoch: 14 , batch: 9 , training loss: 3.895039\n",
      "[INFO] Epoch: 14 , batch: 10 , training loss: 3.820689\n",
      "[INFO] Epoch: 14 , batch: 11 , training loss: 3.748906\n",
      "[INFO] Epoch: 14 , batch: 12 , training loss: 3.717790\n",
      "[INFO] Epoch: 14 , batch: 13 , training loss: 3.749368\n",
      "[INFO] Epoch: 14 , batch: 14 , training loss: 3.601786\n",
      "[INFO] Epoch: 14 , batch: 15 , training loss: 3.803029\n",
      "[INFO] Epoch: 14 , batch: 16 , training loss: 3.683748\n",
      "[INFO] Epoch: 14 , batch: 17 , training loss: 3.803463\n",
      "[INFO] Epoch: 14 , batch: 18 , training loss: 3.733511\n",
      "[INFO] Epoch: 14 , batch: 19 , training loss: 3.510492\n",
      "[INFO] Epoch: 14 , batch: 20 , training loss: 3.490009\n",
      "[INFO] Epoch: 14 , batch: 21 , training loss: 3.637784\n",
      "[INFO] Epoch: 14 , batch: 22 , training loss: 3.553272\n",
      "[INFO] Epoch: 14 , batch: 23 , training loss: 3.731751\n",
      "[INFO] Epoch: 14 , batch: 24 , training loss: 3.642518\n",
      "[INFO] Epoch: 14 , batch: 25 , training loss: 3.753483\n",
      "[INFO] Epoch: 14 , batch: 26 , training loss: 3.576419\n",
      "[INFO] Epoch: 14 , batch: 27 , training loss: 3.564028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 14 , batch: 28 , training loss: 3.788234\n",
      "[INFO] Epoch: 14 , batch: 29 , training loss: 3.599698\n",
      "[INFO] Epoch: 14 , batch: 30 , training loss: 3.579078\n",
      "[INFO] Epoch: 14 , batch: 31 , training loss: 3.708307\n",
      "[INFO] Epoch: 14 , batch: 32 , training loss: 3.696507\n",
      "[INFO] Epoch: 14 , batch: 33 , training loss: 3.750276\n",
      "[INFO] Epoch: 14 , batch: 34 , training loss: 3.737886\n",
      "[INFO] Epoch: 14 , batch: 35 , training loss: 3.664732\n",
      "[INFO] Epoch: 14 , batch: 36 , training loss: 3.746484\n",
      "[INFO] Epoch: 14 , batch: 37 , training loss: 3.604632\n",
      "[INFO] Epoch: 14 , batch: 38 , training loss: 3.730110\n",
      "[INFO] Epoch: 14 , batch: 39 , training loss: 3.488271\n",
      "[INFO] Epoch: 14 , batch: 40 , training loss: 3.710576\n",
      "[INFO] Epoch: 14 , batch: 41 , training loss: 3.753841\n",
      "[INFO] Epoch: 14 , batch: 42 , training loss: 4.201716\n",
      "[INFO] Epoch: 14 , batch: 43 , training loss: 3.916013\n",
      "[INFO] Epoch: 14 , batch: 44 , training loss: 4.209367\n",
      "[INFO] Epoch: 14 , batch: 45 , training loss: 4.215310\n",
      "[INFO] Epoch: 14 , batch: 46 , training loss: 4.249444\n",
      "[INFO] Epoch: 14 , batch: 47 , training loss: 3.916296\n",
      "[INFO] Epoch: 14 , batch: 48 , training loss: 3.845438\n",
      "[INFO] Epoch: 14 , batch: 49 , training loss: 4.033400\n",
      "[INFO] Epoch: 14 , batch: 50 , training loss: 3.757560\n",
      "[INFO] Epoch: 14 , batch: 51 , training loss: 3.969966\n",
      "[INFO] Epoch: 14 , batch: 52 , training loss: 3.810976\n",
      "[INFO] Epoch: 14 , batch: 53 , training loss: 3.941065\n",
      "[INFO] Epoch: 14 , batch: 54 , training loss: 3.942750\n",
      "[INFO] Epoch: 14 , batch: 55 , training loss: 3.993531\n",
      "[INFO] Epoch: 14 , batch: 56 , training loss: 3.850367\n",
      "[INFO] Epoch: 14 , batch: 57 , training loss: 3.773856\n",
      "[INFO] Epoch: 14 , batch: 58 , training loss: 3.819743\n",
      "[INFO] Epoch: 14 , batch: 59 , training loss: 3.900494\n",
      "[INFO] Epoch: 14 , batch: 60 , training loss: 3.822674\n",
      "[INFO] Epoch: 14 , batch: 61 , training loss: 3.921922\n",
      "[INFO] Epoch: 14 , batch: 62 , training loss: 3.760563\n",
      "[INFO] Epoch: 14 , batch: 63 , training loss: 3.972097\n",
      "[INFO] Epoch: 14 , batch: 64 , training loss: 4.205158\n",
      "[INFO] Epoch: 14 , batch: 65 , training loss: 3.844553\n",
      "[INFO] Epoch: 14 , batch: 66 , training loss: 3.708957\n",
      "[INFO] Epoch: 14 , batch: 67 , training loss: 3.706812\n",
      "[INFO] Epoch: 14 , batch: 68 , training loss: 3.969353\n",
      "[INFO] Epoch: 14 , batch: 69 , training loss: 3.821199\n",
      "[INFO] Epoch: 14 , batch: 70 , training loss: 4.060303\n",
      "[INFO] Epoch: 14 , batch: 71 , training loss: 3.907274\n",
      "[INFO] Epoch: 14 , batch: 72 , training loss: 3.998119\n",
      "[INFO] Epoch: 14 , batch: 73 , training loss: 3.914364\n",
      "[INFO] Epoch: 14 , batch: 74 , training loss: 4.043497\n",
      "[INFO] Epoch: 14 , batch: 75 , training loss: 3.831387\n",
      "[INFO] Epoch: 14 , batch: 76 , training loss: 3.982954\n",
      "[INFO] Epoch: 14 , batch: 77 , training loss: 3.917965\n",
      "[INFO] Epoch: 14 , batch: 78 , training loss: 4.015048\n",
      "[INFO] Epoch: 14 , batch: 79 , training loss: 3.826732\n",
      "[INFO] Epoch: 14 , batch: 80 , training loss: 4.046497\n",
      "[INFO] Epoch: 14 , batch: 81 , training loss: 3.992346\n",
      "[INFO] Epoch: 14 , batch: 82 , training loss: 3.945077\n",
      "[INFO] Epoch: 14 , batch: 83 , training loss: 4.058614\n",
      "[INFO] Epoch: 14 , batch: 84 , training loss: 3.998443\n",
      "[INFO] Epoch: 14 , batch: 85 , training loss: 4.122014\n",
      "[INFO] Epoch: 14 , batch: 86 , training loss: 4.019038\n",
      "[INFO] Epoch: 14 , batch: 87 , training loss: 4.001223\n",
      "[INFO] Epoch: 14 , batch: 88 , training loss: 4.148899\n",
      "[INFO] Epoch: 14 , batch: 89 , training loss: 3.929152\n",
      "[INFO] Epoch: 14 , batch: 90 , training loss: 4.027293\n",
      "[INFO] Epoch: 14 , batch: 91 , training loss: 3.956811\n",
      "[INFO] Epoch: 14 , batch: 92 , training loss: 3.970438\n",
      "[INFO] Epoch: 14 , batch: 93 , training loss: 4.058496\n",
      "[INFO] Epoch: 14 , batch: 94 , training loss: 4.204766\n",
      "[INFO] Epoch: 14 , batch: 95 , training loss: 3.998882\n",
      "[INFO] Epoch: 14 , batch: 96 , training loss: 3.940034\n",
      "[INFO] Epoch: 14 , batch: 97 , training loss: 3.942599\n",
      "[INFO] Epoch: 14 , batch: 98 , training loss: 3.861082\n",
      "[INFO] Epoch: 14 , batch: 99 , training loss: 3.980804\n",
      "[INFO] Epoch: 14 , batch: 100 , training loss: 3.834755\n",
      "[INFO] Epoch: 14 , batch: 101 , training loss: 3.884017\n",
      "[INFO] Epoch: 14 , batch: 102 , training loss: 4.055276\n",
      "[INFO] Epoch: 14 , batch: 103 , training loss: 3.810273\n",
      "[INFO] Epoch: 14 , batch: 104 , training loss: 3.771373\n",
      "[INFO] Epoch: 14 , batch: 105 , training loss: 4.017193\n",
      "[INFO] Epoch: 14 , batch: 106 , training loss: 4.048442\n",
      "[INFO] Epoch: 14 , batch: 107 , training loss: 3.897864\n",
      "[INFO] Epoch: 14 , batch: 108 , training loss: 3.853961\n",
      "[INFO] Epoch: 14 , batch: 109 , training loss: 3.722038\n",
      "[INFO] Epoch: 14 , batch: 110 , training loss: 3.923140\n",
      "[INFO] Epoch: 14 , batch: 111 , training loss: 4.016240\n",
      "[INFO] Epoch: 14 , batch: 112 , training loss: 3.957858\n",
      "[INFO] Epoch: 14 , batch: 113 , training loss: 3.922447\n",
      "[INFO] Epoch: 14 , batch: 114 , training loss: 3.987322\n",
      "[INFO] Epoch: 14 , batch: 115 , training loss: 3.926128\n",
      "[INFO] Epoch: 14 , batch: 116 , training loss: 3.845701\n",
      "[INFO] Epoch: 14 , batch: 117 , training loss: 4.091757\n",
      "[INFO] Epoch: 14 , batch: 118 , training loss: 4.049105\n",
      "[INFO] Epoch: 14 , batch: 119 , training loss: 4.225651\n",
      "[INFO] Epoch: 14 , batch: 120 , training loss: 4.175722\n",
      "[INFO] Epoch: 14 , batch: 121 , training loss: 4.023042\n",
      "[INFO] Epoch: 14 , batch: 122 , training loss: 3.913098\n",
      "[INFO] Epoch: 14 , batch: 123 , training loss: 3.945558\n",
      "[INFO] Epoch: 14 , batch: 124 , training loss: 4.057073\n",
      "[INFO] Epoch: 14 , batch: 125 , training loss: 3.810133\n",
      "[INFO] Epoch: 14 , batch: 126 , training loss: 3.850692\n",
      "[INFO] Epoch: 14 , batch: 127 , training loss: 3.867597\n",
      "[INFO] Epoch: 14 , batch: 128 , training loss: 4.003809\n",
      "[INFO] Epoch: 14 , batch: 129 , training loss: 3.951837\n",
      "[INFO] Epoch: 14 , batch: 130 , training loss: 3.944669\n",
      "[INFO] Epoch: 14 , batch: 131 , training loss: 3.941652\n",
      "[INFO] Epoch: 14 , batch: 132 , training loss: 3.986776\n",
      "[INFO] Epoch: 14 , batch: 133 , training loss: 3.927272\n",
      "[INFO] Epoch: 14 , batch: 134 , training loss: 3.694865\n",
      "[INFO] Epoch: 14 , batch: 135 , training loss: 3.758851\n",
      "[INFO] Epoch: 14 , batch: 136 , training loss: 4.057846\n",
      "[INFO] Epoch: 14 , batch: 137 , training loss: 3.980855\n",
      "[INFO] Epoch: 14 , batch: 138 , training loss: 4.043684\n",
      "[INFO] Epoch: 14 , batch: 139 , training loss: 4.616138\n",
      "[INFO] Epoch: 14 , batch: 140 , training loss: 4.447827\n",
      "[INFO] Epoch: 14 , batch: 141 , training loss: 4.184694\n",
      "[INFO] Epoch: 14 , batch: 142 , training loss: 3.889886\n",
      "[INFO] Epoch: 14 , batch: 143 , training loss: 3.993788\n",
      "[INFO] Epoch: 14 , batch: 144 , training loss: 3.865828\n",
      "[INFO] Epoch: 14 , batch: 145 , training loss: 3.929602\n",
      "[INFO] Epoch: 14 , batch: 146 , training loss: 4.124630\n",
      "[INFO] Epoch: 14 , batch: 147 , training loss: 3.800154\n",
      "[INFO] Epoch: 14 , batch: 148 , training loss: 3.793867\n",
      "[INFO] Epoch: 14 , batch: 149 , training loss: 3.889994\n",
      "[INFO] Epoch: 14 , batch: 150 , training loss: 4.151840\n",
      "[INFO] Epoch: 14 , batch: 151 , training loss: 3.933544\n",
      "[INFO] Epoch: 14 , batch: 152 , training loss: 3.942180\n",
      "[INFO] Epoch: 14 , batch: 153 , training loss: 3.988309\n",
      "[INFO] Epoch: 14 , batch: 154 , training loss: 4.060942\n",
      "[INFO] Epoch: 14 , batch: 155 , training loss: 4.320366\n",
      "[INFO] Epoch: 14 , batch: 156 , training loss: 3.998439\n",
      "[INFO] Epoch: 14 , batch: 157 , training loss: 3.995126\n",
      "[INFO] Epoch: 14 , batch: 158 , training loss: 4.191554\n",
      "[INFO] Epoch: 14 , batch: 159 , training loss: 4.063116\n",
      "[INFO] Epoch: 14 , batch: 160 , training loss: 4.439808\n",
      "[INFO] Epoch: 14 , batch: 161 , training loss: 4.471594\n",
      "[INFO] Epoch: 14 , batch: 162 , training loss: 4.394962\n",
      "[INFO] Epoch: 14 , batch: 163 , training loss: 4.564577\n",
      "[INFO] Epoch: 14 , batch: 164 , training loss: 4.466770\n",
      "[INFO] Epoch: 14 , batch: 165 , training loss: 4.390012\n",
      "[INFO] Epoch: 14 , batch: 166 , training loss: 4.363797\n",
      "[INFO] Epoch: 14 , batch: 167 , training loss: 4.630518\n",
      "[INFO] Epoch: 14 , batch: 168 , training loss: 4.297613\n",
      "[INFO] Epoch: 14 , batch: 169 , training loss: 4.242154\n",
      "[INFO] Epoch: 14 , batch: 170 , training loss: 4.325319\n",
      "[INFO] Epoch: 14 , batch: 171 , training loss: 3.786704\n",
      "[INFO] Epoch: 14 , batch: 172 , training loss: 3.976751\n",
      "[INFO] Epoch: 14 , batch: 173 , training loss: 4.289043\n",
      "[INFO] Epoch: 14 , batch: 174 , training loss: 4.690766\n",
      "[INFO] Epoch: 14 , batch: 175 , training loss: 4.950522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 14 , batch: 176 , training loss: 4.700583\n",
      "[INFO] Epoch: 14 , batch: 177 , training loss: 4.296338\n",
      "[INFO] Epoch: 14 , batch: 178 , training loss: 4.238719\n",
      "[INFO] Epoch: 14 , batch: 179 , training loss: 4.300426\n",
      "[INFO] Epoch: 14 , batch: 180 , training loss: 4.208127\n",
      "[INFO] Epoch: 14 , batch: 181 , training loss: 4.503217\n",
      "[INFO] Epoch: 14 , batch: 182 , training loss: 4.428805\n",
      "[INFO] Epoch: 14 , batch: 183 , training loss: 4.391326\n",
      "[INFO] Epoch: 14 , batch: 184 , training loss: 4.307433\n",
      "[INFO] Epoch: 14 , batch: 185 , training loss: 4.262400\n",
      "[INFO] Epoch: 14 , batch: 186 , training loss: 4.375571\n",
      "[INFO] Epoch: 14 , batch: 187 , training loss: 4.489426\n",
      "[INFO] Epoch: 14 , batch: 188 , training loss: 4.479970\n",
      "[INFO] Epoch: 14 , batch: 189 , training loss: 4.359026\n",
      "[INFO] Epoch: 14 , batch: 190 , training loss: 4.369009\n",
      "[INFO] Epoch: 14 , batch: 191 , training loss: 4.533482\n",
      "[INFO] Epoch: 14 , batch: 192 , training loss: 4.338364\n",
      "[INFO] Epoch: 14 , batch: 193 , training loss: 4.455102\n",
      "[INFO] Epoch: 14 , batch: 194 , training loss: 4.367278\n",
      "[INFO] Epoch: 14 , batch: 195 , training loss: 4.321995\n",
      "[INFO] Epoch: 14 , batch: 196 , training loss: 4.175176\n",
      "[INFO] Epoch: 14 , batch: 197 , training loss: 4.280220\n",
      "[INFO] Epoch: 14 , batch: 198 , training loss: 4.167770\n",
      "[INFO] Epoch: 14 , batch: 199 , training loss: 4.299038\n",
      "[INFO] Epoch: 14 , batch: 200 , training loss: 4.199663\n",
      "[INFO] Epoch: 14 , batch: 201 , training loss: 4.100388\n",
      "[INFO] Epoch: 14 , batch: 202 , training loss: 4.106204\n",
      "[INFO] Epoch: 14 , batch: 203 , training loss: 4.199465\n",
      "[INFO] Epoch: 14 , batch: 204 , training loss: 4.291304\n",
      "[INFO] Epoch: 14 , batch: 205 , training loss: 3.900687\n",
      "[INFO] Epoch: 14 , batch: 206 , training loss: 3.818810\n",
      "[INFO] Epoch: 14 , batch: 207 , training loss: 3.826505\n",
      "[INFO] Epoch: 14 , batch: 208 , training loss: 4.179136\n",
      "[INFO] Epoch: 14 , batch: 209 , training loss: 4.086314\n",
      "[INFO] Epoch: 14 , batch: 210 , training loss: 4.145551\n",
      "[INFO] Epoch: 14 , batch: 211 , training loss: 4.160296\n",
      "[INFO] Epoch: 14 , batch: 212 , training loss: 4.256735\n",
      "[INFO] Epoch: 14 , batch: 213 , training loss: 4.208129\n",
      "[INFO] Epoch: 14 , batch: 214 , training loss: 4.281743\n",
      "[INFO] Epoch: 14 , batch: 215 , training loss: 4.489722\n",
      "[INFO] Epoch: 14 , batch: 216 , training loss: 4.188957\n",
      "[INFO] Epoch: 14 , batch: 217 , training loss: 4.130877\n",
      "[INFO] Epoch: 14 , batch: 218 , training loss: 4.151520\n",
      "[INFO] Epoch: 14 , batch: 219 , training loss: 4.241800\n",
      "[INFO] Epoch: 14 , batch: 220 , training loss: 4.046304\n",
      "[INFO] Epoch: 14 , batch: 221 , training loss: 4.066855\n",
      "[INFO] Epoch: 14 , batch: 222 , training loss: 4.207055\n",
      "[INFO] Epoch: 14 , batch: 223 , training loss: 4.314479\n",
      "[INFO] Epoch: 14 , batch: 224 , training loss: 4.333986\n",
      "[INFO] Epoch: 14 , batch: 225 , training loss: 4.220037\n",
      "[INFO] Epoch: 14 , batch: 226 , training loss: 4.370801\n",
      "[INFO] Epoch: 14 , batch: 227 , training loss: 4.328057\n",
      "[INFO] Epoch: 14 , batch: 228 , training loss: 4.369410\n",
      "[INFO] Epoch: 14 , batch: 229 , training loss: 4.195220\n",
      "[INFO] Epoch: 14 , batch: 230 , training loss: 4.090675\n",
      "[INFO] Epoch: 14 , batch: 231 , training loss: 3.937438\n",
      "[INFO] Epoch: 14 , batch: 232 , training loss: 4.081873\n",
      "[INFO] Epoch: 14 , batch: 233 , training loss: 4.112221\n",
      "[INFO] Epoch: 14 , batch: 234 , training loss: 3.788303\n",
      "[INFO] Epoch: 14 , batch: 235 , training loss: 3.910784\n",
      "[INFO] Epoch: 14 , batch: 236 , training loss: 4.035594\n",
      "[INFO] Epoch: 14 , batch: 237 , training loss: 4.248898\n",
      "[INFO] Epoch: 14 , batch: 238 , training loss: 3.993269\n",
      "[INFO] Epoch: 14 , batch: 239 , training loss: 4.035137\n",
      "[INFO] Epoch: 14 , batch: 240 , training loss: 4.087936\n",
      "[INFO] Epoch: 14 , batch: 241 , training loss: 3.893435\n",
      "[INFO] Epoch: 14 , batch: 242 , training loss: 3.897014\n",
      "[INFO] Epoch: 14 , batch: 243 , training loss: 4.210200\n",
      "[INFO] Epoch: 14 , batch: 244 , training loss: 4.168983\n",
      "[INFO] Epoch: 14 , batch: 245 , training loss: 4.154277\n",
      "[INFO] Epoch: 14 , batch: 246 , training loss: 3.821676\n",
      "[INFO] Epoch: 14 , batch: 247 , training loss: 3.980294\n",
      "[INFO] Epoch: 14 , batch: 248 , training loss: 4.063771\n",
      "[INFO] Epoch: 14 , batch: 249 , training loss: 4.048841\n",
      "[INFO] Epoch: 14 , batch: 250 , training loss: 3.822155\n",
      "[INFO] Epoch: 14 , batch: 251 , training loss: 4.318336\n",
      "[INFO] Epoch: 14 , batch: 252 , training loss: 3.992639\n",
      "[INFO] Epoch: 14 , batch: 253 , training loss: 3.923878\n",
      "[INFO] Epoch: 14 , batch: 254 , training loss: 4.219580\n",
      "[INFO] Epoch: 14 , batch: 255 , training loss: 4.154068\n",
      "[INFO] Epoch: 14 , batch: 256 , training loss: 4.159895\n",
      "[INFO] Epoch: 14 , batch: 257 , training loss: 4.343483\n",
      "[INFO] Epoch: 14 , batch: 258 , training loss: 4.364660\n",
      "[INFO] Epoch: 14 , batch: 259 , training loss: 4.394739\n",
      "[INFO] Epoch: 14 , batch: 260 , training loss: 4.139571\n",
      "[INFO] Epoch: 14 , batch: 261 , training loss: 4.321285\n",
      "[INFO] Epoch: 14 , batch: 262 , training loss: 4.498490\n",
      "[INFO] Epoch: 14 , batch: 263 , training loss: 4.636271\n",
      "[INFO] Epoch: 14 , batch: 264 , training loss: 3.998794\n",
      "[INFO] Epoch: 14 , batch: 265 , training loss: 4.098517\n",
      "[INFO] Epoch: 14 , batch: 266 , training loss: 4.563179\n",
      "[INFO] Epoch: 14 , batch: 267 , training loss: 4.267395\n",
      "[INFO] Epoch: 14 , batch: 268 , training loss: 4.179424\n",
      "[INFO] Epoch: 14 , batch: 269 , training loss: 4.180144\n",
      "[INFO] Epoch: 14 , batch: 270 , training loss: 4.193790\n",
      "[INFO] Epoch: 14 , batch: 271 , training loss: 4.243197\n",
      "[INFO] Epoch: 14 , batch: 272 , training loss: 4.222925\n",
      "[INFO] Epoch: 14 , batch: 273 , training loss: 4.223495\n",
      "[INFO] Epoch: 14 , batch: 274 , training loss: 4.317646\n",
      "[INFO] Epoch: 14 , batch: 275 , training loss: 4.163324\n",
      "[INFO] Epoch: 14 , batch: 276 , training loss: 4.248113\n",
      "[INFO] Epoch: 14 , batch: 277 , training loss: 4.393675\n",
      "[INFO] Epoch: 14 , batch: 278 , training loss: 4.048571\n",
      "[INFO] Epoch: 14 , batch: 279 , training loss: 4.088480\n",
      "[INFO] Epoch: 14 , batch: 280 , training loss: 4.010852\n",
      "[INFO] Epoch: 14 , batch: 281 , training loss: 4.168868\n",
      "[INFO] Epoch: 14 , batch: 282 , training loss: 4.067287\n",
      "[INFO] Epoch: 14 , batch: 283 , training loss: 4.088491\n",
      "[INFO] Epoch: 14 , batch: 284 , training loss: 4.128894\n",
      "[INFO] Epoch: 14 , batch: 285 , training loss: 4.084508\n",
      "[INFO] Epoch: 14 , batch: 286 , training loss: 4.069742\n",
      "[INFO] Epoch: 14 , batch: 287 , training loss: 3.998213\n",
      "[INFO] Epoch: 14 , batch: 288 , training loss: 3.975179\n",
      "[INFO] Epoch: 14 , batch: 289 , training loss: 4.057017\n",
      "[INFO] Epoch: 14 , batch: 290 , training loss: 3.813679\n",
      "[INFO] Epoch: 14 , batch: 291 , training loss: 3.822194\n",
      "[INFO] Epoch: 14 , batch: 292 , training loss: 3.940795\n",
      "[INFO] Epoch: 14 , batch: 293 , training loss: 3.843451\n",
      "[INFO] Epoch: 14 , batch: 294 , training loss: 4.531868\n",
      "[INFO] Epoch: 14 , batch: 295 , training loss: 4.289225\n",
      "[INFO] Epoch: 14 , batch: 296 , training loss: 4.217594\n",
      "[INFO] Epoch: 14 , batch: 297 , training loss: 4.177702\n",
      "[INFO] Epoch: 14 , batch: 298 , training loss: 4.009221\n",
      "[INFO] Epoch: 14 , batch: 299 , training loss: 4.024824\n",
      "[INFO] Epoch: 14 , batch: 300 , training loss: 4.025299\n",
      "[INFO] Epoch: 14 , batch: 301 , training loss: 3.953991\n",
      "[INFO] Epoch: 14 , batch: 302 , training loss: 4.132591\n",
      "[INFO] Epoch: 14 , batch: 303 , training loss: 4.147225\n",
      "[INFO] Epoch: 14 , batch: 304 , training loss: 4.294798\n",
      "[INFO] Epoch: 14 , batch: 305 , training loss: 4.087811\n",
      "[INFO] Epoch: 14 , batch: 306 , training loss: 4.220155\n",
      "[INFO] Epoch: 14 , batch: 307 , training loss: 4.211153\n",
      "[INFO] Epoch: 14 , batch: 308 , training loss: 4.050360\n",
      "[INFO] Epoch: 14 , batch: 309 , training loss: 4.058492\n",
      "[INFO] Epoch: 14 , batch: 310 , training loss: 3.951213\n",
      "[INFO] Epoch: 14 , batch: 311 , training loss: 3.940703\n",
      "[INFO] Epoch: 14 , batch: 312 , training loss: 3.843904\n",
      "[INFO] Epoch: 14 , batch: 313 , training loss: 3.962283\n",
      "[INFO] Epoch: 14 , batch: 314 , training loss: 4.047781\n",
      "[INFO] Epoch: 14 , batch: 315 , training loss: 4.106181\n",
      "[INFO] Epoch: 14 , batch: 316 , training loss: 4.389930\n",
      "[INFO] Epoch: 14 , batch: 317 , training loss: 4.801651\n",
      "[INFO] Epoch: 14 , batch: 318 , training loss: 4.932386\n",
      "[INFO] Epoch: 14 , batch: 319 , training loss: 4.594540\n",
      "[INFO] Epoch: 14 , batch: 320 , training loss: 4.092937\n",
      "[INFO] Epoch: 14 , batch: 321 , training loss: 3.892449\n",
      "[INFO] Epoch: 14 , batch: 322 , training loss: 4.018364\n",
      "[INFO] Epoch: 14 , batch: 323 , training loss: 4.033296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 14 , batch: 324 , training loss: 4.026775\n",
      "[INFO] Epoch: 14 , batch: 325 , training loss: 4.157578\n",
      "[INFO] Epoch: 14 , batch: 326 , training loss: 4.193125\n",
      "[INFO] Epoch: 14 , batch: 327 , training loss: 4.127174\n",
      "[INFO] Epoch: 14 , batch: 328 , training loss: 4.129472\n",
      "[INFO] Epoch: 14 , batch: 329 , training loss: 4.026940\n",
      "[INFO] Epoch: 14 , batch: 330 , training loss: 4.029696\n",
      "[INFO] Epoch: 14 , batch: 331 , training loss: 4.189321\n",
      "[INFO] Epoch: 14 , batch: 332 , training loss: 4.010966\n",
      "[INFO] Epoch: 14 , batch: 333 , training loss: 3.993567\n",
      "[INFO] Epoch: 14 , batch: 334 , training loss: 4.024692\n",
      "[INFO] Epoch: 14 , batch: 335 , training loss: 4.150734\n",
      "[INFO] Epoch: 14 , batch: 336 , training loss: 4.148491\n",
      "[INFO] Epoch: 14 , batch: 337 , training loss: 4.215075\n",
      "[INFO] Epoch: 14 , batch: 338 , training loss: 4.401670\n",
      "[INFO] Epoch: 14 , batch: 339 , training loss: 4.221914\n",
      "[INFO] Epoch: 14 , batch: 340 , training loss: 4.433508\n",
      "[INFO] Epoch: 14 , batch: 341 , training loss: 4.164729\n",
      "[INFO] Epoch: 14 , batch: 342 , training loss: 3.953280\n",
      "[INFO] Epoch: 14 , batch: 343 , training loss: 4.036750\n",
      "[INFO] Epoch: 14 , batch: 344 , training loss: 3.871503\n",
      "[INFO] Epoch: 14 , batch: 345 , training loss: 4.028688\n",
      "[INFO] Epoch: 14 , batch: 346 , training loss: 4.065126\n",
      "[INFO] Epoch: 14 , batch: 347 , training loss: 3.996419\n",
      "[INFO] Epoch: 14 , batch: 348 , training loss: 4.108252\n",
      "[INFO] Epoch: 14 , batch: 349 , training loss: 4.190733\n",
      "[INFO] Epoch: 14 , batch: 350 , training loss: 4.019438\n",
      "[INFO] Epoch: 14 , batch: 351 , training loss: 4.101660\n",
      "[INFO] Epoch: 14 , batch: 352 , training loss: 4.111656\n",
      "[INFO] Epoch: 14 , batch: 353 , training loss: 4.083403\n",
      "[INFO] Epoch: 14 , batch: 354 , training loss: 4.187462\n",
      "[INFO] Epoch: 14 , batch: 355 , training loss: 4.193623\n",
      "[INFO] Epoch: 14 , batch: 356 , training loss: 4.078389\n",
      "[INFO] Epoch: 14 , batch: 357 , training loss: 4.151685\n",
      "[INFO] Epoch: 14 , batch: 358 , training loss: 4.078943\n",
      "[INFO] Epoch: 14 , batch: 359 , training loss: 4.037805\n",
      "[INFO] Epoch: 14 , batch: 360 , training loss: 4.129365\n",
      "[INFO] Epoch: 14 , batch: 361 , training loss: 4.104374\n",
      "[INFO] Epoch: 14 , batch: 362 , training loss: 4.200953\n",
      "[INFO] Epoch: 14 , batch: 363 , training loss: 4.094533\n",
      "[INFO] Epoch: 14 , batch: 364 , training loss: 4.153503\n",
      "[INFO] Epoch: 14 , batch: 365 , training loss: 4.046130\n",
      "[INFO] Epoch: 14 , batch: 366 , training loss: 4.172838\n",
      "[INFO] Epoch: 14 , batch: 367 , training loss: 4.222058\n",
      "[INFO] Epoch: 14 , batch: 368 , training loss: 4.672490\n",
      "[INFO] Epoch: 14 , batch: 369 , training loss: 4.329694\n",
      "[INFO] Epoch: 14 , batch: 370 , training loss: 4.081010\n",
      "[INFO] Epoch: 14 , batch: 371 , training loss: 4.536818\n",
      "[INFO] Epoch: 14 , batch: 372 , training loss: 4.825905\n",
      "[INFO] Epoch: 14 , batch: 373 , training loss: 4.836847\n",
      "[INFO] Epoch: 14 , batch: 374 , training loss: 4.949338\n",
      "[INFO] Epoch: 14 , batch: 375 , training loss: 4.939512\n",
      "[INFO] Epoch: 14 , batch: 376 , training loss: 4.847496\n",
      "[INFO] Epoch: 14 , batch: 377 , training loss: 4.551160\n",
      "[INFO] Epoch: 14 , batch: 378 , training loss: 4.633841\n",
      "[INFO] Epoch: 14 , batch: 379 , training loss: 4.633710\n",
      "[INFO] Epoch: 14 , batch: 380 , training loss: 4.746948\n",
      "[INFO] Epoch: 14 , batch: 381 , training loss: 4.516352\n",
      "[INFO] Epoch: 14 , batch: 382 , training loss: 4.795683\n",
      "[INFO] Epoch: 14 , batch: 383 , training loss: 4.804868\n",
      "[INFO] Epoch: 14 , batch: 384 , training loss: 4.832684\n",
      "[INFO] Epoch: 14 , batch: 385 , training loss: 4.503720\n",
      "[INFO] Epoch: 14 , batch: 386 , training loss: 4.730849\n",
      "[INFO] Epoch: 14 , batch: 387 , training loss: 4.685154\n",
      "[INFO] Epoch: 14 , batch: 388 , training loss: 4.474033\n",
      "[INFO] Epoch: 14 , batch: 389 , training loss: 4.303622\n",
      "[INFO] Epoch: 14 , batch: 390 , training loss: 4.294225\n",
      "[INFO] Epoch: 14 , batch: 391 , training loss: 4.324740\n",
      "[INFO] Epoch: 14 , batch: 392 , training loss: 4.680329\n",
      "[INFO] Epoch: 14 , batch: 393 , training loss: 4.591323\n",
      "[INFO] Epoch: 14 , batch: 394 , training loss: 4.654096\n",
      "[INFO] Epoch: 14 , batch: 395 , training loss: 4.470697\n",
      "[INFO] Epoch: 14 , batch: 396 , training loss: 4.268773\n",
      "[INFO] Epoch: 14 , batch: 397 , training loss: 4.441905\n",
      "[INFO] Epoch: 14 , batch: 398 , training loss: 4.306041\n",
      "[INFO] Epoch: 14 , batch: 399 , training loss: 4.373040\n",
      "[INFO] Epoch: 14 , batch: 400 , training loss: 4.352130\n",
      "[INFO] Epoch: 14 , batch: 401 , training loss: 4.781359\n",
      "[INFO] Epoch: 14 , batch: 402 , training loss: 4.502200\n",
      "[INFO] Epoch: 14 , batch: 403 , training loss: 4.331164\n",
      "[INFO] Epoch: 14 , batch: 404 , training loss: 4.477277\n",
      "[INFO] Epoch: 14 , batch: 405 , training loss: 4.552562\n",
      "[INFO] Epoch: 14 , batch: 406 , training loss: 4.466931\n",
      "[INFO] Epoch: 14 , batch: 407 , training loss: 4.524953\n",
      "[INFO] Epoch: 14 , batch: 408 , training loss: 4.436162\n",
      "[INFO] Epoch: 14 , batch: 409 , training loss: 4.460294\n",
      "[INFO] Epoch: 14 , batch: 410 , training loss: 4.536659\n",
      "[INFO] Epoch: 14 , batch: 411 , training loss: 4.697115\n",
      "[INFO] Epoch: 14 , batch: 412 , training loss: 4.536994\n",
      "[INFO] Epoch: 14 , batch: 413 , training loss: 4.397779\n",
      "[INFO] Epoch: 14 , batch: 414 , training loss: 4.437904\n",
      "[INFO] Epoch: 14 , batch: 415 , training loss: 4.467236\n",
      "[INFO] Epoch: 14 , batch: 416 , training loss: 4.553839\n",
      "[INFO] Epoch: 14 , batch: 417 , training loss: 4.446600\n",
      "[INFO] Epoch: 14 , batch: 418 , training loss: 4.502233\n",
      "[INFO] Epoch: 14 , batch: 419 , training loss: 4.433494\n",
      "[INFO] Epoch: 14 , batch: 420 , training loss: 4.412127\n",
      "[INFO] Epoch: 14 , batch: 421 , training loss: 4.413550\n",
      "[INFO] Epoch: 14 , batch: 422 , training loss: 4.282859\n",
      "[INFO] Epoch: 14 , batch: 423 , training loss: 4.490691\n",
      "[INFO] Epoch: 14 , batch: 424 , training loss: 4.659296\n",
      "[INFO] Epoch: 14 , batch: 425 , training loss: 4.540875\n",
      "[INFO] Epoch: 14 , batch: 426 , training loss: 4.247655\n",
      "[INFO] Epoch: 14 , batch: 427 , training loss: 4.509842\n",
      "[INFO] Epoch: 14 , batch: 428 , training loss: 4.369197\n",
      "[INFO] Epoch: 14 , batch: 429 , training loss: 4.265973\n",
      "[INFO] Epoch: 14 , batch: 430 , training loss: 4.512077\n",
      "[INFO] Epoch: 14 , batch: 431 , training loss: 4.103751\n",
      "[INFO] Epoch: 14 , batch: 432 , training loss: 4.151476\n",
      "[INFO] Epoch: 14 , batch: 433 , training loss: 4.186010\n",
      "[INFO] Epoch: 14 , batch: 434 , training loss: 4.082729\n",
      "[INFO] Epoch: 14 , batch: 435 , training loss: 4.432375\n",
      "[INFO] Epoch: 14 , batch: 436 , training loss: 4.488507\n",
      "[INFO] Epoch: 14 , batch: 437 , training loss: 4.268263\n",
      "[INFO] Epoch: 14 , batch: 438 , training loss: 4.099475\n",
      "[INFO] Epoch: 14 , batch: 439 , training loss: 4.344406\n",
      "[INFO] Epoch: 14 , batch: 440 , training loss: 4.485196\n",
      "[INFO] Epoch: 14 , batch: 441 , training loss: 4.541243\n",
      "[INFO] Epoch: 14 , batch: 442 , training loss: 4.310091\n",
      "[INFO] Epoch: 14 , batch: 443 , training loss: 4.523362\n",
      "[INFO] Epoch: 14 , batch: 444 , training loss: 4.115956\n",
      "[INFO] Epoch: 14 , batch: 445 , training loss: 4.041557\n",
      "[INFO] Epoch: 14 , batch: 446 , training loss: 3.963726\n",
      "[INFO] Epoch: 14 , batch: 447 , training loss: 4.140774\n",
      "[INFO] Epoch: 14 , batch: 448 , training loss: 4.281752\n",
      "[INFO] Epoch: 14 , batch: 449 , training loss: 4.671344\n",
      "[INFO] Epoch: 14 , batch: 450 , training loss: 4.749300\n",
      "[INFO] Epoch: 14 , batch: 451 , training loss: 4.621404\n",
      "[INFO] Epoch: 14 , batch: 452 , training loss: 4.429876\n",
      "[INFO] Epoch: 14 , batch: 453 , training loss: 4.214451\n",
      "[INFO] Epoch: 14 , batch: 454 , training loss: 4.366670\n",
      "[INFO] Epoch: 14 , batch: 455 , training loss: 4.406668\n",
      "[INFO] Epoch: 14 , batch: 456 , training loss: 4.385773\n",
      "[INFO] Epoch: 14 , batch: 457 , training loss: 4.485168\n",
      "[INFO] Epoch: 14 , batch: 458 , training loss: 4.229181\n",
      "[INFO] Epoch: 14 , batch: 459 , training loss: 4.177449\n",
      "[INFO] Epoch: 14 , batch: 460 , training loss: 4.307819\n",
      "[INFO] Epoch: 14 , batch: 461 , training loss: 4.274852\n",
      "[INFO] Epoch: 14 , batch: 462 , training loss: 4.333269\n",
      "[INFO] Epoch: 14 , batch: 463 , training loss: 4.234302\n",
      "[INFO] Epoch: 14 , batch: 464 , training loss: 4.415800\n",
      "[INFO] Epoch: 14 , batch: 465 , training loss: 4.348966\n",
      "[INFO] Epoch: 14 , batch: 466 , training loss: 4.459419\n",
      "[INFO] Epoch: 14 , batch: 467 , training loss: 4.430757\n",
      "[INFO] Epoch: 14 , batch: 468 , training loss: 4.386803\n",
      "[INFO] Epoch: 14 , batch: 469 , training loss: 4.420769\n",
      "[INFO] Epoch: 14 , batch: 470 , training loss: 4.211114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 14 , batch: 471 , training loss: 4.337523\n",
      "[INFO] Epoch: 14 , batch: 472 , training loss: 4.388178\n",
      "[INFO] Epoch: 14 , batch: 473 , training loss: 4.318898\n",
      "[INFO] Epoch: 14 , batch: 474 , training loss: 4.097387\n",
      "[INFO] Epoch: 14 , batch: 475 , training loss: 3.970760\n",
      "[INFO] Epoch: 14 , batch: 476 , training loss: 4.383717\n",
      "[INFO] Epoch: 14 , batch: 477 , training loss: 4.475430\n",
      "[INFO] Epoch: 14 , batch: 478 , training loss: 4.504587\n",
      "[INFO] Epoch: 14 , batch: 479 , training loss: 4.453959\n",
      "[INFO] Epoch: 14 , batch: 480 , training loss: 4.585990\n",
      "[INFO] Epoch: 14 , batch: 481 , training loss: 4.462615\n",
      "[INFO] Epoch: 14 , batch: 482 , training loss: 4.580815\n",
      "[INFO] Epoch: 14 , batch: 483 , training loss: 4.427330\n",
      "[INFO] Epoch: 14 , batch: 484 , training loss: 4.200761\n",
      "[INFO] Epoch: 14 , batch: 485 , training loss: 4.325254\n",
      "[INFO] Epoch: 14 , batch: 486 , training loss: 4.215709\n",
      "[INFO] Epoch: 14 , batch: 487 , training loss: 4.208628\n",
      "[INFO] Epoch: 14 , batch: 488 , training loss: 4.378531\n",
      "[INFO] Epoch: 14 , batch: 489 , training loss: 4.292178\n",
      "[INFO] Epoch: 14 , batch: 490 , training loss: 4.359001\n",
      "[INFO] Epoch: 14 , batch: 491 , training loss: 4.298703\n",
      "[INFO] Epoch: 14 , batch: 492 , training loss: 4.235604\n",
      "[INFO] Epoch: 14 , batch: 493 , training loss: 4.413097\n",
      "[INFO] Epoch: 14 , batch: 494 , training loss: 4.309132\n",
      "[INFO] Epoch: 14 , batch: 495 , training loss: 4.453341\n",
      "[INFO] Epoch: 14 , batch: 496 , training loss: 4.346913\n",
      "[INFO] Epoch: 14 , batch: 497 , training loss: 4.366633\n",
      "[INFO] Epoch: 14 , batch: 498 , training loss: 4.381125\n",
      "[INFO] Epoch: 14 , batch: 499 , training loss: 4.436168\n",
      "[INFO] Epoch: 14 , batch: 500 , training loss: 4.590198\n",
      "[INFO] Epoch: 14 , batch: 501 , training loss: 4.948102\n",
      "[INFO] Epoch: 14 , batch: 502 , training loss: 5.040565\n",
      "[INFO] Epoch: 14 , batch: 503 , training loss: 4.760141\n",
      "[INFO] Epoch: 14 , batch: 504 , training loss: 4.862689\n",
      "[INFO] Epoch: 14 , batch: 505 , training loss: 4.823783\n",
      "[INFO] Epoch: 14 , batch: 506 , training loss: 4.763432\n",
      "[INFO] Epoch: 14 , batch: 507 , training loss: 4.828404\n",
      "[INFO] Epoch: 14 , batch: 508 , training loss: 4.728371\n",
      "[INFO] Epoch: 14 , batch: 509 , training loss: 4.502953\n",
      "[INFO] Epoch: 14 , batch: 510 , training loss: 4.603229\n",
      "[INFO] Epoch: 14 , batch: 511 , training loss: 4.517977\n",
      "[INFO] Epoch: 14 , batch: 512 , training loss: 4.568288\n",
      "[INFO] Epoch: 14 , batch: 513 , training loss: 4.837836\n",
      "[INFO] Epoch: 14 , batch: 514 , training loss: 4.496373\n",
      "[INFO] Epoch: 14 , batch: 515 , training loss: 4.734595\n",
      "[INFO] Epoch: 14 , batch: 516 , training loss: 4.519957\n",
      "[INFO] Epoch: 14 , batch: 517 , training loss: 4.502742\n",
      "[INFO] Epoch: 14 , batch: 518 , training loss: 4.459614\n",
      "[INFO] Epoch: 14 , batch: 519 , training loss: 4.313920\n",
      "[INFO] Epoch: 14 , batch: 520 , training loss: 4.554305\n",
      "[INFO] Epoch: 14 , batch: 521 , training loss: 4.538310\n",
      "[INFO] Epoch: 14 , batch: 522 , training loss: 4.626031\n",
      "[INFO] Epoch: 14 , batch: 523 , training loss: 4.510272\n",
      "[INFO] Epoch: 14 , batch: 524 , training loss: 4.805332\n",
      "[INFO] Epoch: 14 , batch: 525 , training loss: 4.676280\n",
      "[INFO] Epoch: 14 , batch: 526 , training loss: 4.456536\n",
      "[INFO] Epoch: 14 , batch: 527 , training loss: 4.505775\n",
      "[INFO] Epoch: 14 , batch: 528 , training loss: 4.537349\n",
      "[INFO] Epoch: 14 , batch: 529 , training loss: 4.496495\n",
      "[INFO] Epoch: 14 , batch: 530 , training loss: 4.345303\n",
      "[INFO] Epoch: 14 , batch: 531 , training loss: 4.498357\n",
      "[INFO] Epoch: 14 , batch: 532 , training loss: 4.400000\n",
      "[INFO] Epoch: 14 , batch: 533 , training loss: 4.544487\n",
      "[INFO] Epoch: 14 , batch: 534 , training loss: 4.522798\n",
      "[INFO] Epoch: 14 , batch: 535 , training loss: 4.544586\n",
      "[INFO] Epoch: 14 , batch: 536 , training loss: 4.394720\n",
      "[INFO] Epoch: 14 , batch: 537 , training loss: 4.342265\n",
      "[INFO] Epoch: 14 , batch: 538 , training loss: 4.437098\n",
      "[INFO] Epoch: 14 , batch: 539 , training loss: 4.577810\n",
      "[INFO] Epoch: 14 , batch: 540 , training loss: 5.132005\n",
      "[INFO] Epoch: 14 , batch: 541 , training loss: 5.000499\n",
      "[INFO] Epoch: 14 , batch: 542 , training loss: 4.842367\n",
      "[INFO] Epoch: 15 , batch: 0 , training loss: 4.031100\n",
      "[INFO] Epoch: 15 , batch: 1 , training loss: 3.813847\n",
      "[INFO] Epoch: 15 , batch: 2 , training loss: 3.924776\n",
      "[INFO] Epoch: 15 , batch: 3 , training loss: 3.746164\n",
      "[INFO] Epoch: 15 , batch: 4 , training loss: 4.126916\n",
      "[INFO] Epoch: 15 , batch: 5 , training loss: 3.775205\n",
      "[INFO] Epoch: 15 , batch: 6 , training loss: 4.078986\n",
      "[INFO] Epoch: 15 , batch: 7 , training loss: 3.966413\n",
      "[INFO] Epoch: 15 , batch: 8 , training loss: 3.602504\n",
      "[INFO] Epoch: 15 , batch: 9 , training loss: 3.852504\n",
      "[INFO] Epoch: 15 , batch: 10 , training loss: 3.780428\n",
      "[INFO] Epoch: 15 , batch: 11 , training loss: 3.738402\n",
      "[INFO] Epoch: 15 , batch: 12 , training loss: 3.709139\n",
      "[INFO] Epoch: 15 , batch: 13 , training loss: 3.720766\n",
      "[INFO] Epoch: 15 , batch: 14 , training loss: 3.546446\n",
      "[INFO] Epoch: 15 , batch: 15 , training loss: 3.767275\n",
      "[INFO] Epoch: 15 , batch: 16 , training loss: 3.643453\n",
      "[INFO] Epoch: 15 , batch: 17 , training loss: 3.773869\n",
      "[INFO] Epoch: 15 , batch: 18 , training loss: 3.731364\n",
      "[INFO] Epoch: 15 , batch: 19 , training loss: 3.491716\n",
      "[INFO] Epoch: 15 , batch: 20 , training loss: 3.450715\n",
      "[INFO] Epoch: 15 , batch: 21 , training loss: 3.635203\n",
      "[INFO] Epoch: 15 , batch: 22 , training loss: 3.527939\n",
      "[INFO] Epoch: 15 , batch: 23 , training loss: 3.722513\n",
      "[INFO] Epoch: 15 , batch: 24 , training loss: 3.614052\n",
      "[INFO] Epoch: 15 , batch: 25 , training loss: 3.718935\n",
      "[INFO] Epoch: 15 , batch: 26 , training loss: 3.593633\n",
      "[INFO] Epoch: 15 , batch: 27 , training loss: 3.564003\n",
      "[INFO] Epoch: 15 , batch: 28 , training loss: 3.757882\n",
      "[INFO] Epoch: 15 , batch: 29 , training loss: 3.569233\n",
      "[INFO] Epoch: 15 , batch: 30 , training loss: 3.558258\n",
      "[INFO] Epoch: 15 , batch: 31 , training loss: 3.691499\n",
      "[INFO] Epoch: 15 , batch: 32 , training loss: 3.673769\n",
      "[INFO] Epoch: 15 , batch: 33 , training loss: 3.740671\n",
      "[INFO] Epoch: 15 , batch: 34 , training loss: 3.690679\n",
      "[INFO] Epoch: 15 , batch: 35 , training loss: 3.644246\n",
      "[INFO] Epoch: 15 , batch: 36 , training loss: 3.741140\n",
      "[INFO] Epoch: 15 , batch: 37 , training loss: 3.603914\n",
      "[INFO] Epoch: 15 , batch: 38 , training loss: 3.704463\n",
      "[INFO] Epoch: 15 , batch: 39 , training loss: 3.490025\n",
      "[INFO] Epoch: 15 , batch: 40 , training loss: 3.680425\n",
      "[INFO] Epoch: 15 , batch: 41 , training loss: 3.717813\n",
      "[INFO] Epoch: 15 , batch: 42 , training loss: 4.173057\n",
      "[INFO] Epoch: 15 , batch: 43 , training loss: 3.913437\n",
      "[INFO] Epoch: 15 , batch: 44 , training loss: 4.217475\n",
      "[INFO] Epoch: 15 , batch: 45 , training loss: 4.213119\n",
      "[INFO] Epoch: 15 , batch: 46 , training loss: 4.213978\n",
      "[INFO] Epoch: 15 , batch: 47 , training loss: 3.847620\n",
      "[INFO] Epoch: 15 , batch: 48 , training loss: 3.766605\n",
      "[INFO] Epoch: 15 , batch: 49 , training loss: 3.994853\n",
      "[INFO] Epoch: 15 , batch: 50 , training loss: 3.758778\n",
      "[INFO] Epoch: 15 , batch: 51 , training loss: 3.928860\n",
      "[INFO] Epoch: 15 , batch: 52 , training loss: 3.786427\n",
      "[INFO] Epoch: 15 , batch: 53 , training loss: 3.899318\n",
      "[INFO] Epoch: 15 , batch: 54 , training loss: 3.896630\n",
      "[INFO] Epoch: 15 , batch: 55 , training loss: 3.973733\n",
      "[INFO] Epoch: 15 , batch: 56 , training loss: 3.820995\n",
      "[INFO] Epoch: 15 , batch: 57 , training loss: 3.758882\n",
      "[INFO] Epoch: 15 , batch: 58 , training loss: 3.775659\n",
      "[INFO] Epoch: 15 , batch: 59 , training loss: 3.875226\n",
      "[INFO] Epoch: 15 , batch: 60 , training loss: 3.809580\n",
      "[INFO] Epoch: 15 , batch: 61 , training loss: 3.886698\n",
      "[INFO] Epoch: 15 , batch: 62 , training loss: 3.752786\n",
      "[INFO] Epoch: 15 , batch: 63 , training loss: 3.931856\n",
      "[INFO] Epoch: 15 , batch: 64 , training loss: 4.182328\n",
      "[INFO] Epoch: 15 , batch: 65 , training loss: 3.818041\n",
      "[INFO] Epoch: 15 , batch: 66 , training loss: 3.707258\n",
      "[INFO] Epoch: 15 , batch: 67 , training loss: 3.695340\n",
      "[INFO] Epoch: 15 , batch: 68 , training loss: 3.963863\n",
      "[INFO] Epoch: 15 , batch: 69 , training loss: 3.823467\n",
      "[INFO] Epoch: 15 , batch: 70 , training loss: 4.039533\n",
      "[INFO] Epoch: 15 , batch: 71 , training loss: 3.893912\n",
      "[INFO] Epoch: 15 , batch: 72 , training loss: 3.953532\n",
      "[INFO] Epoch: 15 , batch: 73 , training loss: 3.878619\n",
      "[INFO] Epoch: 15 , batch: 74 , training loss: 3.996183\n",
      "[INFO] Epoch: 15 , batch: 75 , training loss: 3.798961\n",
      "[INFO] Epoch: 15 , batch: 76 , training loss: 3.977040\n",
      "[INFO] Epoch: 15 , batch: 77 , training loss: 3.897527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 15 , batch: 78 , training loss: 3.982911\n",
      "[INFO] Epoch: 15 , batch: 79 , training loss: 3.810980\n",
      "[INFO] Epoch: 15 , batch: 80 , training loss: 4.032346\n",
      "[INFO] Epoch: 15 , batch: 81 , training loss: 3.975892\n",
      "[INFO] Epoch: 15 , batch: 82 , training loss: 3.936960\n",
      "[INFO] Epoch: 15 , batch: 83 , training loss: 4.060012\n",
      "[INFO] Epoch: 15 , batch: 84 , training loss: 3.978759\n",
      "[INFO] Epoch: 15 , batch: 85 , training loss: 4.093199\n",
      "[INFO] Epoch: 15 , batch: 86 , training loss: 4.006842\n",
      "[INFO] Epoch: 15 , batch: 87 , training loss: 3.986681\n",
      "[INFO] Epoch: 15 , batch: 88 , training loss: 4.127181\n",
      "[INFO] Epoch: 15 , batch: 89 , training loss: 3.909037\n",
      "[INFO] Epoch: 15 , batch: 90 , training loss: 4.029957\n",
      "[INFO] Epoch: 15 , batch: 91 , training loss: 3.938264\n",
      "[INFO] Epoch: 15 , batch: 92 , training loss: 3.959250\n",
      "[INFO] Epoch: 15 , batch: 93 , training loss: 4.041650\n",
      "[INFO] Epoch: 15 , batch: 94 , training loss: 4.198277\n",
      "[INFO] Epoch: 15 , batch: 95 , training loss: 3.960537\n",
      "[INFO] Epoch: 15 , batch: 96 , training loss: 3.937627\n",
      "[INFO] Epoch: 15 , batch: 97 , training loss: 3.914498\n",
      "[INFO] Epoch: 15 , batch: 98 , training loss: 3.830698\n",
      "[INFO] Epoch: 15 , batch: 99 , training loss: 3.967297\n",
      "[INFO] Epoch: 15 , batch: 100 , training loss: 3.806849\n",
      "[INFO] Epoch: 15 , batch: 101 , training loss: 3.874273\n",
      "[INFO] Epoch: 15 , batch: 102 , training loss: 4.025405\n",
      "[INFO] Epoch: 15 , batch: 103 , training loss: 3.789530\n",
      "[INFO] Epoch: 15 , batch: 104 , training loss: 3.766317\n",
      "[INFO] Epoch: 15 , batch: 105 , training loss: 4.019594\n",
      "[INFO] Epoch: 15 , batch: 106 , training loss: 4.040722\n",
      "[INFO] Epoch: 15 , batch: 107 , training loss: 3.884167\n",
      "[INFO] Epoch: 15 , batch: 108 , training loss: 3.847676\n",
      "[INFO] Epoch: 15 , batch: 109 , training loss: 3.721711\n",
      "[INFO] Epoch: 15 , batch: 110 , training loss: 3.940691\n",
      "[INFO] Epoch: 15 , batch: 111 , training loss: 4.002209\n",
      "[INFO] Epoch: 15 , batch: 112 , training loss: 3.954663\n",
      "[INFO] Epoch: 15 , batch: 113 , training loss: 3.913326\n",
      "[INFO] Epoch: 15 , batch: 114 , training loss: 3.943352\n",
      "[INFO] Epoch: 15 , batch: 115 , training loss: 3.906955\n",
      "[INFO] Epoch: 15 , batch: 116 , training loss: 3.819252\n",
      "[INFO] Epoch: 15 , batch: 117 , training loss: 4.067926\n",
      "[INFO] Epoch: 15 , batch: 118 , training loss: 4.017984\n",
      "[INFO] Epoch: 15 , batch: 119 , training loss: 4.187335\n",
      "[INFO] Epoch: 15 , batch: 120 , training loss: 4.149696\n",
      "[INFO] Epoch: 15 , batch: 121 , training loss: 3.998231\n",
      "[INFO] Epoch: 15 , batch: 122 , training loss: 3.896542\n",
      "[INFO] Epoch: 15 , batch: 123 , training loss: 3.927738\n",
      "[INFO] Epoch: 15 , batch: 124 , training loss: 4.057896\n",
      "[INFO] Epoch: 15 , batch: 125 , training loss: 3.793055\n",
      "[INFO] Epoch: 15 , batch: 126 , training loss: 3.826733\n",
      "[INFO] Epoch: 15 , batch: 127 , training loss: 3.879486\n",
      "[INFO] Epoch: 15 , batch: 128 , training loss: 4.006704\n",
      "[INFO] Epoch: 15 , batch: 129 , training loss: 3.939971\n",
      "[INFO] Epoch: 15 , batch: 130 , training loss: 3.920865\n",
      "[INFO] Epoch: 15 , batch: 131 , training loss: 3.927718\n",
      "[INFO] Epoch: 15 , batch: 132 , training loss: 3.952154\n",
      "[INFO] Epoch: 15 , batch: 133 , training loss: 3.934707\n",
      "[INFO] Epoch: 15 , batch: 134 , training loss: 3.684803\n",
      "[INFO] Epoch: 15 , batch: 135 , training loss: 3.751045\n",
      "[INFO] Epoch: 15 , batch: 136 , training loss: 4.039475\n",
      "[INFO] Epoch: 15 , batch: 137 , training loss: 3.970443\n",
      "[INFO] Epoch: 15 , batch: 138 , training loss: 4.020433\n",
      "[INFO] Epoch: 15 , batch: 139 , training loss: 4.622566\n",
      "[INFO] Epoch: 15 , batch: 140 , training loss: 4.395195\n",
      "[INFO] Epoch: 15 , batch: 141 , training loss: 4.159895\n",
      "[INFO] Epoch: 15 , batch: 142 , training loss: 3.853399\n",
      "[INFO] Epoch: 15 , batch: 143 , training loss: 3.993792\n",
      "[INFO] Epoch: 15 , batch: 144 , training loss: 3.849263\n",
      "[INFO] Epoch: 15 , batch: 145 , training loss: 3.919948\n",
      "[INFO] Epoch: 15 , batch: 146 , training loss: 4.130739\n",
      "[INFO] Epoch: 15 , batch: 147 , training loss: 3.783113\n",
      "[INFO] Epoch: 15 , batch: 148 , training loss: 3.768642\n",
      "[INFO] Epoch: 15 , batch: 149 , training loss: 3.866248\n",
      "[INFO] Epoch: 15 , batch: 150 , training loss: 4.108313\n",
      "[INFO] Epoch: 15 , batch: 151 , training loss: 3.916757\n",
      "[INFO] Epoch: 15 , batch: 152 , training loss: 3.920145\n",
      "[INFO] Epoch: 15 , batch: 153 , training loss: 3.984656\n",
      "[INFO] Epoch: 15 , batch: 154 , training loss: 4.042976\n",
      "[INFO] Epoch: 15 , batch: 155 , training loss: 4.243917\n",
      "[INFO] Epoch: 15 , batch: 156 , training loss: 4.014550\n",
      "[INFO] Epoch: 15 , batch: 157 , training loss: 3.963228\n",
      "[INFO] Epoch: 15 , batch: 158 , training loss: 4.164424\n",
      "[INFO] Epoch: 15 , batch: 159 , training loss: 4.052786\n",
      "[INFO] Epoch: 15 , batch: 160 , training loss: 4.377434\n",
      "[INFO] Epoch: 15 , batch: 161 , training loss: 4.415309\n",
      "[INFO] Epoch: 15 , batch: 162 , training loss: 4.365461\n",
      "[INFO] Epoch: 15 , batch: 163 , training loss: 4.545946\n",
      "[INFO] Epoch: 15 , batch: 164 , training loss: 4.452658\n",
      "[INFO] Epoch: 15 , batch: 165 , training loss: 4.384908\n",
      "[INFO] Epoch: 15 , batch: 166 , training loss: 4.283972\n",
      "[INFO] Epoch: 15 , batch: 167 , training loss: 4.526527\n",
      "[INFO] Epoch: 15 , batch: 168 , training loss: 4.228060\n",
      "[INFO] Epoch: 15 , batch: 169 , training loss: 4.163448\n",
      "[INFO] Epoch: 15 , batch: 170 , training loss: 4.288240\n",
      "[INFO] Epoch: 15 , batch: 171 , training loss: 3.739668\n",
      "[INFO] Epoch: 15 , batch: 172 , training loss: 3.907860\n",
      "[INFO] Epoch: 15 , batch: 173 , training loss: 4.204953\n",
      "[INFO] Epoch: 15 , batch: 174 , training loss: 4.631892\n",
      "[INFO] Epoch: 15 , batch: 175 , training loss: 4.924425\n",
      "[INFO] Epoch: 15 , batch: 176 , training loss: 4.694607\n",
      "[INFO] Epoch: 15 , batch: 177 , training loss: 4.265975\n",
      "[INFO] Epoch: 15 , batch: 178 , training loss: 4.211072\n",
      "[INFO] Epoch: 15 , batch: 179 , training loss: 4.261578\n",
      "[INFO] Epoch: 15 , batch: 180 , training loss: 4.184890\n",
      "[INFO] Epoch: 15 , batch: 181 , training loss: 4.469209\n",
      "[INFO] Epoch: 15 , batch: 182 , training loss: 4.401969\n",
      "[INFO] Epoch: 15 , batch: 183 , training loss: 4.366625\n",
      "[INFO] Epoch: 15 , batch: 184 , training loss: 4.263587\n",
      "[INFO] Epoch: 15 , batch: 185 , training loss: 4.202394\n",
      "[INFO] Epoch: 15 , batch: 186 , training loss: 4.339566\n",
      "[INFO] Epoch: 15 , batch: 187 , training loss: 4.478406\n",
      "[INFO] Epoch: 15 , batch: 188 , training loss: 4.462930\n",
      "[INFO] Epoch: 15 , batch: 189 , training loss: 4.332493\n",
      "[INFO] Epoch: 15 , batch: 190 , training loss: 4.365107\n",
      "[INFO] Epoch: 15 , batch: 191 , training loss: 4.517628\n",
      "[INFO] Epoch: 15 , batch: 192 , training loss: 4.309883\n",
      "[INFO] Epoch: 15 , batch: 193 , training loss: 4.415503\n",
      "[INFO] Epoch: 15 , batch: 194 , training loss: 4.353667\n",
      "[INFO] Epoch: 15 , batch: 195 , training loss: 4.296116\n",
      "[INFO] Epoch: 15 , batch: 196 , training loss: 4.150832\n",
      "[INFO] Epoch: 15 , batch: 197 , training loss: 4.256497\n",
      "[INFO] Epoch: 15 , batch: 198 , training loss: 4.161554\n",
      "[INFO] Epoch: 15 , batch: 199 , training loss: 4.285592\n",
      "[INFO] Epoch: 15 , batch: 200 , training loss: 4.185781\n",
      "[INFO] Epoch: 15 , batch: 201 , training loss: 4.096834\n",
      "[INFO] Epoch: 15 , batch: 202 , training loss: 4.090213\n",
      "[INFO] Epoch: 15 , batch: 203 , training loss: 4.193450\n",
      "[INFO] Epoch: 15 , batch: 204 , training loss: 4.296623\n",
      "[INFO] Epoch: 15 , batch: 205 , training loss: 3.894047\n",
      "[INFO] Epoch: 15 , batch: 206 , training loss: 3.819531\n",
      "[INFO] Epoch: 15 , batch: 207 , training loss: 3.821419\n",
      "[INFO] Epoch: 15 , batch: 208 , training loss: 4.151444\n",
      "[INFO] Epoch: 15 , batch: 209 , training loss: 4.092882\n",
      "[INFO] Epoch: 15 , batch: 210 , training loss: 4.133244\n",
      "[INFO] Epoch: 15 , batch: 211 , training loss: 4.121541\n",
      "[INFO] Epoch: 15 , batch: 212 , training loss: 4.251422\n",
      "[INFO] Epoch: 15 , batch: 213 , training loss: 4.187141\n",
      "[INFO] Epoch: 15 , batch: 214 , training loss: 4.260443\n",
      "[INFO] Epoch: 15 , batch: 215 , training loss: 4.473877\n",
      "[INFO] Epoch: 15 , batch: 216 , training loss: 4.183134\n",
      "[INFO] Epoch: 15 , batch: 217 , training loss: 4.121176\n",
      "[INFO] Epoch: 15 , batch: 218 , training loss: 4.126711\n",
      "[INFO] Epoch: 15 , batch: 219 , training loss: 4.221559\n",
      "[INFO] Epoch: 15 , batch: 220 , training loss: 4.042374\n",
      "[INFO] Epoch: 15 , batch: 221 , training loss: 4.060213\n",
      "[INFO] Epoch: 15 , batch: 222 , training loss: 4.189896\n",
      "[INFO] Epoch: 15 , batch: 223 , training loss: 4.297365\n",
      "[INFO] Epoch: 15 , batch: 224 , training loss: 4.334331\n",
      "[INFO] Epoch: 15 , batch: 225 , training loss: 4.209414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 15 , batch: 226 , training loss: 4.346987\n",
      "[INFO] Epoch: 15 , batch: 227 , training loss: 4.312497\n",
      "[INFO] Epoch: 15 , batch: 228 , training loss: 4.355155\n",
      "[INFO] Epoch: 15 , batch: 229 , training loss: 4.207404\n",
      "[INFO] Epoch: 15 , batch: 230 , training loss: 4.085590\n",
      "[INFO] Epoch: 15 , batch: 231 , training loss: 3.919000\n",
      "[INFO] Epoch: 15 , batch: 232 , training loss: 4.066038\n",
      "[INFO] Epoch: 15 , batch: 233 , training loss: 4.096734\n",
      "[INFO] Epoch: 15 , batch: 234 , training loss: 3.784431\n",
      "[INFO] Epoch: 15 , batch: 235 , training loss: 3.905282\n",
      "[INFO] Epoch: 15 , batch: 236 , training loss: 4.014460\n",
      "[INFO] Epoch: 15 , batch: 237 , training loss: 4.234213\n",
      "[INFO] Epoch: 15 , batch: 238 , training loss: 3.988793\n",
      "[INFO] Epoch: 15 , batch: 239 , training loss: 4.036092\n",
      "[INFO] Epoch: 15 , batch: 240 , training loss: 4.087033\n",
      "[INFO] Epoch: 15 , batch: 241 , training loss: 3.882962\n",
      "[INFO] Epoch: 15 , batch: 242 , training loss: 3.890992\n",
      "[INFO] Epoch: 15 , batch: 243 , training loss: 4.185855\n",
      "[INFO] Epoch: 15 , batch: 244 , training loss: 4.160666\n",
      "[INFO] Epoch: 15 , batch: 245 , training loss: 4.130914\n",
      "[INFO] Epoch: 15 , batch: 246 , training loss: 3.822723\n",
      "[INFO] Epoch: 15 , batch: 247 , training loss: 3.966250\n",
      "[INFO] Epoch: 15 , batch: 248 , training loss: 4.059011\n",
      "[INFO] Epoch: 15 , batch: 249 , training loss: 4.039842\n",
      "[INFO] Epoch: 15 , batch: 250 , training loss: 3.822713\n",
      "[INFO] Epoch: 15 , batch: 251 , training loss: 4.304811\n",
      "[INFO] Epoch: 15 , batch: 252 , training loss: 3.993137\n",
      "[INFO] Epoch: 15 , batch: 253 , training loss: 3.916390\n",
      "[INFO] Epoch: 15 , batch: 254 , training loss: 4.209154\n",
      "[INFO] Epoch: 15 , batch: 255 , training loss: 4.142978\n",
      "[INFO] Epoch: 15 , batch: 256 , training loss: 4.131586\n",
      "[INFO] Epoch: 15 , batch: 257 , training loss: 4.308076\n",
      "[INFO] Epoch: 15 , batch: 258 , training loss: 4.340563\n",
      "[INFO] Epoch: 15 , batch: 259 , training loss: 4.381781\n",
      "[INFO] Epoch: 15 , batch: 260 , training loss: 4.139249\n",
      "[INFO] Epoch: 15 , batch: 261 , training loss: 4.303987\n",
      "[INFO] Epoch: 15 , batch: 262 , training loss: 4.475501\n",
      "[INFO] Epoch: 15 , batch: 263 , training loss: 4.613536\n",
      "[INFO] Epoch: 15 , batch: 264 , training loss: 3.984796\n",
      "[INFO] Epoch: 15 , batch: 265 , training loss: 4.085691\n",
      "[INFO] Epoch: 15 , batch: 266 , training loss: 4.543985\n",
      "[INFO] Epoch: 15 , batch: 267 , training loss: 4.244091\n",
      "[INFO] Epoch: 15 , batch: 268 , training loss: 4.174491\n",
      "[INFO] Epoch: 15 , batch: 269 , training loss: 4.166389\n",
      "[INFO] Epoch: 15 , batch: 270 , training loss: 4.180620\n",
      "[INFO] Epoch: 15 , batch: 271 , training loss: 4.225010\n",
      "[INFO] Epoch: 15 , batch: 272 , training loss: 4.190996\n",
      "[INFO] Epoch: 15 , batch: 273 , training loss: 4.201521\n",
      "[INFO] Epoch: 15 , batch: 274 , training loss: 4.292514\n",
      "[INFO] Epoch: 15 , batch: 275 , training loss: 4.155536\n",
      "[INFO] Epoch: 15 , batch: 276 , training loss: 4.226718\n",
      "[INFO] Epoch: 15 , batch: 277 , training loss: 4.388197\n",
      "[INFO] Epoch: 15 , batch: 278 , training loss: 4.024515\n",
      "[INFO] Epoch: 15 , batch: 279 , training loss: 4.068934\n",
      "[INFO] Epoch: 15 , batch: 280 , training loss: 3.999153\n",
      "[INFO] Epoch: 15 , batch: 281 , training loss: 4.152822\n",
      "[INFO] Epoch: 15 , batch: 282 , training loss: 4.052216\n",
      "[INFO] Epoch: 15 , batch: 283 , training loss: 4.085917\n",
      "[INFO] Epoch: 15 , batch: 284 , training loss: 4.113313\n",
      "[INFO] Epoch: 15 , batch: 285 , training loss: 4.076786\n",
      "[INFO] Epoch: 15 , batch: 286 , training loss: 4.057290\n",
      "[INFO] Epoch: 15 , batch: 287 , training loss: 3.996825\n",
      "[INFO] Epoch: 15 , batch: 288 , training loss: 3.967305\n",
      "[INFO] Epoch: 15 , batch: 289 , training loss: 4.039114\n",
      "[INFO] Epoch: 15 , batch: 290 , training loss: 3.815283\n",
      "[INFO] Epoch: 15 , batch: 291 , training loss: 3.813197\n",
      "[INFO] Epoch: 15 , batch: 292 , training loss: 3.931473\n",
      "[INFO] Epoch: 15 , batch: 293 , training loss: 3.845080\n",
      "[INFO] Epoch: 15 , batch: 294 , training loss: 4.517440\n",
      "[INFO] Epoch: 15 , batch: 295 , training loss: 4.276190\n",
      "[INFO] Epoch: 15 , batch: 296 , training loss: 4.213218\n",
      "[INFO] Epoch: 15 , batch: 297 , training loss: 4.156111\n",
      "[INFO] Epoch: 15 , batch: 298 , training loss: 4.005944\n",
      "[INFO] Epoch: 15 , batch: 299 , training loss: 4.021821\n",
      "[INFO] Epoch: 15 , batch: 300 , training loss: 4.013354\n",
      "[INFO] Epoch: 15 , batch: 301 , training loss: 3.944231\n",
      "[INFO] Epoch: 15 , batch: 302 , training loss: 4.102983\n",
      "[INFO] Epoch: 15 , batch: 303 , training loss: 4.134138\n",
      "[INFO] Epoch: 15 , batch: 304 , training loss: 4.294065\n",
      "[INFO] Epoch: 15 , batch: 305 , training loss: 4.074782\n",
      "[INFO] Epoch: 15 , batch: 306 , training loss: 4.212024\n",
      "[INFO] Epoch: 15 , batch: 307 , training loss: 4.206538\n",
      "[INFO] Epoch: 15 , batch: 308 , training loss: 4.044253\n",
      "[INFO] Epoch: 15 , batch: 309 , training loss: 4.042662\n",
      "[INFO] Epoch: 15 , batch: 310 , training loss: 3.940254\n",
      "[INFO] Epoch: 15 , batch: 311 , training loss: 3.934946\n",
      "[INFO] Epoch: 15 , batch: 312 , training loss: 3.845548\n",
      "[INFO] Epoch: 15 , batch: 313 , training loss: 3.945697\n",
      "[INFO] Epoch: 15 , batch: 314 , training loss: 4.030176\n",
      "[INFO] Epoch: 15 , batch: 315 , training loss: 4.102672\n",
      "[INFO] Epoch: 15 , batch: 316 , training loss: 4.371385\n",
      "[INFO] Epoch: 15 , batch: 317 , training loss: 4.772982\n",
      "[INFO] Epoch: 15 , batch: 318 , training loss: 4.925771\n",
      "[INFO] Epoch: 15 , batch: 319 , training loss: 4.571939\n",
      "[INFO] Epoch: 15 , batch: 320 , training loss: 4.079562\n",
      "[INFO] Epoch: 15 , batch: 321 , training loss: 3.893802\n",
      "[INFO] Epoch: 15 , batch: 322 , training loss: 4.010735\n",
      "[INFO] Epoch: 15 , batch: 323 , training loss: 4.039020\n",
      "[INFO] Epoch: 15 , batch: 324 , training loss: 4.020728\n",
      "[INFO] Epoch: 15 , batch: 325 , training loss: 4.152525\n",
      "[INFO] Epoch: 15 , batch: 326 , training loss: 4.191569\n",
      "[INFO] Epoch: 15 , batch: 327 , training loss: 4.118702\n",
      "[INFO] Epoch: 15 , batch: 328 , training loss: 4.122070\n",
      "[INFO] Epoch: 15 , batch: 329 , training loss: 4.022950\n",
      "[INFO] Epoch: 15 , batch: 330 , training loss: 4.016553\n",
      "[INFO] Epoch: 15 , batch: 331 , training loss: 4.180964\n",
      "[INFO] Epoch: 15 , batch: 332 , training loss: 4.022147\n",
      "[INFO] Epoch: 15 , batch: 333 , training loss: 3.991787\n",
      "[INFO] Epoch: 15 , batch: 334 , training loss: 4.007093\n",
      "[INFO] Epoch: 15 , batch: 335 , training loss: 4.146812\n",
      "[INFO] Epoch: 15 , batch: 336 , training loss: 4.133393\n",
      "[INFO] Epoch: 15 , batch: 337 , training loss: 4.210717\n",
      "[INFO] Epoch: 15 , batch: 338 , training loss: 4.393596\n",
      "[INFO] Epoch: 15 , batch: 339 , training loss: 4.206463\n",
      "[INFO] Epoch: 15 , batch: 340 , training loss: 4.433226\n",
      "[INFO] Epoch: 15 , batch: 341 , training loss: 4.146738\n",
      "[INFO] Epoch: 15 , batch: 342 , training loss: 3.937115\n",
      "[INFO] Epoch: 15 , batch: 343 , training loss: 4.016612\n",
      "[INFO] Epoch: 15 , batch: 344 , training loss: 3.866535\n",
      "[INFO] Epoch: 15 , batch: 345 , training loss: 4.026106\n",
      "[INFO] Epoch: 15 , batch: 346 , training loss: 4.046322\n",
      "[INFO] Epoch: 15 , batch: 347 , training loss: 3.973463\n",
      "[INFO] Epoch: 15 , batch: 348 , training loss: 4.109183\n",
      "[INFO] Epoch: 15 , batch: 349 , training loss: 4.193960\n",
      "[INFO] Epoch: 15 , batch: 350 , training loss: 4.020864\n",
      "[INFO] Epoch: 15 , batch: 351 , training loss: 4.088053\n",
      "[INFO] Epoch: 15 , batch: 352 , training loss: 4.101133\n",
      "[INFO] Epoch: 15 , batch: 353 , training loss: 4.080708\n",
      "[INFO] Epoch: 15 , batch: 354 , training loss: 4.185178\n",
      "[INFO] Epoch: 15 , batch: 355 , training loss: 4.169608\n",
      "[INFO] Epoch: 15 , batch: 356 , training loss: 4.055405\n",
      "[INFO] Epoch: 15 , batch: 357 , training loss: 4.148901\n",
      "[INFO] Epoch: 15 , batch: 358 , training loss: 4.054831\n",
      "[INFO] Epoch: 15 , batch: 359 , training loss: 4.040171\n",
      "[INFO] Epoch: 15 , batch: 360 , training loss: 4.114858\n",
      "[INFO] Epoch: 15 , batch: 361 , training loss: 4.099352\n",
      "[INFO] Epoch: 15 , batch: 362 , training loss: 4.205639\n",
      "[INFO] Epoch: 15 , batch: 363 , training loss: 4.085253\n",
      "[INFO] Epoch: 15 , batch: 364 , training loss: 4.143554\n",
      "[INFO] Epoch: 15 , batch: 365 , training loss: 4.051939\n",
      "[INFO] Epoch: 15 , batch: 366 , training loss: 4.145344\n",
      "[INFO] Epoch: 15 , batch: 367 , training loss: 4.217023\n",
      "[INFO] Epoch: 15 , batch: 368 , training loss: 4.658828\n",
      "[INFO] Epoch: 15 , batch: 369 , training loss: 4.339911\n",
      "[INFO] Epoch: 15 , batch: 370 , training loss: 4.070561\n",
      "[INFO] Epoch: 15 , batch: 371 , training loss: 4.513517\n",
      "[INFO] Epoch: 15 , batch: 372 , training loss: 4.800161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 15 , batch: 373 , training loss: 4.825866\n",
      "[INFO] Epoch: 15 , batch: 374 , training loss: 4.917651\n",
      "[INFO] Epoch: 15 , batch: 375 , training loss: 4.904196\n",
      "[INFO] Epoch: 15 , batch: 376 , training loss: 4.824498\n",
      "[INFO] Epoch: 15 , batch: 377 , training loss: 4.549853\n",
      "[INFO] Epoch: 15 , batch: 378 , training loss: 4.622560\n",
      "[INFO] Epoch: 15 , batch: 379 , training loss: 4.625360\n",
      "[INFO] Epoch: 15 , batch: 380 , training loss: 4.727179\n",
      "[INFO] Epoch: 15 , batch: 381 , training loss: 4.478483\n",
      "[INFO] Epoch: 15 , batch: 382 , training loss: 4.778922\n",
      "[INFO] Epoch: 15 , batch: 383 , training loss: 4.786074\n",
      "[INFO] Epoch: 15 , batch: 384 , training loss: 4.816297\n",
      "[INFO] Epoch: 15 , batch: 385 , training loss: 4.494547\n",
      "[INFO] Epoch: 15 , batch: 386 , training loss: 4.726507\n",
      "[INFO] Epoch: 15 , batch: 387 , training loss: 4.670157\n",
      "[INFO] Epoch: 15 , batch: 388 , training loss: 4.452979\n",
      "[INFO] Epoch: 15 , batch: 389 , training loss: 4.270765\n",
      "[INFO] Epoch: 15 , batch: 390 , training loss: 4.288023\n",
      "[INFO] Epoch: 15 , batch: 391 , training loss: 4.303090\n",
      "[INFO] Epoch: 15 , batch: 392 , training loss: 4.678918\n",
      "[INFO] Epoch: 15 , batch: 393 , training loss: 4.591030\n",
      "[INFO] Epoch: 15 , batch: 394 , training loss: 4.653994\n",
      "[INFO] Epoch: 15 , batch: 395 , training loss: 4.462893\n",
      "[INFO] Epoch: 15 , batch: 396 , training loss: 4.273149\n",
      "[INFO] Epoch: 15 , batch: 397 , training loss: 4.436277\n",
      "[INFO] Epoch: 15 , batch: 398 , training loss: 4.293392\n",
      "[INFO] Epoch: 15 , batch: 399 , training loss: 4.354944\n",
      "[INFO] Epoch: 15 , batch: 400 , training loss: 4.341189\n",
      "[INFO] Epoch: 15 , batch: 401 , training loss: 4.794771\n",
      "[INFO] Epoch: 15 , batch: 402 , training loss: 4.484016\n",
      "[INFO] Epoch: 15 , batch: 403 , training loss: 4.320440\n",
      "[INFO] Epoch: 15 , batch: 404 , training loss: 4.472313\n",
      "[INFO] Epoch: 15 , batch: 405 , training loss: 4.542631\n",
      "[INFO] Epoch: 15 , batch: 406 , training loss: 4.452214\n",
      "[INFO] Epoch: 15 , batch: 407 , training loss: 4.497808\n",
      "[INFO] Epoch: 15 , batch: 408 , training loss: 4.429761\n",
      "[INFO] Epoch: 15 , batch: 409 , training loss: 4.465994\n",
      "[INFO] Epoch: 15 , batch: 410 , training loss: 4.533790\n",
      "[INFO] Epoch: 15 , batch: 411 , training loss: 4.687003\n",
      "[INFO] Epoch: 15 , batch: 412 , training loss: 4.527440\n",
      "[INFO] Epoch: 15 , batch: 413 , training loss: 4.384762\n",
      "[INFO] Epoch: 15 , batch: 414 , training loss: 4.427803\n",
      "[INFO] Epoch: 15 , batch: 415 , training loss: 4.463050\n",
      "[INFO] Epoch: 15 , batch: 416 , training loss: 4.542653\n",
      "[INFO] Epoch: 15 , batch: 417 , training loss: 4.446500\n",
      "[INFO] Epoch: 15 , batch: 418 , training loss: 4.498665\n",
      "[INFO] Epoch: 15 , batch: 419 , training loss: 4.430031\n",
      "[INFO] Epoch: 15 , batch: 420 , training loss: 4.415078\n",
      "[INFO] Epoch: 15 , batch: 421 , training loss: 4.417457\n",
      "[INFO] Epoch: 15 , batch: 422 , training loss: 4.267622\n",
      "[INFO] Epoch: 15 , batch: 423 , training loss: 4.483786\n",
      "[INFO] Epoch: 15 , batch: 424 , training loss: 4.660027\n",
      "[INFO] Epoch: 15 , batch: 425 , training loss: 4.525182\n",
      "[INFO] Epoch: 15 , batch: 426 , training loss: 4.233807\n",
      "[INFO] Epoch: 15 , batch: 427 , training loss: 4.496333\n",
      "[INFO] Epoch: 15 , batch: 428 , training loss: 4.361048\n",
      "[INFO] Epoch: 15 , batch: 429 , training loss: 4.256863\n",
      "[INFO] Epoch: 15 , batch: 430 , training loss: 4.507185\n",
      "[INFO] Epoch: 15 , batch: 431 , training loss: 4.098184\n",
      "[INFO] Epoch: 15 , batch: 432 , training loss: 4.144502\n",
      "[INFO] Epoch: 15 , batch: 433 , training loss: 4.178186\n",
      "[INFO] Epoch: 15 , batch: 434 , training loss: 4.062387\n",
      "[INFO] Epoch: 15 , batch: 435 , training loss: 4.423175\n",
      "[INFO] Epoch: 15 , batch: 436 , training loss: 4.480307\n",
      "[INFO] Epoch: 15 , batch: 437 , training loss: 4.266573\n",
      "[INFO] Epoch: 15 , batch: 438 , training loss: 4.101923\n",
      "[INFO] Epoch: 15 , batch: 439 , training loss: 4.334234\n",
      "[INFO] Epoch: 15 , batch: 440 , training loss: 4.478013\n",
      "[INFO] Epoch: 15 , batch: 441 , training loss: 4.547405\n",
      "[INFO] Epoch: 15 , batch: 442 , training loss: 4.305291\n",
      "[INFO] Epoch: 15 , batch: 443 , training loss: 4.509027\n",
      "[INFO] Epoch: 15 , batch: 444 , training loss: 4.106037\n",
      "[INFO] Epoch: 15 , batch: 445 , training loss: 4.041252\n",
      "[INFO] Epoch: 15 , batch: 446 , training loss: 3.947710\n",
      "[INFO] Epoch: 15 , batch: 447 , training loss: 4.137283\n",
      "[INFO] Epoch: 15 , batch: 448 , training loss: 4.286069\n",
      "[INFO] Epoch: 15 , batch: 449 , training loss: 4.660355\n",
      "[INFO] Epoch: 15 , batch: 450 , training loss: 4.733258\n",
      "[INFO] Epoch: 15 , batch: 451 , training loss: 4.618252\n",
      "[INFO] Epoch: 15 , batch: 452 , training loss: 4.419153\n",
      "[INFO] Epoch: 15 , batch: 453 , training loss: 4.197062\n",
      "[INFO] Epoch: 15 , batch: 454 , training loss: 4.349014\n",
      "[INFO] Epoch: 15 , batch: 455 , training loss: 4.388928\n",
      "[INFO] Epoch: 15 , batch: 456 , training loss: 4.388798\n",
      "[INFO] Epoch: 15 , batch: 457 , training loss: 4.473891\n",
      "[INFO] Epoch: 15 , batch: 458 , training loss: 4.210538\n",
      "[INFO] Epoch: 15 , batch: 459 , training loss: 4.181224\n",
      "[INFO] Epoch: 15 , batch: 460 , training loss: 4.302851\n",
      "[INFO] Epoch: 15 , batch: 461 , training loss: 4.258198\n",
      "[INFO] Epoch: 15 , batch: 462 , training loss: 4.316660\n",
      "[INFO] Epoch: 15 , batch: 463 , training loss: 4.237899\n",
      "[INFO] Epoch: 15 , batch: 464 , training loss: 4.402965\n",
      "[INFO] Epoch: 15 , batch: 465 , training loss: 4.346829\n",
      "[INFO] Epoch: 15 , batch: 466 , training loss: 4.449295\n",
      "[INFO] Epoch: 15 , batch: 467 , training loss: 4.423090\n",
      "[INFO] Epoch: 15 , batch: 468 , training loss: 4.380639\n",
      "[INFO] Epoch: 15 , batch: 469 , training loss: 4.425855\n",
      "[INFO] Epoch: 15 , batch: 470 , training loss: 4.211327\n",
      "[INFO] Epoch: 15 , batch: 471 , training loss: 4.334501\n",
      "[INFO] Epoch: 15 , batch: 472 , training loss: 4.383097\n",
      "[INFO] Epoch: 15 , batch: 473 , training loss: 4.313614\n",
      "[INFO] Epoch: 15 , batch: 474 , training loss: 4.089186\n",
      "[INFO] Epoch: 15 , batch: 475 , training loss: 3.964371\n",
      "[INFO] Epoch: 15 , batch: 476 , training loss: 4.359789\n",
      "[INFO] Epoch: 15 , batch: 477 , training loss: 4.458624\n",
      "[INFO] Epoch: 15 , batch: 478 , training loss: 4.480552\n",
      "[INFO] Epoch: 15 , batch: 479 , training loss: 4.457350\n",
      "[INFO] Epoch: 15 , batch: 480 , training loss: 4.576201\n",
      "[INFO] Epoch: 15 , batch: 481 , training loss: 4.450772\n",
      "[INFO] Epoch: 15 , batch: 482 , training loss: 4.564476\n",
      "[INFO] Epoch: 15 , batch: 483 , training loss: 4.412289\n",
      "[INFO] Epoch: 15 , batch: 484 , training loss: 4.199101\n",
      "[INFO] Epoch: 15 , batch: 485 , training loss: 4.315457\n",
      "[INFO] Epoch: 15 , batch: 486 , training loss: 4.207912\n",
      "[INFO] Epoch: 15 , batch: 487 , training loss: 4.211357\n",
      "[INFO] Epoch: 15 , batch: 488 , training loss: 4.371241\n",
      "[INFO] Epoch: 15 , batch: 489 , training loss: 4.273901\n",
      "[INFO] Epoch: 15 , batch: 490 , training loss: 4.347741\n",
      "[INFO] Epoch: 15 , batch: 491 , training loss: 4.269894\n",
      "[INFO] Epoch: 15 , batch: 492 , training loss: 4.235355\n",
      "[INFO] Epoch: 15 , batch: 493 , training loss: 4.404873\n",
      "[INFO] Epoch: 15 , batch: 494 , training loss: 4.306635\n",
      "[INFO] Epoch: 15 , batch: 495 , training loss: 4.443694\n",
      "[INFO] Epoch: 15 , batch: 496 , training loss: 4.340168\n",
      "[INFO] Epoch: 15 , batch: 497 , training loss: 4.367085\n",
      "[INFO] Epoch: 15 , batch: 498 , training loss: 4.368029\n",
      "[INFO] Epoch: 15 , batch: 499 , training loss: 4.434489\n",
      "[INFO] Epoch: 15 , batch: 500 , training loss: 4.590308\n",
      "[INFO] Epoch: 15 , batch: 501 , training loss: 4.937289\n",
      "[INFO] Epoch: 15 , batch: 502 , training loss: 5.001991\n",
      "[INFO] Epoch: 15 , batch: 503 , training loss: 4.717807\n",
      "[INFO] Epoch: 15 , batch: 504 , training loss: 4.831404\n",
      "[INFO] Epoch: 15 , batch: 505 , training loss: 4.800214\n",
      "[INFO] Epoch: 15 , batch: 506 , training loss: 4.744438\n",
      "[INFO] Epoch: 15 , batch: 507 , training loss: 4.804378\n",
      "[INFO] Epoch: 15 , batch: 508 , training loss: 4.704604\n",
      "[INFO] Epoch: 15 , batch: 509 , training loss: 4.497466\n",
      "[INFO] Epoch: 15 , batch: 510 , training loss: 4.579974\n",
      "[INFO] Epoch: 15 , batch: 511 , training loss: 4.503604\n",
      "[INFO] Epoch: 15 , batch: 512 , training loss: 4.557493\n",
      "[INFO] Epoch: 15 , batch: 513 , training loss: 4.822480\n",
      "[INFO] Epoch: 15 , batch: 514 , training loss: 4.480103\n",
      "[INFO] Epoch: 15 , batch: 515 , training loss: 4.731002\n",
      "[INFO] Epoch: 15 , batch: 516 , training loss: 4.518617\n",
      "[INFO] Epoch: 15 , batch: 517 , training loss: 4.490731\n",
      "[INFO] Epoch: 15 , batch: 518 , training loss: 4.449959\n",
      "[INFO] Epoch: 15 , batch: 519 , training loss: 4.296729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 15 , batch: 520 , training loss: 4.530680\n",
      "[INFO] Epoch: 15 , batch: 521 , training loss: 4.533531\n",
      "[INFO] Epoch: 15 , batch: 522 , training loss: 4.611287\n",
      "[INFO] Epoch: 15 , batch: 523 , training loss: 4.497860\n",
      "[INFO] Epoch: 15 , batch: 524 , training loss: 4.784404\n",
      "[INFO] Epoch: 15 , batch: 525 , training loss: 4.657282\n",
      "[INFO] Epoch: 15 , batch: 526 , training loss: 4.446730\n",
      "[INFO] Epoch: 15 , batch: 527 , training loss: 4.484023\n",
      "[INFO] Epoch: 15 , batch: 528 , training loss: 4.519535\n",
      "[INFO] Epoch: 15 , batch: 529 , training loss: 4.491716\n",
      "[INFO] Epoch: 15 , batch: 530 , training loss: 4.344973\n",
      "[INFO] Epoch: 15 , batch: 531 , training loss: 4.483239\n",
      "[INFO] Epoch: 15 , batch: 532 , training loss: 4.382670\n",
      "[INFO] Epoch: 15 , batch: 533 , training loss: 4.530871\n",
      "[INFO] Epoch: 15 , batch: 534 , training loss: 4.511503\n",
      "[INFO] Epoch: 15 , batch: 535 , training loss: 4.527724\n",
      "[INFO] Epoch: 15 , batch: 536 , training loss: 4.377889\n",
      "[INFO] Epoch: 15 , batch: 537 , training loss: 4.334070\n",
      "[INFO] Epoch: 15 , batch: 538 , training loss: 4.426039\n",
      "[INFO] Epoch: 15 , batch: 539 , training loss: 4.570779\n",
      "[INFO] Epoch: 15 , batch: 540 , training loss: 5.112637\n",
      "[INFO] Epoch: 15 , batch: 541 , training loss: 4.961710\n",
      "[INFO] Epoch: 15 , batch: 542 , training loss: 4.816890\n",
      "[INFO] Epoch: 16 , batch: 0 , training loss: 3.977284\n",
      "[INFO] Epoch: 16 , batch: 1 , training loss: 3.822543\n",
      "[INFO] Epoch: 16 , batch: 2 , training loss: 3.882599\n",
      "[INFO] Epoch: 16 , batch: 3 , training loss: 3.712636\n",
      "[INFO] Epoch: 16 , batch: 4 , training loss: 4.115862\n",
      "[INFO] Epoch: 16 , batch: 5 , training loss: 3.748049\n",
      "[INFO] Epoch: 16 , batch: 6 , training loss: 4.071848\n",
      "[INFO] Epoch: 16 , batch: 7 , training loss: 3.965673\n",
      "[INFO] Epoch: 16 , batch: 8 , training loss: 3.594537\n",
      "[INFO] Epoch: 16 , batch: 9 , training loss: 3.847122\n",
      "[INFO] Epoch: 16 , batch: 10 , training loss: 3.793099\n",
      "[INFO] Epoch: 16 , batch: 11 , training loss: 3.734649\n",
      "[INFO] Epoch: 16 , batch: 12 , training loss: 3.660169\n",
      "[INFO] Epoch: 16 , batch: 13 , training loss: 3.702651\n",
      "[INFO] Epoch: 16 , batch: 14 , training loss: 3.558370\n",
      "[INFO] Epoch: 16 , batch: 15 , training loss: 3.790884\n",
      "[INFO] Epoch: 16 , batch: 16 , training loss: 3.643754\n",
      "[INFO] Epoch: 16 , batch: 17 , training loss: 3.784466\n",
      "[INFO] Epoch: 16 , batch: 18 , training loss: 3.717095\n",
      "[INFO] Epoch: 16 , batch: 19 , training loss: 3.489950\n",
      "[INFO] Epoch: 16 , batch: 20 , training loss: 3.434026\n",
      "[INFO] Epoch: 16 , batch: 21 , training loss: 3.613595\n",
      "[INFO] Epoch: 16 , batch: 22 , training loss: 3.509321\n",
      "[INFO] Epoch: 16 , batch: 23 , training loss: 3.696214\n",
      "[INFO] Epoch: 16 , batch: 24 , training loss: 3.602208\n",
      "[INFO] Epoch: 16 , batch: 25 , training loss: 3.678660\n",
      "[INFO] Epoch: 16 , batch: 26 , training loss: 3.576995\n",
      "[INFO] Epoch: 16 , batch: 27 , training loss: 3.520941\n",
      "[INFO] Epoch: 16 , batch: 28 , training loss: 3.763371\n",
      "[INFO] Epoch: 16 , batch: 29 , training loss: 3.561298\n",
      "[INFO] Epoch: 16 , batch: 30 , training loss: 3.533509\n",
      "[INFO] Epoch: 16 , batch: 31 , training loss: 3.666830\n",
      "[INFO] Epoch: 16 , batch: 32 , training loss: 3.652672\n",
      "[INFO] Epoch: 16 , batch: 33 , training loss: 3.733230\n",
      "[INFO] Epoch: 16 , batch: 34 , training loss: 3.701272\n",
      "[INFO] Epoch: 16 , batch: 35 , training loss: 3.632126\n",
      "[INFO] Epoch: 16 , batch: 36 , training loss: 3.732389\n",
      "[INFO] Epoch: 16 , batch: 37 , training loss: 3.568565\n",
      "[INFO] Epoch: 16 , batch: 38 , training loss: 3.676167\n",
      "[INFO] Epoch: 16 , batch: 39 , training loss: 3.460965\n",
      "[INFO] Epoch: 16 , batch: 40 , training loss: 3.689299\n",
      "[INFO] Epoch: 16 , batch: 41 , training loss: 3.683951\n",
      "[INFO] Epoch: 16 , batch: 42 , training loss: 4.134705\n",
      "[INFO] Epoch: 16 , batch: 43 , training loss: 3.895465\n",
      "[INFO] Epoch: 16 , batch: 44 , training loss: 4.197408\n",
      "[INFO] Epoch: 16 , batch: 45 , training loss: 4.205547\n",
      "[INFO] Epoch: 16 , batch: 46 , training loss: 4.216555\n",
      "[INFO] Epoch: 16 , batch: 47 , training loss: 3.825596\n",
      "[INFO] Epoch: 16 , batch: 48 , training loss: 3.772886\n",
      "[INFO] Epoch: 16 , batch: 49 , training loss: 3.982131\n",
      "[INFO] Epoch: 16 , batch: 50 , training loss: 3.731534\n",
      "[INFO] Epoch: 16 , batch: 51 , training loss: 3.923590\n",
      "[INFO] Epoch: 16 , batch: 52 , training loss: 3.778536\n",
      "[INFO] Epoch: 16 , batch: 53 , training loss: 3.876464\n",
      "[INFO] Epoch: 16 , batch: 54 , training loss: 3.886927\n",
      "[INFO] Epoch: 16 , batch: 55 , training loss: 4.002962\n",
      "[INFO] Epoch: 16 , batch: 56 , training loss: 3.842800\n",
      "[INFO] Epoch: 16 , batch: 57 , training loss: 3.736652\n",
      "[INFO] Epoch: 16 , batch: 58 , training loss: 3.780172\n",
      "[INFO] Epoch: 16 , batch: 59 , training loss: 3.873796\n",
      "[INFO] Epoch: 16 , batch: 60 , training loss: 3.802186\n",
      "[INFO] Epoch: 16 , batch: 61 , training loss: 3.902297\n",
      "[INFO] Epoch: 16 , batch: 62 , training loss: 3.768715\n",
      "[INFO] Epoch: 16 , batch: 63 , training loss: 3.946852\n",
      "[INFO] Epoch: 16 , batch: 64 , training loss: 4.188262\n",
      "[INFO] Epoch: 16 , batch: 65 , training loss: 3.827712\n",
      "[INFO] Epoch: 16 , batch: 66 , training loss: 3.710351\n",
      "[INFO] Epoch: 16 , batch: 67 , training loss: 3.670938\n",
      "[INFO] Epoch: 16 , batch: 68 , training loss: 3.941449\n",
      "[INFO] Epoch: 16 , batch: 69 , training loss: 3.826233\n",
      "[INFO] Epoch: 16 , batch: 70 , training loss: 4.006307\n",
      "[INFO] Epoch: 16 , batch: 71 , training loss: 3.870845\n",
      "[INFO] Epoch: 16 , batch: 72 , training loss: 3.947542\n",
      "[INFO] Epoch: 16 , batch: 73 , training loss: 3.874011\n",
      "[INFO] Epoch: 16 , batch: 74 , training loss: 3.991671\n",
      "[INFO] Epoch: 16 , batch: 75 , training loss: 3.812092\n",
      "[INFO] Epoch: 16 , batch: 76 , training loss: 3.966449\n",
      "[INFO] Epoch: 16 , batch: 77 , training loss: 3.901542\n",
      "[INFO] Epoch: 16 , batch: 78 , training loss: 3.991767\n",
      "[INFO] Epoch: 16 , batch: 79 , training loss: 3.828742\n",
      "[INFO] Epoch: 16 , batch: 80 , training loss: 4.018865\n",
      "[INFO] Epoch: 16 , batch: 81 , training loss: 3.971169\n",
      "[INFO] Epoch: 16 , batch: 82 , training loss: 3.938687\n",
      "[INFO] Epoch: 16 , batch: 83 , training loss: 4.079883\n",
      "[INFO] Epoch: 16 , batch: 84 , training loss: 3.984051\n",
      "[INFO] Epoch: 16 , batch: 85 , training loss: 4.088341\n",
      "[INFO] Epoch: 16 , batch: 86 , training loss: 4.045543\n",
      "[INFO] Epoch: 16 , batch: 87 , training loss: 3.988106\n",
      "[INFO] Epoch: 16 , batch: 88 , training loss: 4.120263\n",
      "[INFO] Epoch: 16 , batch: 89 , training loss: 3.911656\n",
      "[INFO] Epoch: 16 , batch: 90 , training loss: 4.015104\n",
      "[INFO] Epoch: 16 , batch: 91 , training loss: 3.939080\n",
      "[INFO] Epoch: 16 , batch: 92 , training loss: 3.972302\n",
      "[INFO] Epoch: 16 , batch: 93 , training loss: 4.059318\n",
      "[INFO] Epoch: 16 , batch: 94 , training loss: 4.184244\n",
      "[INFO] Epoch: 16 , batch: 95 , training loss: 3.968389\n",
      "[INFO] Epoch: 16 , batch: 96 , training loss: 3.935178\n",
      "[INFO] Epoch: 16 , batch: 97 , training loss: 3.912425\n",
      "[INFO] Epoch: 16 , batch: 98 , training loss: 3.826063\n",
      "[INFO] Epoch: 16 , batch: 99 , training loss: 3.952743\n",
      "[INFO] Epoch: 16 , batch: 100 , training loss: 3.820321\n",
      "[INFO] Epoch: 16 , batch: 101 , training loss: 3.870219\n",
      "[INFO] Epoch: 16 , batch: 102 , training loss: 4.034236\n",
      "[INFO] Epoch: 16 , batch: 103 , training loss: 3.818146\n",
      "[INFO] Epoch: 16 , batch: 104 , training loss: 3.749984\n",
      "[INFO] Epoch: 16 , batch: 105 , training loss: 3.999154\n",
      "[INFO] Epoch: 16 , batch: 106 , training loss: 4.062314\n",
      "[INFO] Epoch: 16 , batch: 107 , training loss: 3.870742\n",
      "[INFO] Epoch: 16 , batch: 108 , training loss: 3.821211\n",
      "[INFO] Epoch: 16 , batch: 109 , training loss: 3.713224\n",
      "[INFO] Epoch: 16 , batch: 110 , training loss: 3.922778\n",
      "[INFO] Epoch: 16 , batch: 111 , training loss: 3.986893\n",
      "[INFO] Epoch: 16 , batch: 112 , training loss: 3.933153\n",
      "[INFO] Epoch: 16 , batch: 113 , training loss: 3.915811\n",
      "[INFO] Epoch: 16 , batch: 114 , training loss: 3.937707\n",
      "[INFO] Epoch: 16 , batch: 115 , training loss: 3.922948\n",
      "[INFO] Epoch: 16 , batch: 116 , training loss: 3.813851\n",
      "[INFO] Epoch: 16 , batch: 117 , training loss: 4.056296\n",
      "[INFO] Epoch: 16 , batch: 118 , training loss: 4.003602\n",
      "[INFO] Epoch: 16 , batch: 119 , training loss: 4.184471\n",
      "[INFO] Epoch: 16 , batch: 120 , training loss: 4.149675\n",
      "[INFO] Epoch: 16 , batch: 121 , training loss: 4.017045\n",
      "[INFO] Epoch: 16 , batch: 122 , training loss: 3.896660\n",
      "[INFO] Epoch: 16 , batch: 123 , training loss: 3.919199\n",
      "[INFO] Epoch: 16 , batch: 124 , training loss: 4.047587\n",
      "[INFO] Epoch: 16 , batch: 125 , training loss: 3.782595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 16 , batch: 126 , training loss: 3.834366\n",
      "[INFO] Epoch: 16 , batch: 127 , training loss: 3.860108\n",
      "[INFO] Epoch: 16 , batch: 128 , training loss: 4.009470\n",
      "[INFO] Epoch: 16 , batch: 129 , training loss: 3.936835\n",
      "[INFO] Epoch: 16 , batch: 130 , training loss: 3.918628\n",
      "[INFO] Epoch: 16 , batch: 131 , training loss: 3.924016\n",
      "[INFO] Epoch: 16 , batch: 132 , training loss: 3.963846\n",
      "[INFO] Epoch: 16 , batch: 133 , training loss: 3.924221\n",
      "[INFO] Epoch: 16 , batch: 134 , training loss: 3.683513\n",
      "[INFO] Epoch: 16 , batch: 135 , training loss: 3.739008\n",
      "[INFO] Epoch: 16 , batch: 136 , training loss: 4.028193\n",
      "[INFO] Epoch: 16 , batch: 137 , training loss: 3.970371\n",
      "[INFO] Epoch: 16 , batch: 138 , training loss: 4.010914\n",
      "[INFO] Epoch: 16 , batch: 139 , training loss: 4.610808\n",
      "[INFO] Epoch: 16 , batch: 140 , training loss: 4.422452\n",
      "[INFO] Epoch: 16 , batch: 141 , training loss: 4.155666\n",
      "[INFO] Epoch: 16 , batch: 142 , training loss: 3.861489\n",
      "[INFO] Epoch: 16 , batch: 143 , training loss: 3.979239\n",
      "[INFO] Epoch: 16 , batch: 144 , training loss: 3.830014\n",
      "[INFO] Epoch: 16 , batch: 145 , training loss: 3.916804\n",
      "[INFO] Epoch: 16 , batch: 146 , training loss: 4.100898\n",
      "[INFO] Epoch: 16 , batch: 147 , training loss: 3.766954\n",
      "[INFO] Epoch: 16 , batch: 148 , training loss: 3.744742\n",
      "[INFO] Epoch: 16 , batch: 149 , training loss: 3.834275\n",
      "[INFO] Epoch: 16 , batch: 150 , training loss: 4.075487\n",
      "[INFO] Epoch: 16 , batch: 151 , training loss: 3.892058\n",
      "[INFO] Epoch: 16 , batch: 152 , training loss: 3.920400\n",
      "[INFO] Epoch: 16 , batch: 153 , training loss: 3.956441\n",
      "[INFO] Epoch: 16 , batch: 154 , training loss: 4.044208\n",
      "[INFO] Epoch: 16 , batch: 155 , training loss: 4.245643\n",
      "[INFO] Epoch: 16 , batch: 156 , training loss: 4.018589\n",
      "[INFO] Epoch: 16 , batch: 157 , training loss: 3.968585\n",
      "[INFO] Epoch: 16 , batch: 158 , training loss: 4.131115\n",
      "[INFO] Epoch: 16 , batch: 159 , training loss: 4.053047\n",
      "[INFO] Epoch: 16 , batch: 160 , training loss: 4.388712\n",
      "[INFO] Epoch: 16 , batch: 161 , training loss: 4.440104\n",
      "[INFO] Epoch: 16 , batch: 162 , training loss: 4.366052\n",
      "[INFO] Epoch: 16 , batch: 163 , training loss: 4.549032\n",
      "[INFO] Epoch: 16 , batch: 164 , training loss: 4.485969\n",
      "[INFO] Epoch: 16 , batch: 165 , training loss: 4.390631\n",
      "[INFO] Epoch: 16 , batch: 166 , training loss: 4.283849\n",
      "[INFO] Epoch: 16 , batch: 167 , training loss: 4.515235\n",
      "[INFO] Epoch: 16 , batch: 168 , training loss: 4.203055\n",
      "[INFO] Epoch: 16 , batch: 169 , training loss: 4.160152\n",
      "[INFO] Epoch: 16 , batch: 170 , training loss: 4.266731\n",
      "[INFO] Epoch: 16 , batch: 171 , training loss: 3.735479\n",
      "[INFO] Epoch: 16 , batch: 172 , training loss: 3.904817\n",
      "[INFO] Epoch: 16 , batch: 173 , training loss: 4.194297\n",
      "[INFO] Epoch: 16 , batch: 174 , training loss: 4.633899\n",
      "[INFO] Epoch: 16 , batch: 175 , training loss: 4.905139\n",
      "[INFO] Epoch: 16 , batch: 176 , training loss: 4.702090\n",
      "[INFO] Epoch: 16 , batch: 177 , training loss: 4.240829\n",
      "[INFO] Epoch: 16 , batch: 178 , training loss: 4.198410\n",
      "[INFO] Epoch: 16 , batch: 179 , training loss: 4.280914\n",
      "[INFO] Epoch: 16 , batch: 180 , training loss: 4.186297\n",
      "[INFO] Epoch: 16 , batch: 181 , training loss: 4.460495\n",
      "[INFO] Epoch: 16 , batch: 182 , training loss: 4.400011\n",
      "[INFO] Epoch: 16 , batch: 183 , training loss: 4.349895\n",
      "[INFO] Epoch: 16 , batch: 184 , training loss: 4.270074\n",
      "[INFO] Epoch: 16 , batch: 185 , training loss: 4.211792\n",
      "[INFO] Epoch: 16 , batch: 186 , training loss: 4.331688\n",
      "[INFO] Epoch: 16 , batch: 187 , training loss: 4.490496\n",
      "[INFO] Epoch: 16 , batch: 188 , training loss: 4.434961\n",
      "[INFO] Epoch: 16 , batch: 189 , training loss: 4.329041\n",
      "[INFO] Epoch: 16 , batch: 190 , training loss: 4.366118\n",
      "[INFO] Epoch: 16 , batch: 191 , training loss: 4.520103\n",
      "[INFO] Epoch: 16 , batch: 192 , training loss: 4.311456\n",
      "[INFO] Epoch: 16 , batch: 193 , training loss: 4.417964\n",
      "[INFO] Epoch: 16 , batch: 194 , training loss: 4.360934\n",
      "[INFO] Epoch: 16 , batch: 195 , training loss: 4.299058\n",
      "[INFO] Epoch: 16 , batch: 196 , training loss: 4.148468\n",
      "[INFO] Epoch: 16 , batch: 197 , training loss: 4.271956\n",
      "[INFO] Epoch: 16 , batch: 198 , training loss: 4.166897\n",
      "[INFO] Epoch: 16 , batch: 199 , training loss: 4.278727\n",
      "[INFO] Epoch: 16 , batch: 200 , training loss: 4.172282\n",
      "[INFO] Epoch: 16 , batch: 201 , training loss: 4.084971\n",
      "[INFO] Epoch: 16 , batch: 202 , training loss: 4.087820\n",
      "[INFO] Epoch: 16 , batch: 203 , training loss: 4.203285\n",
      "[INFO] Epoch: 16 , batch: 204 , training loss: 4.281590\n",
      "[INFO] Epoch: 16 , batch: 205 , training loss: 3.898632\n",
      "[INFO] Epoch: 16 , batch: 206 , training loss: 3.812954\n",
      "[INFO] Epoch: 16 , batch: 207 , training loss: 3.812245\n",
      "[INFO] Epoch: 16 , batch: 208 , training loss: 4.144612\n",
      "[INFO] Epoch: 16 , batch: 209 , training loss: 4.091529\n",
      "[INFO] Epoch: 16 , batch: 210 , training loss: 4.135759\n",
      "[INFO] Epoch: 16 , batch: 211 , training loss: 4.105806\n",
      "[INFO] Epoch: 16 , batch: 212 , training loss: 4.234375\n",
      "[INFO] Epoch: 16 , batch: 213 , training loss: 4.190425\n",
      "[INFO] Epoch: 16 , batch: 214 , training loss: 4.263411\n",
      "[INFO] Epoch: 16 , batch: 215 , training loss: 4.468350\n",
      "[INFO] Epoch: 16 , batch: 216 , training loss: 4.186209\n",
      "[INFO] Epoch: 16 , batch: 217 , training loss: 4.108639\n",
      "[INFO] Epoch: 16 , batch: 218 , training loss: 4.110780\n",
      "[INFO] Epoch: 16 , batch: 219 , training loss: 4.218129\n",
      "[INFO] Epoch: 16 , batch: 220 , training loss: 4.036628\n",
      "[INFO] Epoch: 16 , batch: 221 , training loss: 4.056263\n",
      "[INFO] Epoch: 16 , batch: 222 , training loss: 4.170503\n",
      "[INFO] Epoch: 16 , batch: 223 , training loss: 4.302264\n",
      "[INFO] Epoch: 16 , batch: 224 , training loss: 4.320658\n",
      "[INFO] Epoch: 16 , batch: 225 , training loss: 4.205107\n",
      "[INFO] Epoch: 16 , batch: 226 , training loss: 4.344461\n",
      "[INFO] Epoch: 16 , batch: 227 , training loss: 4.311989\n",
      "[INFO] Epoch: 16 , batch: 228 , training loss: 4.339202\n",
      "[INFO] Epoch: 16 , batch: 229 , training loss: 4.187377\n",
      "[INFO] Epoch: 16 , batch: 230 , training loss: 4.071634\n",
      "[INFO] Epoch: 16 , batch: 231 , training loss: 3.920332\n",
      "[INFO] Epoch: 16 , batch: 232 , training loss: 4.068480\n",
      "[INFO] Epoch: 16 , batch: 233 , training loss: 4.097136\n",
      "[INFO] Epoch: 16 , batch: 234 , training loss: 3.780380\n",
      "[INFO] Epoch: 16 , batch: 235 , training loss: 3.912568\n",
      "[INFO] Epoch: 16 , batch: 236 , training loss: 4.005533\n",
      "[INFO] Epoch: 16 , batch: 237 , training loss: 4.220732\n",
      "[INFO] Epoch: 16 , batch: 238 , training loss: 3.976411\n",
      "[INFO] Epoch: 16 , batch: 239 , training loss: 4.034729\n",
      "[INFO] Epoch: 16 , batch: 240 , training loss: 4.075094\n",
      "[INFO] Epoch: 16 , batch: 241 , training loss: 3.885062\n",
      "[INFO] Epoch: 16 , batch: 242 , training loss: 3.888598\n",
      "[INFO] Epoch: 16 , batch: 243 , training loss: 4.180405\n",
      "[INFO] Epoch: 16 , batch: 244 , training loss: 4.145981\n",
      "[INFO] Epoch: 16 , batch: 245 , training loss: 4.134984\n",
      "[INFO] Epoch: 16 , batch: 246 , training loss: 3.801876\n",
      "[INFO] Epoch: 16 , batch: 247 , training loss: 3.967109\n",
      "[INFO] Epoch: 16 , batch: 248 , training loss: 4.055580\n",
      "[INFO] Epoch: 16 , batch: 249 , training loss: 4.036345\n",
      "[INFO] Epoch: 16 , batch: 250 , training loss: 3.831389\n",
      "[INFO] Epoch: 16 , batch: 251 , training loss: 4.294373\n",
      "[INFO] Epoch: 16 , batch: 252 , training loss: 3.977649\n",
      "[INFO] Epoch: 16 , batch: 253 , training loss: 3.921698\n",
      "[INFO] Epoch: 16 , batch: 254 , training loss: 4.192330\n",
      "[INFO] Epoch: 16 , batch: 255 , training loss: 4.138231\n",
      "[INFO] Epoch: 16 , batch: 256 , training loss: 4.143770\n",
      "[INFO] Epoch: 16 , batch: 257 , training loss: 4.313284\n",
      "[INFO] Epoch: 16 , batch: 258 , training loss: 4.323458\n",
      "[INFO] Epoch: 16 , batch: 259 , training loss: 4.370972\n",
      "[INFO] Epoch: 16 , batch: 260 , training loss: 4.118674\n",
      "[INFO] Epoch: 16 , batch: 261 , training loss: 4.290860\n",
      "[INFO] Epoch: 16 , batch: 262 , training loss: 4.460563\n",
      "[INFO] Epoch: 16 , batch: 263 , training loss: 4.618654\n",
      "[INFO] Epoch: 16 , batch: 264 , training loss: 3.966603\n",
      "[INFO] Epoch: 16 , batch: 265 , training loss: 4.070921\n",
      "[INFO] Epoch: 16 , batch: 266 , training loss: 4.541492\n",
      "[INFO] Epoch: 16 , batch: 267 , training loss: 4.240738\n",
      "[INFO] Epoch: 16 , batch: 268 , training loss: 4.161720\n",
      "[INFO] Epoch: 16 , batch: 269 , training loss: 4.154184\n",
      "[INFO] Epoch: 16 , batch: 270 , training loss: 4.171000\n",
      "[INFO] Epoch: 16 , batch: 271 , training loss: 4.211187\n",
      "[INFO] Epoch: 16 , batch: 272 , training loss: 4.190650\n",
      "[INFO] Epoch: 16 , batch: 273 , training loss: 4.191591\n",
      "[INFO] Epoch: 16 , batch: 274 , training loss: 4.296641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 16 , batch: 275 , training loss: 4.154451\n",
      "[INFO] Epoch: 16 , batch: 276 , training loss: 4.219041\n",
      "[INFO] Epoch: 16 , batch: 277 , training loss: 4.384176\n",
      "[INFO] Epoch: 16 , batch: 278 , training loss: 4.030091\n",
      "[INFO] Epoch: 16 , batch: 279 , training loss: 4.060083\n",
      "[INFO] Epoch: 16 , batch: 280 , training loss: 3.992284\n",
      "[INFO] Epoch: 16 , batch: 281 , training loss: 4.145844\n",
      "[INFO] Epoch: 16 , batch: 282 , training loss: 4.047511\n",
      "[INFO] Epoch: 16 , batch: 283 , training loss: 4.055261\n",
      "[INFO] Epoch: 16 , batch: 284 , training loss: 4.093659\n",
      "[INFO] Epoch: 16 , batch: 285 , training loss: 4.058003\n",
      "[INFO] Epoch: 16 , batch: 286 , training loss: 4.055176\n",
      "[INFO] Epoch: 16 , batch: 287 , training loss: 3.982932\n",
      "[INFO] Epoch: 16 , batch: 288 , training loss: 3.966577\n",
      "[INFO] Epoch: 16 , batch: 289 , training loss: 4.031453\n",
      "[INFO] Epoch: 16 , batch: 290 , training loss: 3.807172\n",
      "[INFO] Epoch: 16 , batch: 291 , training loss: 3.804189\n",
      "[INFO] Epoch: 16 , batch: 292 , training loss: 3.916979\n",
      "[INFO] Epoch: 16 , batch: 293 , training loss: 3.841137\n",
      "[INFO] Epoch: 16 , batch: 294 , training loss: 4.491954\n",
      "[INFO] Epoch: 16 , batch: 295 , training loss: 4.268781\n",
      "[INFO] Epoch: 16 , batch: 296 , training loss: 4.208476\n",
      "[INFO] Epoch: 16 , batch: 297 , training loss: 4.149812\n",
      "[INFO] Epoch: 16 , batch: 298 , training loss: 3.989735\n",
      "[INFO] Epoch: 16 , batch: 299 , training loss: 4.022878\n",
      "[INFO] Epoch: 16 , batch: 300 , training loss: 4.013724\n",
      "[INFO] Epoch: 16 , batch: 301 , training loss: 3.944662\n",
      "[INFO] Epoch: 16 , batch: 302 , training loss: 4.107215\n",
      "[INFO] Epoch: 16 , batch: 303 , training loss: 4.124121\n",
      "[INFO] Epoch: 16 , batch: 304 , training loss: 4.279824\n",
      "[INFO] Epoch: 16 , batch: 305 , training loss: 4.080964\n",
      "[INFO] Epoch: 16 , batch: 306 , training loss: 4.201544\n",
      "[INFO] Epoch: 16 , batch: 307 , training loss: 4.202799\n",
      "[INFO] Epoch: 16 , batch: 308 , training loss: 4.029059\n",
      "[INFO] Epoch: 16 , batch: 309 , training loss: 4.040102\n",
      "[INFO] Epoch: 16 , batch: 310 , training loss: 3.953549\n",
      "[INFO] Epoch: 16 , batch: 311 , training loss: 3.930456\n",
      "[INFO] Epoch: 16 , batch: 312 , training loss: 3.844092\n",
      "[INFO] Epoch: 16 , batch: 313 , training loss: 3.941984\n",
      "[INFO] Epoch: 16 , batch: 314 , training loss: 4.037412\n",
      "[INFO] Epoch: 16 , batch: 315 , training loss: 4.090763\n",
      "[INFO] Epoch: 16 , batch: 316 , training loss: 4.371012\n",
      "[INFO] Epoch: 16 , batch: 317 , training loss: 4.745351\n",
      "[INFO] Epoch: 16 , batch: 318 , training loss: 4.903072\n",
      "[INFO] Epoch: 16 , batch: 319 , training loss: 4.552779\n",
      "[INFO] Epoch: 16 , batch: 320 , training loss: 4.076031\n",
      "[INFO] Epoch: 16 , batch: 321 , training loss: 3.879038\n",
      "[INFO] Epoch: 16 , batch: 322 , training loss: 3.998678\n",
      "[INFO] Epoch: 16 , batch: 323 , training loss: 4.021922\n",
      "[INFO] Epoch: 16 , batch: 324 , training loss: 3.993661\n",
      "[INFO] Epoch: 16 , batch: 325 , training loss: 4.131283\n",
      "[INFO] Epoch: 16 , batch: 326 , training loss: 4.171633\n",
      "[INFO] Epoch: 16 , batch: 327 , training loss: 4.107746\n",
      "[INFO] Epoch: 16 , batch: 328 , training loss: 4.107639\n",
      "[INFO] Epoch: 16 , batch: 329 , training loss: 4.013899\n",
      "[INFO] Epoch: 16 , batch: 330 , training loss: 4.009223\n",
      "[INFO] Epoch: 16 , batch: 331 , training loss: 4.175921\n",
      "[INFO] Epoch: 16 , batch: 332 , training loss: 3.998527\n",
      "[INFO] Epoch: 16 , batch: 333 , training loss: 3.979729\n",
      "[INFO] Epoch: 16 , batch: 334 , training loss: 4.004295\n",
      "[INFO] Epoch: 16 , batch: 335 , training loss: 4.140069\n",
      "[INFO] Epoch: 16 , batch: 336 , training loss: 4.144823\n",
      "[INFO] Epoch: 16 , batch: 337 , training loss: 4.195546\n",
      "[INFO] Epoch: 16 , batch: 338 , training loss: 4.385482\n",
      "[INFO] Epoch: 16 , batch: 339 , training loss: 4.202692\n",
      "[INFO] Epoch: 16 , batch: 340 , training loss: 4.420471\n",
      "[INFO] Epoch: 16 , batch: 341 , training loss: 4.144444\n",
      "[INFO] Epoch: 16 , batch: 342 , training loss: 3.928309\n",
      "[INFO] Epoch: 16 , batch: 343 , training loss: 3.997227\n",
      "[INFO] Epoch: 16 , batch: 344 , training loss: 3.867793\n",
      "[INFO] Epoch: 16 , batch: 345 , training loss: 4.015222\n",
      "[INFO] Epoch: 16 , batch: 346 , training loss: 4.035954\n",
      "[INFO] Epoch: 16 , batch: 347 , training loss: 3.961095\n",
      "[INFO] Epoch: 16 , batch: 348 , training loss: 4.092028\n",
      "[INFO] Epoch: 16 , batch: 349 , training loss: 4.180631\n",
      "[INFO] Epoch: 16 , batch: 350 , training loss: 4.014055\n",
      "[INFO] Epoch: 16 , batch: 351 , training loss: 4.074688\n",
      "[INFO] Epoch: 16 , batch: 352 , training loss: 4.096106\n",
      "[INFO] Epoch: 16 , batch: 353 , training loss: 4.067298\n",
      "[INFO] Epoch: 16 , batch: 354 , training loss: 4.170410\n",
      "[INFO] Epoch: 16 , batch: 355 , training loss: 4.175414\n",
      "[INFO] Epoch: 16 , batch: 356 , training loss: 4.066958\n",
      "[INFO] Epoch: 16 , batch: 357 , training loss: 4.128126\n",
      "[INFO] Epoch: 16 , batch: 358 , training loss: 4.050657\n",
      "[INFO] Epoch: 16 , batch: 359 , training loss: 4.036975\n",
      "[INFO] Epoch: 16 , batch: 360 , training loss: 4.120253\n",
      "[INFO] Epoch: 16 , batch: 361 , training loss: 4.091267\n",
      "[INFO] Epoch: 16 , batch: 362 , training loss: 4.199359\n",
      "[INFO] Epoch: 16 , batch: 363 , training loss: 4.081481\n",
      "[INFO] Epoch: 16 , batch: 364 , training loss: 4.144463\n",
      "[INFO] Epoch: 16 , batch: 365 , training loss: 4.036777\n",
      "[INFO] Epoch: 16 , batch: 366 , training loss: 4.143198\n",
      "[INFO] Epoch: 16 , batch: 367 , training loss: 4.205277\n",
      "[INFO] Epoch: 16 , batch: 368 , training loss: 4.650061\n",
      "[INFO] Epoch: 16 , batch: 369 , training loss: 4.319889\n",
      "[INFO] Epoch: 16 , batch: 370 , training loss: 4.064574\n",
      "[INFO] Epoch: 16 , batch: 371 , training loss: 4.516569\n",
      "[INFO] Epoch: 16 , batch: 372 , training loss: 4.772322\n",
      "[INFO] Epoch: 16 , batch: 373 , training loss: 4.834821\n",
      "[INFO] Epoch: 16 , batch: 374 , training loss: 4.926483\n",
      "[INFO] Epoch: 16 , batch: 375 , training loss: 4.916495\n",
      "[INFO] Epoch: 16 , batch: 376 , training loss: 4.822849\n",
      "[INFO] Epoch: 16 , batch: 377 , training loss: 4.541456\n",
      "[INFO] Epoch: 16 , batch: 378 , training loss: 4.628199\n",
      "[INFO] Epoch: 16 , batch: 379 , training loss: 4.605755\n",
      "[INFO] Epoch: 16 , batch: 380 , training loss: 4.726086\n",
      "[INFO] Epoch: 16 , batch: 381 , training loss: 4.477319\n",
      "[INFO] Epoch: 16 , batch: 382 , training loss: 4.766004\n",
      "[INFO] Epoch: 16 , batch: 383 , training loss: 4.797717\n",
      "[INFO] Epoch: 16 , batch: 384 , training loss: 4.799246\n",
      "[INFO] Epoch: 16 , batch: 385 , training loss: 4.453515\n",
      "[INFO] Epoch: 16 , batch: 386 , training loss: 4.707314\n",
      "[INFO] Epoch: 16 , batch: 387 , training loss: 4.663700\n",
      "[INFO] Epoch: 16 , batch: 388 , training loss: 4.449574\n",
      "[INFO] Epoch: 16 , batch: 389 , training loss: 4.274833\n",
      "[INFO] Epoch: 16 , batch: 390 , training loss: 4.284876\n",
      "[INFO] Epoch: 16 , batch: 391 , training loss: 4.295836\n",
      "[INFO] Epoch: 16 , batch: 392 , training loss: 4.669523\n",
      "[INFO] Epoch: 16 , batch: 393 , training loss: 4.578137\n",
      "[INFO] Epoch: 16 , batch: 394 , training loss: 4.658418\n",
      "[INFO] Epoch: 16 , batch: 395 , training loss: 4.473203\n",
      "[INFO] Epoch: 16 , batch: 396 , training loss: 4.265345\n",
      "[INFO] Epoch: 16 , batch: 397 , training loss: 4.429950\n",
      "[INFO] Epoch: 16 , batch: 398 , training loss: 4.281656\n",
      "[INFO] Epoch: 16 , batch: 399 , training loss: 4.356746\n",
      "[INFO] Epoch: 16 , batch: 400 , training loss: 4.348742\n",
      "[INFO] Epoch: 16 , batch: 401 , training loss: 4.784812\n",
      "[INFO] Epoch: 16 , batch: 402 , training loss: 4.481900\n",
      "[INFO] Epoch: 16 , batch: 403 , training loss: 4.317135\n",
      "[INFO] Epoch: 16 , batch: 404 , training loss: 4.476371\n",
      "[INFO] Epoch: 16 , batch: 405 , training loss: 4.540306\n",
      "[INFO] Epoch: 16 , batch: 406 , training loss: 4.442258\n",
      "[INFO] Epoch: 16 , batch: 407 , training loss: 4.487398\n",
      "[INFO] Epoch: 16 , batch: 408 , training loss: 4.423301\n",
      "[INFO] Epoch: 16 , batch: 409 , training loss: 4.446863\n",
      "[INFO] Epoch: 16 , batch: 410 , training loss: 4.515445\n",
      "[INFO] Epoch: 16 , batch: 411 , training loss: 4.689931\n",
      "[INFO] Epoch: 16 , batch: 412 , training loss: 4.525500\n",
      "[INFO] Epoch: 16 , batch: 413 , training loss: 4.385181\n",
      "[INFO] Epoch: 16 , batch: 414 , training loss: 4.421566\n",
      "[INFO] Epoch: 16 , batch: 415 , training loss: 4.467831\n",
      "[INFO] Epoch: 16 , batch: 416 , training loss: 4.548911\n",
      "[INFO] Epoch: 16 , batch: 417 , training loss: 4.433891\n",
      "[INFO] Epoch: 16 , batch: 418 , training loss: 4.504847\n",
      "[INFO] Epoch: 16 , batch: 419 , training loss: 4.430361\n",
      "[INFO] Epoch: 16 , batch: 420 , training loss: 4.420471\n",
      "[INFO] Epoch: 16 , batch: 421 , training loss: 4.413765\n",
      "[INFO] Epoch: 16 , batch: 422 , training loss: 4.268384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 16 , batch: 423 , training loss: 4.476855\n",
      "[INFO] Epoch: 16 , batch: 424 , training loss: 4.647721\n",
      "[INFO] Epoch: 16 , batch: 425 , training loss: 4.532372\n",
      "[INFO] Epoch: 16 , batch: 426 , training loss: 4.233217\n",
      "[INFO] Epoch: 16 , batch: 427 , training loss: 4.491981\n",
      "[INFO] Epoch: 16 , batch: 428 , training loss: 4.365429\n",
      "[INFO] Epoch: 16 , batch: 429 , training loss: 4.258247\n",
      "[INFO] Epoch: 16 , batch: 430 , training loss: 4.498435\n",
      "[INFO] Epoch: 16 , batch: 431 , training loss: 4.093265\n",
      "[INFO] Epoch: 16 , batch: 432 , training loss: 4.147953\n",
      "[INFO] Epoch: 16 , batch: 433 , training loss: 4.189678\n",
      "[INFO] Epoch: 16 , batch: 434 , training loss: 4.062699\n",
      "[INFO] Epoch: 16 , batch: 435 , training loss: 4.418284\n",
      "[INFO] Epoch: 16 , batch: 436 , training loss: 4.492241\n",
      "[INFO] Epoch: 16 , batch: 437 , training loss: 4.274754\n",
      "[INFO] Epoch: 16 , batch: 438 , training loss: 4.100432\n",
      "[INFO] Epoch: 16 , batch: 439 , training loss: 4.351417\n",
      "[INFO] Epoch: 16 , batch: 440 , training loss: 4.487465\n",
      "[INFO] Epoch: 16 , batch: 441 , training loss: 4.534134\n",
      "[INFO] Epoch: 16 , batch: 442 , training loss: 4.300077\n",
      "[INFO] Epoch: 16 , batch: 443 , training loss: 4.523488\n",
      "[INFO] Epoch: 16 , batch: 444 , training loss: 4.106054\n",
      "[INFO] Epoch: 16 , batch: 445 , training loss: 4.047554\n",
      "[INFO] Epoch: 16 , batch: 446 , training loss: 3.954971\n",
      "[INFO] Epoch: 16 , batch: 447 , training loss: 4.144695\n",
      "[INFO] Epoch: 16 , batch: 448 , training loss: 4.295488\n",
      "[INFO] Epoch: 16 , batch: 449 , training loss: 4.656939\n",
      "[INFO] Epoch: 16 , batch: 450 , training loss: 4.738646\n",
      "[INFO] Epoch: 16 , batch: 451 , training loss: 4.607378\n",
      "[INFO] Epoch: 16 , batch: 452 , training loss: 4.415782\n",
      "[INFO] Epoch: 16 , batch: 453 , training loss: 4.201174\n",
      "[INFO] Epoch: 16 , batch: 454 , training loss: 4.351432\n",
      "[INFO] Epoch: 16 , batch: 455 , training loss: 4.398259\n",
      "[INFO] Epoch: 16 , batch: 456 , training loss: 4.374196\n",
      "[INFO] Epoch: 16 , batch: 457 , training loss: 4.471565\n",
      "[INFO] Epoch: 16 , batch: 458 , training loss: 4.215467\n",
      "[INFO] Epoch: 16 , batch: 459 , training loss: 4.184132\n",
      "[INFO] Epoch: 16 , batch: 460 , training loss: 4.305260\n",
      "[INFO] Epoch: 16 , batch: 461 , training loss: 4.264068\n",
      "[INFO] Epoch: 16 , batch: 462 , training loss: 4.322896\n",
      "[INFO] Epoch: 16 , batch: 463 , training loss: 4.238931\n",
      "[INFO] Epoch: 16 , batch: 464 , training loss: 4.397726\n",
      "[INFO] Epoch: 16 , batch: 465 , training loss: 4.345638\n",
      "[INFO] Epoch: 16 , batch: 466 , training loss: 4.446769\n",
      "[INFO] Epoch: 16 , batch: 467 , training loss: 4.411235\n",
      "[INFO] Epoch: 16 , batch: 468 , training loss: 4.394524\n",
      "[INFO] Epoch: 16 , batch: 469 , training loss: 4.430991\n",
      "[INFO] Epoch: 16 , batch: 470 , training loss: 4.204666\n",
      "[INFO] Epoch: 16 , batch: 471 , training loss: 4.324070\n",
      "[INFO] Epoch: 16 , batch: 472 , training loss: 4.382541\n",
      "[INFO] Epoch: 16 , batch: 473 , training loss: 4.303381\n",
      "[INFO] Epoch: 16 , batch: 474 , training loss: 4.081953\n",
      "[INFO] Epoch: 16 , batch: 475 , training loss: 3.970254\n",
      "[INFO] Epoch: 16 , batch: 476 , training loss: 4.367105\n",
      "[INFO] Epoch: 16 , batch: 477 , training loss: 4.459310\n",
      "[INFO] Epoch: 16 , batch: 478 , training loss: 4.491567\n",
      "[INFO] Epoch: 16 , batch: 479 , training loss: 4.445623\n",
      "[INFO] Epoch: 16 , batch: 480 , training loss: 4.575080\n",
      "[INFO] Epoch: 16 , batch: 481 , training loss: 4.457599\n",
      "[INFO] Epoch: 16 , batch: 482 , training loss: 4.574616\n",
      "[INFO] Epoch: 16 , batch: 483 , training loss: 4.417940\n",
      "[INFO] Epoch: 16 , batch: 484 , training loss: 4.187656\n",
      "[INFO] Epoch: 16 , batch: 485 , training loss: 4.309624\n",
      "[INFO] Epoch: 16 , batch: 486 , training loss: 4.218017\n",
      "[INFO] Epoch: 16 , batch: 487 , training loss: 4.193806\n",
      "[INFO] Epoch: 16 , batch: 488 , training loss: 4.378453\n",
      "[INFO] Epoch: 16 , batch: 489 , training loss: 4.270357\n",
      "[INFO] Epoch: 16 , batch: 490 , training loss: 4.346387\n",
      "[INFO] Epoch: 16 , batch: 491 , training loss: 4.275149\n",
      "[INFO] Epoch: 16 , batch: 492 , training loss: 4.233150\n",
      "[INFO] Epoch: 16 , batch: 493 , training loss: 4.410827\n",
      "[INFO] Epoch: 16 , batch: 494 , training loss: 4.297825\n",
      "[INFO] Epoch: 16 , batch: 495 , training loss: 4.436373\n",
      "[INFO] Epoch: 16 , batch: 496 , training loss: 4.332273\n",
      "[INFO] Epoch: 16 , batch: 497 , training loss: 4.369441\n",
      "[INFO] Epoch: 16 , batch: 498 , training loss: 4.364224\n",
      "[INFO] Epoch: 16 , batch: 499 , training loss: 4.417314\n",
      "[INFO] Epoch: 16 , batch: 500 , training loss: 4.567124\n",
      "[INFO] Epoch: 16 , batch: 501 , training loss: 4.957660\n",
      "[INFO] Epoch: 16 , batch: 502 , training loss: 5.024338\n",
      "[INFO] Epoch: 16 , batch: 503 , training loss: 4.725816\n",
      "[INFO] Epoch: 16 , batch: 504 , training loss: 4.831438\n",
      "[INFO] Epoch: 16 , batch: 505 , training loss: 4.785827\n",
      "[INFO] Epoch: 16 , batch: 506 , training loss: 4.733980\n",
      "[INFO] Epoch: 16 , batch: 507 , training loss: 4.777007\n",
      "[INFO] Epoch: 16 , batch: 508 , training loss: 4.696822\n",
      "[INFO] Epoch: 16 , batch: 509 , training loss: 4.499214\n",
      "[INFO] Epoch: 16 , batch: 510 , training loss: 4.590193\n",
      "[INFO] Epoch: 16 , batch: 511 , training loss: 4.501386\n",
      "[INFO] Epoch: 16 , batch: 512 , training loss: 4.547535\n",
      "[INFO] Epoch: 16 , batch: 513 , training loss: 4.819882\n",
      "[INFO] Epoch: 16 , batch: 514 , training loss: 4.471203\n",
      "[INFO] Epoch: 16 , batch: 515 , training loss: 4.722389\n",
      "[INFO] Epoch: 16 , batch: 516 , training loss: 4.507800\n",
      "[INFO] Epoch: 16 , batch: 517 , training loss: 4.478704\n",
      "[INFO] Epoch: 16 , batch: 518 , training loss: 4.443417\n",
      "[INFO] Epoch: 16 , batch: 519 , training loss: 4.293379\n",
      "[INFO] Epoch: 16 , batch: 520 , training loss: 4.542194\n",
      "[INFO] Epoch: 16 , batch: 521 , training loss: 4.524284\n",
      "[INFO] Epoch: 16 , batch: 522 , training loss: 4.606599\n",
      "[INFO] Epoch: 16 , batch: 523 , training loss: 4.486404\n",
      "[INFO] Epoch: 16 , batch: 524 , training loss: 4.780763\n",
      "[INFO] Epoch: 16 , batch: 525 , training loss: 4.658030\n",
      "[INFO] Epoch: 16 , batch: 526 , training loss: 4.446278\n",
      "[INFO] Epoch: 16 , batch: 527 , training loss: 4.502002\n",
      "[INFO] Epoch: 16 , batch: 528 , training loss: 4.510368\n",
      "[INFO] Epoch: 16 , batch: 529 , training loss: 4.488525\n",
      "[INFO] Epoch: 16 , batch: 530 , training loss: 4.332683\n",
      "[INFO] Epoch: 16 , batch: 531 , training loss: 4.491438\n",
      "[INFO] Epoch: 16 , batch: 532 , training loss: 4.378786\n",
      "[INFO] Epoch: 16 , batch: 533 , training loss: 4.527098\n",
      "[INFO] Epoch: 16 , batch: 534 , training loss: 4.516281\n",
      "[INFO] Epoch: 16 , batch: 535 , training loss: 4.523100\n",
      "[INFO] Epoch: 16 , batch: 536 , training loss: 4.369768\n",
      "[INFO] Epoch: 16 , batch: 537 , training loss: 4.335649\n",
      "[INFO] Epoch: 16 , batch: 538 , training loss: 4.426763\n",
      "[INFO] Epoch: 16 , batch: 539 , training loss: 4.567735\n",
      "[INFO] Epoch: 16 , batch: 540 , training loss: 5.110158\n",
      "[INFO] Epoch: 16 , batch: 541 , training loss: 4.981440\n",
      "[INFO] Epoch: 16 , batch: 542 , training loss: 4.817706\n",
      "[INFO] Epoch: 17 , batch: 0 , training loss: 3.998175\n",
      "[INFO] Epoch: 17 , batch: 1 , training loss: 3.765234\n",
      "[INFO] Epoch: 17 , batch: 2 , training loss: 3.884583\n",
      "[INFO] Epoch: 17 , batch: 3 , training loss: 3.738745\n",
      "[INFO] Epoch: 17 , batch: 4 , training loss: 4.069593\n",
      "[INFO] Epoch: 17 , batch: 5 , training loss: 3.758307\n",
      "[INFO] Epoch: 17 , batch: 6 , training loss: 4.090650\n",
      "[INFO] Epoch: 17 , batch: 7 , training loss: 3.968256\n",
      "[INFO] Epoch: 17 , batch: 8 , training loss: 3.593731\n",
      "[INFO] Epoch: 17 , batch: 9 , training loss: 3.833469\n",
      "[INFO] Epoch: 17 , batch: 10 , training loss: 3.777957\n",
      "[INFO] Epoch: 17 , batch: 11 , training loss: 3.724973\n",
      "[INFO] Epoch: 17 , batch: 12 , training loss: 3.625257\n",
      "[INFO] Epoch: 17 , batch: 13 , training loss: 3.706473\n",
      "[INFO] Epoch: 17 , batch: 14 , training loss: 3.554147\n",
      "[INFO] Epoch: 17 , batch: 15 , training loss: 3.796241\n",
      "[INFO] Epoch: 17 , batch: 16 , training loss: 3.649793\n",
      "[INFO] Epoch: 17 , batch: 17 , training loss: 3.775740\n",
      "[INFO] Epoch: 17 , batch: 18 , training loss: 3.702063\n",
      "[INFO] Epoch: 17 , batch: 19 , training loss: 3.483395\n",
      "[INFO] Epoch: 17 , batch: 20 , training loss: 3.444512\n",
      "[INFO] Epoch: 17 , batch: 21 , training loss: 3.624016\n",
      "[INFO] Epoch: 17 , batch: 22 , training loss: 3.508048\n",
      "[INFO] Epoch: 17 , batch: 23 , training loss: 3.697721\n",
      "[INFO] Epoch: 17 , batch: 24 , training loss: 3.588328\n",
      "[INFO] Epoch: 17 , batch: 25 , training loss: 3.687494\n",
      "[INFO] Epoch: 17 , batch: 26 , training loss: 3.563703\n",
      "[INFO] Epoch: 17 , batch: 27 , training loss: 3.499419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 17 , batch: 28 , training loss: 3.756529\n",
      "[INFO] Epoch: 17 , batch: 29 , training loss: 3.551490\n",
      "[INFO] Epoch: 17 , batch: 30 , training loss: 3.539080\n",
      "[INFO] Epoch: 17 , batch: 31 , training loss: 3.674671\n",
      "[INFO] Epoch: 17 , batch: 32 , training loss: 3.622870\n",
      "[INFO] Epoch: 17 , batch: 33 , training loss: 3.691180\n",
      "[INFO] Epoch: 17 , batch: 34 , training loss: 3.690590\n",
      "[INFO] Epoch: 17 , batch: 35 , training loss: 3.631747\n",
      "[INFO] Epoch: 17 , batch: 36 , training loss: 3.702945\n",
      "[INFO] Epoch: 17 , batch: 37 , training loss: 3.571987\n",
      "[INFO] Epoch: 17 , batch: 38 , training loss: 3.693142\n",
      "[INFO] Epoch: 17 , batch: 39 , training loss: 3.467772\n",
      "[INFO] Epoch: 17 , batch: 40 , training loss: 3.675226\n",
      "[INFO] Epoch: 17 , batch: 41 , training loss: 3.678533\n",
      "[INFO] Epoch: 17 , batch: 42 , training loss: 4.132658\n",
      "[INFO] Epoch: 17 , batch: 43 , training loss: 3.841998\n",
      "[INFO] Epoch: 17 , batch: 44 , training loss: 4.230761\n",
      "[INFO] Epoch: 17 , batch: 45 , training loss: 4.174985\n",
      "[INFO] Epoch: 17 , batch: 46 , training loss: 4.174395\n",
      "[INFO] Epoch: 17 , batch: 47 , training loss: 3.798662\n",
      "[INFO] Epoch: 17 , batch: 48 , training loss: 3.725581\n",
      "[INFO] Epoch: 17 , batch: 49 , training loss: 3.963449\n",
      "[INFO] Epoch: 17 , batch: 50 , training loss: 3.724953\n",
      "[INFO] Epoch: 17 , batch: 51 , training loss: 3.904595\n",
      "[INFO] Epoch: 17 , batch: 52 , training loss: 3.768169\n",
      "[INFO] Epoch: 17 , batch: 53 , training loss: 3.873175\n",
      "[INFO] Epoch: 17 , batch: 54 , training loss: 3.891492\n",
      "[INFO] Epoch: 17 , batch: 55 , training loss: 3.996255\n",
      "[INFO] Epoch: 17 , batch: 56 , training loss: 3.806245\n",
      "[INFO] Epoch: 17 , batch: 57 , training loss: 3.732246\n",
      "[INFO] Epoch: 17 , batch: 58 , training loss: 3.799100\n",
      "[INFO] Epoch: 17 , batch: 59 , training loss: 3.880979\n",
      "[INFO] Epoch: 17 , batch: 60 , training loss: 3.811417\n",
      "[INFO] Epoch: 17 , batch: 61 , training loss: 3.868028\n",
      "[INFO] Epoch: 17 , batch: 62 , training loss: 3.775737\n",
      "[INFO] Epoch: 17 , batch: 63 , training loss: 3.927143\n",
      "[INFO] Epoch: 17 , batch: 64 , training loss: 4.178369\n",
      "[INFO] Epoch: 17 , batch: 65 , training loss: 3.811941\n",
      "[INFO] Epoch: 17 , batch: 66 , training loss: 3.703380\n",
      "[INFO] Epoch: 17 , batch: 67 , training loss: 3.685090\n",
      "[INFO] Epoch: 17 , batch: 68 , training loss: 3.921976\n",
      "[INFO] Epoch: 17 , batch: 69 , training loss: 3.822987\n",
      "[INFO] Epoch: 17 , batch: 70 , training loss: 4.026947\n",
      "[INFO] Epoch: 17 , batch: 71 , training loss: 3.877868\n",
      "[INFO] Epoch: 17 , batch: 72 , training loss: 3.947890\n",
      "[INFO] Epoch: 17 , batch: 73 , training loss: 3.895452\n",
      "[INFO] Epoch: 17 , batch: 74 , training loss: 3.986684\n",
      "[INFO] Epoch: 17 , batch: 75 , training loss: 3.815790\n",
      "[INFO] Epoch: 17 , batch: 76 , training loss: 3.954506\n",
      "[INFO] Epoch: 17 , batch: 77 , training loss: 3.876249\n",
      "[INFO] Epoch: 17 , batch: 78 , training loss: 3.971564\n",
      "[INFO] Epoch: 17 , batch: 79 , training loss: 3.827726\n",
      "[INFO] Epoch: 17 , batch: 80 , training loss: 4.012596\n",
      "[INFO] Epoch: 17 , batch: 81 , training loss: 3.964145\n",
      "[INFO] Epoch: 17 , batch: 82 , training loss: 3.966887\n",
      "[INFO] Epoch: 17 , batch: 83 , training loss: 4.071903\n",
      "[INFO] Epoch: 17 , batch: 84 , training loss: 3.977962\n",
      "[INFO] Epoch: 17 , batch: 85 , training loss: 4.093359\n",
      "[INFO] Epoch: 17 , batch: 86 , training loss: 4.006112\n",
      "[INFO] Epoch: 17 , batch: 87 , training loss: 3.991598\n",
      "[INFO] Epoch: 17 , batch: 88 , training loss: 4.146020\n",
      "[INFO] Epoch: 17 , batch: 89 , training loss: 3.925271\n",
      "[INFO] Epoch: 17 , batch: 90 , training loss: 4.008816\n",
      "[INFO] Epoch: 17 , batch: 91 , training loss: 3.931309\n",
      "[INFO] Epoch: 17 , batch: 92 , training loss: 3.957800\n",
      "[INFO] Epoch: 17 , batch: 93 , training loss: 4.048878\n",
      "[INFO] Epoch: 17 , batch: 94 , training loss: 4.165263\n",
      "[INFO] Epoch: 17 , batch: 95 , training loss: 3.950228\n",
      "[INFO] Epoch: 17 , batch: 96 , training loss: 3.939429\n",
      "[INFO] Epoch: 17 , batch: 97 , training loss: 3.910945\n",
      "[INFO] Epoch: 17 , batch: 98 , training loss: 3.832731\n",
      "[INFO] Epoch: 17 , batch: 99 , training loss: 3.948985\n",
      "[INFO] Epoch: 17 , batch: 100 , training loss: 3.795165\n",
      "[INFO] Epoch: 17 , batch: 101 , training loss: 3.854670\n",
      "[INFO] Epoch: 17 , batch: 102 , training loss: 4.021530\n",
      "[INFO] Epoch: 17 , batch: 103 , training loss: 3.810214\n",
      "[INFO] Epoch: 17 , batch: 104 , training loss: 3.752504\n",
      "[INFO] Epoch: 17 , batch: 105 , training loss: 4.007873\n",
      "[INFO] Epoch: 17 , batch: 106 , training loss: 4.020055\n",
      "[INFO] Epoch: 17 , batch: 107 , training loss: 3.869398\n",
      "[INFO] Epoch: 17 , batch: 108 , training loss: 3.816816\n",
      "[INFO] Epoch: 17 , batch: 109 , training loss: 3.730283\n",
      "[INFO] Epoch: 17 , batch: 110 , training loss: 3.924952\n",
      "[INFO] Epoch: 17 , batch: 111 , training loss: 3.996551\n",
      "[INFO] Epoch: 17 , batch: 112 , training loss: 3.936546\n",
      "[INFO] Epoch: 17 , batch: 113 , training loss: 3.917570\n",
      "[INFO] Epoch: 17 , batch: 114 , training loss: 3.911836\n",
      "[INFO] Epoch: 17 , batch: 115 , training loss: 3.933546\n",
      "[INFO] Epoch: 17 , batch: 116 , training loss: 3.809464\n",
      "[INFO] Epoch: 17 , batch: 117 , training loss: 4.052307\n",
      "[INFO] Epoch: 17 , batch: 118 , training loss: 4.012515\n",
      "[INFO] Epoch: 17 , batch: 119 , training loss: 4.174045\n",
      "[INFO] Epoch: 17 , batch: 120 , training loss: 4.138298\n",
      "[INFO] Epoch: 17 , batch: 121 , training loss: 4.007731\n",
      "[INFO] Epoch: 17 , batch: 122 , training loss: 3.878973\n",
      "[INFO] Epoch: 17 , batch: 123 , training loss: 3.936373\n",
      "[INFO] Epoch: 17 , batch: 124 , training loss: 4.037466\n",
      "[INFO] Epoch: 17 , batch: 125 , training loss: 3.781621\n",
      "[INFO] Epoch: 17 , batch: 126 , training loss: 3.831015\n",
      "[INFO] Epoch: 17 , batch: 127 , training loss: 3.857262\n",
      "[INFO] Epoch: 17 , batch: 128 , training loss: 3.978104\n",
      "[INFO] Epoch: 17 , batch: 129 , training loss: 3.927306\n",
      "[INFO] Epoch: 17 , batch: 130 , training loss: 3.909024\n",
      "[INFO] Epoch: 17 , batch: 131 , training loss: 3.940261\n",
      "[INFO] Epoch: 17 , batch: 132 , training loss: 3.951890\n",
      "[INFO] Epoch: 17 , batch: 133 , training loss: 3.912755\n",
      "[INFO] Epoch: 17 , batch: 134 , training loss: 3.660256\n",
      "[INFO] Epoch: 17 , batch: 135 , training loss: 3.750559\n",
      "[INFO] Epoch: 17 , batch: 136 , training loss: 4.033061\n",
      "[INFO] Epoch: 17 , batch: 137 , training loss: 3.943742\n",
      "[INFO] Epoch: 17 , batch: 138 , training loss: 4.002789\n",
      "[INFO] Epoch: 17 , batch: 139 , training loss: 4.571271\n",
      "[INFO] Epoch: 17 , batch: 140 , training loss: 4.394946\n",
      "[INFO] Epoch: 17 , batch: 141 , training loss: 4.162692\n",
      "[INFO] Epoch: 17 , batch: 142 , training loss: 3.847432\n",
      "[INFO] Epoch: 17 , batch: 143 , training loss: 3.980219\n",
      "[INFO] Epoch: 17 , batch: 144 , training loss: 3.829921\n",
      "[INFO] Epoch: 17 , batch: 145 , training loss: 3.896347\n",
      "[INFO] Epoch: 17 , batch: 146 , training loss: 4.074101\n",
      "[INFO] Epoch: 17 , batch: 147 , training loss: 3.758514\n",
      "[INFO] Epoch: 17 , batch: 148 , training loss: 3.757220\n",
      "[INFO] Epoch: 17 , batch: 149 , training loss: 3.819815\n",
      "[INFO] Epoch: 17 , batch: 150 , training loss: 4.084689\n",
      "[INFO] Epoch: 17 , batch: 151 , training loss: 3.902048\n",
      "[INFO] Epoch: 17 , batch: 152 , training loss: 3.921674\n",
      "[INFO] Epoch: 17 , batch: 153 , training loss: 3.962213\n",
      "[INFO] Epoch: 17 , batch: 154 , training loss: 4.033537\n",
      "[INFO] Epoch: 17 , batch: 155 , training loss: 4.230289\n",
      "[INFO] Epoch: 17 , batch: 156 , training loss: 4.011906\n",
      "[INFO] Epoch: 17 , batch: 157 , training loss: 3.944048\n",
      "[INFO] Epoch: 17 , batch: 158 , training loss: 4.135668\n",
      "[INFO] Epoch: 17 , batch: 159 , training loss: 4.020974\n",
      "[INFO] Epoch: 17 , batch: 160 , training loss: 4.337112\n",
      "[INFO] Epoch: 17 , batch: 161 , training loss: 4.421843\n",
      "[INFO] Epoch: 17 , batch: 162 , training loss: 4.341074\n",
      "[INFO] Epoch: 17 , batch: 163 , training loss: 4.553814\n",
      "[INFO] Epoch: 17 , batch: 164 , training loss: 4.427978\n",
      "[INFO] Epoch: 17 , batch: 165 , training loss: 4.372387\n",
      "[INFO] Epoch: 17 , batch: 166 , training loss: 4.280011\n",
      "[INFO] Epoch: 17 , batch: 167 , training loss: 4.513330\n",
      "[INFO] Epoch: 17 , batch: 168 , training loss: 4.163666\n",
      "[INFO] Epoch: 17 , batch: 169 , training loss: 4.142103\n",
      "[INFO] Epoch: 17 , batch: 170 , training loss: 4.274982\n",
      "[INFO] Epoch: 17 , batch: 171 , training loss: 3.703671\n",
      "[INFO] Epoch: 17 , batch: 172 , training loss: 3.908246\n",
      "[INFO] Epoch: 17 , batch: 173 , training loss: 4.177728\n",
      "[INFO] Epoch: 17 , batch: 174 , training loss: 4.604045\n",
      "[INFO] Epoch: 17 , batch: 175 , training loss: 4.901358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 17 , batch: 176 , training loss: 4.641814\n",
      "[INFO] Epoch: 17 , batch: 177 , training loss: 4.241554\n",
      "[INFO] Epoch: 17 , batch: 178 , training loss: 4.188927\n",
      "[INFO] Epoch: 17 , batch: 179 , training loss: 4.265892\n",
      "[INFO] Epoch: 17 , batch: 180 , training loss: 4.196024\n",
      "[INFO] Epoch: 17 , batch: 181 , training loss: 4.471216\n",
      "[INFO] Epoch: 17 , batch: 182 , training loss: 4.405558\n",
      "[INFO] Epoch: 17 , batch: 183 , training loss: 4.366345\n",
      "[INFO] Epoch: 17 , batch: 184 , training loss: 4.267008\n",
      "[INFO] Epoch: 17 , batch: 185 , training loss: 4.228224\n",
      "[INFO] Epoch: 17 , batch: 186 , training loss: 4.343516\n",
      "[INFO] Epoch: 17 , batch: 187 , training loss: 4.479158\n",
      "[INFO] Epoch: 17 , batch: 188 , training loss: 4.451609\n",
      "[INFO] Epoch: 17 , batch: 189 , training loss: 4.346868\n",
      "[INFO] Epoch: 17 , batch: 190 , training loss: 4.385687\n",
      "[INFO] Epoch: 17 , batch: 191 , training loss: 4.531826\n",
      "[INFO] Epoch: 17 , batch: 192 , training loss: 4.313005\n",
      "[INFO] Epoch: 17 , batch: 193 , training loss: 4.410621\n",
      "[INFO] Epoch: 17 , batch: 194 , training loss: 4.363777\n",
      "[INFO] Epoch: 17 , batch: 195 , training loss: 4.300131\n",
      "[INFO] Epoch: 17 , batch: 196 , training loss: 4.162261\n",
      "[INFO] Epoch: 17 , batch: 197 , training loss: 4.267707\n",
      "[INFO] Epoch: 17 , batch: 198 , training loss: 4.144414\n",
      "[INFO] Epoch: 17 , batch: 199 , training loss: 4.277966\n",
      "[INFO] Epoch: 17 , batch: 200 , training loss: 4.201049\n",
      "[INFO] Epoch: 17 , batch: 201 , training loss: 4.087579\n",
      "[INFO] Epoch: 17 , batch: 202 , training loss: 4.086180\n",
      "[INFO] Epoch: 17 , batch: 203 , training loss: 4.207699\n",
      "[INFO] Epoch: 17 , batch: 204 , training loss: 4.288905\n",
      "[INFO] Epoch: 17 , batch: 205 , training loss: 3.899550\n",
      "[INFO] Epoch: 17 , batch: 206 , training loss: 3.814586\n",
      "[INFO] Epoch: 17 , batch: 207 , training loss: 3.817736\n",
      "[INFO] Epoch: 17 , batch: 208 , training loss: 4.140298\n",
      "[INFO] Epoch: 17 , batch: 209 , training loss: 4.084920\n",
      "[INFO] Epoch: 17 , batch: 210 , training loss: 4.146059\n",
      "[INFO] Epoch: 17 , batch: 211 , training loss: 4.112165\n",
      "[INFO] Epoch: 17 , batch: 212 , training loss: 4.212043\n",
      "[INFO] Epoch: 17 , batch: 213 , training loss: 4.188201\n",
      "[INFO] Epoch: 17 , batch: 214 , training loss: 4.253595\n",
      "[INFO] Epoch: 17 , batch: 215 , training loss: 4.478851\n",
      "[INFO] Epoch: 17 , batch: 216 , training loss: 4.180029\n",
      "[INFO] Epoch: 17 , batch: 217 , training loss: 4.109027\n",
      "[INFO] Epoch: 17 , batch: 218 , training loss: 4.118048\n",
      "[INFO] Epoch: 17 , batch: 219 , training loss: 4.211184\n",
      "[INFO] Epoch: 17 , batch: 220 , training loss: 4.033892\n",
      "[INFO] Epoch: 17 , batch: 221 , training loss: 4.043187\n",
      "[INFO] Epoch: 17 , batch: 222 , training loss: 4.180957\n",
      "[INFO] Epoch: 17 , batch: 223 , training loss: 4.299057\n",
      "[INFO] Epoch: 17 , batch: 224 , training loss: 4.333296\n",
      "[INFO] Epoch: 17 , batch: 225 , training loss: 4.220829\n",
      "[INFO] Epoch: 17 , batch: 226 , training loss: 4.336275\n",
      "[INFO] Epoch: 17 , batch: 227 , training loss: 4.306360\n",
      "[INFO] Epoch: 17 , batch: 228 , training loss: 4.353421\n",
      "[INFO] Epoch: 17 , batch: 229 , training loss: 4.194643\n",
      "[INFO] Epoch: 17 , batch: 230 , training loss: 4.060791\n",
      "[INFO] Epoch: 17 , batch: 231 , training loss: 3.922915\n",
      "[INFO] Epoch: 17 , batch: 232 , training loss: 4.074508\n",
      "[INFO] Epoch: 17 , batch: 233 , training loss: 4.083860\n",
      "[INFO] Epoch: 17 , batch: 234 , training loss: 3.774764\n",
      "[INFO] Epoch: 17 , batch: 235 , training loss: 3.891646\n",
      "[INFO] Epoch: 17 , batch: 236 , training loss: 4.004587\n",
      "[INFO] Epoch: 17 , batch: 237 , training loss: 4.207224\n",
      "[INFO] Epoch: 17 , batch: 238 , training loss: 3.978604\n",
      "[INFO] Epoch: 17 , batch: 239 , training loss: 4.040475\n",
      "[INFO] Epoch: 17 , batch: 240 , training loss: 4.072987\n",
      "[INFO] Epoch: 17 , batch: 241 , training loss: 3.870936\n",
      "[INFO] Epoch: 17 , batch: 242 , training loss: 3.880285\n",
      "[INFO] Epoch: 17 , batch: 243 , training loss: 4.182594\n",
      "[INFO] Epoch: 17 , batch: 244 , training loss: 4.151607\n",
      "[INFO] Epoch: 17 , batch: 245 , training loss: 4.128209\n",
      "[INFO] Epoch: 17 , batch: 246 , training loss: 3.804073\n",
      "[INFO] Epoch: 17 , batch: 247 , training loss: 3.954910\n",
      "[INFO] Epoch: 17 , batch: 248 , training loss: 4.042684\n",
      "[INFO] Epoch: 17 , batch: 249 , training loss: 4.017900\n",
      "[INFO] Epoch: 17 , batch: 250 , training loss: 3.829956\n",
      "[INFO] Epoch: 17 , batch: 251 , training loss: 4.285448\n",
      "[INFO] Epoch: 17 , batch: 252 , training loss: 3.978166\n",
      "[INFO] Epoch: 17 , batch: 253 , training loss: 3.914710\n",
      "[INFO] Epoch: 17 , batch: 254 , training loss: 4.195446\n",
      "[INFO] Epoch: 17 , batch: 255 , training loss: 4.137047\n",
      "[INFO] Epoch: 17 , batch: 256 , training loss: 4.134585\n",
      "[INFO] Epoch: 17 , batch: 257 , training loss: 4.310604\n",
      "[INFO] Epoch: 17 , batch: 258 , training loss: 4.339659\n",
      "[INFO] Epoch: 17 , batch: 259 , training loss: 4.369049\n",
      "[INFO] Epoch: 17 , batch: 260 , training loss: 4.123541\n",
      "[INFO] Epoch: 17 , batch: 261 , training loss: 4.290420\n",
      "[INFO] Epoch: 17 , batch: 262 , training loss: 4.450770\n",
      "[INFO] Epoch: 17 , batch: 263 , training loss: 4.605143\n",
      "[INFO] Epoch: 17 , batch: 264 , training loss: 3.965507\n",
      "[INFO] Epoch: 17 , batch: 265 , training loss: 4.066113\n",
      "[INFO] Epoch: 17 , batch: 266 , training loss: 4.529914\n",
      "[INFO] Epoch: 17 , batch: 267 , training loss: 4.226229\n",
      "[INFO] Epoch: 17 , batch: 268 , training loss: 4.135114\n",
      "[INFO] Epoch: 17 , batch: 269 , training loss: 4.140038\n",
      "[INFO] Epoch: 17 , batch: 270 , training loss: 4.153884\n",
      "[INFO] Epoch: 17 , batch: 271 , training loss: 4.207258\n",
      "[INFO] Epoch: 17 , batch: 272 , training loss: 4.195005\n",
      "[INFO] Epoch: 17 , batch: 273 , training loss: 4.187783\n",
      "[INFO] Epoch: 17 , batch: 274 , training loss: 4.287747\n",
      "[INFO] Epoch: 17 , batch: 275 , training loss: 4.145216\n",
      "[INFO] Epoch: 17 , batch: 276 , training loss: 4.223719\n",
      "[INFO] Epoch: 17 , batch: 277 , training loss: 4.370578\n",
      "[INFO] Epoch: 17 , batch: 278 , training loss: 4.022599\n",
      "[INFO] Epoch: 17 , batch: 279 , training loss: 4.066689\n",
      "[INFO] Epoch: 17 , batch: 280 , training loss: 4.000347\n",
      "[INFO] Epoch: 17 , batch: 281 , training loss: 4.146930\n",
      "[INFO] Epoch: 17 , batch: 282 , training loss: 4.052916\n",
      "[INFO] Epoch: 17 , batch: 283 , training loss: 4.074169\n",
      "[INFO] Epoch: 17 , batch: 284 , training loss: 4.101985\n",
      "[INFO] Epoch: 17 , batch: 285 , training loss: 4.058439\n",
      "[INFO] Epoch: 17 , batch: 286 , training loss: 4.043476\n",
      "[INFO] Epoch: 17 , batch: 287 , training loss: 3.975879\n",
      "[INFO] Epoch: 17 , batch: 288 , training loss: 3.947578\n",
      "[INFO] Epoch: 17 , batch: 289 , training loss: 4.034875\n",
      "[INFO] Epoch: 17 , batch: 290 , training loss: 3.804661\n",
      "[INFO] Epoch: 17 , batch: 291 , training loss: 3.780855\n",
      "[INFO] Epoch: 17 , batch: 292 , training loss: 3.907392\n",
      "[INFO] Epoch: 17 , batch: 293 , training loss: 3.821978\n",
      "[INFO] Epoch: 17 , batch: 294 , training loss: 4.499794\n",
      "[INFO] Epoch: 17 , batch: 295 , training loss: 4.262559\n",
      "[INFO] Epoch: 17 , batch: 296 , training loss: 4.202690\n",
      "[INFO] Epoch: 17 , batch: 297 , training loss: 4.152590\n",
      "[INFO] Epoch: 17 , batch: 298 , training loss: 3.995150\n",
      "[INFO] Epoch: 17 , batch: 299 , training loss: 4.021956\n",
      "[INFO] Epoch: 17 , batch: 300 , training loss: 4.009272\n",
      "[INFO] Epoch: 17 , batch: 301 , training loss: 3.927928\n",
      "[INFO] Epoch: 17 , batch: 302 , training loss: 4.106417\n",
      "[INFO] Epoch: 17 , batch: 303 , training loss: 4.122275\n",
      "[INFO] Epoch: 17 , batch: 304 , training loss: 4.285670\n",
      "[INFO] Epoch: 17 , batch: 305 , training loss: 4.078909\n",
      "[INFO] Epoch: 17 , batch: 306 , training loss: 4.194329\n",
      "[INFO] Epoch: 17 , batch: 307 , training loss: 4.197386\n",
      "[INFO] Epoch: 17 , batch: 308 , training loss: 4.031611\n",
      "[INFO] Epoch: 17 , batch: 309 , training loss: 4.036191\n",
      "[INFO] Epoch: 17 , batch: 310 , training loss: 3.949668\n",
      "[INFO] Epoch: 17 , batch: 311 , training loss: 3.935633\n",
      "[INFO] Epoch: 17 , batch: 312 , training loss: 3.844808\n",
      "[INFO] Epoch: 17 , batch: 313 , training loss: 3.944454\n",
      "[INFO] Epoch: 17 , batch: 314 , training loss: 4.031974\n",
      "[INFO] Epoch: 17 , batch: 315 , training loss: 4.096376\n",
      "[INFO] Epoch: 17 , batch: 316 , training loss: 4.362212\n",
      "[INFO] Epoch: 17 , batch: 317 , training loss: 4.735279\n",
      "[INFO] Epoch: 17 , batch: 318 , training loss: 4.891599\n",
      "[INFO] Epoch: 17 , batch: 319 , training loss: 4.562587\n",
      "[INFO] Epoch: 17 , batch: 320 , training loss: 4.067705\n",
      "[INFO] Epoch: 17 , batch: 321 , training loss: 3.875495\n",
      "[INFO] Epoch: 17 , batch: 322 , training loss: 3.990659\n",
      "[INFO] Epoch: 17 , batch: 323 , training loss: 4.025935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 17 , batch: 324 , training loss: 4.002338\n",
      "[INFO] Epoch: 17 , batch: 325 , training loss: 4.131430\n",
      "[INFO] Epoch: 17 , batch: 326 , training loss: 4.170353\n",
      "[INFO] Epoch: 17 , batch: 327 , training loss: 4.109294\n",
      "[INFO] Epoch: 17 , batch: 328 , training loss: 4.104168\n",
      "[INFO] Epoch: 17 , batch: 329 , training loss: 4.028389\n",
      "[INFO] Epoch: 17 , batch: 330 , training loss: 4.017253\n",
      "[INFO] Epoch: 17 , batch: 331 , training loss: 4.174471\n",
      "[INFO] Epoch: 17 , batch: 332 , training loss: 4.008721\n",
      "[INFO] Epoch: 17 , batch: 333 , training loss: 3.991177\n",
      "[INFO] Epoch: 17 , batch: 334 , training loss: 3.995780\n",
      "[INFO] Epoch: 17 , batch: 335 , training loss: 4.138065\n",
      "[INFO] Epoch: 17 , batch: 336 , training loss: 4.130862\n",
      "[INFO] Epoch: 17 , batch: 337 , training loss: 4.201014\n",
      "[INFO] Epoch: 17 , batch: 338 , training loss: 4.387362\n",
      "[INFO] Epoch: 17 , batch: 339 , training loss: 4.208745\n",
      "[INFO] Epoch: 17 , batch: 340 , training loss: 4.400518\n",
      "[INFO] Epoch: 17 , batch: 341 , training loss: 4.163957\n",
      "[INFO] Epoch: 17 , batch: 342 , training loss: 3.926312\n",
      "[INFO] Epoch: 17 , batch: 343 , training loss: 4.002264\n",
      "[INFO] Epoch: 17 , batch: 344 , training loss: 3.868923\n",
      "[INFO] Epoch: 17 , batch: 345 , training loss: 4.002263\n",
      "[INFO] Epoch: 17 , batch: 346 , training loss: 4.045267\n",
      "[INFO] Epoch: 17 , batch: 347 , training loss: 3.965594\n",
      "[INFO] Epoch: 17 , batch: 348 , training loss: 4.078352\n",
      "[INFO] Epoch: 17 , batch: 349 , training loss: 4.185289\n",
      "[INFO] Epoch: 17 , batch: 350 , training loss: 4.007294\n",
      "[INFO] Epoch: 17 , batch: 351 , training loss: 4.093442\n",
      "[INFO] Epoch: 17 , batch: 352 , training loss: 4.092388\n",
      "[INFO] Epoch: 17 , batch: 353 , training loss: 4.072025\n",
      "[INFO] Epoch: 17 , batch: 354 , training loss: 4.175481\n",
      "[INFO] Epoch: 17 , batch: 355 , training loss: 4.163955\n",
      "[INFO] Epoch: 17 , batch: 356 , training loss: 4.048214\n",
      "[INFO] Epoch: 17 , batch: 357 , training loss: 4.127741\n",
      "[INFO] Epoch: 17 , batch: 358 , training loss: 4.040603\n",
      "[INFO] Epoch: 17 , batch: 359 , training loss: 4.036516\n",
      "[INFO] Epoch: 17 , batch: 360 , training loss: 4.110161\n",
      "[INFO] Epoch: 17 , batch: 361 , training loss: 4.090712\n",
      "[INFO] Epoch: 17 , batch: 362 , training loss: 4.195131\n",
      "[INFO] Epoch: 17 , batch: 363 , training loss: 4.073909\n",
      "[INFO] Epoch: 17 , batch: 364 , training loss: 4.139506\n",
      "[INFO] Epoch: 17 , batch: 365 , training loss: 4.028039\n",
      "[INFO] Epoch: 17 , batch: 366 , training loss: 4.147147\n",
      "[INFO] Epoch: 17 , batch: 367 , training loss: 4.201895\n",
      "[INFO] Epoch: 17 , batch: 368 , training loss: 4.656322\n",
      "[INFO] Epoch: 17 , batch: 369 , training loss: 4.332001\n",
      "[INFO] Epoch: 17 , batch: 370 , training loss: 4.061361\n",
      "[INFO] Epoch: 17 , batch: 371 , training loss: 4.499827\n",
      "[INFO] Epoch: 17 , batch: 372 , training loss: 4.790677\n",
      "[INFO] Epoch: 17 , batch: 373 , training loss: 4.836864\n",
      "[INFO] Epoch: 17 , batch: 374 , training loss: 4.945840\n",
      "[INFO] Epoch: 17 , batch: 375 , training loss: 4.904545\n",
      "[INFO] Epoch: 17 , batch: 376 , training loss: 4.813219\n",
      "[INFO] Epoch: 17 , batch: 377 , training loss: 4.542920\n",
      "[INFO] Epoch: 17 , batch: 378 , training loss: 4.625243\n",
      "[INFO] Epoch: 17 , batch: 379 , training loss: 4.616846\n",
      "[INFO] Epoch: 17 , batch: 380 , training loss: 4.742321\n",
      "[INFO] Epoch: 17 , batch: 381 , training loss: 4.482985\n",
      "[INFO] Epoch: 17 , batch: 382 , training loss: 4.763009\n",
      "[INFO] Epoch: 17 , batch: 383 , training loss: 4.772255\n",
      "[INFO] Epoch: 17 , batch: 384 , training loss: 4.796317\n",
      "[INFO] Epoch: 17 , batch: 385 , training loss: 4.471038\n",
      "[INFO] Epoch: 17 , batch: 386 , training loss: 4.727217\n",
      "[INFO] Epoch: 17 , batch: 387 , training loss: 4.659875\n",
      "[INFO] Epoch: 17 , batch: 388 , training loss: 4.473278\n",
      "[INFO] Epoch: 17 , batch: 389 , training loss: 4.315283\n",
      "[INFO] Epoch: 17 , batch: 390 , training loss: 4.287961\n",
      "[INFO] Epoch: 17 , batch: 391 , training loss: 4.302711\n",
      "[INFO] Epoch: 17 , batch: 392 , training loss: 4.666369\n",
      "[INFO] Epoch: 17 , batch: 393 , training loss: 4.583887\n",
      "[INFO] Epoch: 17 , batch: 394 , training loss: 4.654365\n",
      "[INFO] Epoch: 17 , batch: 395 , training loss: 4.476890\n",
      "[INFO] Epoch: 17 , batch: 396 , training loss: 4.276282\n",
      "[INFO] Epoch: 17 , batch: 397 , training loss: 4.428262\n",
      "[INFO] Epoch: 17 , batch: 398 , training loss: 4.296730\n",
      "[INFO] Epoch: 17 , batch: 399 , training loss: 4.340765\n",
      "[INFO] Epoch: 17 , batch: 400 , training loss: 4.350094\n",
      "[INFO] Epoch: 17 , batch: 401 , training loss: 4.809120\n",
      "[INFO] Epoch: 17 , batch: 402 , training loss: 4.485945\n",
      "[INFO] Epoch: 17 , batch: 403 , training loss: 4.325312\n",
      "[INFO] Epoch: 17 , batch: 404 , training loss: 4.476369\n",
      "[INFO] Epoch: 17 , batch: 405 , training loss: 4.532658\n",
      "[INFO] Epoch: 17 , batch: 406 , training loss: 4.454852\n",
      "[INFO] Epoch: 17 , batch: 407 , training loss: 4.484842\n",
      "[INFO] Epoch: 17 , batch: 408 , training loss: 4.426614\n",
      "[INFO] Epoch: 17 , batch: 409 , training loss: 4.446671\n",
      "[INFO] Epoch: 17 , batch: 410 , training loss: 4.527959\n",
      "[INFO] Epoch: 17 , batch: 411 , training loss: 4.704518\n",
      "[INFO] Epoch: 17 , batch: 412 , training loss: 4.526763\n",
      "[INFO] Epoch: 17 , batch: 413 , training loss: 4.383757\n",
      "[INFO] Epoch: 17 , batch: 414 , training loss: 4.420815\n",
      "[INFO] Epoch: 17 , batch: 415 , training loss: 4.460582\n",
      "[INFO] Epoch: 17 , batch: 416 , training loss: 4.550239\n",
      "[INFO] Epoch: 17 , batch: 417 , training loss: 4.434337\n",
      "[INFO] Epoch: 17 , batch: 418 , training loss: 4.493584\n",
      "[INFO] Epoch: 17 , batch: 419 , training loss: 4.420792\n",
      "[INFO] Epoch: 17 , batch: 420 , training loss: 4.419060\n",
      "[INFO] Epoch: 17 , batch: 421 , training loss: 4.417805\n",
      "[INFO] Epoch: 17 , batch: 422 , training loss: 4.253761\n",
      "[INFO] Epoch: 17 , batch: 423 , training loss: 4.466956\n",
      "[INFO] Epoch: 17 , batch: 424 , training loss: 4.635884\n",
      "[INFO] Epoch: 17 , batch: 425 , training loss: 4.529349\n",
      "[INFO] Epoch: 17 , batch: 426 , training loss: 4.215477\n",
      "[INFO] Epoch: 17 , batch: 427 , training loss: 4.486178\n",
      "[INFO] Epoch: 17 , batch: 428 , training loss: 4.351559\n",
      "[INFO] Epoch: 17 , batch: 429 , training loss: 4.256834\n",
      "[INFO] Epoch: 17 , batch: 430 , training loss: 4.490259\n",
      "[INFO] Epoch: 17 , batch: 431 , training loss: 4.105495\n",
      "[INFO] Epoch: 17 , batch: 432 , training loss: 4.155128\n",
      "[INFO] Epoch: 17 , batch: 433 , training loss: 4.185667\n",
      "[INFO] Epoch: 17 , batch: 434 , training loss: 4.063788\n",
      "[INFO] Epoch: 17 , batch: 435 , training loss: 4.414530\n",
      "[INFO] Epoch: 17 , batch: 436 , training loss: 4.482405\n",
      "[INFO] Epoch: 17 , batch: 437 , training loss: 4.257145\n",
      "[INFO] Epoch: 17 , batch: 438 , training loss: 4.105924\n",
      "[INFO] Epoch: 17 , batch: 439 , training loss: 4.336510\n",
      "[INFO] Epoch: 17 , batch: 440 , training loss: 4.466201\n",
      "[INFO] Epoch: 17 , batch: 441 , training loss: 4.536982\n",
      "[INFO] Epoch: 17 , batch: 442 , training loss: 4.300364\n",
      "[INFO] Epoch: 17 , batch: 443 , training loss: 4.506021\n",
      "[INFO] Epoch: 17 , batch: 444 , training loss: 4.099249\n",
      "[INFO] Epoch: 17 , batch: 445 , training loss: 4.033302\n",
      "[INFO] Epoch: 17 , batch: 446 , training loss: 3.955970\n",
      "[INFO] Epoch: 17 , batch: 447 , training loss: 4.134555\n",
      "[INFO] Epoch: 17 , batch: 448 , training loss: 4.289042\n",
      "[INFO] Epoch: 17 , batch: 449 , training loss: 4.647011\n",
      "[INFO] Epoch: 17 , batch: 450 , training loss: 4.726008\n",
      "[INFO] Epoch: 17 , batch: 451 , training loss: 4.614020\n",
      "[INFO] Epoch: 17 , batch: 452 , training loss: 4.417840\n",
      "[INFO] Epoch: 17 , batch: 453 , training loss: 4.192956\n",
      "[INFO] Epoch: 17 , batch: 454 , training loss: 4.342165\n",
      "[INFO] Epoch: 17 , batch: 455 , training loss: 4.390539\n",
      "[INFO] Epoch: 17 , batch: 456 , training loss: 4.382963\n",
      "[INFO] Epoch: 17 , batch: 457 , training loss: 4.463566\n",
      "[INFO] Epoch: 17 , batch: 458 , training loss: 4.204988\n",
      "[INFO] Epoch: 17 , batch: 459 , training loss: 4.175723\n",
      "[INFO] Epoch: 17 , batch: 460 , training loss: 4.298458\n",
      "[INFO] Epoch: 17 , batch: 461 , training loss: 4.271389\n",
      "[INFO] Epoch: 17 , batch: 462 , training loss: 4.326519\n",
      "[INFO] Epoch: 17 , batch: 463 , training loss: 4.232269\n",
      "[INFO] Epoch: 17 , batch: 464 , training loss: 4.396832\n",
      "[INFO] Epoch: 17 , batch: 465 , training loss: 4.357109\n",
      "[INFO] Epoch: 17 , batch: 466 , training loss: 4.435457\n",
      "[INFO] Epoch: 17 , batch: 467 , training loss: 4.402102\n",
      "[INFO] Epoch: 17 , batch: 468 , training loss: 4.383342\n",
      "[INFO] Epoch: 17 , batch: 469 , training loss: 4.430273\n",
      "[INFO] Epoch: 17 , batch: 470 , training loss: 4.203222\n",
      "[INFO] Epoch: 17 , batch: 471 , training loss: 4.320395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 17 , batch: 472 , training loss: 4.385079\n",
      "[INFO] Epoch: 17 , batch: 473 , training loss: 4.298803\n",
      "[INFO] Epoch: 17 , batch: 474 , training loss: 4.088262\n",
      "[INFO] Epoch: 17 , batch: 475 , training loss: 3.955725\n",
      "[INFO] Epoch: 17 , batch: 476 , training loss: 4.364964\n",
      "[INFO] Epoch: 17 , batch: 477 , training loss: 4.459463\n",
      "[INFO] Epoch: 17 , batch: 478 , training loss: 4.494148\n",
      "[INFO] Epoch: 17 , batch: 479 , training loss: 4.444752\n",
      "[INFO] Epoch: 17 , batch: 480 , training loss: 4.584808\n",
      "[INFO] Epoch: 17 , batch: 481 , training loss: 4.451155\n",
      "[INFO] Epoch: 17 , batch: 482 , training loss: 4.575809\n",
      "[INFO] Epoch: 17 , batch: 483 , training loss: 4.401030\n",
      "[INFO] Epoch: 17 , batch: 484 , training loss: 4.189614\n",
      "[INFO] Epoch: 17 , batch: 485 , training loss: 4.304293\n",
      "[INFO] Epoch: 17 , batch: 486 , training loss: 4.210945\n",
      "[INFO] Epoch: 17 , batch: 487 , training loss: 4.192517\n",
      "[INFO] Epoch: 17 , batch: 488 , training loss: 4.371974\n",
      "[INFO] Epoch: 17 , batch: 489 , training loss: 4.274631\n",
      "[INFO] Epoch: 17 , batch: 490 , training loss: 4.345046\n",
      "[INFO] Epoch: 17 , batch: 491 , training loss: 4.283671\n",
      "[INFO] Epoch: 17 , batch: 492 , training loss: 4.232402\n",
      "[INFO] Epoch: 17 , batch: 493 , training loss: 4.402396\n",
      "[INFO] Epoch: 17 , batch: 494 , training loss: 4.299626\n",
      "[INFO] Epoch: 17 , batch: 495 , training loss: 4.443474\n",
      "[INFO] Epoch: 17 , batch: 496 , training loss: 4.327839\n",
      "[INFO] Epoch: 17 , batch: 497 , training loss: 4.377525\n",
      "[INFO] Epoch: 17 , batch: 498 , training loss: 4.359636\n",
      "[INFO] Epoch: 17 , batch: 499 , training loss: 4.421521\n",
      "[INFO] Epoch: 17 , batch: 500 , training loss: 4.562444\n",
      "[INFO] Epoch: 17 , batch: 501 , training loss: 4.920693\n",
      "[INFO] Epoch: 17 , batch: 502 , training loss: 4.999894\n",
      "[INFO] Epoch: 17 , batch: 503 , training loss: 4.705634\n",
      "[INFO] Epoch: 17 , batch: 504 , training loss: 4.839439\n",
      "[INFO] Epoch: 17 , batch: 505 , training loss: 4.778988\n",
      "[INFO] Epoch: 17 , batch: 506 , training loss: 4.720151\n",
      "[INFO] Epoch: 17 , batch: 507 , training loss: 4.783203\n",
      "[INFO] Epoch: 17 , batch: 508 , training loss: 4.715466\n",
      "[INFO] Epoch: 17 , batch: 509 , training loss: 4.507985\n",
      "[INFO] Epoch: 17 , batch: 510 , training loss: 4.575057\n",
      "[INFO] Epoch: 17 , batch: 511 , training loss: 4.478723\n",
      "[INFO] Epoch: 17 , batch: 512 , training loss: 4.546905\n",
      "[INFO] Epoch: 17 , batch: 513 , training loss: 4.819932\n",
      "[INFO] Epoch: 17 , batch: 514 , training loss: 4.474835\n",
      "[INFO] Epoch: 17 , batch: 515 , training loss: 4.732776\n",
      "[INFO] Epoch: 17 , batch: 516 , training loss: 4.515784\n",
      "[INFO] Epoch: 17 , batch: 517 , training loss: 4.472343\n",
      "[INFO] Epoch: 17 , batch: 518 , training loss: 4.435063\n",
      "[INFO] Epoch: 17 , batch: 519 , training loss: 4.288399\n",
      "[INFO] Epoch: 17 , batch: 520 , training loss: 4.540879\n",
      "[INFO] Epoch: 17 , batch: 521 , training loss: 4.508548\n",
      "[INFO] Epoch: 17 , batch: 522 , training loss: 4.593649\n",
      "[INFO] Epoch: 17 , batch: 523 , training loss: 4.483773\n",
      "[INFO] Epoch: 17 , batch: 524 , training loss: 4.773231\n",
      "[INFO] Epoch: 17 , batch: 525 , training loss: 4.651782\n",
      "[INFO] Epoch: 17 , batch: 526 , training loss: 4.441991\n",
      "[INFO] Epoch: 17 , batch: 527 , training loss: 4.491511\n",
      "[INFO] Epoch: 17 , batch: 528 , training loss: 4.489037\n",
      "[INFO] Epoch: 17 , batch: 529 , training loss: 4.465073\n",
      "[INFO] Epoch: 17 , batch: 530 , training loss: 4.327175\n",
      "[INFO] Epoch: 17 , batch: 531 , training loss: 4.473464\n",
      "[INFO] Epoch: 17 , batch: 532 , training loss: 4.365841\n",
      "[INFO] Epoch: 17 , batch: 533 , training loss: 4.519217\n",
      "[INFO] Epoch: 17 , batch: 534 , training loss: 4.513240\n",
      "[INFO] Epoch: 17 , batch: 535 , training loss: 4.502511\n",
      "[INFO] Epoch: 17 , batch: 536 , training loss: 4.365159\n",
      "[INFO] Epoch: 17 , batch: 537 , training loss: 4.319317\n",
      "[INFO] Epoch: 17 , batch: 538 , training loss: 4.425477\n",
      "[INFO] Epoch: 17 , batch: 539 , training loss: 4.552879\n",
      "[INFO] Epoch: 17 , batch: 540 , training loss: 5.099895\n",
      "[INFO] Epoch: 17 , batch: 541 , training loss: 4.958856\n",
      "[INFO] Epoch: 17 , batch: 542 , training loss: 4.804588\n",
      "[INFO] Epoch: 18 , batch: 0 , training loss: 3.993874\n",
      "[INFO] Epoch: 18 , batch: 1 , training loss: 3.743414\n",
      "[INFO] Epoch: 18 , batch: 2 , training loss: 3.837656\n",
      "[INFO] Epoch: 18 , batch: 3 , training loss: 3.706578\n",
      "[INFO] Epoch: 18 , batch: 4 , training loss: 4.036010\n",
      "[INFO] Epoch: 18 , batch: 5 , training loss: 3.721905\n",
      "[INFO] Epoch: 18 , batch: 6 , training loss: 4.057262\n",
      "[INFO] Epoch: 18 , batch: 7 , training loss: 3.935232\n",
      "[INFO] Epoch: 18 , batch: 8 , training loss: 3.602005\n",
      "[INFO] Epoch: 18 , batch: 9 , training loss: 3.867053\n",
      "[INFO] Epoch: 18 , batch: 10 , training loss: 3.772092\n",
      "[INFO] Epoch: 18 , batch: 11 , training loss: 3.728683\n",
      "[INFO] Epoch: 18 , batch: 12 , training loss: 3.619786\n",
      "[INFO] Epoch: 18 , batch: 13 , training loss: 3.679100\n",
      "[INFO] Epoch: 18 , batch: 14 , training loss: 3.548335\n",
      "[INFO] Epoch: 18 , batch: 15 , training loss: 3.788378\n",
      "[INFO] Epoch: 18 , batch: 16 , training loss: 3.653701\n",
      "[INFO] Epoch: 18 , batch: 17 , training loss: 3.784351\n",
      "[INFO] Epoch: 18 , batch: 18 , training loss: 3.684410\n",
      "[INFO] Epoch: 18 , batch: 19 , training loss: 3.477216\n",
      "[INFO] Epoch: 18 , batch: 20 , training loss: 3.409046\n",
      "[INFO] Epoch: 18 , batch: 21 , training loss: 3.605186\n",
      "[INFO] Epoch: 18 , batch: 22 , training loss: 3.503215\n",
      "[INFO] Epoch: 18 , batch: 23 , training loss: 3.679638\n",
      "[INFO] Epoch: 18 , batch: 24 , training loss: 3.559272\n",
      "[INFO] Epoch: 18 , batch: 25 , training loss: 3.672242\n",
      "[INFO] Epoch: 18 , batch: 26 , training loss: 3.533378\n",
      "[INFO] Epoch: 18 , batch: 27 , training loss: 3.479649\n",
      "[INFO] Epoch: 18 , batch: 28 , training loss: 3.720274\n",
      "[INFO] Epoch: 18 , batch: 29 , training loss: 3.533261\n",
      "[INFO] Epoch: 18 , batch: 30 , training loss: 3.512135\n",
      "[INFO] Epoch: 18 , batch: 31 , training loss: 3.649938\n",
      "[INFO] Epoch: 18 , batch: 32 , training loss: 3.602249\n",
      "[INFO] Epoch: 18 , batch: 33 , training loss: 3.684247\n",
      "[INFO] Epoch: 18 , batch: 34 , training loss: 3.680841\n",
      "[INFO] Epoch: 18 , batch: 35 , training loss: 3.578315\n",
      "[INFO] Epoch: 18 , batch: 36 , training loss: 3.693862\n",
      "[INFO] Epoch: 18 , batch: 37 , training loss: 3.534203\n",
      "[INFO] Epoch: 18 , batch: 38 , training loss: 3.636007\n",
      "[INFO] Epoch: 18 , batch: 39 , training loss: 3.442823\n",
      "[INFO] Epoch: 18 , batch: 40 , training loss: 3.647920\n",
      "[INFO] Epoch: 18 , batch: 41 , training loss: 3.631263\n",
      "[INFO] Epoch: 18 , batch: 42 , training loss: 4.124944\n",
      "[INFO] Epoch: 18 , batch: 43 , training loss: 3.837008\n",
      "[INFO] Epoch: 18 , batch: 44 , training loss: 4.178256\n",
      "[INFO] Epoch: 18 , batch: 45 , training loss: 4.160381\n",
      "[INFO] Epoch: 18 , batch: 46 , training loss: 4.187550\n",
      "[INFO] Epoch: 18 , batch: 47 , training loss: 3.799009\n",
      "[INFO] Epoch: 18 , batch: 48 , training loss: 3.708544\n",
      "[INFO] Epoch: 18 , batch: 49 , training loss: 3.951333\n",
      "[INFO] Epoch: 18 , batch: 50 , training loss: 3.692514\n",
      "[INFO] Epoch: 18 , batch: 51 , training loss: 3.890461\n",
      "[INFO] Epoch: 18 , batch: 52 , training loss: 3.738722\n",
      "[INFO] Epoch: 18 , batch: 53 , training loss: 3.858315\n",
      "[INFO] Epoch: 18 , batch: 54 , training loss: 3.875889\n",
      "[INFO] Epoch: 18 , batch: 55 , training loss: 3.965902\n",
      "[INFO] Epoch: 18 , batch: 56 , training loss: 3.790135\n",
      "[INFO] Epoch: 18 , batch: 57 , training loss: 3.733643\n",
      "[INFO] Epoch: 18 , batch: 58 , training loss: 3.761364\n",
      "[INFO] Epoch: 18 , batch: 59 , training loss: 3.839707\n",
      "[INFO] Epoch: 18 , batch: 60 , training loss: 3.794140\n",
      "[INFO] Epoch: 18 , batch: 61 , training loss: 3.860787\n",
      "[INFO] Epoch: 18 , batch: 62 , training loss: 3.721350\n",
      "[INFO] Epoch: 18 , batch: 63 , training loss: 3.908727\n",
      "[INFO] Epoch: 18 , batch: 64 , training loss: 4.158695\n",
      "[INFO] Epoch: 18 , batch: 65 , training loss: 3.804627\n",
      "[INFO] Epoch: 18 , batch: 66 , training loss: 3.697650\n",
      "[INFO] Epoch: 18 , batch: 67 , training loss: 3.654078\n",
      "[INFO] Epoch: 18 , batch: 68 , training loss: 3.915593\n",
      "[INFO] Epoch: 18 , batch: 69 , training loss: 3.802457\n",
      "[INFO] Epoch: 18 , batch: 70 , training loss: 4.023744\n",
      "[INFO] Epoch: 18 , batch: 71 , training loss: 3.857852\n",
      "[INFO] Epoch: 18 , batch: 72 , training loss: 3.931887\n",
      "[INFO] Epoch: 18 , batch: 73 , training loss: 3.856192\n",
      "[INFO] Epoch: 18 , batch: 74 , training loss: 3.959751\n",
      "[INFO] Epoch: 18 , batch: 75 , training loss: 3.819169\n",
      "[INFO] Epoch: 18 , batch: 76 , training loss: 3.970962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 18 , batch: 77 , training loss: 3.869779\n",
      "[INFO] Epoch: 18 , batch: 78 , training loss: 3.972852\n",
      "[INFO] Epoch: 18 , batch: 79 , training loss: 3.808974\n",
      "[INFO] Epoch: 18 , batch: 80 , training loss: 4.029937\n",
      "[INFO] Epoch: 18 , batch: 81 , training loss: 3.935261\n",
      "[INFO] Epoch: 18 , batch: 82 , training loss: 3.933433\n",
      "[INFO] Epoch: 18 , batch: 83 , training loss: 4.035532\n",
      "[INFO] Epoch: 18 , batch: 84 , training loss: 3.976180\n",
      "[INFO] Epoch: 18 , batch: 85 , training loss: 4.059702\n",
      "[INFO] Epoch: 18 , batch: 86 , training loss: 3.996663\n",
      "[INFO] Epoch: 18 , batch: 87 , training loss: 3.960926\n",
      "[INFO] Epoch: 18 , batch: 88 , training loss: 4.102168\n",
      "[INFO] Epoch: 18 , batch: 89 , training loss: 3.910036\n",
      "[INFO] Epoch: 18 , batch: 90 , training loss: 3.984998\n",
      "[INFO] Epoch: 18 , batch: 91 , training loss: 3.922989\n",
      "[INFO] Epoch: 18 , batch: 92 , training loss: 3.926777\n",
      "[INFO] Epoch: 18 , batch: 93 , training loss: 4.027844\n",
      "[INFO] Epoch: 18 , batch: 94 , training loss: 4.176879\n",
      "[INFO] Epoch: 18 , batch: 95 , training loss: 3.951758\n",
      "[INFO] Epoch: 18 , batch: 96 , training loss: 3.896530\n",
      "[INFO] Epoch: 18 , batch: 97 , training loss: 3.891649\n",
      "[INFO] Epoch: 18 , batch: 98 , training loss: 3.799544\n",
      "[INFO] Epoch: 18 , batch: 99 , training loss: 3.928410\n",
      "[INFO] Epoch: 18 , batch: 100 , training loss: 3.787178\n",
      "[INFO] Epoch: 18 , batch: 101 , training loss: 3.849576\n",
      "[INFO] Epoch: 18 , batch: 102 , training loss: 4.017290\n",
      "[INFO] Epoch: 18 , batch: 103 , training loss: 3.793356\n",
      "[INFO] Epoch: 18 , batch: 104 , training loss: 3.746630\n",
      "[INFO] Epoch: 18 , batch: 105 , training loss: 3.973569\n",
      "[INFO] Epoch: 18 , batch: 106 , training loss: 4.039738\n",
      "[INFO] Epoch: 18 , batch: 107 , training loss: 3.845787\n",
      "[INFO] Epoch: 18 , batch: 108 , training loss: 3.783749\n",
      "[INFO] Epoch: 18 , batch: 109 , training loss: 3.722162\n",
      "[INFO] Epoch: 18 , batch: 110 , training loss: 3.911634\n",
      "[INFO] Epoch: 18 , batch: 111 , training loss: 3.983531\n",
      "[INFO] Epoch: 18 , batch: 112 , training loss: 3.902156\n",
      "[INFO] Epoch: 18 , batch: 113 , training loss: 3.900395\n",
      "[INFO] Epoch: 18 , batch: 114 , training loss: 3.935300\n",
      "[INFO] Epoch: 18 , batch: 115 , training loss: 3.919645\n",
      "[INFO] Epoch: 18 , batch: 116 , training loss: 3.791937\n",
      "[INFO] Epoch: 18 , batch: 117 , training loss: 4.043591\n",
      "[INFO] Epoch: 18 , batch: 118 , training loss: 3.976796\n",
      "[INFO] Epoch: 18 , batch: 119 , training loss: 4.172453\n",
      "[INFO] Epoch: 18 , batch: 120 , training loss: 4.142589\n",
      "[INFO] Epoch: 18 , batch: 121 , training loss: 3.992518\n",
      "[INFO] Epoch: 18 , batch: 122 , training loss: 3.878299\n",
      "[INFO] Epoch: 18 , batch: 123 , training loss: 3.920781\n",
      "[INFO] Epoch: 18 , batch: 124 , training loss: 4.027566\n",
      "[INFO] Epoch: 18 , batch: 125 , training loss: 3.782585\n",
      "[INFO] Epoch: 18 , batch: 126 , training loss: 3.820025\n",
      "[INFO] Epoch: 18 , batch: 127 , training loss: 3.824389\n",
      "[INFO] Epoch: 18 , batch: 128 , training loss: 3.974464\n",
      "[INFO] Epoch: 18 , batch: 129 , training loss: 3.923055\n",
      "[INFO] Epoch: 18 , batch: 130 , training loss: 3.904164\n",
      "[INFO] Epoch: 18 , batch: 131 , training loss: 3.920156\n",
      "[INFO] Epoch: 18 , batch: 132 , training loss: 3.948362\n",
      "[INFO] Epoch: 18 , batch: 133 , training loss: 3.904968\n",
      "[INFO] Epoch: 18 , batch: 134 , training loss: 3.660864\n",
      "[INFO] Epoch: 18 , batch: 135 , training loss: 3.736869\n",
      "[INFO] Epoch: 18 , batch: 136 , training loss: 4.021722\n",
      "[INFO] Epoch: 18 , batch: 137 , training loss: 3.942061\n",
      "[INFO] Epoch: 18 , batch: 138 , training loss: 3.999231\n",
      "[INFO] Epoch: 18 , batch: 139 , training loss: 4.584148\n",
      "[INFO] Epoch: 18 , batch: 140 , training loss: 4.406472\n",
      "[INFO] Epoch: 18 , batch: 141 , training loss: 4.132550\n",
      "[INFO] Epoch: 18 , batch: 142 , training loss: 3.866994\n",
      "[INFO] Epoch: 18 , batch: 143 , training loss: 3.976897\n",
      "[INFO] Epoch: 18 , batch: 144 , training loss: 3.828778\n",
      "[INFO] Epoch: 18 , batch: 145 , training loss: 3.890412\n",
      "[INFO] Epoch: 18 , batch: 146 , training loss: 4.089657\n",
      "[INFO] Epoch: 18 , batch: 147 , training loss: 3.736014\n",
      "[INFO] Epoch: 18 , batch: 148 , training loss: 3.745900\n",
      "[INFO] Epoch: 18 , batch: 149 , training loss: 3.841871\n",
      "[INFO] Epoch: 18 , batch: 150 , training loss: 4.072697\n",
      "[INFO] Epoch: 18 , batch: 151 , training loss: 3.897786\n",
      "[INFO] Epoch: 18 , batch: 152 , training loss: 3.898066\n",
      "[INFO] Epoch: 18 , batch: 153 , training loss: 3.951717\n",
      "[INFO] Epoch: 18 , batch: 154 , training loss: 4.027828\n",
      "[INFO] Epoch: 18 , batch: 155 , training loss: 4.269052\n",
      "[INFO] Epoch: 18 , batch: 156 , training loss: 4.015184\n",
      "[INFO] Epoch: 18 , batch: 157 , training loss: 3.949105\n",
      "[INFO] Epoch: 18 , batch: 158 , training loss: 4.117458\n",
      "[INFO] Epoch: 18 , batch: 159 , training loss: 4.039176\n",
      "[INFO] Epoch: 18 , batch: 160 , training loss: 4.365297\n",
      "[INFO] Epoch: 18 , batch: 161 , training loss: 4.385764\n",
      "[INFO] Epoch: 18 , batch: 162 , training loss: 4.332143\n",
      "[INFO] Epoch: 18 , batch: 163 , training loss: 4.523553\n",
      "[INFO] Epoch: 18 , batch: 164 , training loss: 4.433011\n",
      "[INFO] Epoch: 18 , batch: 165 , training loss: 4.367020\n",
      "[INFO] Epoch: 18 , batch: 166 , training loss: 4.288115\n",
      "[INFO] Epoch: 18 , batch: 167 , training loss: 4.489275\n",
      "[INFO] Epoch: 18 , batch: 168 , training loss: 4.117269\n",
      "[INFO] Epoch: 18 , batch: 169 , training loss: 4.111864\n",
      "[INFO] Epoch: 18 , batch: 170 , training loss: 4.261512\n",
      "[INFO] Epoch: 18 , batch: 171 , training loss: 3.681554\n",
      "[INFO] Epoch: 18 , batch: 172 , training loss: 3.877337\n",
      "[INFO] Epoch: 18 , batch: 173 , training loss: 4.208467\n",
      "[INFO] Epoch: 18 , batch: 174 , training loss: 4.609612\n",
      "[INFO] Epoch: 18 , batch: 175 , training loss: 4.893231\n",
      "[INFO] Epoch: 18 , batch: 176 , training loss: 4.618539\n",
      "[INFO] Epoch: 18 , batch: 177 , training loss: 4.233376\n",
      "[INFO] Epoch: 18 , batch: 178 , training loss: 4.202242\n",
      "[INFO] Epoch: 18 , batch: 179 , training loss: 4.265641\n",
      "[INFO] Epoch: 18 , batch: 180 , training loss: 4.212149\n",
      "[INFO] Epoch: 18 , batch: 181 , training loss: 4.467082\n",
      "[INFO] Epoch: 18 , batch: 182 , training loss: 4.394157\n",
      "[INFO] Epoch: 18 , batch: 183 , training loss: 4.369490\n",
      "[INFO] Epoch: 18 , batch: 184 , training loss: 4.253891\n",
      "[INFO] Epoch: 18 , batch: 185 , training loss: 4.223555\n",
      "[INFO] Epoch: 18 , batch: 186 , training loss: 4.346508\n",
      "[INFO] Epoch: 18 , batch: 187 , training loss: 4.459810\n",
      "[INFO] Epoch: 18 , batch: 188 , training loss: 4.443484\n",
      "[INFO] Epoch: 18 , batch: 189 , training loss: 4.334707\n",
      "[INFO] Epoch: 18 , batch: 190 , training loss: 4.373823\n",
      "[INFO] Epoch: 18 , batch: 191 , training loss: 4.492349\n",
      "[INFO] Epoch: 18 , batch: 192 , training loss: 4.314494\n",
      "[INFO] Epoch: 18 , batch: 193 , training loss: 4.412877\n",
      "[INFO] Epoch: 18 , batch: 194 , training loss: 4.347002\n",
      "[INFO] Epoch: 18 , batch: 195 , training loss: 4.300615\n",
      "[INFO] Epoch: 18 , batch: 196 , training loss: 4.154354\n",
      "[INFO] Epoch: 18 , batch: 197 , training loss: 4.255077\n",
      "[INFO] Epoch: 18 , batch: 198 , training loss: 4.132380\n",
      "[INFO] Epoch: 18 , batch: 199 , training loss: 4.267498\n",
      "[INFO] Epoch: 18 , batch: 200 , training loss: 4.188221\n",
      "[INFO] Epoch: 18 , batch: 201 , training loss: 4.087618\n",
      "[INFO] Epoch: 18 , batch: 202 , training loss: 4.068426\n",
      "[INFO] Epoch: 18 , batch: 203 , training loss: 4.206114\n",
      "[INFO] Epoch: 18 , batch: 204 , training loss: 4.276979\n",
      "[INFO] Epoch: 18 , batch: 205 , training loss: 3.890784\n",
      "[INFO] Epoch: 18 , batch: 206 , training loss: 3.807415\n",
      "[INFO] Epoch: 18 , batch: 207 , training loss: 3.788193\n",
      "[INFO] Epoch: 18 , batch: 208 , training loss: 4.137816\n",
      "[INFO] Epoch: 18 , batch: 209 , training loss: 4.093927\n",
      "[INFO] Epoch: 18 , batch: 210 , training loss: 4.114183\n",
      "[INFO] Epoch: 18 , batch: 211 , training loss: 4.119697\n",
      "[INFO] Epoch: 18 , batch: 212 , training loss: 4.224560\n",
      "[INFO] Epoch: 18 , batch: 213 , training loss: 4.178679\n",
      "[INFO] Epoch: 18 , batch: 214 , training loss: 4.256498\n",
      "[INFO] Epoch: 18 , batch: 215 , training loss: 4.472775\n",
      "[INFO] Epoch: 18 , batch: 216 , training loss: 4.168586\n",
      "[INFO] Epoch: 18 , batch: 217 , training loss: 4.106965\n",
      "[INFO] Epoch: 18 , batch: 218 , training loss: 4.112650\n",
      "[INFO] Epoch: 18 , batch: 219 , training loss: 4.196530\n",
      "[INFO] Epoch: 18 , batch: 220 , training loss: 4.031341\n",
      "[INFO] Epoch: 18 , batch: 221 , training loss: 4.029709\n",
      "[INFO] Epoch: 18 , batch: 222 , training loss: 4.175667\n",
      "[INFO] Epoch: 18 , batch: 223 , training loss: 4.301670\n",
      "[INFO] Epoch: 18 , batch: 224 , training loss: 4.325599\n",
      "[INFO] Epoch: 18 , batch: 225 , training loss: 4.199336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 18 , batch: 226 , training loss: 4.324953\n",
      "[INFO] Epoch: 18 , batch: 227 , training loss: 4.301498\n",
      "[INFO] Epoch: 18 , batch: 228 , training loss: 4.344481\n",
      "[INFO] Epoch: 18 , batch: 229 , training loss: 4.183723\n",
      "[INFO] Epoch: 18 , batch: 230 , training loss: 4.070456\n",
      "[INFO] Epoch: 18 , batch: 231 , training loss: 3.917375\n",
      "[INFO] Epoch: 18 , batch: 232 , training loss: 4.060206\n",
      "[INFO] Epoch: 18 , batch: 233 , training loss: 4.087814\n",
      "[INFO] Epoch: 18 , batch: 234 , training loss: 3.770297\n",
      "[INFO] Epoch: 18 , batch: 235 , training loss: 3.900342\n",
      "[INFO] Epoch: 18 , batch: 236 , training loss: 4.005104\n",
      "[INFO] Epoch: 18 , batch: 237 , training loss: 4.211483\n",
      "[INFO] Epoch: 18 , batch: 238 , training loss: 3.982423\n",
      "[INFO] Epoch: 18 , batch: 239 , training loss: 4.020799\n",
      "[INFO] Epoch: 18 , batch: 240 , training loss: 4.057004\n",
      "[INFO] Epoch: 18 , batch: 241 , training loss: 3.868600\n",
      "[INFO] Epoch: 18 , batch: 242 , training loss: 3.887960\n",
      "[INFO] Epoch: 18 , batch: 243 , training loss: 4.159646\n",
      "[INFO] Epoch: 18 , batch: 244 , training loss: 4.118848\n",
      "[INFO] Epoch: 18 , batch: 245 , training loss: 4.113323\n",
      "[INFO] Epoch: 18 , batch: 246 , training loss: 3.790637\n",
      "[INFO] Epoch: 18 , batch: 247 , training loss: 3.953497\n",
      "[INFO] Epoch: 18 , batch: 248 , training loss: 4.038654\n",
      "[INFO] Epoch: 18 , batch: 249 , training loss: 4.018543\n",
      "[INFO] Epoch: 18 , batch: 250 , training loss: 3.813255\n",
      "[INFO] Epoch: 18 , batch: 251 , training loss: 4.284016\n",
      "[INFO] Epoch: 18 , batch: 252 , training loss: 3.968848\n",
      "[INFO] Epoch: 18 , batch: 253 , training loss: 3.908194\n",
      "[INFO] Epoch: 18 , batch: 254 , training loss: 4.190270\n",
      "[INFO] Epoch: 18 , batch: 255 , training loss: 4.137069\n",
      "[INFO] Epoch: 18 , batch: 256 , training loss: 4.130741\n",
      "[INFO] Epoch: 18 , batch: 257 , training loss: 4.305242\n",
      "[INFO] Epoch: 18 , batch: 258 , training loss: 4.305385\n",
      "[INFO] Epoch: 18 , batch: 259 , training loss: 4.345227\n",
      "[INFO] Epoch: 18 , batch: 260 , training loss: 4.115552\n",
      "[INFO] Epoch: 18 , batch: 261 , training loss: 4.280630\n",
      "[INFO] Epoch: 18 , batch: 262 , training loss: 4.456997\n",
      "[INFO] Epoch: 18 , batch: 263 , training loss: 4.586863\n",
      "[INFO] Epoch: 18 , batch: 264 , training loss: 3.956426\n",
      "[INFO] Epoch: 18 , batch: 265 , training loss: 4.073479\n",
      "[INFO] Epoch: 18 , batch: 266 , training loss: 4.528079\n",
      "[INFO] Epoch: 18 , batch: 267 , training loss: 4.230818\n",
      "[INFO] Epoch: 18 , batch: 268 , training loss: 4.133440\n",
      "[INFO] Epoch: 18 , batch: 269 , training loss: 4.142909\n",
      "[INFO] Epoch: 18 , batch: 270 , training loss: 4.161406\n",
      "[INFO] Epoch: 18 , batch: 271 , training loss: 4.188068\n",
      "[INFO] Epoch: 18 , batch: 272 , training loss: 4.181768\n",
      "[INFO] Epoch: 18 , batch: 273 , training loss: 4.179702\n",
      "[INFO] Epoch: 18 , batch: 274 , training loss: 4.295216\n",
      "[INFO] Epoch: 18 , batch: 275 , training loss: 4.134424\n",
      "[INFO] Epoch: 18 , batch: 276 , training loss: 4.207098\n",
      "[INFO] Epoch: 18 , batch: 277 , training loss: 4.363795\n",
      "[INFO] Epoch: 18 , batch: 278 , training loss: 4.011942\n",
      "[INFO] Epoch: 18 , batch: 279 , training loss: 4.042190\n",
      "[INFO] Epoch: 18 , batch: 280 , training loss: 3.991576\n",
      "[INFO] Epoch: 18 , batch: 281 , training loss: 4.128731\n",
      "[INFO] Epoch: 18 , batch: 282 , training loss: 4.031650\n",
      "[INFO] Epoch: 18 , batch: 283 , training loss: 4.062066\n",
      "[INFO] Epoch: 18 , batch: 284 , training loss: 4.103674\n",
      "[INFO] Epoch: 18 , batch: 285 , training loss: 4.056715\n",
      "[INFO] Epoch: 18 , batch: 286 , training loss: 4.035100\n",
      "[INFO] Epoch: 18 , batch: 287 , training loss: 3.971707\n",
      "[INFO] Epoch: 18 , batch: 288 , training loss: 3.939026\n",
      "[INFO] Epoch: 18 , batch: 289 , training loss: 4.038604\n",
      "[INFO] Epoch: 18 , batch: 290 , training loss: 3.791442\n",
      "[INFO] Epoch: 18 , batch: 291 , training loss: 3.774634\n",
      "[INFO] Epoch: 18 , batch: 292 , training loss: 3.901342\n",
      "[INFO] Epoch: 18 , batch: 293 , training loss: 3.819532\n",
      "[INFO] Epoch: 18 , batch: 294 , training loss: 4.490932\n",
      "[INFO] Epoch: 18 , batch: 295 , training loss: 4.249647\n",
      "[INFO] Epoch: 18 , batch: 296 , training loss: 4.201946\n",
      "[INFO] Epoch: 18 , batch: 297 , training loss: 4.150298\n",
      "[INFO] Epoch: 18 , batch: 298 , training loss: 3.978190\n",
      "[INFO] Epoch: 18 , batch: 299 , training loss: 4.018301\n",
      "[INFO] Epoch: 18 , batch: 300 , training loss: 4.020892\n",
      "[INFO] Epoch: 18 , batch: 301 , training loss: 3.943958\n",
      "[INFO] Epoch: 18 , batch: 302 , training loss: 4.104036\n",
      "[INFO] Epoch: 18 , batch: 303 , training loss: 4.108832\n",
      "[INFO] Epoch: 18 , batch: 304 , training loss: 4.261686\n",
      "[INFO] Epoch: 18 , batch: 305 , training loss: 4.059775\n",
      "[INFO] Epoch: 18 , batch: 306 , training loss: 4.205392\n",
      "[INFO] Epoch: 18 , batch: 307 , training loss: 4.199813\n",
      "[INFO] Epoch: 18 , batch: 308 , training loss: 4.024145\n",
      "[INFO] Epoch: 18 , batch: 309 , training loss: 4.022857\n",
      "[INFO] Epoch: 18 , batch: 310 , training loss: 3.954882\n",
      "[INFO] Epoch: 18 , batch: 311 , training loss: 3.938069\n",
      "[INFO] Epoch: 18 , batch: 312 , training loss: 3.835919\n",
      "[INFO] Epoch: 18 , batch: 313 , training loss: 3.944551\n",
      "[INFO] Epoch: 18 , batch: 314 , training loss: 4.033056\n",
      "[INFO] Epoch: 18 , batch: 315 , training loss: 4.084177\n",
      "[INFO] Epoch: 18 , batch: 316 , training loss: 4.371303\n",
      "[INFO] Epoch: 18 , batch: 317 , training loss: 4.727434\n",
      "[INFO] Epoch: 18 , batch: 318 , training loss: 4.907665\n",
      "[INFO] Epoch: 18 , batch: 319 , training loss: 4.549409\n",
      "[INFO] Epoch: 18 , batch: 320 , training loss: 4.061772\n",
      "[INFO] Epoch: 18 , batch: 321 , training loss: 3.882885\n",
      "[INFO] Epoch: 18 , batch: 322 , training loss: 3.998234\n",
      "[INFO] Epoch: 18 , batch: 323 , training loss: 4.022670\n",
      "[INFO] Epoch: 18 , batch: 324 , training loss: 4.004428\n",
      "[INFO] Epoch: 18 , batch: 325 , training loss: 4.119658\n",
      "[INFO] Epoch: 18 , batch: 326 , training loss: 4.177643\n",
      "[INFO] Epoch: 18 , batch: 327 , training loss: 4.091062\n",
      "[INFO] Epoch: 18 , batch: 328 , training loss: 4.105376\n",
      "[INFO] Epoch: 18 , batch: 329 , training loss: 4.007375\n",
      "[INFO] Epoch: 18 , batch: 330 , training loss: 3.999707\n",
      "[INFO] Epoch: 18 , batch: 331 , training loss: 4.179629\n",
      "[INFO] Epoch: 18 , batch: 332 , training loss: 3.999214\n",
      "[INFO] Epoch: 18 , batch: 333 , training loss: 3.990294\n",
      "[INFO] Epoch: 18 , batch: 334 , training loss: 3.993458\n",
      "[INFO] Epoch: 18 , batch: 335 , training loss: 4.118651\n",
      "[INFO] Epoch: 18 , batch: 336 , training loss: 4.130018\n",
      "[INFO] Epoch: 18 , batch: 337 , training loss: 4.194808\n",
      "[INFO] Epoch: 18 , batch: 338 , training loss: 4.378840\n",
      "[INFO] Epoch: 18 , batch: 339 , training loss: 4.189238\n",
      "[INFO] Epoch: 18 , batch: 340 , training loss: 4.393305\n",
      "[INFO] Epoch: 18 , batch: 341 , training loss: 4.137219\n",
      "[INFO] Epoch: 18 , batch: 342 , training loss: 3.918402\n",
      "[INFO] Epoch: 18 , batch: 343 , training loss: 4.004543\n",
      "[INFO] Epoch: 18 , batch: 344 , training loss: 3.867527\n",
      "[INFO] Epoch: 18 , batch: 345 , training loss: 4.014153\n",
      "[INFO] Epoch: 18 , batch: 346 , training loss: 4.043063\n",
      "[INFO] Epoch: 18 , batch: 347 , training loss: 3.949551\n",
      "[INFO] Epoch: 18 , batch: 348 , training loss: 4.074342\n",
      "[INFO] Epoch: 18 , batch: 349 , training loss: 4.157522\n",
      "[INFO] Epoch: 18 , batch: 350 , training loss: 4.006465\n",
      "[INFO] Epoch: 18 , batch: 351 , training loss: 4.085260\n",
      "[INFO] Epoch: 18 , batch: 352 , training loss: 4.091211\n",
      "[INFO] Epoch: 18 , batch: 353 , training loss: 4.067822\n",
      "[INFO] Epoch: 18 , batch: 354 , training loss: 4.182171\n",
      "[INFO] Epoch: 18 , batch: 355 , training loss: 4.161490\n",
      "[INFO] Epoch: 18 , batch: 356 , training loss: 4.037151\n",
      "[INFO] Epoch: 18 , batch: 357 , training loss: 4.126380\n",
      "[INFO] Epoch: 18 , batch: 358 , training loss: 4.025608\n",
      "[INFO] Epoch: 18 , batch: 359 , training loss: 4.023377\n",
      "[INFO] Epoch: 18 , batch: 360 , training loss: 4.097863\n",
      "[INFO] Epoch: 18 , batch: 361 , training loss: 4.080192\n",
      "[INFO] Epoch: 18 , batch: 362 , training loss: 4.192767\n",
      "[INFO] Epoch: 18 , batch: 363 , training loss: 4.072858\n",
      "[INFO] Epoch: 18 , batch: 364 , training loss: 4.138689\n",
      "[INFO] Epoch: 18 , batch: 365 , training loss: 4.025306\n",
      "[INFO] Epoch: 18 , batch: 366 , training loss: 4.157844\n",
      "[INFO] Epoch: 18 , batch: 367 , training loss: 4.187954\n",
      "[INFO] Epoch: 18 , batch: 368 , training loss: 4.639373\n",
      "[INFO] Epoch: 18 , batch: 369 , training loss: 4.305234\n",
      "[INFO] Epoch: 18 , batch: 370 , training loss: 4.054169\n",
      "[INFO] Epoch: 18 , batch: 371 , training loss: 4.496780\n",
      "[INFO] Epoch: 18 , batch: 372 , training loss: 4.765931\n",
      "[INFO] Epoch: 18 , batch: 373 , training loss: 4.839486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 18 , batch: 374 , training loss: 4.929464\n",
      "[INFO] Epoch: 18 , batch: 375 , training loss: 4.901322\n",
      "[INFO] Epoch: 18 , batch: 376 , training loss: 4.794160\n",
      "[INFO] Epoch: 18 , batch: 377 , training loss: 4.515418\n",
      "[INFO] Epoch: 18 , batch: 378 , training loss: 4.610963\n",
      "[INFO] Epoch: 18 , batch: 379 , training loss: 4.581439\n",
      "[INFO] Epoch: 18 , batch: 380 , training loss: 4.728293\n",
      "[INFO] Epoch: 18 , batch: 381 , training loss: 4.461877\n",
      "[INFO] Epoch: 18 , batch: 382 , training loss: 4.754439\n",
      "[INFO] Epoch: 18 , batch: 383 , training loss: 4.760083\n",
      "[INFO] Epoch: 18 , batch: 384 , training loss: 4.783566\n",
      "[INFO] Epoch: 18 , batch: 385 , training loss: 4.442952\n",
      "[INFO] Epoch: 18 , batch: 386 , training loss: 4.692177\n",
      "[INFO] Epoch: 18 , batch: 387 , training loss: 4.658129\n",
      "[INFO] Epoch: 18 , batch: 388 , training loss: 4.446135\n",
      "[INFO] Epoch: 18 , batch: 389 , training loss: 4.269528\n",
      "[INFO] Epoch: 18 , batch: 390 , training loss: 4.271916\n",
      "[INFO] Epoch: 18 , batch: 391 , training loss: 4.295020\n",
      "[INFO] Epoch: 18 , batch: 392 , training loss: 4.654331\n",
      "[INFO] Epoch: 18 , batch: 393 , training loss: 4.559350\n",
      "[INFO] Epoch: 18 , batch: 394 , training loss: 4.644979\n",
      "[INFO] Epoch: 18 , batch: 395 , training loss: 4.429772\n",
      "[INFO] Epoch: 18 , batch: 396 , training loss: 4.268872\n",
      "[INFO] Epoch: 18 , batch: 397 , training loss: 4.414927\n",
      "[INFO] Epoch: 18 , batch: 398 , training loss: 4.273895\n",
      "[INFO] Epoch: 18 , batch: 399 , training loss: 4.346728\n",
      "[INFO] Epoch: 18 , batch: 400 , training loss: 4.319654\n",
      "[INFO] Epoch: 18 , batch: 401 , training loss: 4.766502\n",
      "[INFO] Epoch: 18 , batch: 402 , training loss: 4.474024\n",
      "[INFO] Epoch: 18 , batch: 403 , training loss: 4.299650\n",
      "[INFO] Epoch: 18 , batch: 404 , training loss: 4.484782\n",
      "[INFO] Epoch: 18 , batch: 405 , training loss: 4.526870\n",
      "[INFO] Epoch: 18 , batch: 406 , training loss: 4.440791\n",
      "[INFO] Epoch: 18 , batch: 407 , training loss: 4.476082\n",
      "[INFO] Epoch: 18 , batch: 408 , training loss: 4.419533\n",
      "[INFO] Epoch: 18 , batch: 409 , training loss: 4.436970\n",
      "[INFO] Epoch: 18 , batch: 410 , training loss: 4.513103\n",
      "[INFO] Epoch: 18 , batch: 411 , training loss: 4.684505\n",
      "[INFO] Epoch: 18 , batch: 412 , training loss: 4.513747\n",
      "[INFO] Epoch: 18 , batch: 413 , training loss: 4.380670\n",
      "[INFO] Epoch: 18 , batch: 414 , training loss: 4.411368\n",
      "[INFO] Epoch: 18 , batch: 415 , training loss: 4.453850\n",
      "[INFO] Epoch: 18 , batch: 416 , training loss: 4.524224\n",
      "[INFO] Epoch: 18 , batch: 417 , training loss: 4.434561\n",
      "[INFO] Epoch: 18 , batch: 418 , training loss: 4.489635\n",
      "[INFO] Epoch: 18 , batch: 419 , training loss: 4.427850\n",
      "[INFO] Epoch: 18 , batch: 420 , training loss: 4.400045\n",
      "[INFO] Epoch: 18 , batch: 421 , training loss: 4.406011\n",
      "[INFO] Epoch: 18 , batch: 422 , training loss: 4.258673\n",
      "[INFO] Epoch: 18 , batch: 423 , training loss: 4.466184\n",
      "[INFO] Epoch: 18 , batch: 424 , training loss: 4.628911\n",
      "[INFO] Epoch: 18 , batch: 425 , training loss: 4.507789\n",
      "[INFO] Epoch: 18 , batch: 426 , training loss: 4.219604\n",
      "[INFO] Epoch: 18 , batch: 427 , training loss: 4.498313\n",
      "[INFO] Epoch: 18 , batch: 428 , training loss: 4.348041\n",
      "[INFO] Epoch: 18 , batch: 429 , training loss: 4.257253\n",
      "[INFO] Epoch: 18 , batch: 430 , training loss: 4.483923\n",
      "[INFO] Epoch: 18 , batch: 431 , training loss: 4.084871\n",
      "[INFO] Epoch: 18 , batch: 432 , training loss: 4.136772\n",
      "[INFO] Epoch: 18 , batch: 433 , training loss: 4.171091\n",
      "[INFO] Epoch: 18 , batch: 434 , training loss: 4.058091\n",
      "[INFO] Epoch: 18 , batch: 435 , training loss: 4.411969\n",
      "[INFO] Epoch: 18 , batch: 436 , training loss: 4.473917\n",
      "[INFO] Epoch: 18 , batch: 437 , training loss: 4.247646\n",
      "[INFO] Epoch: 18 , batch: 438 , training loss: 4.090055\n",
      "[INFO] Epoch: 18 , batch: 439 , training loss: 4.331725\n",
      "[INFO] Epoch: 18 , batch: 440 , training loss: 4.471198\n",
      "[INFO] Epoch: 18 , batch: 441 , training loss: 4.522819\n",
      "[INFO] Epoch: 18 , batch: 442 , training loss: 4.302756\n",
      "[INFO] Epoch: 18 , batch: 443 , training loss: 4.513051\n",
      "[INFO] Epoch: 18 , batch: 444 , training loss: 4.098419\n",
      "[INFO] Epoch: 18 , batch: 445 , training loss: 4.027661\n",
      "[INFO] Epoch: 18 , batch: 446 , training loss: 3.944197\n",
      "[INFO] Epoch: 18 , batch: 447 , training loss: 4.138400\n",
      "[INFO] Epoch: 18 , batch: 448 , training loss: 4.275795\n",
      "[INFO] Epoch: 18 , batch: 449 , training loss: 4.632182\n",
      "[INFO] Epoch: 18 , batch: 450 , training loss: 4.708671\n",
      "[INFO] Epoch: 18 , batch: 451 , training loss: 4.588165\n",
      "[INFO] Epoch: 18 , batch: 452 , training loss: 4.411499\n",
      "[INFO] Epoch: 18 , batch: 453 , training loss: 4.192953\n",
      "[INFO] Epoch: 18 , batch: 454 , training loss: 4.339876\n",
      "[INFO] Epoch: 18 , batch: 455 , training loss: 4.383436\n",
      "[INFO] Epoch: 18 , batch: 456 , training loss: 4.363835\n",
      "[INFO] Epoch: 18 , batch: 457 , training loss: 4.466135\n",
      "[INFO] Epoch: 18 , batch: 458 , training loss: 4.189701\n",
      "[INFO] Epoch: 18 , batch: 459 , training loss: 4.158840\n",
      "[INFO] Epoch: 18 , batch: 460 , training loss: 4.300858\n",
      "[INFO] Epoch: 18 , batch: 461 , training loss: 4.246566\n",
      "[INFO] Epoch: 18 , batch: 462 , training loss: 4.315551\n",
      "[INFO] Epoch: 18 , batch: 463 , training loss: 4.228986\n",
      "[INFO] Epoch: 18 , batch: 464 , training loss: 4.391024\n",
      "[INFO] Epoch: 18 , batch: 465 , training loss: 4.329544\n",
      "[INFO] Epoch: 18 , batch: 466 , training loss: 4.434172\n",
      "[INFO] Epoch: 18 , batch: 467 , training loss: 4.403053\n",
      "[INFO] Epoch: 18 , batch: 468 , training loss: 4.363328\n",
      "[INFO] Epoch: 18 , batch: 469 , training loss: 4.424300\n",
      "[INFO] Epoch: 18 , batch: 470 , training loss: 4.203093\n",
      "[INFO] Epoch: 18 , batch: 471 , training loss: 4.320389\n",
      "[INFO] Epoch: 18 , batch: 472 , training loss: 4.379217\n",
      "[INFO] Epoch: 18 , batch: 473 , training loss: 4.292939\n",
      "[INFO] Epoch: 18 , batch: 474 , training loss: 4.078979\n",
      "[INFO] Epoch: 18 , batch: 475 , training loss: 3.950219\n",
      "[INFO] Epoch: 18 , batch: 476 , training loss: 4.366395\n",
      "[INFO] Epoch: 18 , batch: 477 , training loss: 4.461813\n",
      "[INFO] Epoch: 18 , batch: 478 , training loss: 4.487121\n",
      "[INFO] Epoch: 18 , batch: 479 , training loss: 4.432457\n",
      "[INFO] Epoch: 18 , batch: 480 , training loss: 4.567055\n",
      "[INFO] Epoch: 18 , batch: 481 , training loss: 4.450027\n",
      "[INFO] Epoch: 18 , batch: 482 , training loss: 4.551724\n",
      "[INFO] Epoch: 18 , batch: 483 , training loss: 4.398080\n",
      "[INFO] Epoch: 18 , batch: 484 , training loss: 4.191000\n",
      "[INFO] Epoch: 18 , batch: 485 , training loss: 4.293965\n",
      "[INFO] Epoch: 18 , batch: 486 , training loss: 4.187770\n",
      "[INFO] Epoch: 18 , batch: 487 , training loss: 4.187877\n",
      "[INFO] Epoch: 18 , batch: 488 , training loss: 4.363771\n",
      "[INFO] Epoch: 18 , batch: 489 , training loss: 4.264842\n",
      "[INFO] Epoch: 18 , batch: 490 , training loss: 4.338412\n",
      "[INFO] Epoch: 18 , batch: 491 , training loss: 4.259835\n",
      "[INFO] Epoch: 18 , batch: 492 , training loss: 4.216816\n",
      "[INFO] Epoch: 18 , batch: 493 , training loss: 4.393291\n",
      "[INFO] Epoch: 18 , batch: 494 , training loss: 4.295778\n",
      "[INFO] Epoch: 18 , batch: 495 , training loss: 4.436347\n",
      "[INFO] Epoch: 18 , batch: 496 , training loss: 4.313545\n",
      "[INFO] Epoch: 18 , batch: 497 , training loss: 4.362579\n",
      "[INFO] Epoch: 18 , batch: 498 , training loss: 4.346344\n",
      "[INFO] Epoch: 18 , batch: 499 , training loss: 4.416040\n",
      "[INFO] Epoch: 18 , batch: 500 , training loss: 4.556944\n",
      "[INFO] Epoch: 18 , batch: 501 , training loss: 4.906335\n",
      "[INFO] Epoch: 18 , batch: 502 , training loss: 4.983839\n",
      "[INFO] Epoch: 18 , batch: 503 , training loss: 4.657725\n",
      "[INFO] Epoch: 18 , batch: 504 , training loss: 4.801885\n",
      "[INFO] Epoch: 18 , batch: 505 , training loss: 4.745265\n",
      "[INFO] Epoch: 18 , batch: 506 , training loss: 4.714201\n",
      "[INFO] Epoch: 18 , batch: 507 , training loss: 4.772751\n",
      "[INFO] Epoch: 18 , batch: 508 , training loss: 4.680304\n",
      "[INFO] Epoch: 18 , batch: 509 , training loss: 4.483325\n",
      "[INFO] Epoch: 18 , batch: 510 , training loss: 4.550859\n",
      "[INFO] Epoch: 18 , batch: 511 , training loss: 4.491656\n",
      "[INFO] Epoch: 18 , batch: 512 , training loss: 4.546036\n",
      "[INFO] Epoch: 18 , batch: 513 , training loss: 4.814260\n",
      "[INFO] Epoch: 18 , batch: 514 , training loss: 4.451095\n",
      "[INFO] Epoch: 18 , batch: 515 , training loss: 4.708436\n",
      "[INFO] Epoch: 18 , batch: 516 , training loss: 4.506147\n",
      "[INFO] Epoch: 18 , batch: 517 , training loss: 4.460744\n",
      "[INFO] Epoch: 18 , batch: 518 , training loss: 4.419628\n",
      "[INFO] Epoch: 18 , batch: 519 , training loss: 4.280180\n",
      "[INFO] Epoch: 18 , batch: 520 , training loss: 4.523650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 18 , batch: 521 , training loss: 4.506658\n",
      "[INFO] Epoch: 18 , batch: 522 , training loss: 4.576996\n",
      "[INFO] Epoch: 18 , batch: 523 , training loss: 4.489489\n",
      "[INFO] Epoch: 18 , batch: 524 , training loss: 4.755134\n",
      "[INFO] Epoch: 18 , batch: 525 , training loss: 4.630868\n",
      "[INFO] Epoch: 18 , batch: 526 , training loss: 4.432084\n",
      "[INFO] Epoch: 18 , batch: 527 , training loss: 4.470736\n",
      "[INFO] Epoch: 18 , batch: 528 , training loss: 4.482397\n",
      "[INFO] Epoch: 18 , batch: 529 , training loss: 4.460034\n",
      "[INFO] Epoch: 18 , batch: 530 , training loss: 4.304186\n",
      "[INFO] Epoch: 18 , batch: 531 , training loss: 4.460761\n",
      "[INFO] Epoch: 18 , batch: 532 , training loss: 4.370191\n",
      "[INFO] Epoch: 18 , batch: 533 , training loss: 4.506414\n",
      "[INFO] Epoch: 18 , batch: 534 , training loss: 4.495637\n",
      "[INFO] Epoch: 18 , batch: 535 , training loss: 4.503047\n",
      "[INFO] Epoch: 18 , batch: 536 , training loss: 4.362018\n",
      "[INFO] Epoch: 18 , batch: 537 , training loss: 4.309838\n",
      "[INFO] Epoch: 18 , batch: 538 , training loss: 4.424033\n",
      "[INFO] Epoch: 18 , batch: 539 , training loss: 4.543483\n",
      "[INFO] Epoch: 18 , batch: 540 , training loss: 5.072841\n",
      "[INFO] Epoch: 18 , batch: 541 , training loss: 4.933520\n",
      "[INFO] Epoch: 18 , batch: 542 , training loss: 4.776374\n",
      "[INFO] Epoch: 19 , batch: 0 , training loss: 3.905093\n",
      "[INFO] Epoch: 19 , batch: 1 , training loss: 3.687017\n",
      "[INFO] Epoch: 19 , batch: 2 , training loss: 3.825873\n",
      "[INFO] Epoch: 19 , batch: 3 , training loss: 3.685172\n",
      "[INFO] Epoch: 19 , batch: 4 , training loss: 4.009600\n",
      "[INFO] Epoch: 19 , batch: 5 , training loss: 3.689756\n",
      "[INFO] Epoch: 19 , batch: 6 , training loss: 4.044052\n",
      "[INFO] Epoch: 19 , batch: 7 , training loss: 3.966618\n",
      "[INFO] Epoch: 19 , batch: 8 , training loss: 3.599736\n",
      "[INFO] Epoch: 19 , batch: 9 , training loss: 3.836637\n",
      "[INFO] Epoch: 19 , batch: 10 , training loss: 3.760613\n",
      "[INFO] Epoch: 19 , batch: 11 , training loss: 3.705212\n",
      "[INFO] Epoch: 19 , batch: 12 , training loss: 3.591656\n",
      "[INFO] Epoch: 19 , batch: 13 , training loss: 3.646598\n",
      "[INFO] Epoch: 19 , batch: 14 , training loss: 3.503807\n",
      "[INFO] Epoch: 19 , batch: 15 , training loss: 3.764431\n",
      "[INFO] Epoch: 19 , batch: 16 , training loss: 3.626743\n",
      "[INFO] Epoch: 19 , batch: 17 , training loss: 3.758209\n",
      "[INFO] Epoch: 19 , batch: 18 , training loss: 3.669175\n",
      "[INFO] Epoch: 19 , batch: 19 , training loss: 3.462902\n",
      "[INFO] Epoch: 19 , batch: 20 , training loss: 3.434078\n",
      "[INFO] Epoch: 19 , batch: 21 , training loss: 3.582585\n",
      "[INFO] Epoch: 19 , batch: 22 , training loss: 3.489581\n",
      "[INFO] Epoch: 19 , batch: 23 , training loss: 3.675107\n",
      "[INFO] Epoch: 19 , batch: 24 , training loss: 3.537611\n",
      "[INFO] Epoch: 19 , batch: 25 , training loss: 3.644017\n",
      "[INFO] Epoch: 19 , batch: 26 , training loss: 3.526482\n",
      "[INFO] Epoch: 19 , batch: 27 , training loss: 3.486316\n",
      "[INFO] Epoch: 19 , batch: 28 , training loss: 3.696544\n",
      "[INFO] Epoch: 19 , batch: 29 , training loss: 3.522371\n",
      "[INFO] Epoch: 19 , batch: 30 , training loss: 3.504246\n",
      "[INFO] Epoch: 19 , batch: 31 , training loss: 3.646635\n",
      "[INFO] Epoch: 19 , batch: 32 , training loss: 3.618509\n",
      "[INFO] Epoch: 19 , batch: 33 , training loss: 3.661103\n",
      "[INFO] Epoch: 19 , batch: 34 , training loss: 3.670977\n",
      "[INFO] Epoch: 19 , batch: 35 , training loss: 3.580138\n",
      "[INFO] Epoch: 19 , batch: 36 , training loss: 3.674201\n",
      "[INFO] Epoch: 19 , batch: 37 , training loss: 3.535923\n",
      "[INFO] Epoch: 19 , batch: 38 , training loss: 3.631858\n",
      "[INFO] Epoch: 19 , batch: 39 , training loss: 3.421622\n",
      "[INFO] Epoch: 19 , batch: 40 , training loss: 3.626881\n",
      "[INFO] Epoch: 19 , batch: 41 , training loss: 3.625500\n",
      "[INFO] Epoch: 19 , batch: 42 , training loss: 4.076717\n",
      "[INFO] Epoch: 19 , batch: 43 , training loss: 3.772114\n",
      "[INFO] Epoch: 19 , batch: 44 , training loss: 4.159431\n",
      "[INFO] Epoch: 19 , batch: 45 , training loss: 4.122159\n",
      "[INFO] Epoch: 19 , batch: 46 , training loss: 4.064229\n",
      "[INFO] Epoch: 19 , batch: 47 , training loss: 3.749640\n",
      "[INFO] Epoch: 19 , batch: 48 , training loss: 3.701882\n",
      "[INFO] Epoch: 19 , batch: 49 , training loss: 3.964895\n",
      "[INFO] Epoch: 19 , batch: 50 , training loss: 3.687045\n",
      "[INFO] Epoch: 19 , batch: 51 , training loss: 3.902059\n",
      "[INFO] Epoch: 19 , batch: 52 , training loss: 3.724529\n",
      "[INFO] Epoch: 19 , batch: 53 , training loss: 3.878852\n",
      "[INFO] Epoch: 19 , batch: 54 , training loss: 3.917778\n",
      "[INFO] Epoch: 19 , batch: 55 , training loss: 3.997136\n",
      "[INFO] Epoch: 19 , batch: 56 , training loss: 3.788588\n",
      "[INFO] Epoch: 19 , batch: 57 , training loss: 3.748184\n",
      "[INFO] Epoch: 19 , batch: 58 , training loss: 3.802603\n",
      "[INFO] Epoch: 19 , batch: 59 , training loss: 3.871094\n",
      "[INFO] Epoch: 19 , batch: 60 , training loss: 3.834087\n",
      "[INFO] Epoch: 19 , batch: 61 , training loss: 3.891094\n",
      "[INFO] Epoch: 19 , batch: 62 , training loss: 3.755847\n",
      "[INFO] Epoch: 19 , batch: 63 , training loss: 3.929937\n",
      "[INFO] Epoch: 19 , batch: 64 , training loss: 4.187489\n",
      "[INFO] Epoch: 19 , batch: 65 , training loss: 3.823795\n",
      "[INFO] Epoch: 19 , batch: 66 , training loss: 3.727407\n",
      "[INFO] Epoch: 19 , batch: 67 , training loss: 3.691428\n",
      "[INFO] Epoch: 19 , batch: 68 , training loss: 3.975609\n",
      "[INFO] Epoch: 19 , batch: 69 , training loss: 3.843790\n",
      "[INFO] Epoch: 19 , batch: 70 , training loss: 4.041998\n",
      "[INFO] Epoch: 19 , batch: 71 , training loss: 3.895475\n",
      "[INFO] Epoch: 19 , batch: 72 , training loss: 3.973550\n",
      "[INFO] Epoch: 19 , batch: 73 , training loss: 3.878397\n",
      "[INFO] Epoch: 19 , batch: 74 , training loss: 4.023863\n",
      "[INFO] Epoch: 19 , batch: 75 , training loss: 3.865261\n",
      "[INFO] Epoch: 19 , batch: 76 , training loss: 3.979127\n",
      "[INFO] Epoch: 19 , batch: 77 , training loss: 3.912841\n",
      "[INFO] Epoch: 19 , batch: 78 , training loss: 3.987996\n",
      "[INFO] Epoch: 19 , batch: 79 , training loss: 3.818965\n",
      "[INFO] Epoch: 19 , batch: 80 , training loss: 4.057842\n",
      "[INFO] Epoch: 19 , batch: 81 , training loss: 3.996698\n",
      "[INFO] Epoch: 19 , batch: 82 , training loss: 3.938699\n",
      "[INFO] Epoch: 19 , batch: 83 , training loss: 4.058514\n",
      "[INFO] Epoch: 19 , batch: 84 , training loss: 4.010321\n",
      "[INFO] Epoch: 19 , batch: 85 , training loss: 4.084197\n",
      "[INFO] Epoch: 19 , batch: 86 , training loss: 4.026120\n",
      "[INFO] Epoch: 19 , batch: 87 , training loss: 3.991209\n",
      "[INFO] Epoch: 19 , batch: 88 , training loss: 4.144013\n",
      "[INFO] Epoch: 19 , batch: 89 , training loss: 3.915102\n",
      "[INFO] Epoch: 19 , batch: 90 , training loss: 4.016900\n",
      "[INFO] Epoch: 19 , batch: 91 , training loss: 3.934118\n",
      "[INFO] Epoch: 19 , batch: 92 , training loss: 3.963739\n",
      "[INFO] Epoch: 19 , batch: 93 , training loss: 4.049838\n",
      "[INFO] Epoch: 19 , batch: 94 , training loss: 4.213469\n",
      "[INFO] Epoch: 19 , batch: 95 , training loss: 3.985255\n",
      "[INFO] Epoch: 19 , batch: 96 , training loss: 3.930928\n",
      "[INFO] Epoch: 19 , batch: 97 , training loss: 3.919753\n",
      "[INFO] Epoch: 19 , batch: 98 , training loss: 3.844721\n",
      "[INFO] Epoch: 19 , batch: 99 , training loss: 3.966190\n",
      "[INFO] Epoch: 19 , batch: 100 , training loss: 3.825401\n",
      "[INFO] Epoch: 19 , batch: 101 , training loss: 3.866306\n",
      "[INFO] Epoch: 19 , batch: 102 , training loss: 4.033926\n",
      "[INFO] Epoch: 19 , batch: 103 , training loss: 3.783121\n",
      "[INFO] Epoch: 19 , batch: 104 , training loss: 3.755829\n",
      "[INFO] Epoch: 19 , batch: 105 , training loss: 4.005999\n",
      "[INFO] Epoch: 19 , batch: 106 , training loss: 4.028066\n",
      "[INFO] Epoch: 19 , batch: 107 , training loss: 3.888059\n",
      "[INFO] Epoch: 19 , batch: 108 , training loss: 3.822467\n",
      "[INFO] Epoch: 19 , batch: 109 , training loss: 3.737240\n",
      "[INFO] Epoch: 19 , batch: 110 , training loss: 3.938519\n",
      "[INFO] Epoch: 19 , batch: 111 , training loss: 4.001317\n",
      "[INFO] Epoch: 19 , batch: 112 , training loss: 3.949820\n",
      "[INFO] Epoch: 19 , batch: 113 , training loss: 3.920504\n",
      "[INFO] Epoch: 19 , batch: 114 , training loss: 3.955028\n",
      "[INFO] Epoch: 19 , batch: 115 , training loss: 3.940790\n",
      "[INFO] Epoch: 19 , batch: 116 , training loss: 3.817454\n",
      "[INFO] Epoch: 19 , batch: 117 , training loss: 4.052146\n",
      "[INFO] Epoch: 19 , batch: 118 , training loss: 4.013403\n",
      "[INFO] Epoch: 19 , batch: 119 , training loss: 4.184905\n",
      "[INFO] Epoch: 19 , batch: 120 , training loss: 4.160199\n",
      "[INFO] Epoch: 19 , batch: 121 , training loss: 4.013417\n",
      "[INFO] Epoch: 19 , batch: 122 , training loss: 3.896329\n",
      "[INFO] Epoch: 19 , batch: 123 , training loss: 3.940015\n",
      "[INFO] Epoch: 19 , batch: 124 , training loss: 4.051110\n",
      "[INFO] Epoch: 19 , batch: 125 , training loss: 3.815342\n",
      "[INFO] Epoch: 19 , batch: 126 , training loss: 3.835058\n",
      "[INFO] Epoch: 19 , batch: 127 , training loss: 3.848633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 19 , batch: 128 , training loss: 3.996538\n",
      "[INFO] Epoch: 19 , batch: 129 , training loss: 3.942286\n",
      "[INFO] Epoch: 19 , batch: 130 , training loss: 3.931983\n",
      "[INFO] Epoch: 19 , batch: 131 , training loss: 3.937254\n",
      "[INFO] Epoch: 19 , batch: 132 , training loss: 3.960830\n",
      "[INFO] Epoch: 19 , batch: 133 , training loss: 3.932674\n",
      "[INFO] Epoch: 19 , batch: 134 , training loss: 3.664198\n",
      "[INFO] Epoch: 19 , batch: 135 , training loss: 3.755728\n",
      "[INFO] Epoch: 19 , batch: 136 , training loss: 4.033973\n",
      "[INFO] Epoch: 19 , batch: 137 , training loss: 3.951932\n",
      "[INFO] Epoch: 19 , batch: 138 , training loss: 4.012693\n",
      "[INFO] Epoch: 19 , batch: 139 , training loss: 4.614779\n",
      "[INFO] Epoch: 19 , batch: 140 , training loss: 4.396634\n",
      "[INFO] Epoch: 19 , batch: 141 , training loss: 4.178036\n",
      "[INFO] Epoch: 19 , batch: 142 , training loss: 3.855062\n",
      "[INFO] Epoch: 19 , batch: 143 , training loss: 3.962403\n",
      "[INFO] Epoch: 19 , batch: 144 , training loss: 3.837595\n",
      "[INFO] Epoch: 19 , batch: 145 , training loss: 3.929470\n",
      "[INFO] Epoch: 19 , batch: 146 , training loss: 4.079724\n",
      "[INFO] Epoch: 19 , batch: 147 , training loss: 3.768116\n",
      "[INFO] Epoch: 19 , batch: 148 , training loss: 3.749959\n",
      "[INFO] Epoch: 19 , batch: 149 , training loss: 3.858537\n",
      "[INFO] Epoch: 19 , batch: 150 , training loss: 4.098574\n",
      "[INFO] Epoch: 19 , batch: 151 , training loss: 3.912116\n",
      "[INFO] Epoch: 19 , batch: 152 , training loss: 3.920651\n",
      "[INFO] Epoch: 19 , batch: 153 , training loss: 3.947836\n",
      "[INFO] Epoch: 19 , batch: 154 , training loss: 4.018234\n",
      "[INFO] Epoch: 19 , batch: 155 , training loss: 4.233751\n",
      "[INFO] Epoch: 19 , batch: 156 , training loss: 3.998655\n",
      "[INFO] Epoch: 19 , batch: 157 , training loss: 3.966466\n",
      "[INFO] Epoch: 19 , batch: 158 , training loss: 4.149557\n",
      "[INFO] Epoch: 19 , batch: 159 , training loss: 4.066367\n",
      "[INFO] Epoch: 19 , batch: 160 , training loss: 4.343601\n",
      "[INFO] Epoch: 19 , batch: 161 , training loss: 4.372786\n",
      "[INFO] Epoch: 19 , batch: 162 , training loss: 4.355002\n",
      "[INFO] Epoch: 19 , batch: 163 , training loss: 4.555172\n",
      "[INFO] Epoch: 19 , batch: 164 , training loss: 4.448410\n",
      "[INFO] Epoch: 19 , batch: 165 , training loss: 4.370680\n",
      "[INFO] Epoch: 19 , batch: 166 , training loss: 4.291842\n",
      "[INFO] Epoch: 19 , batch: 167 , training loss: 4.491527\n",
      "[INFO] Epoch: 19 , batch: 168 , training loss: 4.168210\n",
      "[INFO] Epoch: 19 , batch: 169 , training loss: 4.170712\n",
      "[INFO] Epoch: 19 , batch: 170 , training loss: 4.276076\n",
      "[INFO] Epoch: 19 , batch: 171 , training loss: 3.683544\n",
      "[INFO] Epoch: 19 , batch: 172 , training loss: 3.925808\n",
      "[INFO] Epoch: 19 , batch: 173 , training loss: 4.191605\n",
      "[INFO] Epoch: 19 , batch: 174 , training loss: 4.633791\n",
      "[INFO] Epoch: 19 , batch: 175 , training loss: 4.894385\n",
      "[INFO] Epoch: 19 , batch: 176 , training loss: 4.639204\n",
      "[INFO] Epoch: 19 , batch: 177 , training loss: 4.271536\n",
      "[INFO] Epoch: 19 , batch: 178 , training loss: 4.208999\n",
      "[INFO] Epoch: 19 , batch: 179 , training loss: 4.262149\n",
      "[INFO] Epoch: 19 , batch: 180 , training loss: 4.198266\n",
      "[INFO] Epoch: 19 , batch: 181 , training loss: 4.447383\n",
      "[INFO] Epoch: 19 , batch: 182 , training loss: 4.391071\n",
      "[INFO] Epoch: 19 , batch: 183 , training loss: 4.343655\n",
      "[INFO] Epoch: 19 , batch: 184 , training loss: 4.262693\n",
      "[INFO] Epoch: 19 , batch: 185 , training loss: 4.222353\n",
      "[INFO] Epoch: 19 , batch: 186 , training loss: 4.335277\n",
      "[INFO] Epoch: 19 , batch: 187 , training loss: 4.453699\n",
      "[INFO] Epoch: 19 , batch: 188 , training loss: 4.444340\n",
      "[INFO] Epoch: 19 , batch: 189 , training loss: 4.330470\n",
      "[INFO] Epoch: 19 , batch: 190 , training loss: 4.362860\n",
      "[INFO] Epoch: 19 , batch: 191 , training loss: 4.492740\n",
      "[INFO] Epoch: 19 , batch: 192 , training loss: 4.300090\n",
      "[INFO] Epoch: 19 , batch: 193 , training loss: 4.407857\n",
      "[INFO] Epoch: 19 , batch: 194 , training loss: 4.343454\n",
      "[INFO] Epoch: 19 , batch: 195 , training loss: 4.298876\n",
      "[INFO] Epoch: 19 , batch: 196 , training loss: 4.157656\n",
      "[INFO] Epoch: 19 , batch: 197 , training loss: 4.241461\n",
      "[INFO] Epoch: 19 , batch: 198 , training loss: 4.117337\n",
      "[INFO] Epoch: 19 , batch: 199 , training loss: 4.259861\n",
      "[INFO] Epoch: 19 , batch: 200 , training loss: 4.191895\n",
      "[INFO] Epoch: 19 , batch: 201 , training loss: 4.087893\n",
      "[INFO] Epoch: 19 , batch: 202 , training loss: 4.068438\n",
      "[INFO] Epoch: 19 , batch: 203 , training loss: 4.195284\n",
      "[INFO] Epoch: 19 , batch: 204 , training loss: 4.303403\n",
      "[INFO] Epoch: 19 , batch: 205 , training loss: 3.882277\n",
      "[INFO] Epoch: 19 , batch: 206 , training loss: 3.796350\n",
      "[INFO] Epoch: 19 , batch: 207 , training loss: 3.815896\n",
      "[INFO] Epoch: 19 , batch: 208 , training loss: 4.135363\n",
      "[INFO] Epoch: 19 , batch: 209 , training loss: 4.081566\n",
      "[INFO] Epoch: 19 , batch: 210 , training loss: 4.115540\n",
      "[INFO] Epoch: 19 , batch: 211 , training loss: 4.107184\n",
      "[INFO] Epoch: 19 , batch: 212 , training loss: 4.218134\n",
      "[INFO] Epoch: 19 , batch: 213 , training loss: 4.187024\n",
      "[INFO] Epoch: 19 , batch: 214 , training loss: 4.265391\n",
      "[INFO] Epoch: 19 , batch: 215 , training loss: 4.458812\n",
      "[INFO] Epoch: 19 , batch: 216 , training loss: 4.150900\n",
      "[INFO] Epoch: 19 , batch: 217 , training loss: 4.106617\n",
      "[INFO] Epoch: 19 , batch: 218 , training loss: 4.097161\n",
      "[INFO] Epoch: 19 , batch: 219 , training loss: 4.203432\n",
      "[INFO] Epoch: 19 , batch: 220 , training loss: 4.031993\n",
      "[INFO] Epoch: 19 , batch: 221 , training loss: 4.044700\n",
      "[INFO] Epoch: 19 , batch: 222 , training loss: 4.174604\n",
      "[INFO] Epoch: 19 , batch: 223 , training loss: 4.297841\n",
      "[INFO] Epoch: 19 , batch: 224 , training loss: 4.313401\n",
      "[INFO] Epoch: 19 , batch: 225 , training loss: 4.204347\n",
      "[INFO] Epoch: 19 , batch: 226 , training loss: 4.321846\n",
      "[INFO] Epoch: 19 , batch: 227 , training loss: 4.293236\n",
      "[INFO] Epoch: 19 , batch: 228 , training loss: 4.338960\n",
      "[INFO] Epoch: 19 , batch: 229 , training loss: 4.189952\n",
      "[INFO] Epoch: 19 , batch: 230 , training loss: 4.059168\n",
      "[INFO] Epoch: 19 , batch: 231 , training loss: 3.899611\n",
      "[INFO] Epoch: 19 , batch: 232 , training loss: 4.060849\n",
      "[INFO] Epoch: 19 , batch: 233 , training loss: 4.082821\n",
      "[INFO] Epoch: 19 , batch: 234 , training loss: 3.767148\n",
      "[INFO] Epoch: 19 , batch: 235 , training loss: 3.899564\n",
      "[INFO] Epoch: 19 , batch: 236 , training loss: 3.985303\n",
      "[INFO] Epoch: 19 , batch: 237 , training loss: 4.217413\n",
      "[INFO] Epoch: 19 , batch: 238 , training loss: 3.974303\n",
      "[INFO] Epoch: 19 , batch: 239 , training loss: 4.022265\n",
      "[INFO] Epoch: 19 , batch: 240 , training loss: 4.049425\n",
      "[INFO] Epoch: 19 , batch: 241 , training loss: 3.874501\n",
      "[INFO] Epoch: 19 , batch: 242 , training loss: 3.885860\n",
      "[INFO] Epoch: 19 , batch: 243 , training loss: 4.165235\n",
      "[INFO] Epoch: 19 , batch: 244 , training loss: 4.129850\n",
      "[INFO] Epoch: 19 , batch: 245 , training loss: 4.112201\n",
      "[INFO] Epoch: 19 , batch: 246 , training loss: 3.798859\n",
      "[INFO] Epoch: 19 , batch: 247 , training loss: 3.952787\n",
      "[INFO] Epoch: 19 , batch: 248 , training loss: 4.033401\n",
      "[INFO] Epoch: 19 , batch: 249 , training loss: 4.006660\n",
      "[INFO] Epoch: 19 , batch: 250 , training loss: 3.815053\n",
      "[INFO] Epoch: 19 , batch: 251 , training loss: 4.287186\n",
      "[INFO] Epoch: 19 , batch: 252 , training loss: 3.982860\n",
      "[INFO] Epoch: 19 , batch: 253 , training loss: 3.915617\n",
      "[INFO] Epoch: 19 , batch: 254 , training loss: 4.192795\n",
      "[INFO] Epoch: 19 , batch: 255 , training loss: 4.141661\n",
      "[INFO] Epoch: 19 , batch: 256 , training loss: 4.116943\n",
      "[INFO] Epoch: 19 , batch: 257 , training loss: 4.296858\n",
      "[INFO] Epoch: 19 , batch: 258 , training loss: 4.312005\n",
      "[INFO] Epoch: 19 , batch: 259 , training loss: 4.354685\n",
      "[INFO] Epoch: 19 , batch: 260 , training loss: 4.119141\n",
      "[INFO] Epoch: 19 , batch: 261 , training loss: 4.275656\n",
      "[INFO] Epoch: 19 , batch: 262 , training loss: 4.458107\n",
      "[INFO] Epoch: 19 , batch: 263 , training loss: 4.592583\n",
      "[INFO] Epoch: 19 , batch: 264 , training loss: 3.961971\n",
      "[INFO] Epoch: 19 , batch: 265 , training loss: 4.068355\n",
      "[INFO] Epoch: 19 , batch: 266 , training loss: 4.506181\n",
      "[INFO] Epoch: 19 , batch: 267 , training loss: 4.221494\n",
      "[INFO] Epoch: 19 , batch: 268 , training loss: 4.128500\n",
      "[INFO] Epoch: 19 , batch: 269 , training loss: 4.142112\n",
      "[INFO] Epoch: 19 , batch: 270 , training loss: 4.164938\n",
      "[INFO] Epoch: 19 , batch: 271 , training loss: 4.209983\n",
      "[INFO] Epoch: 19 , batch: 272 , training loss: 4.179637\n",
      "[INFO] Epoch: 19 , batch: 273 , training loss: 4.184257\n",
      "[INFO] Epoch: 19 , batch: 274 , training loss: 4.275609\n",
      "[INFO] Epoch: 19 , batch: 275 , training loss: 4.139925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 19 , batch: 276 , training loss: 4.213969\n",
      "[INFO] Epoch: 19 , batch: 277 , training loss: 4.356294\n",
      "[INFO] Epoch: 19 , batch: 278 , training loss: 4.029370\n",
      "[INFO] Epoch: 19 , batch: 279 , training loss: 4.042360\n",
      "[INFO] Epoch: 19 , batch: 280 , training loss: 3.994472\n",
      "[INFO] Epoch: 19 , batch: 281 , training loss: 4.136625\n",
      "[INFO] Epoch: 19 , batch: 282 , training loss: 4.037456\n",
      "[INFO] Epoch: 19 , batch: 283 , training loss: 4.071913\n",
      "[INFO] Epoch: 19 , batch: 284 , training loss: 4.091707\n",
      "[INFO] Epoch: 19 , batch: 285 , training loss: 4.067021\n",
      "[INFO] Epoch: 19 , batch: 286 , training loss: 4.029466\n",
      "[INFO] Epoch: 19 , batch: 287 , training loss: 3.970726\n",
      "[INFO] Epoch: 19 , batch: 288 , training loss: 3.949079\n",
      "[INFO] Epoch: 19 , batch: 289 , training loss: 4.027411\n",
      "[INFO] Epoch: 19 , batch: 290 , training loss: 3.797639\n",
      "[INFO] Epoch: 19 , batch: 291 , training loss: 3.779153\n",
      "[INFO] Epoch: 19 , batch: 292 , training loss: 3.891259\n",
      "[INFO] Epoch: 19 , batch: 293 , training loss: 3.825815\n",
      "[INFO] Epoch: 19 , batch: 294 , training loss: 4.502616\n",
      "[INFO] Epoch: 19 , batch: 295 , training loss: 4.293153\n",
      "[INFO] Epoch: 19 , batch: 296 , training loss: 4.191411\n",
      "[INFO] Epoch: 19 , batch: 297 , training loss: 4.150453\n",
      "[INFO] Epoch: 19 , batch: 298 , training loss: 3.983747\n",
      "[INFO] Epoch: 19 , batch: 299 , training loss: 4.010497\n",
      "[INFO] Epoch: 19 , batch: 300 , training loss: 4.007947\n",
      "[INFO] Epoch: 19 , batch: 301 , training loss: 3.928041\n",
      "[INFO] Epoch: 19 , batch: 302 , training loss: 4.092938\n",
      "[INFO] Epoch: 19 , batch: 303 , training loss: 4.117413\n",
      "[INFO] Epoch: 19 , batch: 304 , training loss: 4.275770\n",
      "[INFO] Epoch: 19 , batch: 305 , training loss: 4.075535\n",
      "[INFO] Epoch: 19 , batch: 306 , training loss: 4.196435\n",
      "[INFO] Epoch: 19 , batch: 307 , training loss: 4.206069\n",
      "[INFO] Epoch: 19 , batch: 308 , training loss: 4.019980\n",
      "[INFO] Epoch: 19 , batch: 309 , training loss: 4.036863\n",
      "[INFO] Epoch: 19 , batch: 310 , training loss: 3.940650\n",
      "[INFO] Epoch: 19 , batch: 311 , training loss: 3.935179\n",
      "[INFO] Epoch: 19 , batch: 312 , training loss: 3.844479\n",
      "[INFO] Epoch: 19 , batch: 313 , training loss: 3.946094\n",
      "[INFO] Epoch: 19 , batch: 314 , training loss: 4.009991\n",
      "[INFO] Epoch: 19 , batch: 315 , training loss: 4.097927\n",
      "[INFO] Epoch: 19 , batch: 316 , training loss: 4.356081\n",
      "[INFO] Epoch: 19 , batch: 317 , training loss: 4.738358\n",
      "[INFO] Epoch: 19 , batch: 318 , training loss: 4.875708\n",
      "[INFO] Epoch: 19 , batch: 319 , training loss: 4.534463\n",
      "[INFO] Epoch: 19 , batch: 320 , training loss: 4.068869\n",
      "[INFO] Epoch: 19 , batch: 321 , training loss: 3.873046\n",
      "[INFO] Epoch: 19 , batch: 322 , training loss: 3.988601\n",
      "[INFO] Epoch: 19 , batch: 323 , training loss: 4.020639\n",
      "[INFO] Epoch: 19 , batch: 324 , training loss: 4.017867\n",
      "[INFO] Epoch: 19 , batch: 325 , training loss: 4.136709\n",
      "[INFO] Epoch: 19 , batch: 326 , training loss: 4.173407\n",
      "[INFO] Epoch: 19 , batch: 327 , training loss: 4.101480\n",
      "[INFO] Epoch: 19 , batch: 328 , training loss: 4.109252\n",
      "[INFO] Epoch: 19 , batch: 329 , training loss: 3.992424\n",
      "[INFO] Epoch: 19 , batch: 330 , training loss: 4.006530\n",
      "[INFO] Epoch: 19 , batch: 331 , training loss: 4.171707\n",
      "[INFO] Epoch: 19 , batch: 332 , training loss: 4.002766\n",
      "[INFO] Epoch: 19 , batch: 333 , training loss: 3.985445\n",
      "[INFO] Epoch: 19 , batch: 334 , training loss: 4.000440\n",
      "[INFO] Epoch: 19 , batch: 335 , training loss: 4.128214\n",
      "[INFO] Epoch: 19 , batch: 336 , training loss: 4.120350\n",
      "[INFO] Epoch: 19 , batch: 337 , training loss: 4.191454\n",
      "[INFO] Epoch: 19 , batch: 338 , training loss: 4.397316\n",
      "[INFO] Epoch: 19 , batch: 339 , training loss: 4.200595\n",
      "[INFO] Epoch: 19 , batch: 340 , training loss: 4.396587\n",
      "[INFO] Epoch: 19 , batch: 341 , training loss: 4.130250\n",
      "[INFO] Epoch: 19 , batch: 342 , training loss: 3.919035\n",
      "[INFO] Epoch: 19 , batch: 343 , training loss: 4.012327\n",
      "[INFO] Epoch: 19 , batch: 344 , training loss: 3.863300\n",
      "[INFO] Epoch: 19 , batch: 345 , training loss: 4.002524\n",
      "[INFO] Epoch: 19 , batch: 346 , training loss: 4.029993\n",
      "[INFO] Epoch: 19 , batch: 347 , training loss: 3.932461\n",
      "[INFO] Epoch: 19 , batch: 348 , training loss: 4.072299\n",
      "[INFO] Epoch: 19 , batch: 349 , training loss: 4.165482\n",
      "[INFO] Epoch: 19 , batch: 350 , training loss: 4.009891\n",
      "[INFO] Epoch: 19 , batch: 351 , training loss: 4.081264\n",
      "[INFO] Epoch: 19 , batch: 352 , training loss: 4.090667\n",
      "[INFO] Epoch: 19 , batch: 353 , training loss: 4.075435\n",
      "[INFO] Epoch: 19 , batch: 354 , training loss: 4.165575\n",
      "[INFO] Epoch: 19 , batch: 355 , training loss: 4.159352\n",
      "[INFO] Epoch: 19 , batch: 356 , training loss: 4.028418\n",
      "[INFO] Epoch: 19 , batch: 357 , training loss: 4.124268\n",
      "[INFO] Epoch: 19 , batch: 358 , training loss: 4.030470\n",
      "[INFO] Epoch: 19 , batch: 359 , training loss: 4.027503\n",
      "[INFO] Epoch: 19 , batch: 360 , training loss: 4.100609\n",
      "[INFO] Epoch: 19 , batch: 361 , training loss: 4.072853\n",
      "[INFO] Epoch: 19 , batch: 362 , training loss: 4.191202\n",
      "[INFO] Epoch: 19 , batch: 363 , training loss: 4.070810\n",
      "[INFO] Epoch: 19 , batch: 364 , training loss: 4.146957\n",
      "[INFO] Epoch: 19 , batch: 365 , training loss: 4.020741\n",
      "[INFO] Epoch: 19 , batch: 366 , training loss: 4.155975\n",
      "[INFO] Epoch: 19 , batch: 367 , training loss: 4.192276\n",
      "[INFO] Epoch: 19 , batch: 368 , training loss: 4.651962\n",
      "[INFO] Epoch: 19 , batch: 369 , training loss: 4.303883\n",
      "[INFO] Epoch: 19 , batch: 370 , training loss: 4.063385\n",
      "[INFO] Epoch: 19 , batch: 371 , training loss: 4.482603\n",
      "[INFO] Epoch: 19 , batch: 372 , training loss: 4.780013\n",
      "[INFO] Epoch: 19 , batch: 373 , training loss: 4.827627\n",
      "[INFO] Epoch: 19 , batch: 374 , training loss: 4.888577\n",
      "[INFO] Epoch: 19 , batch: 375 , training loss: 4.885008\n",
      "[INFO] Epoch: 19 , batch: 376 , training loss: 4.791022\n",
      "[INFO] Epoch: 19 , batch: 377 , training loss: 4.496472\n",
      "[INFO] Epoch: 19 , batch: 378 , training loss: 4.589104\n",
      "[INFO] Epoch: 19 , batch: 379 , training loss: 4.575933\n",
      "[INFO] Epoch: 19 , batch: 380 , training loss: 4.713222\n",
      "[INFO] Epoch: 19 , batch: 381 , training loss: 4.464313\n",
      "[INFO] Epoch: 19 , batch: 382 , training loss: 4.750535\n",
      "[INFO] Epoch: 19 , batch: 383 , training loss: 4.756825\n",
      "[INFO] Epoch: 19 , batch: 384 , training loss: 4.767963\n",
      "[INFO] Epoch: 19 , batch: 385 , training loss: 4.441724\n",
      "[INFO] Epoch: 19 , batch: 386 , training loss: 4.675894\n",
      "[INFO] Epoch: 19 , batch: 387 , training loss: 4.625931\n",
      "[INFO] Epoch: 19 , batch: 388 , training loss: 4.431759\n",
      "[INFO] Epoch: 19 , batch: 389 , training loss: 4.261990\n",
      "[INFO] Epoch: 19 , batch: 390 , training loss: 4.280963\n",
      "[INFO] Epoch: 19 , batch: 391 , training loss: 4.299310\n",
      "[INFO] Epoch: 19 , batch: 392 , training loss: 4.656554\n",
      "[INFO] Epoch: 19 , batch: 393 , training loss: 4.555625\n",
      "[INFO] Epoch: 19 , batch: 394 , training loss: 4.651054\n",
      "[INFO] Epoch: 19 , batch: 395 , training loss: 4.450561\n",
      "[INFO] Epoch: 19 , batch: 396 , training loss: 4.268383\n",
      "[INFO] Epoch: 19 , batch: 397 , training loss: 4.415802\n",
      "[INFO] Epoch: 19 , batch: 398 , training loss: 4.270651\n",
      "[INFO] Epoch: 19 , batch: 399 , training loss: 4.348953\n",
      "[INFO] Epoch: 19 , batch: 400 , training loss: 4.327959\n",
      "[INFO] Epoch: 19 , batch: 401 , training loss: 4.763977\n",
      "[INFO] Epoch: 19 , batch: 402 , training loss: 4.468830\n",
      "[INFO] Epoch: 19 , batch: 403 , training loss: 4.302949\n",
      "[INFO] Epoch: 19 , batch: 404 , training loss: 4.468803\n",
      "[INFO] Epoch: 19 , batch: 405 , training loss: 4.525245\n",
      "[INFO] Epoch: 19 , batch: 406 , training loss: 4.443516\n",
      "[INFO] Epoch: 19 , batch: 407 , training loss: 4.497591\n",
      "[INFO] Epoch: 19 , batch: 408 , training loss: 4.407677\n",
      "[INFO] Epoch: 19 , batch: 409 , training loss: 4.446577\n",
      "[INFO] Epoch: 19 , batch: 410 , training loss: 4.517586\n",
      "[INFO] Epoch: 19 , batch: 411 , training loss: 4.680547\n",
      "[INFO] Epoch: 19 , batch: 412 , training loss: 4.504925\n",
      "[INFO] Epoch: 19 , batch: 413 , training loss: 4.374554\n",
      "[INFO] Epoch: 19 , batch: 414 , training loss: 4.406219\n",
      "[INFO] Epoch: 19 , batch: 415 , training loss: 4.460999\n",
      "[INFO] Epoch: 19 , batch: 416 , training loss: 4.532022\n",
      "[INFO] Epoch: 19 , batch: 417 , training loss: 4.422971\n",
      "[INFO] Epoch: 19 , batch: 418 , training loss: 4.495219\n",
      "[INFO] Epoch: 19 , batch: 419 , training loss: 4.433553\n",
      "[INFO] Epoch: 19 , batch: 420 , training loss: 4.408850\n",
      "[INFO] Epoch: 19 , batch: 421 , training loss: 4.405947\n",
      "[INFO] Epoch: 19 , batch: 422 , training loss: 4.259089\n",
      "[INFO] Epoch: 19 , batch: 423 , training loss: 4.463248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 19 , batch: 424 , training loss: 4.629758\n",
      "[INFO] Epoch: 19 , batch: 425 , training loss: 4.511888\n",
      "[INFO] Epoch: 19 , batch: 426 , training loss: 4.220693\n",
      "[INFO] Epoch: 19 , batch: 427 , training loss: 4.494713\n",
      "[INFO] Epoch: 19 , batch: 428 , training loss: 4.340917\n",
      "[INFO] Epoch: 19 , batch: 429 , training loss: 4.255120\n",
      "[INFO] Epoch: 19 , batch: 430 , training loss: 4.475476\n",
      "[INFO] Epoch: 19 , batch: 431 , training loss: 4.095691\n",
      "[INFO] Epoch: 19 , batch: 432 , training loss: 4.148523\n",
      "[INFO] Epoch: 19 , batch: 433 , training loss: 4.177936\n",
      "[INFO] Epoch: 19 , batch: 434 , training loss: 4.062247\n",
      "[INFO] Epoch: 19 , batch: 435 , training loss: 4.419296\n",
      "[INFO] Epoch: 19 , batch: 436 , training loss: 4.466689\n",
      "[INFO] Epoch: 19 , batch: 437 , training loss: 4.256872\n",
      "[INFO] Epoch: 19 , batch: 438 , training loss: 4.099122\n",
      "[INFO] Epoch: 19 , batch: 439 , training loss: 4.330520\n",
      "[INFO] Epoch: 19 , batch: 440 , training loss: 4.468379\n",
      "[INFO] Epoch: 19 , batch: 441 , training loss: 4.543179\n",
      "[INFO] Epoch: 19 , batch: 442 , training loss: 4.302703\n",
      "[INFO] Epoch: 19 , batch: 443 , training loss: 4.520707\n",
      "[INFO] Epoch: 19 , batch: 444 , training loss: 4.095735\n",
      "[INFO] Epoch: 19 , batch: 445 , training loss: 4.015975\n",
      "[INFO] Epoch: 19 , batch: 446 , training loss: 3.949039\n",
      "[INFO] Epoch: 19 , batch: 447 , training loss: 4.132864\n",
      "[INFO] Epoch: 19 , batch: 448 , training loss: 4.264426\n",
      "[INFO] Epoch: 19 , batch: 449 , training loss: 4.648678\n",
      "[INFO] Epoch: 19 , batch: 450 , training loss: 4.708785\n",
      "[INFO] Epoch: 19 , batch: 451 , training loss: 4.587072\n",
      "[INFO] Epoch: 19 , batch: 452 , training loss: 4.416239\n",
      "[INFO] Epoch: 19 , batch: 453 , training loss: 4.183331\n",
      "[INFO] Epoch: 19 , batch: 454 , training loss: 4.332802\n",
      "[INFO] Epoch: 19 , batch: 455 , training loss: 4.388105\n",
      "[INFO] Epoch: 19 , batch: 456 , training loss: 4.382379\n",
      "[INFO] Epoch: 19 , batch: 457 , training loss: 4.470685\n",
      "[INFO] Epoch: 19 , batch: 458 , training loss: 4.200558\n",
      "[INFO] Epoch: 19 , batch: 459 , training loss: 4.169729\n",
      "[INFO] Epoch: 19 , batch: 460 , training loss: 4.298720\n",
      "[INFO] Epoch: 19 , batch: 461 , training loss: 4.251334\n",
      "[INFO] Epoch: 19 , batch: 462 , training loss: 4.305075\n",
      "[INFO] Epoch: 19 , batch: 463 , training loss: 4.222550\n",
      "[INFO] Epoch: 19 , batch: 464 , training loss: 4.394142\n",
      "[INFO] Epoch: 19 , batch: 465 , training loss: 4.335022\n",
      "[INFO] Epoch: 19 , batch: 466 , training loss: 4.439430\n",
      "[INFO] Epoch: 19 , batch: 467 , training loss: 4.411152\n",
      "[INFO] Epoch: 19 , batch: 468 , training loss: 4.371824\n",
      "[INFO] Epoch: 19 , batch: 469 , training loss: 4.419058\n",
      "[INFO] Epoch: 19 , batch: 470 , training loss: 4.198563\n",
      "[INFO] Epoch: 19 , batch: 471 , training loss: 4.325770\n",
      "[INFO] Epoch: 19 , batch: 472 , training loss: 4.376749\n",
      "[INFO] Epoch: 19 , batch: 473 , training loss: 4.295733\n",
      "[INFO] Epoch: 19 , batch: 474 , training loss: 4.076000\n",
      "[INFO] Epoch: 19 , batch: 475 , training loss: 3.955375\n",
      "[INFO] Epoch: 19 , batch: 476 , training loss: 4.362305\n",
      "[INFO] Epoch: 19 , batch: 477 , training loss: 4.458992\n",
      "[INFO] Epoch: 19 , batch: 478 , training loss: 4.475669\n",
      "[INFO] Epoch: 19 , batch: 479 , training loss: 4.430547\n",
      "[INFO] Epoch: 19 , batch: 480 , training loss: 4.558644\n",
      "[INFO] Epoch: 19 , batch: 481 , training loss: 4.459858\n",
      "[INFO] Epoch: 19 , batch: 482 , training loss: 4.554597\n",
      "[INFO] Epoch: 19 , batch: 483 , training loss: 4.399428\n",
      "[INFO] Epoch: 19 , batch: 484 , training loss: 4.184581\n",
      "[INFO] Epoch: 19 , batch: 485 , training loss: 4.288204\n",
      "[INFO] Epoch: 19 , batch: 486 , training loss: 4.195488\n",
      "[INFO] Epoch: 19 , batch: 487 , training loss: 4.188889\n",
      "[INFO] Epoch: 19 , batch: 488 , training loss: 4.354695\n",
      "[INFO] Epoch: 19 , batch: 489 , training loss: 4.265563\n",
      "[INFO] Epoch: 19 , batch: 490 , training loss: 4.328276\n",
      "[INFO] Epoch: 19 , batch: 491 , training loss: 4.265633\n",
      "[INFO] Epoch: 19 , batch: 492 , training loss: 4.226293\n",
      "[INFO] Epoch: 19 , batch: 493 , training loss: 4.388766\n",
      "[INFO] Epoch: 19 , batch: 494 , training loss: 4.292454\n",
      "[INFO] Epoch: 19 , batch: 495 , training loss: 4.434793\n",
      "[INFO] Epoch: 19 , batch: 496 , training loss: 4.324711\n",
      "[INFO] Epoch: 19 , batch: 497 , training loss: 4.365156\n",
      "[INFO] Epoch: 19 , batch: 498 , training loss: 4.353580\n",
      "[INFO] Epoch: 19 , batch: 499 , training loss: 4.414476\n",
      "[INFO] Epoch: 19 , batch: 500 , training loss: 4.555295\n",
      "[INFO] Epoch: 19 , batch: 501 , training loss: 4.916964\n",
      "[INFO] Epoch: 19 , batch: 502 , training loss: 5.008591\n",
      "[INFO] Epoch: 19 , batch: 503 , training loss: 4.692282\n",
      "[INFO] Epoch: 19 , batch: 504 , training loss: 4.786856\n",
      "[INFO] Epoch: 19 , batch: 505 , training loss: 4.750456\n",
      "[INFO] Epoch: 19 , batch: 506 , training loss: 4.708537\n",
      "[INFO] Epoch: 19 , batch: 507 , training loss: 4.763790\n",
      "[INFO] Epoch: 19 , batch: 508 , training loss: 4.701943\n",
      "[INFO] Epoch: 19 , batch: 509 , training loss: 4.472738\n",
      "[INFO] Epoch: 19 , batch: 510 , training loss: 4.531425\n",
      "[INFO] Epoch: 19 , batch: 511 , training loss: 4.467023\n",
      "[INFO] Epoch: 19 , batch: 512 , training loss: 4.544398\n",
      "[INFO] Epoch: 19 , batch: 513 , training loss: 4.810582\n",
      "[INFO] Epoch: 19 , batch: 514 , training loss: 4.457296\n",
      "[INFO] Epoch: 19 , batch: 515 , training loss: 4.714469\n",
      "[INFO] Epoch: 19 , batch: 516 , training loss: 4.497018\n",
      "[INFO] Epoch: 19 , batch: 517 , training loss: 4.464210\n",
      "[INFO] Epoch: 19 , batch: 518 , training loss: 4.411528\n",
      "[INFO] Epoch: 19 , batch: 519 , training loss: 4.270878\n",
      "[INFO] Epoch: 19 , batch: 520 , training loss: 4.518291\n",
      "[INFO] Epoch: 19 , batch: 521 , training loss: 4.492763\n",
      "[INFO] Epoch: 19 , batch: 522 , training loss: 4.574116\n",
      "[INFO] Epoch: 19 , batch: 523 , training loss: 4.480617\n",
      "[INFO] Epoch: 19 , batch: 524 , training loss: 4.752688\n",
      "[INFO] Epoch: 19 , batch: 525 , training loss: 4.628179\n",
      "[INFO] Epoch: 19 , batch: 526 , training loss: 4.422141\n",
      "[INFO] Epoch: 19 , batch: 527 , training loss: 4.473925\n",
      "[INFO] Epoch: 19 , batch: 528 , training loss: 4.485495\n",
      "[INFO] Epoch: 19 , batch: 529 , training loss: 4.457862\n",
      "[INFO] Epoch: 19 , batch: 530 , training loss: 4.313944\n",
      "[INFO] Epoch: 19 , batch: 531 , training loss: 4.463437\n",
      "[INFO] Epoch: 19 , batch: 532 , training loss: 4.362701\n",
      "[INFO] Epoch: 19 , batch: 533 , training loss: 4.502795\n",
      "[INFO] Epoch: 19 , batch: 534 , training loss: 4.491609\n",
      "[INFO] Epoch: 19 , batch: 535 , training loss: 4.498946\n",
      "[INFO] Epoch: 19 , batch: 536 , training loss: 4.353803\n",
      "[INFO] Epoch: 19 , batch: 537 , training loss: 4.316776\n",
      "[INFO] Epoch: 19 , batch: 538 , training loss: 4.409648\n",
      "[INFO] Epoch: 19 , batch: 539 , training loss: 4.531790\n",
      "[INFO] Epoch: 19 , batch: 540 , training loss: 5.059734\n",
      "[INFO] Epoch: 19 , batch: 541 , training loss: 4.905768\n",
      "[INFO] Epoch: 19 , batch: 542 , training loss: 4.777070\n",
      "[INFO] Epoch: 20 , batch: 0 , training loss: 3.853400\n",
      "[INFO] Epoch: 20 , batch: 1 , training loss: 3.704304\n",
      "[INFO] Epoch: 20 , batch: 2 , training loss: 3.811826\n",
      "[INFO] Epoch: 20 , batch: 3 , training loss: 3.664180\n",
      "[INFO] Epoch: 20 , batch: 4 , training loss: 4.015562\n",
      "[INFO] Epoch: 20 , batch: 5 , training loss: 3.653293\n",
      "[INFO] Epoch: 20 , batch: 6 , training loss: 4.020977\n",
      "[INFO] Epoch: 20 , batch: 7 , training loss: 3.914645\n",
      "[INFO] Epoch: 20 , batch: 8 , training loss: 3.585567\n",
      "[INFO] Epoch: 20 , batch: 9 , training loss: 3.857098\n",
      "[INFO] Epoch: 20 , batch: 10 , training loss: 3.755471\n",
      "[INFO] Epoch: 20 , batch: 11 , training loss: 3.689184\n",
      "[INFO] Epoch: 20 , batch: 12 , training loss: 3.592416\n",
      "[INFO] Epoch: 20 , batch: 13 , training loss: 3.646282\n",
      "[INFO] Epoch: 20 , batch: 14 , training loss: 3.508415\n",
      "[INFO] Epoch: 20 , batch: 15 , training loss: 3.774215\n",
      "[INFO] Epoch: 20 , batch: 16 , training loss: 3.599680\n",
      "[INFO] Epoch: 20 , batch: 17 , training loss: 3.764916\n",
      "[INFO] Epoch: 20 , batch: 18 , training loss: 3.656746\n",
      "[INFO] Epoch: 20 , batch: 19 , training loss: 3.474297\n",
      "[INFO] Epoch: 20 , batch: 20 , training loss: 3.413682\n",
      "[INFO] Epoch: 20 , batch: 21 , training loss: 3.553677\n",
      "[INFO] Epoch: 20 , batch: 22 , training loss: 3.464901\n",
      "[INFO] Epoch: 20 , batch: 23 , training loss: 3.666081\n",
      "[INFO] Epoch: 20 , batch: 24 , training loss: 3.552377\n",
      "[INFO] Epoch: 20 , batch: 25 , training loss: 3.647003\n",
      "[INFO] Epoch: 20 , batch: 26 , training loss: 3.515907\n",
      "[INFO] Epoch: 20 , batch: 27 , training loss: 3.481868\n",
      "[INFO] Epoch: 20 , batch: 28 , training loss: 3.709141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 20 , batch: 29 , training loss: 3.514867\n",
      "[INFO] Epoch: 20 , batch: 30 , training loss: 3.491932\n",
      "[INFO] Epoch: 20 , batch: 31 , training loss: 3.638490\n",
      "[INFO] Epoch: 20 , batch: 32 , training loss: 3.569523\n",
      "[INFO] Epoch: 20 , batch: 33 , training loss: 3.654387\n",
      "[INFO] Epoch: 20 , batch: 34 , training loss: 3.647673\n",
      "[INFO] Epoch: 20 , batch: 35 , training loss: 3.557193\n",
      "[INFO] Epoch: 20 , batch: 36 , training loss: 3.667393\n",
      "[INFO] Epoch: 20 , batch: 37 , training loss: 3.542650\n",
      "[INFO] Epoch: 20 , batch: 38 , training loss: 3.642567\n",
      "[INFO] Epoch: 20 , batch: 39 , training loss: 3.427710\n",
      "[INFO] Epoch: 20 , batch: 40 , training loss: 3.626229\n",
      "[INFO] Epoch: 20 , batch: 41 , training loss: 3.600444\n",
      "[INFO] Epoch: 20 , batch: 42 , training loss: 4.078593\n",
      "[INFO] Epoch: 20 , batch: 43 , training loss: 3.771569\n",
      "[INFO] Epoch: 20 , batch: 44 , training loss: 4.150609\n",
      "[INFO] Epoch: 20 , batch: 45 , training loss: 4.104061\n",
      "[INFO] Epoch: 20 , batch: 46 , training loss: 4.122443\n",
      "[INFO] Epoch: 20 , batch: 47 , training loss: 3.736704\n",
      "[INFO] Epoch: 20 , batch: 48 , training loss: 3.685495\n",
      "[INFO] Epoch: 20 , batch: 49 , training loss: 3.908213\n",
      "[INFO] Epoch: 20 , batch: 50 , training loss: 3.684731\n",
      "[INFO] Epoch: 20 , batch: 51 , training loss: 3.897669\n",
      "[INFO] Epoch: 20 , batch: 52 , training loss: 3.717716\n",
      "[INFO] Epoch: 20 , batch: 53 , training loss: 3.841033\n",
      "[INFO] Epoch: 20 , batch: 54 , training loss: 3.850050\n",
      "[INFO] Epoch: 20 , batch: 55 , training loss: 3.946366\n",
      "[INFO] Epoch: 20 , batch: 56 , training loss: 3.796783\n",
      "[INFO] Epoch: 20 , batch: 57 , training loss: 3.691600\n",
      "[INFO] Epoch: 20 , batch: 58 , training loss: 3.744955\n",
      "[INFO] Epoch: 20 , batch: 59 , training loss: 3.827415\n",
      "[INFO] Epoch: 20 , batch: 60 , training loss: 3.770411\n",
      "[INFO] Epoch: 20 , batch: 61 , training loss: 3.824881\n",
      "[INFO] Epoch: 20 , batch: 62 , training loss: 3.725060\n",
      "[INFO] Epoch: 20 , batch: 63 , training loss: 3.878661\n",
      "[INFO] Epoch: 20 , batch: 64 , training loss: 4.129272\n",
      "[INFO] Epoch: 20 , batch: 65 , training loss: 3.789303\n",
      "[INFO] Epoch: 20 , batch: 66 , training loss: 3.660030\n",
      "[INFO] Epoch: 20 , batch: 67 , training loss: 3.645912\n",
      "[INFO] Epoch: 20 , batch: 68 , training loss: 3.906027\n",
      "[INFO] Epoch: 20 , batch: 69 , training loss: 3.772836\n",
      "[INFO] Epoch: 20 , batch: 70 , training loss: 4.000675\n",
      "[INFO] Epoch: 20 , batch: 71 , training loss: 3.855949\n",
      "[INFO] Epoch: 20 , batch: 72 , training loss: 3.903395\n",
      "[INFO] Epoch: 20 , batch: 73 , training loss: 3.860517\n",
      "[INFO] Epoch: 20 , batch: 74 , training loss: 3.973171\n",
      "[INFO] Epoch: 20 , batch: 75 , training loss: 3.823545\n",
      "[INFO] Epoch: 20 , batch: 76 , training loss: 3.941249\n",
      "[INFO] Epoch: 20 , batch: 77 , training loss: 3.837863\n",
      "[INFO] Epoch: 20 , batch: 78 , training loss: 3.955376\n",
      "[INFO] Epoch: 20 , batch: 79 , training loss: 3.788597\n",
      "[INFO] Epoch: 20 , batch: 80 , training loss: 4.004775\n",
      "[INFO] Epoch: 20 , batch: 81 , training loss: 3.931857\n",
      "[INFO] Epoch: 20 , batch: 82 , training loss: 3.912073\n",
      "[INFO] Epoch: 20 , batch: 83 , training loss: 4.027007\n",
      "[INFO] Epoch: 20 , batch: 84 , training loss: 3.941588\n",
      "[INFO] Epoch: 20 , batch: 85 , training loss: 4.046571\n",
      "[INFO] Epoch: 20 , batch: 86 , training loss: 3.990576\n",
      "[INFO] Epoch: 20 , batch: 87 , training loss: 3.936903\n",
      "[INFO] Epoch: 20 , batch: 88 , training loss: 4.086440\n",
      "[INFO] Epoch: 20 , batch: 89 , training loss: 3.861151\n",
      "[INFO] Epoch: 20 , batch: 90 , training loss: 3.988221\n",
      "[INFO] Epoch: 20 , batch: 91 , training loss: 3.879249\n",
      "[INFO] Epoch: 20 , batch: 92 , training loss: 3.910303\n",
      "[INFO] Epoch: 20 , batch: 93 , training loss: 4.004594\n",
      "[INFO] Epoch: 20 , batch: 94 , training loss: 4.155068\n",
      "[INFO] Epoch: 20 , batch: 95 , training loss: 3.937385\n",
      "[INFO] Epoch: 20 , batch: 96 , training loss: 3.902945\n",
      "[INFO] Epoch: 20 , batch: 97 , training loss: 3.870758\n",
      "[INFO] Epoch: 20 , batch: 98 , training loss: 3.805380\n",
      "[INFO] Epoch: 20 , batch: 99 , training loss: 3.925168\n",
      "[INFO] Epoch: 20 , batch: 100 , training loss: 3.762763\n",
      "[INFO] Epoch: 20 , batch: 101 , training loss: 3.816922\n",
      "[INFO] Epoch: 20 , batch: 102 , training loss: 4.008307\n",
      "[INFO] Epoch: 20 , batch: 103 , training loss: 3.759947\n",
      "[INFO] Epoch: 20 , batch: 104 , training loss: 3.716065\n",
      "[INFO] Epoch: 20 , batch: 105 , training loss: 3.975808\n",
      "[INFO] Epoch: 20 , batch: 106 , training loss: 4.002755\n",
      "[INFO] Epoch: 20 , batch: 107 , training loss: 3.835855\n",
      "[INFO] Epoch: 20 , batch: 108 , training loss: 3.771053\n",
      "[INFO] Epoch: 20 , batch: 109 , training loss: 3.715694\n",
      "[INFO] Epoch: 20 , batch: 110 , training loss: 3.877710\n",
      "[INFO] Epoch: 20 , batch: 111 , training loss: 3.941076\n",
      "[INFO] Epoch: 20 , batch: 112 , training loss: 3.898093\n",
      "[INFO] Epoch: 20 , batch: 113 , training loss: 3.887871\n",
      "[INFO] Epoch: 20 , batch: 114 , training loss: 3.919009\n",
      "[INFO] Epoch: 20 , batch: 115 , training loss: 3.887449\n",
      "[INFO] Epoch: 20 , batch: 116 , training loss: 3.791158\n",
      "[INFO] Epoch: 20 , batch: 117 , training loss: 4.014793\n",
      "[INFO] Epoch: 20 , batch: 118 , training loss: 3.975126\n",
      "[INFO] Epoch: 20 , batch: 119 , training loss: 4.142517\n",
      "[INFO] Epoch: 20 , batch: 120 , training loss: 4.101843\n",
      "[INFO] Epoch: 20 , batch: 121 , training loss: 3.968674\n",
      "[INFO] Epoch: 20 , batch: 122 , training loss: 3.840324\n",
      "[INFO] Epoch: 20 , batch: 123 , training loss: 3.893639\n",
      "[INFO] Epoch: 20 , batch: 124 , training loss: 4.019444\n",
      "[INFO] Epoch: 20 , batch: 125 , training loss: 3.767044\n",
      "[INFO] Epoch: 20 , batch: 126 , training loss: 3.800131\n",
      "[INFO] Epoch: 20 , batch: 127 , training loss: 3.829105\n",
      "[INFO] Epoch: 20 , batch: 128 , training loss: 3.970943\n",
      "[INFO] Epoch: 20 , batch: 129 , training loss: 3.930867\n",
      "[INFO] Epoch: 20 , batch: 130 , training loss: 3.892248\n",
      "[INFO] Epoch: 20 , batch: 131 , training loss: 3.929520\n",
      "[INFO] Epoch: 20 , batch: 132 , training loss: 3.921446\n",
      "[INFO] Epoch: 20 , batch: 133 , training loss: 3.887536\n",
      "[INFO] Epoch: 20 , batch: 134 , training loss: 3.626346\n",
      "[INFO] Epoch: 20 , batch: 135 , training loss: 3.723702\n",
      "[INFO] Epoch: 20 , batch: 136 , training loss: 4.008515\n",
      "[INFO] Epoch: 20 , batch: 137 , training loss: 3.928674\n",
      "[INFO] Epoch: 20 , batch: 138 , training loss: 3.959657\n",
      "[INFO] Epoch: 20 , batch: 139 , training loss: 4.552630\n",
      "[INFO] Epoch: 20 , batch: 140 , training loss: 4.341613\n",
      "[INFO] Epoch: 20 , batch: 141 , training loss: 4.105808\n",
      "[INFO] Epoch: 20 , batch: 142 , training loss: 3.805766\n",
      "[INFO] Epoch: 20 , batch: 143 , training loss: 3.939018\n",
      "[INFO] Epoch: 20 , batch: 144 , training loss: 3.803014\n",
      "[INFO] Epoch: 20 , batch: 145 , training loss: 3.885817\n",
      "[INFO] Epoch: 20 , batch: 146 , training loss: 4.088383\n",
      "[INFO] Epoch: 20 , batch: 147 , training loss: 3.742929\n",
      "[INFO] Epoch: 20 , batch: 148 , training loss: 3.716435\n",
      "[INFO] Epoch: 20 , batch: 149 , training loss: 3.794578\n",
      "[INFO] Epoch: 20 , batch: 150 , training loss: 4.059296\n",
      "[INFO] Epoch: 20 , batch: 151 , training loss: 3.888826\n",
      "[INFO] Epoch: 20 , batch: 152 , training loss: 3.903603\n",
      "[INFO] Epoch: 20 , batch: 153 , training loss: 3.907994\n",
      "[INFO] Epoch: 20 , batch: 154 , training loss: 4.025689\n",
      "[INFO] Epoch: 20 , batch: 155 , training loss: 4.205814\n",
      "[INFO] Epoch: 20 , batch: 156 , training loss: 3.972626\n",
      "[INFO] Epoch: 20 , batch: 157 , training loss: 3.927211\n",
      "[INFO] Epoch: 20 , batch: 158 , training loss: 4.099112\n",
      "[INFO] Epoch: 20 , batch: 159 , training loss: 4.007143\n",
      "[INFO] Epoch: 20 , batch: 160 , training loss: 4.278828\n",
      "[INFO] Epoch: 20 , batch: 161 , training loss: 4.347230\n",
      "[INFO] Epoch: 20 , batch: 162 , training loss: 4.326641\n",
      "[INFO] Epoch: 20 , batch: 163 , training loss: 4.511580\n",
      "[INFO] Epoch: 20 , batch: 164 , training loss: 4.376818\n",
      "[INFO] Epoch: 20 , batch: 165 , training loss: 4.336088\n",
      "[INFO] Epoch: 20 , batch: 166 , training loss: 4.246057\n",
      "[INFO] Epoch: 20 , batch: 167 , training loss: 4.381309\n",
      "[INFO] Epoch: 20 , batch: 168 , training loss: 4.065814\n",
      "[INFO] Epoch: 20 , batch: 169 , training loss: 4.013006\n",
      "[INFO] Epoch: 20 , batch: 170 , training loss: 4.184823\n",
      "[INFO] Epoch: 20 , batch: 171 , training loss: 3.608798\n",
      "[INFO] Epoch: 20 , batch: 172 , training loss: 3.819496\n",
      "[INFO] Epoch: 20 , batch: 173 , training loss: 4.123479\n",
      "[INFO] Epoch: 20 , batch: 174 , training loss: 4.551523\n",
      "[INFO] Epoch: 20 , batch: 175 , training loss: 4.869807\n",
      "[INFO] Epoch: 20 , batch: 176 , training loss: 4.582206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 20 , batch: 177 , training loss: 4.187829\n",
      "[INFO] Epoch: 20 , batch: 178 , training loss: 4.174769\n",
      "[INFO] Epoch: 20 , batch: 179 , training loss: 4.229099\n",
      "[INFO] Epoch: 20 , batch: 180 , training loss: 4.155773\n",
      "[INFO] Epoch: 20 , batch: 181 , training loss: 4.428216\n",
      "[INFO] Epoch: 20 , batch: 182 , training loss: 4.372532\n",
      "[INFO] Epoch: 20 , batch: 183 , training loss: 4.338240\n",
      "[INFO] Epoch: 20 , batch: 184 , training loss: 4.234342\n",
      "[INFO] Epoch: 20 , batch: 185 , training loss: 4.202280\n",
      "[INFO] Epoch: 20 , batch: 186 , training loss: 4.315613\n",
      "[INFO] Epoch: 20 , batch: 187 , training loss: 4.423074\n",
      "[INFO] Epoch: 20 , batch: 188 , training loss: 4.426500\n",
      "[INFO] Epoch: 20 , batch: 189 , training loss: 4.317277\n",
      "[INFO] Epoch: 20 , batch: 190 , training loss: 4.336356\n",
      "[INFO] Epoch: 20 , batch: 191 , training loss: 4.467509\n",
      "[INFO] Epoch: 20 , batch: 192 , training loss: 4.291277\n",
      "[INFO] Epoch: 20 , batch: 193 , training loss: 4.388060\n",
      "[INFO] Epoch: 20 , batch: 194 , training loss: 4.339676\n",
      "[INFO] Epoch: 20 , batch: 195 , training loss: 4.269880\n",
      "[INFO] Epoch: 20 , batch: 196 , training loss: 4.152533\n",
      "[INFO] Epoch: 20 , batch: 197 , training loss: 4.218834\n",
      "[INFO] Epoch: 20 , batch: 198 , training loss: 4.113063\n",
      "[INFO] Epoch: 20 , batch: 199 , training loss: 4.256903\n",
      "[INFO] Epoch: 20 , batch: 200 , training loss: 4.174807\n",
      "[INFO] Epoch: 20 , batch: 201 , training loss: 4.087871\n",
      "[INFO] Epoch: 20 , batch: 202 , training loss: 4.065367\n",
      "[INFO] Epoch: 20 , batch: 203 , training loss: 4.191678\n",
      "[INFO] Epoch: 20 , batch: 204 , training loss: 4.283615\n",
      "[INFO] Epoch: 20 , batch: 205 , training loss: 3.903565\n",
      "[INFO] Epoch: 20 , batch: 206 , training loss: 3.797258\n",
      "[INFO] Epoch: 20 , batch: 207 , training loss: 3.804650\n",
      "[INFO] Epoch: 20 , batch: 208 , training loss: 4.118382\n",
      "[INFO] Epoch: 20 , batch: 209 , training loss: 4.079203\n",
      "[INFO] Epoch: 20 , batch: 210 , training loss: 4.100597\n",
      "[INFO] Epoch: 20 , batch: 211 , training loss: 4.091225\n",
      "[INFO] Epoch: 20 , batch: 212 , training loss: 4.201487\n",
      "[INFO] Epoch: 20 , batch: 213 , training loss: 4.157131\n",
      "[INFO] Epoch: 20 , batch: 214 , training loss: 4.215696\n",
      "[INFO] Epoch: 20 , batch: 215 , training loss: 4.462704\n",
      "[INFO] Epoch: 20 , batch: 216 , training loss: 4.154665\n",
      "[INFO] Epoch: 20 , batch: 217 , training loss: 4.096973\n",
      "[INFO] Epoch: 20 , batch: 218 , training loss: 4.090301\n",
      "[INFO] Epoch: 20 , batch: 219 , training loss: 4.180298\n",
      "[INFO] Epoch: 20 , batch: 220 , training loss: 4.025403\n",
      "[INFO] Epoch: 20 , batch: 221 , training loss: 4.027987\n",
      "[INFO] Epoch: 20 , batch: 222 , training loss: 4.167536\n",
      "[INFO] Epoch: 20 , batch: 223 , training loss: 4.280645\n",
      "[INFO] Epoch: 20 , batch: 224 , training loss: 4.315336\n",
      "[INFO] Epoch: 20 , batch: 225 , training loss: 4.196904\n",
      "[INFO] Epoch: 20 , batch: 226 , training loss: 4.311073\n",
      "[INFO] Epoch: 20 , batch: 227 , training loss: 4.284300\n",
      "[INFO] Epoch: 20 , batch: 228 , training loss: 4.309779\n",
      "[INFO] Epoch: 20 , batch: 229 , training loss: 4.183071\n",
      "[INFO] Epoch: 20 , batch: 230 , training loss: 4.035944\n",
      "[INFO] Epoch: 20 , batch: 231 , training loss: 3.905122\n",
      "[INFO] Epoch: 20 , batch: 232 , training loss: 4.049466\n",
      "[INFO] Epoch: 20 , batch: 233 , training loss: 4.078072\n",
      "[INFO] Epoch: 20 , batch: 234 , training loss: 3.772690\n",
      "[INFO] Epoch: 20 , batch: 235 , training loss: 3.882140\n",
      "[INFO] Epoch: 20 , batch: 236 , training loss: 3.971562\n",
      "[INFO] Epoch: 20 , batch: 237 , training loss: 4.195868\n",
      "[INFO] Epoch: 20 , batch: 238 , training loss: 3.970320\n",
      "[INFO] Epoch: 20 , batch: 239 , training loss: 4.028683\n",
      "[INFO] Epoch: 20 , batch: 240 , training loss: 4.040351\n",
      "[INFO] Epoch: 20 , batch: 241 , training loss: 3.871795\n",
      "[INFO] Epoch: 20 , batch: 242 , training loss: 3.886098\n",
      "[INFO] Epoch: 20 , batch: 243 , training loss: 4.160430\n",
      "[INFO] Epoch: 20 , batch: 244 , training loss: 4.114072\n",
      "[INFO] Epoch: 20 , batch: 245 , training loss: 4.083369\n",
      "[INFO] Epoch: 20 , batch: 246 , training loss: 3.786365\n",
      "[INFO] Epoch: 20 , batch: 247 , training loss: 3.951761\n",
      "[INFO] Epoch: 20 , batch: 248 , training loss: 4.038989\n",
      "[INFO] Epoch: 20 , batch: 249 , training loss: 4.005798\n",
      "[INFO] Epoch: 20 , batch: 250 , training loss: 3.801360\n",
      "[INFO] Epoch: 20 , batch: 251 , training loss: 4.272994\n",
      "[INFO] Epoch: 20 , batch: 252 , training loss: 3.967080\n",
      "[INFO] Epoch: 20 , batch: 253 , training loss: 3.899541\n",
      "[INFO] Epoch: 20 , batch: 254 , training loss: 4.162357\n",
      "[INFO] Epoch: 20 , batch: 255 , training loss: 4.120537\n",
      "[INFO] Epoch: 20 , batch: 256 , training loss: 4.111499\n",
      "[INFO] Epoch: 20 , batch: 257 , training loss: 4.293818\n",
      "[INFO] Epoch: 20 , batch: 258 , training loss: 4.300144\n",
      "[INFO] Epoch: 20 , batch: 259 , training loss: 4.328508\n",
      "[INFO] Epoch: 20 , batch: 260 , training loss: 4.101615\n",
      "[INFO] Epoch: 20 , batch: 261 , training loss: 4.276511\n",
      "[INFO] Epoch: 20 , batch: 262 , training loss: 4.444535\n",
      "[INFO] Epoch: 20 , batch: 263 , training loss: 4.563667\n",
      "[INFO] Epoch: 20 , batch: 264 , training loss: 3.965032\n",
      "[INFO] Epoch: 20 , batch: 265 , training loss: 4.045467\n",
      "[INFO] Epoch: 20 , batch: 266 , training loss: 4.499962\n",
      "[INFO] Epoch: 20 , batch: 267 , training loss: 4.231595\n",
      "[INFO] Epoch: 20 , batch: 268 , training loss: 4.124028\n",
      "[INFO] Epoch: 20 , batch: 269 , training loss: 4.112980\n",
      "[INFO] Epoch: 20 , batch: 270 , training loss: 4.142827\n",
      "[INFO] Epoch: 20 , batch: 271 , training loss: 4.179867\n",
      "[INFO] Epoch: 20 , batch: 272 , training loss: 4.150121\n",
      "[INFO] Epoch: 20 , batch: 273 , training loss: 4.163682\n",
      "[INFO] Epoch: 20 , batch: 274 , training loss: 4.280776\n",
      "[INFO] Epoch: 20 , batch: 275 , training loss: 4.123866\n",
      "[INFO] Epoch: 20 , batch: 276 , training loss: 4.195014\n",
      "[INFO] Epoch: 20 , batch: 277 , training loss: 4.346070\n",
      "[INFO] Epoch: 20 , batch: 278 , training loss: 4.017641\n",
      "[INFO] Epoch: 20 , batch: 279 , training loss: 4.034784\n",
      "[INFO] Epoch: 20 , batch: 280 , training loss: 3.983525\n",
      "[INFO] Epoch: 20 , batch: 281 , training loss: 4.131361\n",
      "[INFO] Epoch: 20 , batch: 282 , training loss: 4.031494\n",
      "[INFO] Epoch: 20 , batch: 283 , training loss: 4.045128\n",
      "[INFO] Epoch: 20 , batch: 284 , training loss: 4.070224\n",
      "[INFO] Epoch: 20 , batch: 285 , training loss: 4.019353\n",
      "[INFO] Epoch: 20 , batch: 286 , training loss: 4.014746\n",
      "[INFO] Epoch: 20 , batch: 287 , training loss: 3.961976\n",
      "[INFO] Epoch: 20 , batch: 288 , training loss: 3.939529\n",
      "[INFO] Epoch: 20 , batch: 289 , training loss: 4.000476\n",
      "[INFO] Epoch: 20 , batch: 290 , training loss: 3.781723\n",
      "[INFO] Epoch: 20 , batch: 291 , training loss: 3.759747\n",
      "[INFO] Epoch: 20 , batch: 292 , training loss: 3.887995\n",
      "[INFO] Epoch: 20 , batch: 293 , training loss: 3.803870\n",
      "[INFO] Epoch: 20 , batch: 294 , training loss: 4.479833\n",
      "[INFO] Epoch: 20 , batch: 295 , training loss: 4.255545\n",
      "[INFO] Epoch: 20 , batch: 296 , training loss: 4.198778\n",
      "[INFO] Epoch: 20 , batch: 297 , training loss: 4.139606\n",
      "[INFO] Epoch: 20 , batch: 298 , training loss: 3.962399\n",
      "[INFO] Epoch: 20 , batch: 299 , training loss: 4.008186\n",
      "[INFO] Epoch: 20 , batch: 300 , training loss: 3.994749\n",
      "[INFO] Epoch: 20 , batch: 301 , training loss: 3.914721\n",
      "[INFO] Epoch: 20 , batch: 302 , training loss: 4.078198\n",
      "[INFO] Epoch: 20 , batch: 303 , training loss: 4.087877\n",
      "[INFO] Epoch: 20 , batch: 304 , training loss: 4.260369\n",
      "[INFO] Epoch: 20 , batch: 305 , training loss: 4.071226\n",
      "[INFO] Epoch: 20 , batch: 306 , training loss: 4.184387\n",
      "[INFO] Epoch: 20 , batch: 307 , training loss: 4.190551\n",
      "[INFO] Epoch: 20 , batch: 308 , training loss: 4.008856\n",
      "[INFO] Epoch: 20 , batch: 309 , training loss: 4.003505\n",
      "[INFO] Epoch: 20 , batch: 310 , training loss: 3.937639\n",
      "[INFO] Epoch: 20 , batch: 311 , training loss: 3.941018\n",
      "[INFO] Epoch: 20 , batch: 312 , training loss: 3.839058\n",
      "[INFO] Epoch: 20 , batch: 313 , training loss: 3.922673\n",
      "[INFO] Epoch: 20 , batch: 314 , training loss: 4.021196\n",
      "[INFO] Epoch: 20 , batch: 315 , training loss: 4.082895\n",
      "[INFO] Epoch: 20 , batch: 316 , training loss: 4.345071\n",
      "[INFO] Epoch: 20 , batch: 317 , training loss: 4.714176\n",
      "[INFO] Epoch: 20 , batch: 318 , training loss: 4.852213\n",
      "[INFO] Epoch: 20 , batch: 319 , training loss: 4.537390\n",
      "[INFO] Epoch: 20 , batch: 320 , training loss: 4.046931\n",
      "[INFO] Epoch: 20 , batch: 321 , training loss: 3.868903\n",
      "[INFO] Epoch: 20 , batch: 322 , training loss: 3.977744\n",
      "[INFO] Epoch: 20 , batch: 323 , training loss: 4.020646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 20 , batch: 324 , training loss: 3.997552\n",
      "[INFO] Epoch: 20 , batch: 325 , training loss: 4.116786\n",
      "[INFO] Epoch: 20 , batch: 326 , training loss: 4.156591\n",
      "[INFO] Epoch: 20 , batch: 327 , training loss: 4.085369\n",
      "[INFO] Epoch: 20 , batch: 328 , training loss: 4.096160\n",
      "[INFO] Epoch: 20 , batch: 329 , training loss: 3.981068\n",
      "[INFO] Epoch: 20 , batch: 330 , training loss: 4.000252\n",
      "[INFO] Epoch: 20 , batch: 331 , training loss: 4.164661\n",
      "[INFO] Epoch: 20 , batch: 332 , training loss: 3.986504\n",
      "[INFO] Epoch: 20 , batch: 333 , training loss: 3.983233\n",
      "[INFO] Epoch: 20 , batch: 334 , training loss: 3.984124\n",
      "[INFO] Epoch: 20 , batch: 335 , training loss: 4.118992\n",
      "[INFO] Epoch: 20 , batch: 336 , training loss: 4.124788\n",
      "[INFO] Epoch: 20 , batch: 337 , training loss: 4.179648\n",
      "[INFO] Epoch: 20 , batch: 338 , training loss: 4.398425\n",
      "[INFO] Epoch: 20 , batch: 339 , training loss: 4.186594\n",
      "[INFO] Epoch: 20 , batch: 340 , training loss: 4.390169\n",
      "[INFO] Epoch: 20 , batch: 341 , training loss: 4.115593\n",
      "[INFO] Epoch: 20 , batch: 342 , training loss: 3.907901\n",
      "[INFO] Epoch: 20 , batch: 343 , training loss: 3.995288\n",
      "[INFO] Epoch: 20 , batch: 344 , training loss: 3.857913\n",
      "[INFO] Epoch: 20 , batch: 345 , training loss: 3.991848\n",
      "[INFO] Epoch: 20 , batch: 346 , training loss: 4.023050\n",
      "[INFO] Epoch: 20 , batch: 347 , training loss: 3.939186\n",
      "[INFO] Epoch: 20 , batch: 348 , training loss: 4.053342\n",
      "[INFO] Epoch: 20 , batch: 349 , training loss: 4.148665\n",
      "[INFO] Epoch: 20 , batch: 350 , training loss: 4.000055\n",
      "[INFO] Epoch: 20 , batch: 351 , training loss: 4.069542\n",
      "[INFO] Epoch: 20 , batch: 352 , training loss: 4.089104\n",
      "[INFO] Epoch: 20 , batch: 353 , training loss: 4.063107\n",
      "[INFO] Epoch: 20 , batch: 354 , training loss: 4.165009\n",
      "[INFO] Epoch: 20 , batch: 355 , training loss: 4.143488\n",
      "[INFO] Epoch: 20 , batch: 356 , training loss: 4.016488\n",
      "[INFO] Epoch: 20 , batch: 357 , training loss: 4.109496\n",
      "[INFO] Epoch: 20 , batch: 358 , training loss: 4.012769\n",
      "[INFO] Epoch: 20 , batch: 359 , training loss: 4.016106\n",
      "[INFO] Epoch: 20 , batch: 360 , training loss: 4.100211\n",
      "[INFO] Epoch: 20 , batch: 361 , training loss: 4.076330\n",
      "[INFO] Epoch: 20 , batch: 362 , training loss: 4.172691\n",
      "[INFO] Epoch: 20 , batch: 363 , training loss: 4.050996\n",
      "[INFO] Epoch: 20 , batch: 364 , training loss: 4.135573\n",
      "[INFO] Epoch: 20 , batch: 365 , training loss: 4.013547\n",
      "[INFO] Epoch: 20 , batch: 366 , training loss: 4.134987\n",
      "[INFO] Epoch: 20 , batch: 367 , training loss: 4.195498\n",
      "[INFO] Epoch: 20 , batch: 368 , training loss: 4.616384\n",
      "[INFO] Epoch: 20 , batch: 369 , training loss: 4.285503\n",
      "[INFO] Epoch: 20 , batch: 370 , training loss: 4.052283\n",
      "[INFO] Epoch: 20 , batch: 371 , training loss: 4.479790\n",
      "[INFO] Epoch: 20 , batch: 372 , training loss: 4.747964\n",
      "[INFO] Epoch: 20 , batch: 373 , training loss: 4.802053\n",
      "[INFO] Epoch: 20 , batch: 374 , training loss: 4.913888\n",
      "[INFO] Epoch: 20 , batch: 375 , training loss: 4.862290\n",
      "[INFO] Epoch: 20 , batch: 376 , training loss: 4.785843\n",
      "[INFO] Epoch: 20 , batch: 377 , training loss: 4.521934\n",
      "[INFO] Epoch: 20 , batch: 378 , training loss: 4.591736\n",
      "[INFO] Epoch: 20 , batch: 379 , training loss: 4.581054\n",
      "[INFO] Epoch: 20 , batch: 380 , training loss: 4.721949\n",
      "[INFO] Epoch: 20 , batch: 381 , training loss: 4.445924\n",
      "[INFO] Epoch: 20 , batch: 382 , training loss: 4.746616\n",
      "[INFO] Epoch: 20 , batch: 383 , training loss: 4.749864\n",
      "[INFO] Epoch: 20 , batch: 384 , training loss: 4.743641\n",
      "[INFO] Epoch: 20 , batch: 385 , training loss: 4.405713\n",
      "[INFO] Epoch: 20 , batch: 386 , training loss: 4.664434\n",
      "[INFO] Epoch: 20 , batch: 387 , training loss: 4.625075\n",
      "[INFO] Epoch: 20 , batch: 388 , training loss: 4.418515\n",
      "[INFO] Epoch: 20 , batch: 389 , training loss: 4.247771\n",
      "[INFO] Epoch: 20 , batch: 390 , training loss: 4.259073\n",
      "[INFO] Epoch: 20 , batch: 391 , training loss: 4.268720\n",
      "[INFO] Epoch: 20 , batch: 392 , training loss: 4.645616\n",
      "[INFO] Epoch: 20 , batch: 393 , training loss: 4.545574\n",
      "[INFO] Epoch: 20 , batch: 394 , training loss: 4.629311\n",
      "[INFO] Epoch: 20 , batch: 395 , training loss: 4.431333\n",
      "[INFO] Epoch: 20 , batch: 396 , training loss: 4.246922\n",
      "[INFO] Epoch: 20 , batch: 397 , training loss: 4.397194\n",
      "[INFO] Epoch: 20 , batch: 398 , training loss: 4.275481\n",
      "[INFO] Epoch: 20 , batch: 399 , training loss: 4.343582\n",
      "[INFO] Epoch: 20 , batch: 400 , training loss: 4.318211\n",
      "[INFO] Epoch: 20 , batch: 401 , training loss: 4.764274\n",
      "[INFO] Epoch: 20 , batch: 402 , training loss: 4.454244\n",
      "[INFO] Epoch: 20 , batch: 403 , training loss: 4.293656\n",
      "[INFO] Epoch: 20 , batch: 404 , training loss: 4.437984\n",
      "[INFO] Epoch: 20 , batch: 405 , training loss: 4.519150\n",
      "[INFO] Epoch: 20 , batch: 406 , training loss: 4.436861\n",
      "[INFO] Epoch: 20 , batch: 407 , training loss: 4.464224\n",
      "[INFO] Epoch: 20 , batch: 408 , training loss: 4.416143\n",
      "[INFO] Epoch: 20 , batch: 409 , training loss: 4.424315\n",
      "[INFO] Epoch: 20 , batch: 410 , training loss: 4.509117\n",
      "[INFO] Epoch: 20 , batch: 411 , training loss: 4.682505\n",
      "[INFO] Epoch: 20 , batch: 412 , training loss: 4.503473\n",
      "[INFO] Epoch: 20 , batch: 413 , training loss: 4.378226\n",
      "[INFO] Epoch: 20 , batch: 414 , training loss: 4.406841\n",
      "[INFO] Epoch: 20 , batch: 415 , training loss: 4.453700\n",
      "[INFO] Epoch: 20 , batch: 416 , training loss: 4.523973\n",
      "[INFO] Epoch: 20 , batch: 417 , training loss: 4.427669\n",
      "[INFO] Epoch: 20 , batch: 418 , training loss: 4.468771\n",
      "[INFO] Epoch: 20 , batch: 419 , training loss: 4.427445\n",
      "[INFO] Epoch: 20 , batch: 420 , training loss: 4.411861\n",
      "[INFO] Epoch: 20 , batch: 421 , training loss: 4.406870\n",
      "[INFO] Epoch: 20 , batch: 422 , training loss: 4.257937\n",
      "[INFO] Epoch: 20 , batch: 423 , training loss: 4.461145\n",
      "[INFO] Epoch: 20 , batch: 424 , training loss: 4.631679\n",
      "[INFO] Epoch: 20 , batch: 425 , training loss: 4.509473\n",
      "[INFO] Epoch: 20 , batch: 426 , training loss: 4.224023\n",
      "[INFO] Epoch: 20 , batch: 427 , training loss: 4.487234\n",
      "[INFO] Epoch: 20 , batch: 428 , training loss: 4.349116\n",
      "[INFO] Epoch: 20 , batch: 429 , training loss: 4.247982\n",
      "[INFO] Epoch: 20 , batch: 430 , training loss: 4.490061\n",
      "[INFO] Epoch: 20 , batch: 431 , training loss: 4.079760\n",
      "[INFO] Epoch: 20 , batch: 432 , training loss: 4.141963\n",
      "[INFO] Epoch: 20 , batch: 433 , training loss: 4.173177\n",
      "[INFO] Epoch: 20 , batch: 434 , training loss: 4.055300\n",
      "[INFO] Epoch: 20 , batch: 435 , training loss: 4.406635\n",
      "[INFO] Epoch: 20 , batch: 436 , training loss: 4.470613\n",
      "[INFO] Epoch: 20 , batch: 437 , training loss: 4.238763\n",
      "[INFO] Epoch: 20 , batch: 438 , training loss: 4.097823\n",
      "[INFO] Epoch: 20 , batch: 439 , training loss: 4.324023\n",
      "[INFO] Epoch: 20 , batch: 440 , training loss: 4.459161\n",
      "[INFO] Epoch: 20 , batch: 441 , training loss: 4.530950\n",
      "[INFO] Epoch: 20 , batch: 442 , training loss: 4.284182\n",
      "[INFO] Epoch: 20 , batch: 443 , training loss: 4.521422\n",
      "[INFO] Epoch: 20 , batch: 444 , training loss: 4.099287\n",
      "[INFO] Epoch: 20 , batch: 445 , training loss: 4.015762\n",
      "[INFO] Epoch: 20 , batch: 446 , training loss: 3.938442\n",
      "[INFO] Epoch: 20 , batch: 447 , training loss: 4.143122\n",
      "[INFO] Epoch: 20 , batch: 448 , training loss: 4.261868\n",
      "[INFO] Epoch: 20 , batch: 449 , training loss: 4.642776\n",
      "[INFO] Epoch: 20 , batch: 450 , training loss: 4.706854\n",
      "[INFO] Epoch: 20 , batch: 451 , training loss: 4.604653\n",
      "[INFO] Epoch: 20 , batch: 452 , training loss: 4.416413\n",
      "[INFO] Epoch: 20 , batch: 453 , training loss: 4.186874\n",
      "[INFO] Epoch: 20 , batch: 454 , training loss: 4.332616\n",
      "[INFO] Epoch: 20 , batch: 455 , training loss: 4.387703\n",
      "[INFO] Epoch: 20 , batch: 456 , training loss: 4.375267\n",
      "[INFO] Epoch: 20 , batch: 457 , training loss: 4.452152\n",
      "[INFO] Epoch: 20 , batch: 458 , training loss: 4.192354\n",
      "[INFO] Epoch: 20 , batch: 459 , training loss: 4.166469\n",
      "[INFO] Epoch: 20 , batch: 460 , training loss: 4.278096\n",
      "[INFO] Epoch: 20 , batch: 461 , training loss: 4.242712\n",
      "[INFO] Epoch: 20 , batch: 462 , training loss: 4.297051\n",
      "[INFO] Epoch: 20 , batch: 463 , training loss: 4.228376\n",
      "[INFO] Epoch: 20 , batch: 464 , training loss: 4.393307\n",
      "[INFO] Epoch: 20 , batch: 465 , training loss: 4.335481\n",
      "[INFO] Epoch: 20 , batch: 466 , training loss: 4.440876\n",
      "[INFO] Epoch: 20 , batch: 467 , training loss: 4.399161\n",
      "[INFO] Epoch: 20 , batch: 468 , training loss: 4.362924\n",
      "[INFO] Epoch: 20 , batch: 469 , training loss: 4.401783\n",
      "[INFO] Epoch: 20 , batch: 470 , training loss: 4.198478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 20 , batch: 471 , training loss: 4.307367\n",
      "[INFO] Epoch: 20 , batch: 472 , training loss: 4.364404\n",
      "[INFO] Epoch: 20 , batch: 473 , training loss: 4.271606\n",
      "[INFO] Epoch: 20 , batch: 474 , training loss: 4.072485\n",
      "[INFO] Epoch: 20 , batch: 475 , training loss: 3.947139\n",
      "[INFO] Epoch: 20 , batch: 476 , training loss: 4.358951\n",
      "[INFO] Epoch: 20 , batch: 477 , training loss: 4.450668\n",
      "[INFO] Epoch: 20 , batch: 478 , training loss: 4.459758\n",
      "[INFO] Epoch: 20 , batch: 479 , training loss: 4.439771\n",
      "[INFO] Epoch: 20 , batch: 480 , training loss: 4.572799\n",
      "[INFO] Epoch: 20 , batch: 481 , training loss: 4.447623\n",
      "[INFO] Epoch: 20 , batch: 482 , training loss: 4.557299\n",
      "[INFO] Epoch: 20 , batch: 483 , training loss: 4.390224\n",
      "[INFO] Epoch: 20 , batch: 484 , training loss: 4.186470\n",
      "[INFO] Epoch: 20 , batch: 485 , training loss: 4.285622\n",
      "[INFO] Epoch: 20 , batch: 486 , training loss: 4.195314\n",
      "[INFO] Epoch: 20 , batch: 487 , training loss: 4.177965\n",
      "[INFO] Epoch: 20 , batch: 488 , training loss: 4.353218\n",
      "[INFO] Epoch: 20 , batch: 489 , training loss: 4.265965\n",
      "[INFO] Epoch: 20 , batch: 490 , training loss: 4.314933\n",
      "[INFO] Epoch: 20 , batch: 491 , training loss: 4.266886\n",
      "[INFO] Epoch: 20 , batch: 492 , training loss: 4.204231\n",
      "[INFO] Epoch: 20 , batch: 493 , training loss: 4.384337\n",
      "[INFO] Epoch: 20 , batch: 494 , training loss: 4.284818\n",
      "[INFO] Epoch: 20 , batch: 495 , training loss: 4.438552\n",
      "[INFO] Epoch: 20 , batch: 496 , training loss: 4.315374\n",
      "[INFO] Epoch: 20 , batch: 497 , training loss: 4.348456\n",
      "[INFO] Epoch: 20 , batch: 498 , training loss: 4.345366\n",
      "[INFO] Epoch: 20 , batch: 499 , training loss: 4.404785\n",
      "[INFO] Epoch: 20 , batch: 500 , training loss: 4.557914\n",
      "[INFO] Epoch: 20 , batch: 501 , training loss: 4.896069\n",
      "[INFO] Epoch: 20 , batch: 502 , training loss: 4.963566\n",
      "[INFO] Epoch: 20 , batch: 503 , training loss: 4.634391\n",
      "[INFO] Epoch: 20 , batch: 504 , training loss: 4.778470\n",
      "[INFO] Epoch: 20 , batch: 505 , training loss: 4.739395\n",
      "[INFO] Epoch: 20 , batch: 506 , training loss: 4.702584\n",
      "[INFO] Epoch: 20 , batch: 507 , training loss: 4.750124\n",
      "[INFO] Epoch: 20 , batch: 508 , training loss: 4.658870\n",
      "[INFO] Epoch: 20 , batch: 509 , training loss: 4.449861\n",
      "[INFO] Epoch: 20 , batch: 510 , training loss: 4.528529\n",
      "[INFO] Epoch: 20 , batch: 511 , training loss: 4.457472\n",
      "[INFO] Epoch: 20 , batch: 512 , training loss: 4.539216\n",
      "[INFO] Epoch: 20 , batch: 513 , training loss: 4.805824\n",
      "[INFO] Epoch: 20 , batch: 514 , training loss: 4.445732\n",
      "[INFO] Epoch: 20 , batch: 515 , training loss: 4.697621\n",
      "[INFO] Epoch: 20 , batch: 516 , training loss: 4.491934\n",
      "[INFO] Epoch: 20 , batch: 517 , training loss: 4.454045\n",
      "[INFO] Epoch: 20 , batch: 518 , training loss: 4.419286\n",
      "[INFO] Epoch: 20 , batch: 519 , training loss: 4.266105\n",
      "[INFO] Epoch: 20 , batch: 520 , training loss: 4.512286\n",
      "[INFO] Epoch: 20 , batch: 521 , training loss: 4.501667\n",
      "[INFO] Epoch: 20 , batch: 522 , training loss: 4.575062\n",
      "[INFO] Epoch: 20 , batch: 523 , training loss: 4.480822\n",
      "[INFO] Epoch: 20 , batch: 524 , training loss: 4.767382\n",
      "[INFO] Epoch: 20 , batch: 525 , training loss: 4.623464\n",
      "[INFO] Epoch: 20 , batch: 526 , training loss: 4.411500\n",
      "[INFO] Epoch: 20 , batch: 527 , training loss: 4.467227\n",
      "[INFO] Epoch: 20 , batch: 528 , training loss: 4.482003\n",
      "[INFO] Epoch: 20 , batch: 529 , training loss: 4.459041\n",
      "[INFO] Epoch: 20 , batch: 530 , training loss: 4.307222\n",
      "[INFO] Epoch: 20 , batch: 531 , training loss: 4.461717\n",
      "[INFO] Epoch: 20 , batch: 532 , training loss: 4.360135\n",
      "[INFO] Epoch: 20 , batch: 533 , training loss: 4.504950\n",
      "[INFO] Epoch: 20 , batch: 534 , training loss: 4.494070\n",
      "[INFO] Epoch: 20 , batch: 535 , training loss: 4.493019\n",
      "[INFO] Epoch: 20 , batch: 536 , training loss: 4.360565\n",
      "[INFO] Epoch: 20 , batch: 537 , training loss: 4.323471\n",
      "[INFO] Epoch: 20 , batch: 538 , training loss: 4.419942\n",
      "[INFO] Epoch: 20 , batch: 539 , training loss: 4.541965\n",
      "[INFO] Epoch: 20 , batch: 540 , training loss: 5.055851\n",
      "[INFO] Epoch: 20 , batch: 541 , training loss: 4.903910\n",
      "[INFO] Epoch: 20 , batch: 542 , training loss: 4.762837\n",
      "[INFO] Epoch: 21 , batch: 0 , training loss: 3.823447\n",
      "[INFO] Epoch: 21 , batch: 1 , training loss: 3.673221\n",
      "[INFO] Epoch: 21 , batch: 2 , training loss: 3.785590\n",
      "[INFO] Epoch: 21 , batch: 3 , training loss: 3.656091\n",
      "[INFO] Epoch: 21 , batch: 4 , training loss: 3.994756\n",
      "[INFO] Epoch: 21 , batch: 5 , training loss: 3.674319\n",
      "[INFO] Epoch: 21 , batch: 6 , training loss: 3.965420\n",
      "[INFO] Epoch: 21 , batch: 7 , training loss: 3.886808\n",
      "[INFO] Epoch: 21 , batch: 8 , training loss: 3.587200\n",
      "[INFO] Epoch: 21 , batch: 9 , training loss: 3.835533\n",
      "[INFO] Epoch: 21 , batch: 10 , training loss: 3.748930\n",
      "[INFO] Epoch: 21 , batch: 11 , training loss: 3.719237\n",
      "[INFO] Epoch: 21 , batch: 12 , training loss: 3.598957\n",
      "[INFO] Epoch: 21 , batch: 13 , training loss: 3.661894\n",
      "[INFO] Epoch: 21 , batch: 14 , training loss: 3.526333\n",
      "[INFO] Epoch: 21 , batch: 15 , training loss: 3.765338\n",
      "[INFO] Epoch: 21 , batch: 16 , training loss: 3.622302\n",
      "[INFO] Epoch: 21 , batch: 17 , training loss: 3.767174\n",
      "[INFO] Epoch: 21 , batch: 18 , training loss: 3.666333\n",
      "[INFO] Epoch: 21 , batch: 19 , training loss: 3.452044\n",
      "[INFO] Epoch: 21 , batch: 20 , training loss: 3.421782\n",
      "[INFO] Epoch: 21 , batch: 21 , training loss: 3.571145\n",
      "[INFO] Epoch: 21 , batch: 22 , training loss: 3.478349\n",
      "[INFO] Epoch: 21 , batch: 23 , training loss: 3.683129\n",
      "[INFO] Epoch: 21 , batch: 24 , training loss: 3.559107\n",
      "[INFO] Epoch: 21 , batch: 25 , training loss: 3.635474\n",
      "[INFO] Epoch: 21 , batch: 26 , training loss: 3.543789\n",
      "[INFO] Epoch: 21 , batch: 27 , training loss: 3.456017\n",
      "[INFO] Epoch: 21 , batch: 28 , training loss: 3.677274\n",
      "[INFO] Epoch: 21 , batch: 29 , training loss: 3.510011\n",
      "[INFO] Epoch: 21 , batch: 30 , training loss: 3.482221\n",
      "[INFO] Epoch: 21 , batch: 31 , training loss: 3.636589\n",
      "[INFO] Epoch: 21 , batch: 32 , training loss: 3.574174\n",
      "[INFO] Epoch: 21 , batch: 33 , training loss: 3.624166\n",
      "[INFO] Epoch: 21 , batch: 34 , training loss: 3.633541\n",
      "[INFO] Epoch: 21 , batch: 35 , training loss: 3.544688\n",
      "[INFO] Epoch: 21 , batch: 36 , training loss: 3.654022\n",
      "[INFO] Epoch: 21 , batch: 37 , training loss: 3.505272\n",
      "[INFO] Epoch: 21 , batch: 38 , training loss: 3.619602\n",
      "[INFO] Epoch: 21 , batch: 39 , training loss: 3.422296\n",
      "[INFO] Epoch: 21 , batch: 40 , training loss: 3.624929\n",
      "[INFO] Epoch: 21 , batch: 41 , training loss: 3.594507\n",
      "[INFO] Epoch: 21 , batch: 42 , training loss: 4.067116\n",
      "[INFO] Epoch: 21 , batch: 43 , training loss: 3.802760\n",
      "[INFO] Epoch: 21 , batch: 44 , training loss: 4.131224\n",
      "[INFO] Epoch: 21 , batch: 45 , training loss: 4.089137\n",
      "[INFO] Epoch: 21 , batch: 46 , training loss: 4.046441\n",
      "[INFO] Epoch: 21 , batch: 47 , training loss: 3.703336\n",
      "[INFO] Epoch: 21 , batch: 48 , training loss: 3.678732\n",
      "[INFO] Epoch: 21 , batch: 49 , training loss: 3.891089\n",
      "[INFO] Epoch: 21 , batch: 50 , training loss: 3.677418\n",
      "[INFO] Epoch: 21 , batch: 51 , training loss: 3.882798\n",
      "[INFO] Epoch: 21 , batch: 52 , training loss: 3.732618\n",
      "[INFO] Epoch: 21 , batch: 53 , training loss: 3.840174\n",
      "[INFO] Epoch: 21 , batch: 54 , training loss: 3.835416\n",
      "[INFO] Epoch: 21 , batch: 55 , training loss: 3.942375\n",
      "[INFO] Epoch: 21 , batch: 56 , training loss: 3.753182\n",
      "[INFO] Epoch: 21 , batch: 57 , training loss: 3.688797\n",
      "[INFO] Epoch: 21 , batch: 58 , training loss: 3.724813\n",
      "[INFO] Epoch: 21 , batch: 59 , training loss: 3.834192\n",
      "[INFO] Epoch: 21 , batch: 60 , training loss: 3.771597\n",
      "[INFO] Epoch: 21 , batch: 61 , training loss: 3.832330\n",
      "[INFO] Epoch: 21 , batch: 62 , training loss: 3.696657\n",
      "[INFO] Epoch: 21 , batch: 63 , training loss: 3.898704\n",
      "[INFO] Epoch: 21 , batch: 64 , training loss: 4.132547\n",
      "[INFO] Epoch: 21 , batch: 65 , training loss: 3.786730\n",
      "[INFO] Epoch: 21 , batch: 66 , training loss: 3.673622\n",
      "[INFO] Epoch: 21 , batch: 67 , training loss: 3.659797\n",
      "[INFO] Epoch: 21 , batch: 68 , training loss: 3.915323\n",
      "[INFO] Epoch: 21 , batch: 69 , training loss: 3.804518\n",
      "[INFO] Epoch: 21 , batch: 70 , training loss: 3.997623\n",
      "[INFO] Epoch: 21 , batch: 71 , training loss: 3.872172\n",
      "[INFO] Epoch: 21 , batch: 72 , training loss: 3.922625\n",
      "[INFO] Epoch: 21 , batch: 73 , training loss: 3.851951\n",
      "[INFO] Epoch: 21 , batch: 74 , training loss: 3.971821\n",
      "[INFO] Epoch: 21 , batch: 75 , training loss: 3.845091\n",
      "[INFO] Epoch: 21 , batch: 76 , training loss: 3.929595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 21 , batch: 77 , training loss: 3.862597\n",
      "[INFO] Epoch: 21 , batch: 78 , training loss: 3.969235\n",
      "[INFO] Epoch: 21 , batch: 79 , training loss: 3.798988\n",
      "[INFO] Epoch: 21 , batch: 80 , training loss: 4.004635\n",
      "[INFO] Epoch: 21 , batch: 81 , training loss: 3.950636\n",
      "[INFO] Epoch: 21 , batch: 82 , training loss: 3.918852\n",
      "[INFO] Epoch: 21 , batch: 83 , training loss: 4.034155\n",
      "[INFO] Epoch: 21 , batch: 84 , training loss: 3.938454\n",
      "[INFO] Epoch: 21 , batch: 85 , training loss: 4.059382\n",
      "[INFO] Epoch: 21 , batch: 86 , training loss: 4.011436\n",
      "[INFO] Epoch: 21 , batch: 87 , training loss: 3.937501\n",
      "[INFO] Epoch: 21 , batch: 88 , training loss: 4.099468\n",
      "[INFO] Epoch: 21 , batch: 89 , training loss: 3.883267\n",
      "[INFO] Epoch: 21 , batch: 90 , training loss: 3.987412\n",
      "[INFO] Epoch: 21 , batch: 91 , training loss: 3.867372\n",
      "[INFO] Epoch: 21 , batch: 92 , training loss: 3.943878\n",
      "[INFO] Epoch: 21 , batch: 93 , training loss: 3.996801\n",
      "[INFO] Epoch: 21 , batch: 94 , training loss: 4.141432\n",
      "[INFO] Epoch: 21 , batch: 95 , training loss: 3.937085\n",
      "[INFO] Epoch: 21 , batch: 96 , training loss: 3.907047\n",
      "[INFO] Epoch: 21 , batch: 97 , training loss: 3.862920\n",
      "[INFO] Epoch: 21 , batch: 98 , training loss: 3.798048\n",
      "[INFO] Epoch: 21 , batch: 99 , training loss: 3.930562\n",
      "[INFO] Epoch: 21 , batch: 100 , training loss: 3.776047\n",
      "[INFO] Epoch: 21 , batch: 101 , training loss: 3.828524\n",
      "[INFO] Epoch: 21 , batch: 102 , training loss: 3.998029\n",
      "[INFO] Epoch: 21 , batch: 103 , training loss: 3.756938\n",
      "[INFO] Epoch: 21 , batch: 104 , training loss: 3.722189\n",
      "[INFO] Epoch: 21 , batch: 105 , training loss: 3.975938\n",
      "[INFO] Epoch: 21 , batch: 106 , training loss: 4.025288\n",
      "[INFO] Epoch: 21 , batch: 107 , training loss: 3.841994\n",
      "[INFO] Epoch: 21 , batch: 108 , training loss: 3.781831\n",
      "[INFO] Epoch: 21 , batch: 109 , training loss: 3.721449\n",
      "[INFO] Epoch: 21 , batch: 110 , training loss: 3.900465\n",
      "[INFO] Epoch: 21 , batch: 111 , training loss: 3.959067\n",
      "[INFO] Epoch: 21 , batch: 112 , training loss: 3.897289\n",
      "[INFO] Epoch: 21 , batch: 113 , training loss: 3.882851\n",
      "[INFO] Epoch: 21 , batch: 114 , training loss: 3.911083\n",
      "[INFO] Epoch: 21 , batch: 115 , training loss: 3.895853\n",
      "[INFO] Epoch: 21 , batch: 116 , training loss: 3.786164\n",
      "[INFO] Epoch: 21 , batch: 117 , training loss: 4.010356\n",
      "[INFO] Epoch: 21 , batch: 118 , training loss: 3.993583\n",
      "[INFO] Epoch: 21 , batch: 119 , training loss: 4.156029\n",
      "[INFO] Epoch: 21 , batch: 120 , training loss: 4.114295\n",
      "[INFO] Epoch: 21 , batch: 121 , training loss: 3.956482\n",
      "[INFO] Epoch: 21 , batch: 122 , training loss: 3.853871\n",
      "[INFO] Epoch: 21 , batch: 123 , training loss: 3.905122\n",
      "[INFO] Epoch: 21 , batch: 124 , training loss: 4.014973\n",
      "[INFO] Epoch: 21 , batch: 125 , training loss: 3.763828\n",
      "[INFO] Epoch: 21 , batch: 126 , training loss: 3.794785\n",
      "[INFO] Epoch: 21 , batch: 127 , training loss: 3.824584\n",
      "[INFO] Epoch: 21 , batch: 128 , training loss: 3.980405\n",
      "[INFO] Epoch: 21 , batch: 129 , training loss: 3.935852\n",
      "[INFO] Epoch: 21 , batch: 130 , training loss: 3.886779\n",
      "[INFO] Epoch: 21 , batch: 131 , training loss: 3.901291\n",
      "[INFO] Epoch: 21 , batch: 132 , training loss: 3.921741\n",
      "[INFO] Epoch: 21 , batch: 133 , training loss: 3.889818\n",
      "[INFO] Epoch: 21 , batch: 134 , training loss: 3.645195\n",
      "[INFO] Epoch: 21 , batch: 135 , training loss: 3.727328\n",
      "[INFO] Epoch: 21 , batch: 136 , training loss: 3.988167\n",
      "[INFO] Epoch: 21 , batch: 137 , training loss: 3.933330\n",
      "[INFO] Epoch: 21 , batch: 138 , training loss: 3.972382\n",
      "[INFO] Epoch: 21 , batch: 139 , training loss: 4.556408\n",
      "[INFO] Epoch: 21 , batch: 140 , training loss: 4.364614\n",
      "[INFO] Epoch: 21 , batch: 141 , training loss: 4.109997\n",
      "[INFO] Epoch: 21 , batch: 142 , training loss: 3.803614\n",
      "[INFO] Epoch: 21 , batch: 143 , training loss: 3.944913\n",
      "[INFO] Epoch: 21 , batch: 144 , training loss: 3.791871\n",
      "[INFO] Epoch: 21 , batch: 145 , training loss: 3.891357\n",
      "[INFO] Epoch: 21 , batch: 146 , training loss: 4.071026\n",
      "[INFO] Epoch: 21 , batch: 147 , training loss: 3.751319\n",
      "[INFO] Epoch: 21 , batch: 148 , training loss: 3.721440\n",
      "[INFO] Epoch: 21 , batch: 149 , training loss: 3.802735\n",
      "[INFO] Epoch: 21 , batch: 150 , training loss: 4.053013\n",
      "[INFO] Epoch: 21 , batch: 151 , training loss: 3.874412\n",
      "[INFO] Epoch: 21 , batch: 152 , training loss: 3.886033\n",
      "[INFO] Epoch: 21 , batch: 153 , training loss: 3.918628\n",
      "[INFO] Epoch: 21 , batch: 154 , training loss: 3.990327\n",
      "[INFO] Epoch: 21 , batch: 155 , training loss: 4.220709\n",
      "[INFO] Epoch: 21 , batch: 156 , training loss: 3.972392\n",
      "[INFO] Epoch: 21 , batch: 157 , training loss: 3.932510\n",
      "[INFO] Epoch: 21 , batch: 158 , training loss: 4.092996\n",
      "[INFO] Epoch: 21 , batch: 159 , training loss: 4.034288\n",
      "[INFO] Epoch: 21 , batch: 160 , training loss: 4.274792\n",
      "[INFO] Epoch: 21 , batch: 161 , training loss: 4.346969\n",
      "[INFO] Epoch: 21 , batch: 162 , training loss: 4.296105\n",
      "[INFO] Epoch: 21 , batch: 163 , training loss: 4.480378\n",
      "[INFO] Epoch: 21 , batch: 164 , training loss: 4.400190\n",
      "[INFO] Epoch: 21 , batch: 165 , training loss: 4.327341\n",
      "[INFO] Epoch: 21 , batch: 166 , training loss: 4.216273\n",
      "[INFO] Epoch: 21 , batch: 167 , training loss: 4.380975\n",
      "[INFO] Epoch: 21 , batch: 168 , training loss: 4.048823\n",
      "[INFO] Epoch: 21 , batch: 169 , training loss: 4.033481\n",
      "[INFO] Epoch: 21 , batch: 170 , training loss: 4.166770\n",
      "[INFO] Epoch: 21 , batch: 171 , training loss: 3.589878\n",
      "[INFO] Epoch: 21 , batch: 172 , training loss: 3.824204\n",
      "[INFO] Epoch: 21 , batch: 173 , training loss: 4.119837\n",
      "[INFO] Epoch: 21 , batch: 174 , training loss: 4.522825\n",
      "[INFO] Epoch: 21 , batch: 175 , training loss: 4.864637\n",
      "[INFO] Epoch: 21 , batch: 176 , training loss: 4.551197\n",
      "[INFO] Epoch: 21 , batch: 177 , training loss: 4.190441\n",
      "[INFO] Epoch: 21 , batch: 178 , training loss: 4.168568\n",
      "[INFO] Epoch: 21 , batch: 179 , training loss: 4.212347\n",
      "[INFO] Epoch: 21 , batch: 180 , training loss: 4.147677\n",
      "[INFO] Epoch: 21 , batch: 181 , training loss: 4.420208\n",
      "[INFO] Epoch: 21 , batch: 182 , training loss: 4.360042\n",
      "[INFO] Epoch: 21 , batch: 183 , training loss: 4.314213\n",
      "[INFO] Epoch: 21 , batch: 184 , training loss: 4.216918\n",
      "[INFO] Epoch: 21 , batch: 185 , training loss: 4.178679\n",
      "[INFO] Epoch: 21 , batch: 186 , training loss: 4.318955\n",
      "[INFO] Epoch: 21 , batch: 187 , training loss: 4.430176\n",
      "[INFO] Epoch: 21 , batch: 188 , training loss: 4.422481\n",
      "[INFO] Epoch: 21 , batch: 189 , training loss: 4.310968\n",
      "[INFO] Epoch: 21 , batch: 190 , training loss: 4.319551\n",
      "[INFO] Epoch: 21 , batch: 191 , training loss: 4.464103\n",
      "[INFO] Epoch: 21 , batch: 192 , training loss: 4.253324\n",
      "[INFO] Epoch: 21 , batch: 193 , training loss: 4.376965\n",
      "[INFO] Epoch: 21 , batch: 194 , training loss: 4.329735\n",
      "[INFO] Epoch: 21 , batch: 195 , training loss: 4.260687\n",
      "[INFO] Epoch: 21 , batch: 196 , training loss: 4.129373\n",
      "[INFO] Epoch: 21 , batch: 197 , training loss: 4.183781\n",
      "[INFO] Epoch: 21 , batch: 198 , training loss: 4.099034\n",
      "[INFO] Epoch: 21 , batch: 199 , training loss: 4.242737\n",
      "[INFO] Epoch: 21 , batch: 200 , training loss: 4.168150\n",
      "[INFO] Epoch: 21 , batch: 201 , training loss: 4.061886\n",
      "[INFO] Epoch: 21 , batch: 202 , training loss: 4.059388\n",
      "[INFO] Epoch: 21 , batch: 203 , training loss: 4.184205\n",
      "[INFO] Epoch: 21 , batch: 204 , training loss: 4.260809\n",
      "[INFO] Epoch: 21 , batch: 205 , training loss: 3.878245\n",
      "[INFO] Epoch: 21 , batch: 206 , training loss: 3.779516\n",
      "[INFO] Epoch: 21 , batch: 207 , training loss: 3.773339\n",
      "[INFO] Epoch: 21 , batch: 208 , training loss: 4.101633\n",
      "[INFO] Epoch: 21 , batch: 209 , training loss: 4.070605\n",
      "[INFO] Epoch: 21 , batch: 210 , training loss: 4.091297\n",
      "[INFO] Epoch: 21 , batch: 211 , training loss: 4.078736\n",
      "[INFO] Epoch: 21 , batch: 212 , training loss: 4.191333\n",
      "[INFO] Epoch: 21 , batch: 213 , training loss: 4.140049\n",
      "[INFO] Epoch: 21 , batch: 214 , training loss: 4.211450\n",
      "[INFO] Epoch: 21 , batch: 215 , training loss: 4.442908\n",
      "[INFO] Epoch: 21 , batch: 216 , training loss: 4.154747\n",
      "[INFO] Epoch: 21 , batch: 217 , training loss: 4.078447\n",
      "[INFO] Epoch: 21 , batch: 218 , training loss: 4.090200\n",
      "[INFO] Epoch: 21 , batch: 219 , training loss: 4.193669\n",
      "[INFO] Epoch: 21 , batch: 220 , training loss: 4.010797\n",
      "[INFO] Epoch: 21 , batch: 221 , training loss: 4.029708\n",
      "[INFO] Epoch: 21 , batch: 222 , training loss: 4.154631\n",
      "[INFO] Epoch: 21 , batch: 223 , training loss: 4.256650\n",
      "[INFO] Epoch: 21 , batch: 224 , training loss: 4.297268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 21 , batch: 225 , training loss: 4.183504\n",
      "[INFO] Epoch: 21 , batch: 226 , training loss: 4.315629\n",
      "[INFO] Epoch: 21 , batch: 227 , training loss: 4.289273\n",
      "[INFO] Epoch: 21 , batch: 228 , training loss: 4.315546\n",
      "[INFO] Epoch: 21 , batch: 229 , training loss: 4.181807\n",
      "[INFO] Epoch: 21 , batch: 230 , training loss: 4.033915\n",
      "[INFO] Epoch: 21 , batch: 231 , training loss: 3.885725\n",
      "[INFO] Epoch: 21 , batch: 232 , training loss: 4.035395\n",
      "[INFO] Epoch: 21 , batch: 233 , training loss: 4.056165\n",
      "[INFO] Epoch: 21 , batch: 234 , training loss: 3.743967\n",
      "[INFO] Epoch: 21 , batch: 235 , training loss: 3.881853\n",
      "[INFO] Epoch: 21 , batch: 236 , training loss: 3.967734\n",
      "[INFO] Epoch: 21 , batch: 237 , training loss: 4.193727\n",
      "[INFO] Epoch: 21 , batch: 238 , training loss: 3.964422\n",
      "[INFO] Epoch: 21 , batch: 239 , training loss: 4.006027\n",
      "[INFO] Epoch: 21 , batch: 240 , training loss: 4.043092\n",
      "[INFO] Epoch: 21 , batch: 241 , training loss: 3.856834\n",
      "[INFO] Epoch: 21 , batch: 242 , training loss: 3.873573\n",
      "[INFO] Epoch: 21 , batch: 243 , training loss: 4.135422\n",
      "[INFO] Epoch: 21 , batch: 244 , training loss: 4.114762\n",
      "[INFO] Epoch: 21 , batch: 245 , training loss: 4.084976\n",
      "[INFO] Epoch: 21 , batch: 246 , training loss: 3.782602\n",
      "[INFO] Epoch: 21 , batch: 247 , training loss: 3.938854\n",
      "[INFO] Epoch: 21 , batch: 248 , training loss: 4.013750\n",
      "[INFO] Epoch: 21 , batch: 249 , training loss: 3.990428\n",
      "[INFO] Epoch: 21 , batch: 250 , training loss: 3.787139\n",
      "[INFO] Epoch: 21 , batch: 251 , training loss: 4.257817\n",
      "[INFO] Epoch: 21 , batch: 252 , training loss: 3.947003\n",
      "[INFO] Epoch: 21 , batch: 253 , training loss: 3.887170\n",
      "[INFO] Epoch: 21 , batch: 254 , training loss: 4.149903\n",
      "[INFO] Epoch: 21 , batch: 255 , training loss: 4.115282\n",
      "[INFO] Epoch: 21 , batch: 256 , training loss: 4.116267\n",
      "[INFO] Epoch: 21 , batch: 257 , training loss: 4.275716\n",
      "[INFO] Epoch: 21 , batch: 258 , training loss: 4.288676\n",
      "[INFO] Epoch: 21 , batch: 259 , training loss: 4.331838\n",
      "[INFO] Epoch: 21 , batch: 260 , training loss: 4.082887\n",
      "[INFO] Epoch: 21 , batch: 261 , training loss: 4.254038\n",
      "[INFO] Epoch: 21 , batch: 262 , training loss: 4.431444\n",
      "[INFO] Epoch: 21 , batch: 263 , training loss: 4.546221\n",
      "[INFO] Epoch: 21 , batch: 264 , training loss: 3.945123\n",
      "[INFO] Epoch: 21 , batch: 265 , training loss: 4.038254\n",
      "[INFO] Epoch: 21 , batch: 266 , training loss: 4.478687\n",
      "[INFO] Epoch: 21 , batch: 267 , training loss: 4.215955\n",
      "[INFO] Epoch: 21 , batch: 268 , training loss: 4.116875\n",
      "[INFO] Epoch: 21 , batch: 269 , training loss: 4.122798\n",
      "[INFO] Epoch: 21 , batch: 270 , training loss: 4.128081\n",
      "[INFO] Epoch: 21 , batch: 271 , training loss: 4.165176\n",
      "[INFO] Epoch: 21 , batch: 272 , training loss: 4.169692\n",
      "[INFO] Epoch: 21 , batch: 273 , training loss: 4.147391\n",
      "[INFO] Epoch: 21 , batch: 274 , training loss: 4.240327\n",
      "[INFO] Epoch: 21 , batch: 275 , training loss: 4.091612\n",
      "[INFO] Epoch: 21 , batch: 276 , training loss: 4.189456\n",
      "[INFO] Epoch: 21 , batch: 277 , training loss: 4.340887\n",
      "[INFO] Epoch: 21 , batch: 278 , training loss: 4.011537\n",
      "[INFO] Epoch: 21 , batch: 279 , training loss: 4.024283\n",
      "[INFO] Epoch: 21 , batch: 280 , training loss: 3.973069\n",
      "[INFO] Epoch: 21 , batch: 281 , training loss: 4.130372\n",
      "[INFO] Epoch: 21 , batch: 282 , training loss: 4.026340\n",
      "[INFO] Epoch: 21 , batch: 283 , training loss: 4.043088\n",
      "[INFO] Epoch: 21 , batch: 284 , training loss: 4.072519\n",
      "[INFO] Epoch: 21 , batch: 285 , training loss: 4.032974\n",
      "[INFO] Epoch: 21 , batch: 286 , training loss: 4.015464\n",
      "[INFO] Epoch: 21 , batch: 287 , training loss: 3.963854\n",
      "[INFO] Epoch: 21 , batch: 288 , training loss: 3.923488\n",
      "[INFO] Epoch: 21 , batch: 289 , training loss: 3.998971\n",
      "[INFO] Epoch: 21 , batch: 290 , training loss: 3.770236\n",
      "[INFO] Epoch: 21 , batch: 291 , training loss: 3.761371\n",
      "[INFO] Epoch: 21 , batch: 292 , training loss: 3.880224\n",
      "[INFO] Epoch: 21 , batch: 293 , training loss: 3.792063\n",
      "[INFO] Epoch: 21 , batch: 294 , training loss: 4.476483\n",
      "[INFO] Epoch: 21 , batch: 295 , training loss: 4.246090\n",
      "[INFO] Epoch: 21 , batch: 296 , training loss: 4.176664\n",
      "[INFO] Epoch: 21 , batch: 297 , training loss: 4.120837\n",
      "[INFO] Epoch: 21 , batch: 298 , training loss: 3.956549\n",
      "[INFO] Epoch: 21 , batch: 299 , training loss: 4.003141\n",
      "[INFO] Epoch: 21 , batch: 300 , training loss: 3.987321\n",
      "[INFO] Epoch: 21 , batch: 301 , training loss: 3.919241\n",
      "[INFO] Epoch: 21 , batch: 302 , training loss: 4.084232\n",
      "[INFO] Epoch: 21 , batch: 303 , training loss: 4.078627\n",
      "[INFO] Epoch: 21 , batch: 304 , training loss: 4.239651\n",
      "[INFO] Epoch: 21 , batch: 305 , training loss: 4.052275\n",
      "[INFO] Epoch: 21 , batch: 306 , training loss: 4.158010\n",
      "[INFO] Epoch: 21 , batch: 307 , training loss: 4.179748\n",
      "[INFO] Epoch: 21 , batch: 308 , training loss: 3.984258\n",
      "[INFO] Epoch: 21 , batch: 309 , training loss: 4.010931\n",
      "[INFO] Epoch: 21 , batch: 310 , training loss: 3.934931\n",
      "[INFO] Epoch: 21 , batch: 311 , training loss: 3.930926\n",
      "[INFO] Epoch: 21 , batch: 312 , training loss: 3.832050\n",
      "[INFO] Epoch: 21 , batch: 313 , training loss: 3.918044\n",
      "[INFO] Epoch: 21 , batch: 314 , training loss: 4.020875\n",
      "[INFO] Epoch: 21 , batch: 315 , training loss: 4.078579\n",
      "[INFO] Epoch: 21 , batch: 316 , training loss: 4.346011\n",
      "[INFO] Epoch: 21 , batch: 317 , training loss: 4.683510\n",
      "[INFO] Epoch: 21 , batch: 318 , training loss: 4.835558\n",
      "[INFO] Epoch: 21 , batch: 319 , training loss: 4.516984\n",
      "[INFO] Epoch: 21 , batch: 320 , training loss: 4.043103\n",
      "[INFO] Epoch: 21 , batch: 321 , training loss: 3.863512\n",
      "[INFO] Epoch: 21 , batch: 322 , training loss: 3.970859\n",
      "[INFO] Epoch: 21 , batch: 323 , training loss: 4.001946\n",
      "[INFO] Epoch: 21 , batch: 324 , training loss: 4.000896\n",
      "[INFO] Epoch: 21 , batch: 325 , training loss: 4.112165\n",
      "[INFO] Epoch: 21 , batch: 326 , training loss: 4.154943\n",
      "[INFO] Epoch: 21 , batch: 327 , training loss: 4.085206\n",
      "[INFO] Epoch: 21 , batch: 328 , training loss: 4.080250\n",
      "[INFO] Epoch: 21 , batch: 329 , training loss: 3.981633\n",
      "[INFO] Epoch: 21 , batch: 330 , training loss: 3.993226\n",
      "[INFO] Epoch: 21 , batch: 331 , training loss: 4.144660\n",
      "[INFO] Epoch: 21 , batch: 332 , training loss: 3.994018\n",
      "[INFO] Epoch: 21 , batch: 333 , training loss: 3.954678\n",
      "[INFO] Epoch: 21 , batch: 334 , training loss: 3.971195\n",
      "[INFO] Epoch: 21 , batch: 335 , training loss: 4.102365\n",
      "[INFO] Epoch: 21 , batch: 336 , training loss: 4.118868\n",
      "[INFO] Epoch: 21 , batch: 337 , training loss: 4.176570\n",
      "[INFO] Epoch: 21 , batch: 338 , training loss: 4.373442\n",
      "[INFO] Epoch: 21 , batch: 339 , training loss: 4.180836\n",
      "[INFO] Epoch: 21 , batch: 340 , training loss: 4.386072\n",
      "[INFO] Epoch: 21 , batch: 341 , training loss: 4.112210\n",
      "[INFO] Epoch: 21 , batch: 342 , training loss: 3.904362\n",
      "[INFO] Epoch: 21 , batch: 343 , training loss: 3.972234\n",
      "[INFO] Epoch: 21 , batch: 344 , training loss: 3.849475\n",
      "[INFO] Epoch: 21 , batch: 345 , training loss: 3.991063\n",
      "[INFO] Epoch: 21 , batch: 346 , training loss: 4.021487\n",
      "[INFO] Epoch: 21 , batch: 347 , training loss: 3.936683\n",
      "[INFO] Epoch: 21 , batch: 348 , training loss: 4.053483\n",
      "[INFO] Epoch: 21 , batch: 349 , training loss: 4.128853\n",
      "[INFO] Epoch: 21 , batch: 350 , training loss: 3.988854\n",
      "[INFO] Epoch: 21 , batch: 351 , training loss: 4.067946\n",
      "[INFO] Epoch: 21 , batch: 352 , training loss: 4.091249\n",
      "[INFO] Epoch: 21 , batch: 353 , training loss: 4.055864\n",
      "[INFO] Epoch: 21 , batch: 354 , training loss: 4.150500\n",
      "[INFO] Epoch: 21 , batch: 355 , training loss: 4.132391\n",
      "[INFO] Epoch: 21 , batch: 356 , training loss: 4.027040\n",
      "[INFO] Epoch: 21 , batch: 357 , training loss: 4.101981\n",
      "[INFO] Epoch: 21 , batch: 358 , training loss: 3.995071\n",
      "[INFO] Epoch: 21 , batch: 359 , training loss: 4.005776\n",
      "[INFO] Epoch: 21 , batch: 360 , training loss: 4.102627\n",
      "[INFO] Epoch: 21 , batch: 361 , training loss: 4.064117\n",
      "[INFO] Epoch: 21 , batch: 362 , training loss: 4.172775\n",
      "[INFO] Epoch: 21 , batch: 363 , training loss: 4.048377\n",
      "[INFO] Epoch: 21 , batch: 364 , training loss: 4.131352\n",
      "[INFO] Epoch: 21 , batch: 365 , training loss: 4.012898\n",
      "[INFO] Epoch: 21 , batch: 366 , training loss: 4.121836\n",
      "[INFO] Epoch: 21 , batch: 367 , training loss: 4.168697\n",
      "[INFO] Epoch: 21 , batch: 368 , training loss: 4.617718\n",
      "[INFO] Epoch: 21 , batch: 369 , training loss: 4.279408\n",
      "[INFO] Epoch: 21 , batch: 370 , training loss: 4.038570\n",
      "[INFO] Epoch: 21 , batch: 371 , training loss: 4.460021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 21 , batch: 372 , training loss: 4.711397\n",
      "[INFO] Epoch: 21 , batch: 373 , training loss: 4.796515\n",
      "[INFO] Epoch: 21 , batch: 374 , training loss: 4.883585\n",
      "[INFO] Epoch: 21 , batch: 375 , training loss: 4.850249\n",
      "[INFO] Epoch: 21 , batch: 376 , training loss: 4.744410\n",
      "[INFO] Epoch: 21 , batch: 377 , training loss: 4.493624\n",
      "[INFO] Epoch: 21 , batch: 378 , training loss: 4.573712\n",
      "[INFO] Epoch: 21 , batch: 379 , training loss: 4.567060\n",
      "[INFO] Epoch: 21 , batch: 380 , training loss: 4.691158\n",
      "[INFO] Epoch: 21 , batch: 381 , training loss: 4.438547\n",
      "[INFO] Epoch: 21 , batch: 382 , training loss: 4.720388\n",
      "[INFO] Epoch: 21 , batch: 383 , training loss: 4.709529\n",
      "[INFO] Epoch: 21 , batch: 384 , training loss: 4.703610\n",
      "[INFO] Epoch: 21 , batch: 385 , training loss: 4.381403\n",
      "[INFO] Epoch: 21 , batch: 386 , training loss: 4.647609\n",
      "[INFO] Epoch: 21 , batch: 387 , training loss: 4.606880\n",
      "[INFO] Epoch: 21 , batch: 388 , training loss: 4.418731\n",
      "[INFO] Epoch: 21 , batch: 389 , training loss: 4.232833\n",
      "[INFO] Epoch: 21 , batch: 390 , training loss: 4.250263\n",
      "[INFO] Epoch: 21 , batch: 391 , training loss: 4.270022\n",
      "[INFO] Epoch: 21 , batch: 392 , training loss: 4.635757\n",
      "[INFO] Epoch: 21 , batch: 393 , training loss: 4.542912\n",
      "[INFO] Epoch: 21 , batch: 394 , training loss: 4.621770\n",
      "[INFO] Epoch: 21 , batch: 395 , training loss: 4.422032\n",
      "[INFO] Epoch: 21 , batch: 396 , training loss: 4.242758\n",
      "[INFO] Epoch: 21 , batch: 397 , training loss: 4.389736\n",
      "[INFO] Epoch: 21 , batch: 398 , training loss: 4.266512\n",
      "[INFO] Epoch: 21 , batch: 399 , training loss: 4.330923\n",
      "[INFO] Epoch: 21 , batch: 400 , training loss: 4.316204\n",
      "[INFO] Epoch: 21 , batch: 401 , training loss: 4.753544\n",
      "[INFO] Epoch: 21 , batch: 402 , training loss: 4.441459\n",
      "[INFO] Epoch: 21 , batch: 403 , training loss: 4.271862\n",
      "[INFO] Epoch: 21 , batch: 404 , training loss: 4.442618\n",
      "[INFO] Epoch: 21 , batch: 405 , training loss: 4.514188\n",
      "[INFO] Epoch: 21 , batch: 406 , training loss: 4.418205\n",
      "[INFO] Epoch: 21 , batch: 407 , training loss: 4.467680\n",
      "[INFO] Epoch: 21 , batch: 408 , training loss: 4.393222\n",
      "[INFO] Epoch: 21 , batch: 409 , training loss: 4.417886\n",
      "[INFO] Epoch: 21 , batch: 410 , training loss: 4.504893\n",
      "[INFO] Epoch: 21 , batch: 411 , training loss: 4.669832\n",
      "[INFO] Epoch: 21 , batch: 412 , training loss: 4.485591\n",
      "[INFO] Epoch: 21 , batch: 413 , training loss: 4.349063\n",
      "[INFO] Epoch: 21 , batch: 414 , training loss: 4.404187\n",
      "[INFO] Epoch: 21 , batch: 415 , training loss: 4.437490\n",
      "[INFO] Epoch: 21 , batch: 416 , training loss: 4.517920\n",
      "[INFO] Epoch: 21 , batch: 417 , training loss: 4.415623\n",
      "[INFO] Epoch: 21 , batch: 418 , training loss: 4.458513\n",
      "[INFO] Epoch: 21 , batch: 419 , training loss: 4.417371\n",
      "[INFO] Epoch: 21 , batch: 420 , training loss: 4.403655\n",
      "[INFO] Epoch: 21 , batch: 421 , training loss: 4.394307\n",
      "[INFO] Epoch: 21 , batch: 422 , training loss: 4.248983\n",
      "[INFO] Epoch: 21 , batch: 423 , training loss: 4.446678\n",
      "[INFO] Epoch: 21 , batch: 424 , training loss: 4.621775\n",
      "[INFO] Epoch: 21 , batch: 425 , training loss: 4.492457\n",
      "[INFO] Epoch: 21 , batch: 426 , training loss: 4.205904\n",
      "[INFO] Epoch: 21 , batch: 427 , training loss: 4.477455\n",
      "[INFO] Epoch: 21 , batch: 428 , training loss: 4.344939\n",
      "[INFO] Epoch: 21 , batch: 429 , training loss: 4.241592\n",
      "[INFO] Epoch: 21 , batch: 430 , training loss: 4.460075\n",
      "[INFO] Epoch: 21 , batch: 431 , training loss: 4.076763\n",
      "[INFO] Epoch: 21 , batch: 432 , training loss: 4.140242\n",
      "[INFO] Epoch: 21 , batch: 433 , training loss: 4.152616\n",
      "[INFO] Epoch: 21 , batch: 434 , training loss: 4.048340\n",
      "[INFO] Epoch: 21 , batch: 435 , training loss: 4.396242\n",
      "[INFO] Epoch: 21 , batch: 436 , training loss: 4.450623\n",
      "[INFO] Epoch: 21 , batch: 437 , training loss: 4.221329\n",
      "[INFO] Epoch: 21 , batch: 438 , training loss: 4.085179\n",
      "[INFO] Epoch: 21 , batch: 439 , training loss: 4.325693\n",
      "[INFO] Epoch: 21 , batch: 440 , training loss: 4.439458\n",
      "[INFO] Epoch: 21 , batch: 441 , training loss: 4.515752\n",
      "[INFO] Epoch: 21 , batch: 442 , training loss: 4.273755\n",
      "[INFO] Epoch: 21 , batch: 443 , training loss: 4.503196\n",
      "[INFO] Epoch: 21 , batch: 444 , training loss: 4.080558\n",
      "[INFO] Epoch: 21 , batch: 445 , training loss: 3.998786\n",
      "[INFO] Epoch: 21 , batch: 446 , training loss: 3.931785\n",
      "[INFO] Epoch: 21 , batch: 447 , training loss: 4.121974\n",
      "[INFO] Epoch: 21 , batch: 448 , training loss: 4.242265\n",
      "[INFO] Epoch: 21 , batch: 449 , training loss: 4.629383\n",
      "[INFO] Epoch: 21 , batch: 450 , training loss: 4.690460\n",
      "[INFO] Epoch: 21 , batch: 451 , training loss: 4.582607\n",
      "[INFO] Epoch: 21 , batch: 452 , training loss: 4.397981\n",
      "[INFO] Epoch: 21 , batch: 453 , training loss: 4.173959\n",
      "[INFO] Epoch: 21 , batch: 454 , training loss: 4.321893\n",
      "[INFO] Epoch: 21 , batch: 455 , training loss: 4.372871\n",
      "[INFO] Epoch: 21 , batch: 456 , training loss: 4.372996\n",
      "[INFO] Epoch: 21 , batch: 457 , training loss: 4.441850\n",
      "[INFO] Epoch: 21 , batch: 458 , training loss: 4.192323\n",
      "[INFO] Epoch: 21 , batch: 459 , training loss: 4.157755\n",
      "[INFO] Epoch: 21 , batch: 460 , training loss: 4.275395\n",
      "[INFO] Epoch: 21 , batch: 461 , training loss: 4.234925\n",
      "[INFO] Epoch: 21 , batch: 462 , training loss: 4.297240\n",
      "[INFO] Epoch: 21 , batch: 463 , training loss: 4.222038\n",
      "[INFO] Epoch: 21 , batch: 464 , training loss: 4.380276\n",
      "[INFO] Epoch: 21 , batch: 465 , training loss: 4.328041\n",
      "[INFO] Epoch: 21 , batch: 466 , training loss: 4.417933\n",
      "[INFO] Epoch: 21 , batch: 467 , training loss: 4.392825\n",
      "[INFO] Epoch: 21 , batch: 468 , training loss: 4.351038\n",
      "[INFO] Epoch: 21 , batch: 469 , training loss: 4.395981\n",
      "[INFO] Epoch: 21 , batch: 470 , training loss: 4.185272\n",
      "[INFO] Epoch: 21 , batch: 471 , training loss: 4.307900\n",
      "[INFO] Epoch: 21 , batch: 472 , training loss: 4.360677\n",
      "[INFO] Epoch: 21 , batch: 473 , training loss: 4.267442\n",
      "[INFO] Epoch: 21 , batch: 474 , training loss: 4.067566\n",
      "[INFO] Epoch: 21 , batch: 475 , training loss: 3.940443\n",
      "[INFO] Epoch: 21 , batch: 476 , training loss: 4.344219\n",
      "[INFO] Epoch: 21 , batch: 477 , training loss: 4.446996\n",
      "[INFO] Epoch: 21 , batch: 478 , training loss: 4.465913\n",
      "[INFO] Epoch: 21 , batch: 479 , training loss: 4.427747\n",
      "[INFO] Epoch: 21 , batch: 480 , training loss: 4.554388\n",
      "[INFO] Epoch: 21 , batch: 481 , training loss: 4.430184\n",
      "[INFO] Epoch: 21 , batch: 482 , training loss: 4.538667\n",
      "[INFO] Epoch: 21 , batch: 483 , training loss: 4.386102\n",
      "[INFO] Epoch: 21 , batch: 484 , training loss: 4.178702\n",
      "[INFO] Epoch: 21 , batch: 485 , training loss: 4.274759\n",
      "[INFO] Epoch: 21 , batch: 486 , training loss: 4.182078\n",
      "[INFO] Epoch: 21 , batch: 487 , training loss: 4.169609\n",
      "[INFO] Epoch: 21 , batch: 488 , training loss: 4.344978\n",
      "[INFO] Epoch: 21 , batch: 489 , training loss: 4.248135\n",
      "[INFO] Epoch: 21 , batch: 490 , training loss: 4.315881\n",
      "[INFO] Epoch: 21 , batch: 491 , training loss: 4.240045\n",
      "[INFO] Epoch: 21 , batch: 492 , training loss: 4.205067\n",
      "[INFO] Epoch: 21 , batch: 493 , training loss: 4.387685\n",
      "[INFO] Epoch: 21 , batch: 494 , training loss: 4.270286\n",
      "[INFO] Epoch: 21 , batch: 495 , training loss: 4.429710\n",
      "[INFO] Epoch: 21 , batch: 496 , training loss: 4.309236\n",
      "[INFO] Epoch: 21 , batch: 497 , training loss: 4.345469\n",
      "[INFO] Epoch: 21 , batch: 498 , training loss: 4.332466\n",
      "[INFO] Epoch: 21 , batch: 499 , training loss: 4.398840\n",
      "[INFO] Epoch: 21 , batch: 500 , training loss: 4.532494\n",
      "[INFO] Epoch: 21 , batch: 501 , training loss: 4.883734\n",
      "[INFO] Epoch: 21 , batch: 502 , training loss: 4.947956\n",
      "[INFO] Epoch: 21 , batch: 503 , training loss: 4.613405\n",
      "[INFO] Epoch: 21 , batch: 504 , training loss: 4.767322\n",
      "[INFO] Epoch: 21 , batch: 505 , training loss: 4.714885\n",
      "[INFO] Epoch: 21 , batch: 506 , training loss: 4.686531\n",
      "[INFO] Epoch: 21 , batch: 507 , training loss: 4.735274\n",
      "[INFO] Epoch: 21 , batch: 508 , training loss: 4.651944\n",
      "[INFO] Epoch: 21 , batch: 509 , training loss: 4.452266\n",
      "[INFO] Epoch: 21 , batch: 510 , training loss: 4.530117\n",
      "[INFO] Epoch: 21 , batch: 511 , training loss: 4.455157\n",
      "[INFO] Epoch: 21 , batch: 512 , training loss: 4.522391\n",
      "[INFO] Epoch: 21 , batch: 513 , training loss: 4.790927\n",
      "[INFO] Epoch: 21 , batch: 514 , training loss: 4.439443\n",
      "[INFO] Epoch: 21 , batch: 515 , training loss: 4.697441\n",
      "[INFO] Epoch: 21 , batch: 516 , training loss: 4.476398\n",
      "[INFO] Epoch: 21 , batch: 517 , training loss: 4.456123\n",
      "[INFO] Epoch: 21 , batch: 518 , training loss: 4.406256\n",
      "[INFO] Epoch: 21 , batch: 519 , training loss: 4.258409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 21 , batch: 520 , training loss: 4.501149\n",
      "[INFO] Epoch: 21 , batch: 521 , training loss: 4.480485\n",
      "[INFO] Epoch: 21 , batch: 522 , training loss: 4.559375\n",
      "[INFO] Epoch: 21 , batch: 523 , training loss: 4.462721\n",
      "[INFO] Epoch: 21 , batch: 524 , training loss: 4.742677\n",
      "[INFO] Epoch: 21 , batch: 525 , training loss: 4.616470\n",
      "[INFO] Epoch: 21 , batch: 526 , training loss: 4.392654\n",
      "[INFO] Epoch: 21 , batch: 527 , training loss: 4.462996\n",
      "[INFO] Epoch: 21 , batch: 528 , training loss: 4.475549\n",
      "[INFO] Epoch: 21 , batch: 529 , training loss: 4.444120\n",
      "[INFO] Epoch: 21 , batch: 530 , training loss: 4.288753\n",
      "[INFO] Epoch: 21 , batch: 531 , training loss: 4.449981\n",
      "[INFO] Epoch: 21 , batch: 532 , training loss: 4.348222\n",
      "[INFO] Epoch: 21 , batch: 533 , training loss: 4.497512\n",
      "[INFO] Epoch: 21 , batch: 534 , training loss: 4.487964\n",
      "[INFO] Epoch: 21 , batch: 535 , training loss: 4.481960\n",
      "[INFO] Epoch: 21 , batch: 536 , training loss: 4.349599\n",
      "[INFO] Epoch: 21 , batch: 537 , training loss: 4.310743\n",
      "[INFO] Epoch: 21 , batch: 538 , training loss: 4.398928\n",
      "[INFO] Epoch: 21 , batch: 539 , training loss: 4.512893\n",
      "[INFO] Epoch: 21 , batch: 540 , training loss: 5.058501\n",
      "[INFO] Epoch: 21 , batch: 541 , training loss: 4.886685\n",
      "[INFO] Epoch: 21 , batch: 542 , training loss: 4.744871\n",
      "[INFO] Epoch: 22 , batch: 0 , training loss: 3.813442\n",
      "[INFO] Epoch: 22 , batch: 1 , training loss: 3.655913\n",
      "[INFO] Epoch: 22 , batch: 2 , training loss: 3.781154\n",
      "[INFO] Epoch: 22 , batch: 3 , training loss: 3.634291\n",
      "[INFO] Epoch: 22 , batch: 4 , training loss: 3.952092\n",
      "[INFO] Epoch: 22 , batch: 5 , training loss: 3.637297\n",
      "[INFO] Epoch: 22 , batch: 6 , training loss: 4.002995\n",
      "[INFO] Epoch: 22 , batch: 7 , training loss: 3.876379\n",
      "[INFO] Epoch: 22 , batch: 8 , training loss: 3.553892\n",
      "[INFO] Epoch: 22 , batch: 9 , training loss: 3.828470\n",
      "[INFO] Epoch: 22 , batch: 10 , training loss: 3.729929\n",
      "[INFO] Epoch: 22 , batch: 11 , training loss: 3.703452\n",
      "[INFO] Epoch: 22 , batch: 12 , training loss: 3.592294\n",
      "[INFO] Epoch: 22 , batch: 13 , training loss: 3.653087\n",
      "[INFO] Epoch: 22 , batch: 14 , training loss: 3.517327\n",
      "[INFO] Epoch: 22 , batch: 15 , training loss: 3.748074\n",
      "[INFO] Epoch: 22 , batch: 16 , training loss: 3.612792\n",
      "[INFO] Epoch: 22 , batch: 17 , training loss: 3.736324\n",
      "[INFO] Epoch: 22 , batch: 18 , training loss: 3.654158\n",
      "[INFO] Epoch: 22 , batch: 19 , training loss: 3.421346\n",
      "[INFO] Epoch: 22 , batch: 20 , training loss: 3.394860\n",
      "[INFO] Epoch: 22 , batch: 21 , training loss: 3.560504\n",
      "[INFO] Epoch: 22 , batch: 22 , training loss: 3.439878\n",
      "[INFO] Epoch: 22 , batch: 23 , training loss: 3.646141\n",
      "[INFO] Epoch: 22 , batch: 24 , training loss: 3.534218\n",
      "[INFO] Epoch: 22 , batch: 25 , training loss: 3.634021\n",
      "[INFO] Epoch: 22 , batch: 26 , training loss: 3.503340\n",
      "[INFO] Epoch: 22 , batch: 27 , training loss: 3.442972\n",
      "[INFO] Epoch: 22 , batch: 28 , training loss: 3.674529\n",
      "[INFO] Epoch: 22 , batch: 29 , training loss: 3.473212\n",
      "[INFO] Epoch: 22 , batch: 30 , training loss: 3.484963\n",
      "[INFO] Epoch: 22 , batch: 31 , training loss: 3.601815\n",
      "[INFO] Epoch: 22 , batch: 32 , training loss: 3.577121\n",
      "[INFO] Epoch: 22 , batch: 33 , training loss: 3.591704\n",
      "[INFO] Epoch: 22 , batch: 34 , training loss: 3.617464\n",
      "[INFO] Epoch: 22 , batch: 35 , training loss: 3.563287\n",
      "[INFO] Epoch: 22 , batch: 36 , training loss: 3.638493\n",
      "[INFO] Epoch: 22 , batch: 37 , training loss: 3.480209\n",
      "[INFO] Epoch: 22 , batch: 38 , training loss: 3.586333\n",
      "[INFO] Epoch: 22 , batch: 39 , training loss: 3.422662\n",
      "[INFO] Epoch: 22 , batch: 40 , training loss: 3.600984\n",
      "[INFO] Epoch: 22 , batch: 41 , training loss: 3.572684\n",
      "[INFO] Epoch: 22 , batch: 42 , training loss: 4.012231\n",
      "[INFO] Epoch: 22 , batch: 43 , training loss: 3.753720\n",
      "[INFO] Epoch: 22 , batch: 44 , training loss: 4.117281\n",
      "[INFO] Epoch: 22 , batch: 45 , training loss: 4.047002\n",
      "[INFO] Epoch: 22 , batch: 46 , training loss: 4.031914\n",
      "[INFO] Epoch: 22 , batch: 47 , training loss: 3.673586\n",
      "[INFO] Epoch: 22 , batch: 48 , training loss: 3.663698\n",
      "[INFO] Epoch: 22 , batch: 49 , training loss: 3.927904\n",
      "[INFO] Epoch: 22 , batch: 50 , training loss: 3.671472\n",
      "[INFO] Epoch: 22 , batch: 51 , training loss: 3.878090\n",
      "[INFO] Epoch: 22 , batch: 52 , training loss: 3.721167\n",
      "[INFO] Epoch: 22 , batch: 53 , training loss: 3.827132\n",
      "[INFO] Epoch: 22 , batch: 54 , training loss: 3.830789\n",
      "[INFO] Epoch: 22 , batch: 55 , training loss: 3.957686\n",
      "[INFO] Epoch: 22 , batch: 56 , training loss: 3.758806\n",
      "[INFO] Epoch: 22 , batch: 57 , training loss: 3.669820\n",
      "[INFO] Epoch: 22 , batch: 58 , training loss: 3.718262\n",
      "[INFO] Epoch: 22 , batch: 59 , training loss: 3.824040\n",
      "[INFO] Epoch: 22 , batch: 60 , training loss: 3.782158\n",
      "[INFO] Epoch: 22 , batch: 61 , training loss: 3.797211\n",
      "[INFO] Epoch: 22 , batch: 62 , training loss: 3.712694\n",
      "[INFO] Epoch: 22 , batch: 63 , training loss: 3.877141\n",
      "[INFO] Epoch: 22 , batch: 64 , training loss: 4.124877\n",
      "[INFO] Epoch: 22 , batch: 65 , training loss: 3.781539\n",
      "[INFO] Epoch: 22 , batch: 66 , training loss: 3.643533\n",
      "[INFO] Epoch: 22 , batch: 67 , training loss: 3.641776\n",
      "[INFO] Epoch: 22 , batch: 68 , training loss: 3.904712\n",
      "[INFO] Epoch: 22 , batch: 69 , training loss: 3.773108\n",
      "[INFO] Epoch: 22 , batch: 70 , training loss: 4.003648\n",
      "[INFO] Epoch: 22 , batch: 71 , training loss: 3.851339\n",
      "[INFO] Epoch: 22 , batch: 72 , training loss: 3.894660\n",
      "[INFO] Epoch: 22 , batch: 73 , training loss: 3.855762\n",
      "[INFO] Epoch: 22 , batch: 74 , training loss: 3.970703\n",
      "[INFO] Epoch: 22 , batch: 75 , training loss: 3.815063\n",
      "[INFO] Epoch: 22 , batch: 76 , training loss: 3.930784\n",
      "[INFO] Epoch: 22 , batch: 77 , training loss: 3.845168\n",
      "[INFO] Epoch: 22 , batch: 78 , training loss: 3.948308\n",
      "[INFO] Epoch: 22 , batch: 79 , training loss: 3.787576\n",
      "[INFO] Epoch: 22 , batch: 80 , training loss: 3.994611\n",
      "[INFO] Epoch: 22 , batch: 81 , training loss: 3.925273\n",
      "[INFO] Epoch: 22 , batch: 82 , training loss: 3.893999\n",
      "[INFO] Epoch: 22 , batch: 83 , training loss: 4.030039\n",
      "[INFO] Epoch: 22 , batch: 84 , training loss: 3.955527\n",
      "[INFO] Epoch: 22 , batch: 85 , training loss: 4.027801\n",
      "[INFO] Epoch: 22 , batch: 86 , training loss: 3.985782\n",
      "[INFO] Epoch: 22 , batch: 87 , training loss: 3.943998\n",
      "[INFO] Epoch: 22 , batch: 88 , training loss: 4.095919\n",
      "[INFO] Epoch: 22 , batch: 89 , training loss: 3.868187\n",
      "[INFO] Epoch: 22 , batch: 90 , training loss: 3.962581\n",
      "[INFO] Epoch: 22 , batch: 91 , training loss: 3.874083\n",
      "[INFO] Epoch: 22 , batch: 92 , training loss: 3.905212\n",
      "[INFO] Epoch: 22 , batch: 93 , training loss: 4.001730\n",
      "[INFO] Epoch: 22 , batch: 94 , training loss: 4.138222\n",
      "[INFO] Epoch: 22 , batch: 95 , training loss: 3.907842\n",
      "[INFO] Epoch: 22 , batch: 96 , training loss: 3.887650\n",
      "[INFO] Epoch: 22 , batch: 97 , training loss: 3.856582\n",
      "[INFO] Epoch: 22 , batch: 98 , training loss: 3.789076\n",
      "[INFO] Epoch: 22 , batch: 99 , training loss: 3.899678\n",
      "[INFO] Epoch: 22 , batch: 100 , training loss: 3.774424\n",
      "[INFO] Epoch: 22 , batch: 101 , training loss: 3.798901\n",
      "[INFO] Epoch: 22 , batch: 102 , training loss: 3.994742\n",
      "[INFO] Epoch: 22 , batch: 103 , training loss: 3.755361\n",
      "[INFO] Epoch: 22 , batch: 104 , training loss: 3.707519\n",
      "[INFO] Epoch: 22 , batch: 105 , training loss: 3.977508\n",
      "[INFO] Epoch: 22 , batch: 106 , training loss: 3.985693\n",
      "[INFO] Epoch: 22 , batch: 107 , training loss: 3.839811\n",
      "[INFO] Epoch: 22 , batch: 108 , training loss: 3.761586\n",
      "[INFO] Epoch: 22 , batch: 109 , training loss: 3.710546\n",
      "[INFO] Epoch: 22 , batch: 110 , training loss: 3.880675\n",
      "[INFO] Epoch: 22 , batch: 111 , training loss: 3.966902\n",
      "[INFO] Epoch: 22 , batch: 112 , training loss: 3.875585\n",
      "[INFO] Epoch: 22 , batch: 113 , training loss: 3.874985\n",
      "[INFO] Epoch: 22 , batch: 114 , training loss: 3.901643\n",
      "[INFO] Epoch: 22 , batch: 115 , training loss: 3.879719\n",
      "[INFO] Epoch: 22 , batch: 116 , training loss: 3.786286\n",
      "[INFO] Epoch: 22 , batch: 117 , training loss: 4.017100\n",
      "[INFO] Epoch: 22 , batch: 118 , training loss: 3.960881\n",
      "[INFO] Epoch: 22 , batch: 119 , training loss: 4.136341\n",
      "[INFO] Epoch: 22 , batch: 120 , training loss: 4.109091\n",
      "[INFO] Epoch: 22 , batch: 121 , training loss: 3.964352\n",
      "[INFO] Epoch: 22 , batch: 122 , training loss: 3.841757\n",
      "[INFO] Epoch: 22 , batch: 123 , training loss: 3.884291\n",
      "[INFO] Epoch: 22 , batch: 124 , training loss: 3.992434\n",
      "[INFO] Epoch: 22 , batch: 125 , training loss: 3.775611\n",
      "[INFO] Epoch: 22 , batch: 126 , training loss: 3.812641\n",
      "[INFO] Epoch: 22 , batch: 127 , training loss: 3.799909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 22 , batch: 128 , training loss: 3.974056\n",
      "[INFO] Epoch: 22 , batch: 129 , training loss: 3.930554\n",
      "[INFO] Epoch: 22 , batch: 130 , training loss: 3.879312\n",
      "[INFO] Epoch: 22 , batch: 131 , training loss: 3.911741\n",
      "[INFO] Epoch: 22 , batch: 132 , training loss: 3.927490\n",
      "[INFO] Epoch: 22 , batch: 133 , training loss: 3.869831\n",
      "[INFO] Epoch: 22 , batch: 134 , training loss: 3.625731\n",
      "[INFO] Epoch: 22 , batch: 135 , training loss: 3.728599\n",
      "[INFO] Epoch: 22 , batch: 136 , training loss: 4.014514\n",
      "[INFO] Epoch: 22 , batch: 137 , training loss: 3.908956\n",
      "[INFO] Epoch: 22 , batch: 138 , training loss: 3.979179\n",
      "[INFO] Epoch: 22 , batch: 139 , training loss: 4.557364\n",
      "[INFO] Epoch: 22 , batch: 140 , training loss: 4.341689\n",
      "[INFO] Epoch: 22 , batch: 141 , training loss: 4.116663\n",
      "[INFO] Epoch: 22 , batch: 142 , training loss: 3.839319\n",
      "[INFO] Epoch: 22 , batch: 143 , training loss: 3.931997\n",
      "[INFO] Epoch: 22 , batch: 144 , training loss: 3.793175\n",
      "[INFO] Epoch: 22 , batch: 145 , training loss: 3.869967\n",
      "[INFO] Epoch: 22 , batch: 146 , training loss: 4.102190\n",
      "[INFO] Epoch: 22 , batch: 147 , training loss: 3.731351\n",
      "[INFO] Epoch: 22 , batch: 148 , training loss: 3.719690\n",
      "[INFO] Epoch: 22 , batch: 149 , training loss: 3.799333\n",
      "[INFO] Epoch: 22 , batch: 150 , training loss: 4.092882\n",
      "[INFO] Epoch: 22 , batch: 151 , training loss: 3.883625\n",
      "[INFO] Epoch: 22 , batch: 152 , training loss: 3.909474\n",
      "[INFO] Epoch: 22 , batch: 153 , training loss: 3.920940\n",
      "[INFO] Epoch: 22 , batch: 154 , training loss: 4.005680\n",
      "[INFO] Epoch: 22 , batch: 155 , training loss: 4.205612\n",
      "[INFO] Epoch: 22 , batch: 156 , training loss: 3.981620\n",
      "[INFO] Epoch: 22 , batch: 157 , training loss: 3.939400\n",
      "[INFO] Epoch: 22 , batch: 158 , training loss: 4.107967\n",
      "[INFO] Epoch: 22 , batch: 159 , training loss: 4.002843\n",
      "[INFO] Epoch: 22 , batch: 160 , training loss: 4.284925\n",
      "[INFO] Epoch: 22 , batch: 161 , training loss: 4.307051\n",
      "[INFO] Epoch: 22 , batch: 162 , training loss: 4.283989\n",
      "[INFO] Epoch: 22 , batch: 163 , training loss: 4.478758\n",
      "[INFO] Epoch: 22 , batch: 164 , training loss: 4.391125\n",
      "[INFO] Epoch: 22 , batch: 165 , training loss: 4.333650\n",
      "[INFO] Epoch: 22 , batch: 166 , training loss: 4.252923\n",
      "[INFO] Epoch: 22 , batch: 167 , training loss: 4.438867\n",
      "[INFO] Epoch: 22 , batch: 168 , training loss: 4.115016\n",
      "[INFO] Epoch: 22 , batch: 169 , training loss: 4.048315\n",
      "[INFO] Epoch: 22 , batch: 170 , training loss: 4.161118\n",
      "[INFO] Epoch: 22 , batch: 171 , training loss: 3.605223\n",
      "[INFO] Epoch: 22 , batch: 172 , training loss: 3.839879\n",
      "[INFO] Epoch: 22 , batch: 173 , training loss: 4.109703\n",
      "[INFO] Epoch: 22 , batch: 174 , training loss: 4.571495\n",
      "[INFO] Epoch: 22 , batch: 175 , training loss: 4.872644\n",
      "[INFO] Epoch: 22 , batch: 176 , training loss: 4.570378\n",
      "[INFO] Epoch: 22 , batch: 177 , training loss: 4.206343\n",
      "[INFO] Epoch: 22 , batch: 178 , training loss: 4.160623\n",
      "[INFO] Epoch: 22 , batch: 179 , training loss: 4.205785\n",
      "[INFO] Epoch: 22 , batch: 180 , training loss: 4.150563\n",
      "[INFO] Epoch: 22 , batch: 181 , training loss: 4.421735\n",
      "[INFO] Epoch: 22 , batch: 182 , training loss: 4.364604\n",
      "[INFO] Epoch: 22 , batch: 183 , training loss: 4.325253\n",
      "[INFO] Epoch: 22 , batch: 184 , training loss: 4.213803\n",
      "[INFO] Epoch: 22 , batch: 185 , training loss: 4.169792\n",
      "[INFO] Epoch: 22 , batch: 186 , training loss: 4.301315\n",
      "[INFO] Epoch: 22 , batch: 187 , training loss: 4.437879\n",
      "[INFO] Epoch: 22 , batch: 188 , training loss: 4.401753\n",
      "[INFO] Epoch: 22 , batch: 189 , training loss: 4.312918\n",
      "[INFO] Epoch: 22 , batch: 190 , training loss: 4.340410\n",
      "[INFO] Epoch: 22 , batch: 191 , training loss: 4.451528\n",
      "[INFO] Epoch: 22 , batch: 192 , training loss: 4.272379\n",
      "[INFO] Epoch: 22 , batch: 193 , training loss: 4.369099\n",
      "[INFO] Epoch: 22 , batch: 194 , training loss: 4.355865\n",
      "[INFO] Epoch: 22 , batch: 195 , training loss: 4.264143\n",
      "[INFO] Epoch: 22 , batch: 196 , training loss: 4.131119\n",
      "[INFO] Epoch: 22 , batch: 197 , training loss: 4.218216\n",
      "[INFO] Epoch: 22 , batch: 198 , training loss: 4.116259\n",
      "[INFO] Epoch: 22 , batch: 199 , training loss: 4.250108\n",
      "[INFO] Epoch: 22 , batch: 200 , training loss: 4.146525\n",
      "[INFO] Epoch: 22 , batch: 201 , training loss: 4.065097\n",
      "[INFO] Epoch: 22 , batch: 202 , training loss: 4.042381\n",
      "[INFO] Epoch: 22 , batch: 203 , training loss: 4.169356\n",
      "[INFO] Epoch: 22 , batch: 204 , training loss: 4.262216\n",
      "[INFO] Epoch: 22 , batch: 205 , training loss: 3.855675\n",
      "[INFO] Epoch: 22 , batch: 206 , training loss: 3.769856\n",
      "[INFO] Epoch: 22 , batch: 207 , training loss: 3.791147\n",
      "[INFO] Epoch: 22 , batch: 208 , training loss: 4.123564\n",
      "[INFO] Epoch: 22 , batch: 209 , training loss: 4.063869\n",
      "[INFO] Epoch: 22 , batch: 210 , training loss: 4.089509\n",
      "[INFO] Epoch: 22 , batch: 211 , training loss: 4.096672\n",
      "[INFO] Epoch: 22 , batch: 212 , training loss: 4.190707\n",
      "[INFO] Epoch: 22 , batch: 213 , training loss: 4.161168\n",
      "[INFO] Epoch: 22 , batch: 214 , training loss: 4.231353\n",
      "[INFO] Epoch: 22 , batch: 215 , training loss: 4.436622\n",
      "[INFO] Epoch: 22 , batch: 216 , training loss: 4.160260\n",
      "[INFO] Epoch: 22 , batch: 217 , training loss: 4.073023\n",
      "[INFO] Epoch: 22 , batch: 218 , training loss: 4.084610\n",
      "[INFO] Epoch: 22 , batch: 219 , training loss: 4.194983\n",
      "[INFO] Epoch: 22 , batch: 220 , training loss: 4.010384\n",
      "[INFO] Epoch: 22 , batch: 221 , training loss: 4.015450\n",
      "[INFO] Epoch: 22 , batch: 222 , training loss: 4.153916\n",
      "[INFO] Epoch: 22 , batch: 223 , training loss: 4.271418\n",
      "[INFO] Epoch: 22 , batch: 224 , training loss: 4.309893\n",
      "[INFO] Epoch: 22 , batch: 225 , training loss: 4.178704\n",
      "[INFO] Epoch: 22 , batch: 226 , training loss: 4.319254\n",
      "[INFO] Epoch: 22 , batch: 227 , training loss: 4.280783\n",
      "[INFO] Epoch: 22 , batch: 228 , training loss: 4.302274\n",
      "[INFO] Epoch: 22 , batch: 229 , training loss: 4.175494\n",
      "[INFO] Epoch: 22 , batch: 230 , training loss: 4.047385\n",
      "[INFO] Epoch: 22 , batch: 231 , training loss: 3.890881\n",
      "[INFO] Epoch: 22 , batch: 232 , training loss: 4.035030\n",
      "[INFO] Epoch: 22 , batch: 233 , training loss: 4.056493\n",
      "[INFO] Epoch: 22 , batch: 234 , training loss: 3.749414\n",
      "[INFO] Epoch: 22 , batch: 235 , training loss: 3.880616\n",
      "[INFO] Epoch: 22 , batch: 236 , training loss: 3.973070\n",
      "[INFO] Epoch: 22 , batch: 237 , training loss: 4.186907\n",
      "[INFO] Epoch: 22 , batch: 238 , training loss: 3.956329\n",
      "[INFO] Epoch: 22 , batch: 239 , training loss: 4.005593\n",
      "[INFO] Epoch: 22 , batch: 240 , training loss: 4.046577\n",
      "[INFO] Epoch: 22 , batch: 241 , training loss: 3.860142\n",
      "[INFO] Epoch: 22 , batch: 242 , training loss: 3.882526\n",
      "[INFO] Epoch: 22 , batch: 243 , training loss: 4.145269\n",
      "[INFO] Epoch: 22 , batch: 244 , training loss: 4.108833\n",
      "[INFO] Epoch: 22 , batch: 245 , training loss: 4.077429\n",
      "[INFO] Epoch: 22 , batch: 246 , training loss: 3.781576\n",
      "[INFO] Epoch: 22 , batch: 247 , training loss: 3.943280\n",
      "[INFO] Epoch: 22 , batch: 248 , training loss: 4.024855\n",
      "[INFO] Epoch: 22 , batch: 249 , training loss: 3.992258\n",
      "[INFO] Epoch: 22 , batch: 250 , training loss: 3.796433\n",
      "[INFO] Epoch: 22 , batch: 251 , training loss: 4.251418\n",
      "[INFO] Epoch: 22 , batch: 252 , training loss: 3.959036\n",
      "[INFO] Epoch: 22 , batch: 253 , training loss: 3.879858\n",
      "[INFO] Epoch: 22 , batch: 254 , training loss: 4.148538\n",
      "[INFO] Epoch: 22 , batch: 255 , training loss: 4.123992\n",
      "[INFO] Epoch: 22 , batch: 256 , training loss: 4.106348\n",
      "[INFO] Epoch: 22 , batch: 257 , training loss: 4.271466\n",
      "[INFO] Epoch: 22 , batch: 258 , training loss: 4.288948\n",
      "[INFO] Epoch: 22 , batch: 259 , training loss: 4.320782\n",
      "[INFO] Epoch: 22 , batch: 260 , training loss: 4.103115\n",
      "[INFO] Epoch: 22 , batch: 261 , training loss: 4.259996\n",
      "[INFO] Epoch: 22 , batch: 262 , training loss: 4.425265\n",
      "[INFO] Epoch: 22 , batch: 263 , training loss: 4.566009\n",
      "[INFO] Epoch: 22 , batch: 264 , training loss: 3.953132\n",
      "[INFO] Epoch: 22 , batch: 265 , training loss: 4.053486\n",
      "[INFO] Epoch: 22 , batch: 266 , training loss: 4.465267\n",
      "[INFO] Epoch: 22 , batch: 267 , training loss: 4.208004\n",
      "[INFO] Epoch: 22 , batch: 268 , training loss: 4.128125\n",
      "[INFO] Epoch: 22 , batch: 269 , training loss: 4.117743\n",
      "[INFO] Epoch: 22 , batch: 270 , training loss: 4.135811\n",
      "[INFO] Epoch: 22 , batch: 271 , training loss: 4.169501\n",
      "[INFO] Epoch: 22 , batch: 272 , training loss: 4.160208\n",
      "[INFO] Epoch: 22 , batch: 273 , training loss: 4.154274\n",
      "[INFO] Epoch: 22 , batch: 274 , training loss: 4.258693\n",
      "[INFO] Epoch: 22 , batch: 275 , training loss: 4.119141\n",
      "[INFO] Epoch: 22 , batch: 276 , training loss: 4.175927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 22 , batch: 277 , training loss: 4.348238\n",
      "[INFO] Epoch: 22 , batch: 278 , training loss: 4.005471\n",
      "[INFO] Epoch: 22 , batch: 279 , training loss: 4.035468\n",
      "[INFO] Epoch: 22 , batch: 280 , training loss: 3.969226\n",
      "[INFO] Epoch: 22 , batch: 281 , training loss: 4.123932\n",
      "[INFO] Epoch: 22 , batch: 282 , training loss: 4.028866\n",
      "[INFO] Epoch: 22 , batch: 283 , training loss: 4.022697\n",
      "[INFO] Epoch: 22 , batch: 284 , training loss: 4.080205\n",
      "[INFO] Epoch: 22 , batch: 285 , training loss: 4.031581\n",
      "[INFO] Epoch: 22 , batch: 286 , training loss: 4.018548\n",
      "[INFO] Epoch: 22 , batch: 287 , training loss: 3.953684\n",
      "[INFO] Epoch: 22 , batch: 288 , training loss: 3.924388\n",
      "[INFO] Epoch: 22 , batch: 289 , training loss: 3.998212\n",
      "[INFO] Epoch: 22 , batch: 290 , training loss: 3.760737\n",
      "[INFO] Epoch: 22 , batch: 291 , training loss: 3.759281\n",
      "[INFO] Epoch: 22 , batch: 292 , training loss: 3.878892\n",
      "[INFO] Epoch: 22 , batch: 293 , training loss: 3.786546\n",
      "[INFO] Epoch: 22 , batch: 294 , training loss: 4.472234\n",
      "[INFO] Epoch: 22 , batch: 295 , training loss: 4.239508\n",
      "[INFO] Epoch: 22 , batch: 296 , training loss: 4.172929\n",
      "[INFO] Epoch: 22 , batch: 297 , training loss: 4.135299\n",
      "[INFO] Epoch: 22 , batch: 298 , training loss: 3.953724\n",
      "[INFO] Epoch: 22 , batch: 299 , training loss: 4.000035\n",
      "[INFO] Epoch: 22 , batch: 300 , training loss: 3.986339\n",
      "[INFO] Epoch: 22 , batch: 301 , training loss: 3.913502\n",
      "[INFO] Epoch: 22 , batch: 302 , training loss: 4.093286\n",
      "[INFO] Epoch: 22 , batch: 303 , training loss: 4.090629\n",
      "[INFO] Epoch: 22 , batch: 304 , training loss: 4.260348\n",
      "[INFO] Epoch: 22 , batch: 305 , training loss: 4.043874\n",
      "[INFO] Epoch: 22 , batch: 306 , training loss: 4.158224\n",
      "[INFO] Epoch: 22 , batch: 307 , training loss: 4.182273\n",
      "[INFO] Epoch: 22 , batch: 308 , training loss: 3.985305\n",
      "[INFO] Epoch: 22 , batch: 309 , training loss: 4.008499\n",
      "[INFO] Epoch: 22 , batch: 310 , training loss: 3.923533\n",
      "[INFO] Epoch: 22 , batch: 311 , training loss: 3.913223\n",
      "[INFO] Epoch: 22 , batch: 312 , training loss: 3.830150\n",
      "[INFO] Epoch: 22 , batch: 313 , training loss: 3.917969\n",
      "[INFO] Epoch: 22 , batch: 314 , training loss: 4.014184\n",
      "[INFO] Epoch: 22 , batch: 315 , training loss: 4.071112\n",
      "[INFO] Epoch: 22 , batch: 316 , training loss: 4.338861\n",
      "[INFO] Epoch: 22 , batch: 317 , training loss: 4.702999\n",
      "[INFO] Epoch: 22 , batch: 318 , training loss: 4.856341\n",
      "[INFO] Epoch: 22 , batch: 319 , training loss: 4.533087\n",
      "[INFO] Epoch: 22 , batch: 320 , training loss: 4.031568\n",
      "[INFO] Epoch: 22 , batch: 321 , training loss: 3.871375\n",
      "[INFO] Epoch: 22 , batch: 322 , training loss: 3.974521\n",
      "[INFO] Epoch: 22 , batch: 323 , training loss: 4.014865\n",
      "[INFO] Epoch: 22 , batch: 324 , training loss: 3.984227\n",
      "[INFO] Epoch: 22 , batch: 325 , training loss: 4.115074\n",
      "[INFO] Epoch: 22 , batch: 326 , training loss: 4.135664\n",
      "[INFO] Epoch: 22 , batch: 327 , training loss: 4.084169\n",
      "[INFO] Epoch: 22 , batch: 328 , training loss: 4.086301\n",
      "[INFO] Epoch: 22 , batch: 329 , training loss: 3.990150\n",
      "[INFO] Epoch: 22 , batch: 330 , training loss: 3.996756\n",
      "[INFO] Epoch: 22 , batch: 331 , training loss: 4.131451\n",
      "[INFO] Epoch: 22 , batch: 332 , training loss: 3.988383\n",
      "[INFO] Epoch: 22 , batch: 333 , training loss: 3.960405\n",
      "[INFO] Epoch: 22 , batch: 334 , training loss: 3.968678\n",
      "[INFO] Epoch: 22 , batch: 335 , training loss: 4.107518\n",
      "[INFO] Epoch: 22 , batch: 336 , training loss: 4.102129\n",
      "[INFO] Epoch: 22 , batch: 337 , training loss: 4.179301\n",
      "[INFO] Epoch: 22 , batch: 338 , training loss: 4.368146\n",
      "[INFO] Epoch: 22 , batch: 339 , training loss: 4.165363\n",
      "[INFO] Epoch: 22 , batch: 340 , training loss: 4.371308\n",
      "[INFO] Epoch: 22 , batch: 341 , training loss: 4.111559\n",
      "[INFO] Epoch: 22 , batch: 342 , training loss: 3.902449\n",
      "[INFO] Epoch: 22 , batch: 343 , training loss: 3.977322\n",
      "[INFO] Epoch: 22 , batch: 344 , training loss: 3.841387\n",
      "[INFO] Epoch: 22 , batch: 345 , training loss: 3.986842\n",
      "[INFO] Epoch: 22 , batch: 346 , training loss: 4.023407\n",
      "[INFO] Epoch: 22 , batch: 347 , training loss: 3.927265\n",
      "[INFO] Epoch: 22 , batch: 348 , training loss: 4.042135\n",
      "[INFO] Epoch: 22 , batch: 349 , training loss: 4.135995\n",
      "[INFO] Epoch: 22 , batch: 350 , training loss: 3.988463\n",
      "[INFO] Epoch: 22 , batch: 351 , training loss: 4.073145\n",
      "[INFO] Epoch: 22 , batch: 352 , training loss: 4.081254\n",
      "[INFO] Epoch: 22 , batch: 353 , training loss: 4.063036\n",
      "[INFO] Epoch: 22 , batch: 354 , training loss: 4.153248\n",
      "[INFO] Epoch: 22 , batch: 355 , training loss: 4.131433\n",
      "[INFO] Epoch: 22 , batch: 356 , training loss: 4.017326\n",
      "[INFO] Epoch: 22 , batch: 357 , training loss: 4.095094\n",
      "[INFO] Epoch: 22 , batch: 358 , training loss: 3.997190\n",
      "[INFO] Epoch: 22 , batch: 359 , training loss: 4.010491\n",
      "[INFO] Epoch: 22 , batch: 360 , training loss: 4.107441\n",
      "[INFO] Epoch: 22 , batch: 361 , training loss: 4.068998\n",
      "[INFO] Epoch: 22 , batch: 362 , training loss: 4.176941\n",
      "[INFO] Epoch: 22 , batch: 363 , training loss: 4.055736\n",
      "[INFO] Epoch: 22 , batch: 364 , training loss: 4.134440\n",
      "[INFO] Epoch: 22 , batch: 365 , training loss: 4.003678\n",
      "[INFO] Epoch: 22 , batch: 366 , training loss: 4.131977\n",
      "[INFO] Epoch: 22 , batch: 367 , training loss: 4.172279\n",
      "[INFO] Epoch: 22 , batch: 368 , training loss: 4.614299\n",
      "[INFO] Epoch: 22 , batch: 369 , training loss: 4.272499\n",
      "[INFO] Epoch: 22 , batch: 370 , training loss: 4.040891\n",
      "[INFO] Epoch: 22 , batch: 371 , training loss: 4.474816\n",
      "[INFO] Epoch: 22 , batch: 372 , training loss: 4.708332\n",
      "[INFO] Epoch: 22 , batch: 373 , training loss: 4.788499\n",
      "[INFO] Epoch: 22 , batch: 374 , training loss: 4.897594\n",
      "[INFO] Epoch: 22 , batch: 375 , training loss: 4.868251\n",
      "[INFO] Epoch: 22 , batch: 376 , training loss: 4.753963\n",
      "[INFO] Epoch: 22 , batch: 377 , training loss: 4.496160\n",
      "[INFO] Epoch: 22 , batch: 378 , training loss: 4.587688\n",
      "[INFO] Epoch: 22 , batch: 379 , training loss: 4.564208\n",
      "[INFO] Epoch: 22 , batch: 380 , training loss: 4.702300\n",
      "[INFO] Epoch: 22 , batch: 381 , training loss: 4.413270\n",
      "[INFO] Epoch: 22 , batch: 382 , training loss: 4.716640\n",
      "[INFO] Epoch: 22 , batch: 383 , training loss: 4.717299\n",
      "[INFO] Epoch: 22 , batch: 384 , training loss: 4.734076\n",
      "[INFO] Epoch: 22 , batch: 385 , training loss: 4.401138\n",
      "[INFO] Epoch: 22 , batch: 386 , training loss: 4.642058\n",
      "[INFO] Epoch: 22 , batch: 387 , training loss: 4.612175\n",
      "[INFO] Epoch: 22 , batch: 388 , training loss: 4.402573\n",
      "[INFO] Epoch: 22 , batch: 389 , training loss: 4.236707\n",
      "[INFO] Epoch: 22 , batch: 390 , training loss: 4.257752\n",
      "[INFO] Epoch: 22 , batch: 391 , training loss: 4.265034\n",
      "[INFO] Epoch: 22 , batch: 392 , training loss: 4.635981\n",
      "[INFO] Epoch: 22 , batch: 393 , training loss: 4.547309\n",
      "[INFO] Epoch: 22 , batch: 394 , training loss: 4.622844\n",
      "[INFO] Epoch: 22 , batch: 395 , training loss: 4.438998\n",
      "[INFO] Epoch: 22 , batch: 396 , training loss: 4.254894\n",
      "[INFO] Epoch: 22 , batch: 397 , training loss: 4.395144\n",
      "[INFO] Epoch: 22 , batch: 398 , training loss: 4.270502\n",
      "[INFO] Epoch: 22 , batch: 399 , training loss: 4.342969\n",
      "[INFO] Epoch: 22 , batch: 400 , training loss: 4.318565\n",
      "[INFO] Epoch: 22 , batch: 401 , training loss: 4.754204\n",
      "[INFO] Epoch: 22 , batch: 402 , training loss: 4.450948\n",
      "[INFO] Epoch: 22 , batch: 403 , training loss: 4.280823\n",
      "[INFO] Epoch: 22 , batch: 404 , training loss: 4.434591\n",
      "[INFO] Epoch: 22 , batch: 405 , training loss: 4.523033\n",
      "[INFO] Epoch: 22 , batch: 406 , training loss: 4.428849\n",
      "[INFO] Epoch: 22 , batch: 407 , training loss: 4.472965\n",
      "[INFO] Epoch: 22 , batch: 408 , training loss: 4.404024\n",
      "[INFO] Epoch: 22 , batch: 409 , training loss: 4.423446\n",
      "[INFO] Epoch: 22 , batch: 410 , training loss: 4.509438\n",
      "[INFO] Epoch: 22 , batch: 411 , training loss: 4.672117\n",
      "[INFO] Epoch: 22 , batch: 412 , training loss: 4.483649\n",
      "[INFO] Epoch: 22 , batch: 413 , training loss: 4.361844\n",
      "[INFO] Epoch: 22 , batch: 414 , training loss: 4.396739\n",
      "[INFO] Epoch: 22 , batch: 415 , training loss: 4.433311\n",
      "[INFO] Epoch: 22 , batch: 416 , training loss: 4.513301\n",
      "[INFO] Epoch: 22 , batch: 417 , training loss: 4.412245\n",
      "[INFO] Epoch: 22 , batch: 418 , training loss: 4.471470\n",
      "[INFO] Epoch: 22 , batch: 419 , training loss: 4.442907\n",
      "[INFO] Epoch: 22 , batch: 420 , training loss: 4.407950\n",
      "[INFO] Epoch: 22 , batch: 421 , training loss: 4.383662\n",
      "[INFO] Epoch: 22 , batch: 422 , training loss: 4.247529\n",
      "[INFO] Epoch: 22 , batch: 423 , training loss: 4.447609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 22 , batch: 424 , training loss: 4.631872\n",
      "[INFO] Epoch: 22 , batch: 425 , training loss: 4.491908\n",
      "[INFO] Epoch: 22 , batch: 426 , training loss: 4.209457\n",
      "[INFO] Epoch: 22 , batch: 427 , training loss: 4.488934\n",
      "[INFO] Epoch: 22 , batch: 428 , training loss: 4.343689\n",
      "[INFO] Epoch: 22 , batch: 429 , training loss: 4.244945\n",
      "[INFO] Epoch: 22 , batch: 430 , training loss: 4.471600\n",
      "[INFO] Epoch: 22 , batch: 431 , training loss: 4.079896\n",
      "[INFO] Epoch: 22 , batch: 432 , training loss: 4.146882\n",
      "[INFO] Epoch: 22 , batch: 433 , training loss: 4.153666\n",
      "[INFO] Epoch: 22 , batch: 434 , training loss: 4.041602\n",
      "[INFO] Epoch: 22 , batch: 435 , training loss: 4.387335\n",
      "[INFO] Epoch: 22 , batch: 436 , training loss: 4.449569\n",
      "[INFO] Epoch: 22 , batch: 437 , training loss: 4.228521\n",
      "[INFO] Epoch: 22 , batch: 438 , training loss: 4.091104\n",
      "[INFO] Epoch: 22 , batch: 439 , training loss: 4.307209\n",
      "[INFO] Epoch: 22 , batch: 440 , training loss: 4.437510\n",
      "[INFO] Epoch: 22 , batch: 441 , training loss: 4.513457\n",
      "[INFO] Epoch: 22 , batch: 442 , training loss: 4.271968\n",
      "[INFO] Epoch: 22 , batch: 443 , training loss: 4.506919\n",
      "[INFO] Epoch: 22 , batch: 444 , training loss: 4.073969\n",
      "[INFO] Epoch: 22 , batch: 445 , training loss: 4.010811\n",
      "[INFO] Epoch: 22 , batch: 446 , training loss: 3.936139\n",
      "[INFO] Epoch: 22 , batch: 447 , training loss: 4.120697\n",
      "[INFO] Epoch: 22 , batch: 448 , training loss: 4.248919\n",
      "[INFO] Epoch: 22 , batch: 449 , training loss: 4.620735\n",
      "[INFO] Epoch: 22 , batch: 450 , training loss: 4.703560\n",
      "[INFO] Epoch: 22 , batch: 451 , training loss: 4.579891\n",
      "[INFO] Epoch: 22 , batch: 452 , training loss: 4.394805\n",
      "[INFO] Epoch: 22 , batch: 453 , training loss: 4.184271\n",
      "[INFO] Epoch: 22 , batch: 454 , training loss: 4.308735\n",
      "[INFO] Epoch: 22 , batch: 455 , training loss: 4.376636\n",
      "[INFO] Epoch: 22 , batch: 456 , training loss: 4.359758\n",
      "[INFO] Epoch: 22 , batch: 457 , training loss: 4.445426\n",
      "[INFO] Epoch: 22 , batch: 458 , training loss: 4.184598\n",
      "[INFO] Epoch: 22 , batch: 459 , training loss: 4.155192\n",
      "[INFO] Epoch: 22 , batch: 460 , training loss: 4.280586\n",
      "[INFO] Epoch: 22 , batch: 461 , training loss: 4.244684\n",
      "[INFO] Epoch: 22 , batch: 462 , training loss: 4.290421\n",
      "[INFO] Epoch: 22 , batch: 463 , training loss: 4.213014\n",
      "[INFO] Epoch: 22 , batch: 464 , training loss: 4.370818\n",
      "[INFO] Epoch: 22 , batch: 465 , training loss: 4.336985\n",
      "[INFO] Epoch: 22 , batch: 466 , training loss: 4.421507\n",
      "[INFO] Epoch: 22 , batch: 467 , training loss: 4.402365\n",
      "[INFO] Epoch: 22 , batch: 468 , training loss: 4.350217\n",
      "[INFO] Epoch: 22 , batch: 469 , training loss: 4.395164\n",
      "[INFO] Epoch: 22 , batch: 470 , training loss: 4.190175\n",
      "[INFO] Epoch: 22 , batch: 471 , training loss: 4.288573\n",
      "[INFO] Epoch: 22 , batch: 472 , training loss: 4.361863\n",
      "[INFO] Epoch: 22 , batch: 473 , training loss: 4.278067\n",
      "[INFO] Epoch: 22 , batch: 474 , training loss: 4.062247\n",
      "[INFO] Epoch: 22 , batch: 475 , training loss: 3.950654\n",
      "[INFO] Epoch: 22 , batch: 476 , training loss: 4.345961\n",
      "[INFO] Epoch: 22 , batch: 477 , training loss: 4.453057\n",
      "[INFO] Epoch: 22 , batch: 478 , training loss: 4.456139\n",
      "[INFO] Epoch: 22 , batch: 479 , training loss: 4.417459\n",
      "[INFO] Epoch: 22 , batch: 480 , training loss: 4.546169\n",
      "[INFO] Epoch: 22 , batch: 481 , training loss: 4.444574\n",
      "[INFO] Epoch: 22 , batch: 482 , training loss: 4.533104\n",
      "[INFO] Epoch: 22 , batch: 483 , training loss: 4.381585\n",
      "[INFO] Epoch: 22 , batch: 484 , training loss: 4.179282\n",
      "[INFO] Epoch: 22 , batch: 485 , training loss: 4.278201\n",
      "[INFO] Epoch: 22 , batch: 486 , training loss: 4.177353\n",
      "[INFO] Epoch: 22 , batch: 487 , training loss: 4.173193\n",
      "[INFO] Epoch: 22 , batch: 488 , training loss: 4.348522\n",
      "[INFO] Epoch: 22 , batch: 489 , training loss: 4.255428\n",
      "[INFO] Epoch: 22 , batch: 490 , training loss: 4.309133\n",
      "[INFO] Epoch: 22 , batch: 491 , training loss: 4.247092\n",
      "[INFO] Epoch: 22 , batch: 492 , training loss: 4.208831\n",
      "[INFO] Epoch: 22 , batch: 493 , training loss: 4.363065\n",
      "[INFO] Epoch: 22 , batch: 494 , training loss: 4.268751\n",
      "[INFO] Epoch: 22 , batch: 495 , training loss: 4.433078\n",
      "[INFO] Epoch: 22 , batch: 496 , training loss: 4.307108\n",
      "[INFO] Epoch: 22 , batch: 497 , training loss: 4.346701\n",
      "[INFO] Epoch: 22 , batch: 498 , training loss: 4.339806\n",
      "[INFO] Epoch: 22 , batch: 499 , training loss: 4.393307\n",
      "[INFO] Epoch: 22 , batch: 500 , training loss: 4.545774\n",
      "[INFO] Epoch: 22 , batch: 501 , training loss: 4.884926\n",
      "[INFO] Epoch: 22 , batch: 502 , training loss: 4.949231\n",
      "[INFO] Epoch: 22 , batch: 503 , training loss: 4.628968\n",
      "[INFO] Epoch: 22 , batch: 504 , training loss: 4.757008\n",
      "[INFO] Epoch: 22 , batch: 505 , training loss: 4.720031\n",
      "[INFO] Epoch: 22 , batch: 506 , training loss: 4.672696\n",
      "[INFO] Epoch: 22 , batch: 507 , training loss: 4.753623\n",
      "[INFO] Epoch: 22 , batch: 508 , training loss: 4.649256\n",
      "[INFO] Epoch: 22 , batch: 509 , training loss: 4.436425\n",
      "[INFO] Epoch: 22 , batch: 510 , training loss: 4.535144\n",
      "[INFO] Epoch: 22 , batch: 511 , training loss: 4.452175\n",
      "[INFO] Epoch: 22 , batch: 512 , training loss: 4.530966\n",
      "[INFO] Epoch: 22 , batch: 513 , training loss: 4.794581\n",
      "[INFO] Epoch: 22 , batch: 514 , training loss: 4.433778\n",
      "[INFO] Epoch: 22 , batch: 515 , training loss: 4.696675\n",
      "[INFO] Epoch: 22 , batch: 516 , training loss: 4.476929\n",
      "[INFO] Epoch: 22 , batch: 517 , training loss: 4.447761\n",
      "[INFO] Epoch: 22 , batch: 518 , training loss: 4.409638\n",
      "[INFO] Epoch: 22 , batch: 519 , training loss: 4.252893\n",
      "[INFO] Epoch: 22 , batch: 520 , training loss: 4.498862\n",
      "[INFO] Epoch: 22 , batch: 521 , training loss: 4.478836\n",
      "[INFO] Epoch: 22 , batch: 522 , training loss: 4.558110\n",
      "[INFO] Epoch: 22 , batch: 523 , training loss: 4.461183\n",
      "[INFO] Epoch: 22 , batch: 524 , training loss: 4.736397\n",
      "[INFO] Epoch: 22 , batch: 525 , training loss: 4.607466\n",
      "[INFO] Epoch: 22 , batch: 526 , training loss: 4.409107\n",
      "[INFO] Epoch: 22 , batch: 527 , training loss: 4.462607\n",
      "[INFO] Epoch: 22 , batch: 528 , training loss: 4.477151\n",
      "[INFO] Epoch: 22 , batch: 529 , training loss: 4.446290\n",
      "[INFO] Epoch: 22 , batch: 530 , training loss: 4.299736\n",
      "[INFO] Epoch: 22 , batch: 531 , training loss: 4.434343\n",
      "[INFO] Epoch: 22 , batch: 532 , training loss: 4.347322\n",
      "[INFO] Epoch: 22 , batch: 533 , training loss: 4.488347\n",
      "[INFO] Epoch: 22 , batch: 534 , training loss: 4.469834\n",
      "[INFO] Epoch: 22 , batch: 535 , training loss: 4.475060\n",
      "[INFO] Epoch: 22 , batch: 536 , training loss: 4.346788\n",
      "[INFO] Epoch: 22 , batch: 537 , training loss: 4.296128\n",
      "[INFO] Epoch: 22 , batch: 538 , training loss: 4.395179\n",
      "[INFO] Epoch: 22 , batch: 539 , training loss: 4.498076\n",
      "[INFO] Epoch: 22 , batch: 540 , training loss: 5.049923\n",
      "[INFO] Epoch: 22 , batch: 541 , training loss: 4.875316\n",
      "[INFO] Epoch: 22 , batch: 542 , training loss: 4.733969\n",
      "[INFO] Epoch: 23 , batch: 0 , training loss: 3.760698\n",
      "[INFO] Epoch: 23 , batch: 1 , training loss: 3.633406\n",
      "[INFO] Epoch: 23 , batch: 2 , training loss: 3.768486\n",
      "[INFO] Epoch: 23 , batch: 3 , training loss: 3.604764\n",
      "[INFO] Epoch: 23 , batch: 4 , training loss: 3.940328\n",
      "[INFO] Epoch: 23 , batch: 5 , training loss: 3.604657\n",
      "[INFO] Epoch: 23 , batch: 6 , training loss: 3.984313\n",
      "[INFO] Epoch: 23 , batch: 7 , training loss: 3.852633\n",
      "[INFO] Epoch: 23 , batch: 8 , training loss: 3.560015\n",
      "[INFO] Epoch: 23 , batch: 9 , training loss: 3.790719\n",
      "[INFO] Epoch: 23 , batch: 10 , training loss: 3.703493\n",
      "[INFO] Epoch: 23 , batch: 11 , training loss: 3.678354\n",
      "[INFO] Epoch: 23 , batch: 12 , training loss: 3.583937\n",
      "[INFO] Epoch: 23 , batch: 13 , training loss: 3.654350\n",
      "[INFO] Epoch: 23 , batch: 14 , training loss: 3.525546\n",
      "[INFO] Epoch: 23 , batch: 15 , training loss: 3.738683\n",
      "[INFO] Epoch: 23 , batch: 16 , training loss: 3.607926\n",
      "[INFO] Epoch: 23 , batch: 17 , training loss: 3.712666\n",
      "[INFO] Epoch: 23 , batch: 18 , training loss: 3.622492\n",
      "[INFO] Epoch: 23 , batch: 19 , training loss: 3.409246\n",
      "[INFO] Epoch: 23 , batch: 20 , training loss: 3.403949\n",
      "[INFO] Epoch: 23 , batch: 21 , training loss: 3.537503\n",
      "[INFO] Epoch: 23 , batch: 22 , training loss: 3.432674\n",
      "[INFO] Epoch: 23 , batch: 23 , training loss: 3.653962\n",
      "[INFO] Epoch: 23 , batch: 24 , training loss: 3.491676\n",
      "[INFO] Epoch: 23 , batch: 25 , training loss: 3.597995\n",
      "[INFO] Epoch: 23 , batch: 26 , training loss: 3.470333\n",
      "[INFO] Epoch: 23 , batch: 27 , training loss: 3.457017\n",
      "[INFO] Epoch: 23 , batch: 28 , training loss: 3.664848\n",
      "[INFO] Epoch: 23 , batch: 29 , training loss: 3.476677\n",
      "[INFO] Epoch: 23 , batch: 30 , training loss: 3.478279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 23 , batch: 31 , training loss: 3.610145\n",
      "[INFO] Epoch: 23 , batch: 32 , training loss: 3.559124\n",
      "[INFO] Epoch: 23 , batch: 33 , training loss: 3.589638\n",
      "[INFO] Epoch: 23 , batch: 34 , training loss: 3.612518\n",
      "[INFO] Epoch: 23 , batch: 35 , training loss: 3.535846\n",
      "[INFO] Epoch: 23 , batch: 36 , training loss: 3.647683\n",
      "[INFO] Epoch: 23 , batch: 37 , training loss: 3.474272\n",
      "[INFO] Epoch: 23 , batch: 38 , training loss: 3.575886\n",
      "[INFO] Epoch: 23 , batch: 39 , training loss: 3.390769\n",
      "[INFO] Epoch: 23 , batch: 40 , training loss: 3.579064\n",
      "[INFO] Epoch: 23 , batch: 41 , training loss: 3.569448\n",
      "[INFO] Epoch: 23 , batch: 42 , training loss: 4.013593\n",
      "[INFO] Epoch: 23 , batch: 43 , training loss: 3.688102\n",
      "[INFO] Epoch: 23 , batch: 44 , training loss: 4.087116\n",
      "[INFO] Epoch: 23 , batch: 45 , training loss: 4.051021\n",
      "[INFO] Epoch: 23 , batch: 46 , training loss: 4.020503\n",
      "[INFO] Epoch: 23 , batch: 47 , training loss: 3.663846\n",
      "[INFO] Epoch: 23 , batch: 48 , training loss: 3.652827\n",
      "[INFO] Epoch: 23 , batch: 49 , training loss: 3.910216\n",
      "[INFO] Epoch: 23 , batch: 50 , training loss: 3.656390\n",
      "[INFO] Epoch: 23 , batch: 51 , training loss: 3.884948\n",
      "[INFO] Epoch: 23 , batch: 52 , training loss: 3.687954\n",
      "[INFO] Epoch: 23 , batch: 53 , training loss: 3.810918\n",
      "[INFO] Epoch: 23 , batch: 54 , training loss: 3.817902\n",
      "[INFO] Epoch: 23 , batch: 55 , training loss: 3.928223\n",
      "[INFO] Epoch: 23 , batch: 56 , training loss: 3.725713\n",
      "[INFO] Epoch: 23 , batch: 57 , training loss: 3.674387\n",
      "[INFO] Epoch: 23 , batch: 58 , training loss: 3.736492\n",
      "[INFO] Epoch: 23 , batch: 59 , training loss: 3.823780\n",
      "[INFO] Epoch: 23 , batch: 60 , training loss: 3.760906\n",
      "[INFO] Epoch: 23 , batch: 61 , training loss: 3.803617\n",
      "[INFO] Epoch: 23 , batch: 62 , training loss: 3.694913\n",
      "[INFO] Epoch: 23 , batch: 63 , training loss: 3.851732\n",
      "[INFO] Epoch: 23 , batch: 64 , training loss: 4.113704\n",
      "[INFO] Epoch: 23 , batch: 65 , training loss: 3.763529\n",
      "[INFO] Epoch: 23 , batch: 66 , training loss: 3.626009\n",
      "[INFO] Epoch: 23 , batch: 67 , training loss: 3.645035\n",
      "[INFO] Epoch: 23 , batch: 68 , training loss: 3.881464\n",
      "[INFO] Epoch: 23 , batch: 69 , training loss: 3.767926\n",
      "[INFO] Epoch: 23 , batch: 70 , training loss: 3.949733\n",
      "[INFO] Epoch: 23 , batch: 71 , training loss: 3.862314\n",
      "[INFO] Epoch: 23 , batch: 72 , training loss: 3.908018\n",
      "[INFO] Epoch: 23 , batch: 73 , training loss: 3.827353\n",
      "[INFO] Epoch: 23 , batch: 74 , training loss: 3.947863\n",
      "[INFO] Epoch: 23 , batch: 75 , training loss: 3.819882\n",
      "[INFO] Epoch: 23 , batch: 76 , training loss: 3.930783\n",
      "[INFO] Epoch: 23 , batch: 77 , training loss: 3.838379\n",
      "[INFO] Epoch: 23 , batch: 78 , training loss: 3.941369\n",
      "[INFO] Epoch: 23 , batch: 79 , training loss: 3.772877\n",
      "[INFO] Epoch: 23 , batch: 80 , training loss: 3.991688\n",
      "[INFO] Epoch: 23 , batch: 81 , training loss: 3.932968\n",
      "[INFO] Epoch: 23 , batch: 82 , training loss: 3.903193\n",
      "[INFO] Epoch: 23 , batch: 83 , training loss: 4.006924\n",
      "[INFO] Epoch: 23 , batch: 84 , training loss: 3.932504\n",
      "[INFO] Epoch: 23 , batch: 85 , training loss: 4.019732\n",
      "[INFO] Epoch: 23 , batch: 86 , training loss: 3.985640\n",
      "[INFO] Epoch: 23 , batch: 87 , training loss: 3.923900\n",
      "[INFO] Epoch: 23 , batch: 88 , training loss: 4.044009\n",
      "[INFO] Epoch: 23 , batch: 89 , training loss: 3.879961\n",
      "[INFO] Epoch: 23 , batch: 90 , training loss: 3.974945\n",
      "[INFO] Epoch: 23 , batch: 91 , training loss: 3.862780\n",
      "[INFO] Epoch: 23 , batch: 92 , training loss: 3.912957\n",
      "[INFO] Epoch: 23 , batch: 93 , training loss: 4.005383\n",
      "[INFO] Epoch: 23 , batch: 94 , training loss: 4.129049\n",
      "[INFO] Epoch: 23 , batch: 95 , training loss: 3.931294\n",
      "[INFO] Epoch: 23 , batch: 96 , training loss: 3.888454\n",
      "[INFO] Epoch: 23 , batch: 97 , training loss: 3.838103\n",
      "[INFO] Epoch: 23 , batch: 98 , training loss: 3.799376\n",
      "[INFO] Epoch: 23 , batch: 99 , training loss: 3.875558\n",
      "[INFO] Epoch: 23 , batch: 100 , training loss: 3.767837\n",
      "[INFO] Epoch: 23 , batch: 101 , training loss: 3.803375\n",
      "[INFO] Epoch: 23 , batch: 102 , training loss: 3.966950\n",
      "[INFO] Epoch: 23 , batch: 103 , training loss: 3.744537\n",
      "[INFO] Epoch: 23 , batch: 104 , training loss: 3.688422\n",
      "[INFO] Epoch: 23 , batch: 105 , training loss: 3.943447\n",
      "[INFO] Epoch: 23 , batch: 106 , training loss: 3.974532\n",
      "[INFO] Epoch: 23 , batch: 107 , training loss: 3.820802\n",
      "[INFO] Epoch: 23 , batch: 108 , training loss: 3.779746\n",
      "[INFO] Epoch: 23 , batch: 109 , training loss: 3.685893\n",
      "[INFO] Epoch: 23 , batch: 110 , training loss: 3.894038\n",
      "[INFO] Epoch: 23 , batch: 111 , training loss: 3.952228\n",
      "[INFO] Epoch: 23 , batch: 112 , training loss: 3.892842\n",
      "[INFO] Epoch: 23 , batch: 113 , training loss: 3.856068\n",
      "[INFO] Epoch: 23 , batch: 114 , training loss: 3.895445\n",
      "[INFO] Epoch: 23 , batch: 115 , training loss: 3.852741\n",
      "[INFO] Epoch: 23 , batch: 116 , training loss: 3.778395\n",
      "[INFO] Epoch: 23 , batch: 117 , training loss: 4.000136\n",
      "[INFO] Epoch: 23 , batch: 118 , training loss: 3.962697\n",
      "[INFO] Epoch: 23 , batch: 119 , training loss: 4.147283\n",
      "[INFO] Epoch: 23 , batch: 120 , training loss: 4.089057\n",
      "[INFO] Epoch: 23 , batch: 121 , training loss: 3.937633\n",
      "[INFO] Epoch: 23 , batch: 122 , training loss: 3.844830\n",
      "[INFO] Epoch: 23 , batch: 123 , training loss: 3.871226\n",
      "[INFO] Epoch: 23 , batch: 124 , training loss: 3.977659\n",
      "[INFO] Epoch: 23 , batch: 125 , training loss: 3.748564\n",
      "[INFO] Epoch: 23 , batch: 126 , training loss: 3.804050\n",
      "[INFO] Epoch: 23 , batch: 127 , training loss: 3.792236\n",
      "[INFO] Epoch: 23 , batch: 128 , training loss: 3.952528\n",
      "[INFO] Epoch: 23 , batch: 129 , training loss: 3.896707\n",
      "[INFO] Epoch: 23 , batch: 130 , training loss: 3.874826\n",
      "[INFO] Epoch: 23 , batch: 131 , training loss: 3.883367\n",
      "[INFO] Epoch: 23 , batch: 132 , training loss: 3.898382\n",
      "[INFO] Epoch: 23 , batch: 133 , training loss: 3.876753\n",
      "[INFO] Epoch: 23 , batch: 134 , training loss: 3.600056\n",
      "[INFO] Epoch: 23 , batch: 135 , training loss: 3.709405\n",
      "[INFO] Epoch: 23 , batch: 136 , training loss: 3.996075\n",
      "[INFO] Epoch: 23 , batch: 137 , training loss: 3.909942\n",
      "[INFO] Epoch: 23 , batch: 138 , training loss: 3.944354\n",
      "[INFO] Epoch: 23 , batch: 139 , training loss: 4.531332\n",
      "[INFO] Epoch: 23 , batch: 140 , training loss: 4.330317\n",
      "[INFO] Epoch: 23 , batch: 141 , training loss: 4.088892\n",
      "[INFO] Epoch: 23 , batch: 142 , training loss: 3.806603\n",
      "[INFO] Epoch: 23 , batch: 143 , training loss: 3.941299\n",
      "[INFO] Epoch: 23 , batch: 144 , training loss: 3.789616\n",
      "[INFO] Epoch: 23 , batch: 145 , training loss: 3.849221\n",
      "[INFO] Epoch: 23 , batch: 146 , training loss: 4.080962\n",
      "[INFO] Epoch: 23 , batch: 147 , training loss: 3.713782\n",
      "[INFO] Epoch: 23 , batch: 148 , training loss: 3.716121\n",
      "[INFO] Epoch: 23 , batch: 149 , training loss: 3.770239\n",
      "[INFO] Epoch: 23 , batch: 150 , training loss: 4.041391\n",
      "[INFO] Epoch: 23 , batch: 151 , training loss: 3.873475\n",
      "[INFO] Epoch: 23 , batch: 152 , training loss: 3.897493\n",
      "[INFO] Epoch: 23 , batch: 153 , training loss: 3.889446\n",
      "[INFO] Epoch: 23 , batch: 154 , training loss: 4.015003\n",
      "[INFO] Epoch: 23 , batch: 155 , training loss: 4.182346\n",
      "[INFO] Epoch: 23 , batch: 156 , training loss: 3.947357\n",
      "[INFO] Epoch: 23 , batch: 157 , training loss: 3.921174\n",
      "[INFO] Epoch: 23 , batch: 158 , training loss: 4.033658\n",
      "[INFO] Epoch: 23 , batch: 159 , training loss: 3.996508\n",
      "[INFO] Epoch: 23 , batch: 160 , training loss: 4.237226\n",
      "[INFO] Epoch: 23 , batch: 161 , training loss: 4.286851\n",
      "[INFO] Epoch: 23 , batch: 162 , training loss: 4.278367\n",
      "[INFO] Epoch: 23 , batch: 163 , training loss: 4.469107\n",
      "[INFO] Epoch: 23 , batch: 164 , training loss: 4.390771\n",
      "[INFO] Epoch: 23 , batch: 165 , training loss: 4.307823\n",
      "[INFO] Epoch: 23 , batch: 166 , training loss: 4.187478\n",
      "[INFO] Epoch: 23 , batch: 167 , training loss: 4.323134\n",
      "[INFO] Epoch: 23 , batch: 168 , training loss: 3.976889\n",
      "[INFO] Epoch: 23 , batch: 169 , training loss: 3.950156\n",
      "[INFO] Epoch: 23 , batch: 170 , training loss: 4.104959\n",
      "[INFO] Epoch: 23 , batch: 171 , training loss: 3.554559\n",
      "[INFO] Epoch: 23 , batch: 172 , training loss: 3.774288\n",
      "[INFO] Epoch: 23 , batch: 173 , training loss: 4.096855\n",
      "[INFO] Epoch: 23 , batch: 174 , training loss: 4.526968\n",
      "[INFO] Epoch: 23 , batch: 175 , training loss: 4.834783\n",
      "[INFO] Epoch: 23 , batch: 176 , training loss: 4.494436\n",
      "[INFO] Epoch: 23 , batch: 177 , training loss: 4.153891\n",
      "[INFO] Epoch: 23 , batch: 178 , training loss: 4.126988\n",
      "[INFO] Epoch: 23 , batch: 179 , training loss: 4.166512\n",
      "[INFO] Epoch: 23 , batch: 180 , training loss: 4.123583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 23 , batch: 181 , training loss: 4.392841\n",
      "[INFO] Epoch: 23 , batch: 182 , training loss: 4.344582\n",
      "[INFO] Epoch: 23 , batch: 183 , training loss: 4.298043\n",
      "[INFO] Epoch: 23 , batch: 184 , training loss: 4.210872\n",
      "[INFO] Epoch: 23 , batch: 185 , training loss: 4.187486\n",
      "[INFO] Epoch: 23 , batch: 186 , training loss: 4.319258\n",
      "[INFO] Epoch: 23 , batch: 187 , training loss: 4.416069\n",
      "[INFO] Epoch: 23 , batch: 188 , training loss: 4.401603\n",
      "[INFO] Epoch: 23 , batch: 189 , training loss: 4.282888\n",
      "[INFO] Epoch: 23 , batch: 190 , training loss: 4.310382\n",
      "[INFO] Epoch: 23 , batch: 191 , training loss: 4.444664\n",
      "[INFO] Epoch: 23 , batch: 192 , training loss: 4.268460\n",
      "[INFO] Epoch: 23 , batch: 193 , training loss: 4.388878\n",
      "[INFO] Epoch: 23 , batch: 194 , training loss: 4.306974\n",
      "[INFO] Epoch: 23 , batch: 195 , training loss: 4.269413\n",
      "[INFO] Epoch: 23 , batch: 196 , training loss: 4.112221\n",
      "[INFO] Epoch: 23 , batch: 197 , training loss: 4.191052\n",
      "[INFO] Epoch: 23 , batch: 198 , training loss: 4.093866\n",
      "[INFO] Epoch: 23 , batch: 199 , training loss: 4.232714\n",
      "[INFO] Epoch: 23 , batch: 200 , training loss: 4.144832\n",
      "[INFO] Epoch: 23 , batch: 201 , training loss: 4.052607\n",
      "[INFO] Epoch: 23 , batch: 202 , training loss: 4.057090\n",
      "[INFO] Epoch: 23 , batch: 203 , training loss: 4.173948\n",
      "[INFO] Epoch: 23 , batch: 204 , training loss: 4.251464\n",
      "[INFO] Epoch: 23 , batch: 205 , training loss: 3.871783\n",
      "[INFO] Epoch: 23 , batch: 206 , training loss: 3.766159\n",
      "[INFO] Epoch: 23 , batch: 207 , training loss: 3.788251\n",
      "[INFO] Epoch: 23 , batch: 208 , training loss: 4.107473\n",
      "[INFO] Epoch: 23 , batch: 209 , training loss: 4.054382\n",
      "[INFO] Epoch: 23 , batch: 210 , training loss: 4.091517\n",
      "[INFO] Epoch: 23 , batch: 211 , training loss: 4.071542\n",
      "[INFO] Epoch: 23 , batch: 212 , training loss: 4.175250\n",
      "[INFO] Epoch: 23 , batch: 213 , training loss: 4.153949\n",
      "[INFO] Epoch: 23 , batch: 214 , training loss: 4.217592\n",
      "[INFO] Epoch: 23 , batch: 215 , training loss: 4.447589\n",
      "[INFO] Epoch: 23 , batch: 216 , training loss: 4.146429\n",
      "[INFO] Epoch: 23 , batch: 217 , training loss: 4.068594\n",
      "[INFO] Epoch: 23 , batch: 218 , training loss: 4.077638\n",
      "[INFO] Epoch: 23 , batch: 219 , training loss: 4.167449\n",
      "[INFO] Epoch: 23 , batch: 220 , training loss: 4.008791\n",
      "[INFO] Epoch: 23 , batch: 221 , training loss: 4.002966\n",
      "[INFO] Epoch: 23 , batch: 222 , training loss: 4.143155\n",
      "[INFO] Epoch: 23 , batch: 223 , training loss: 4.250875\n",
      "[INFO] Epoch: 23 , batch: 224 , training loss: 4.294676\n",
      "[INFO] Epoch: 23 , batch: 225 , training loss: 4.170030\n",
      "[INFO] Epoch: 23 , batch: 226 , training loss: 4.314468\n",
      "[INFO] Epoch: 23 , batch: 227 , training loss: 4.268768\n",
      "[INFO] Epoch: 23 , batch: 228 , training loss: 4.279672\n",
      "[INFO] Epoch: 23 , batch: 229 , training loss: 4.178314\n",
      "[INFO] Epoch: 23 , batch: 230 , training loss: 4.041770\n",
      "[INFO] Epoch: 23 , batch: 231 , training loss: 3.885319\n",
      "[INFO] Epoch: 23 , batch: 232 , training loss: 4.026580\n",
      "[INFO] Epoch: 23 , batch: 233 , training loss: 4.051781\n",
      "[INFO] Epoch: 23 , batch: 234 , training loss: 3.748201\n",
      "[INFO] Epoch: 23 , batch: 235 , training loss: 3.878624\n",
      "[INFO] Epoch: 23 , batch: 236 , training loss: 3.965798\n",
      "[INFO] Epoch: 23 , batch: 237 , training loss: 4.189848\n",
      "[INFO] Epoch: 23 , batch: 238 , training loss: 3.953743\n",
      "[INFO] Epoch: 23 , batch: 239 , training loss: 3.989933\n",
      "[INFO] Epoch: 23 , batch: 240 , training loss: 4.047177\n",
      "[INFO] Epoch: 23 , batch: 241 , training loss: 3.862263\n",
      "[INFO] Epoch: 23 , batch: 242 , training loss: 3.870491\n",
      "[INFO] Epoch: 23 , batch: 243 , training loss: 4.145852\n",
      "[INFO] Epoch: 23 , batch: 244 , training loss: 4.113311\n",
      "[INFO] Epoch: 23 , batch: 245 , training loss: 4.063502\n",
      "[INFO] Epoch: 23 , batch: 246 , training loss: 3.772580\n",
      "[INFO] Epoch: 23 , batch: 247 , training loss: 3.927223\n",
      "[INFO] Epoch: 23 , batch: 248 , training loss: 4.004807\n",
      "[INFO] Epoch: 23 , batch: 249 , training loss: 3.992966\n",
      "[INFO] Epoch: 23 , batch: 250 , training loss: 3.786309\n",
      "[INFO] Epoch: 23 , batch: 251 , training loss: 4.247066\n",
      "[INFO] Epoch: 23 , batch: 252 , training loss: 3.950320\n",
      "[INFO] Epoch: 23 , batch: 253 , training loss: 3.878393\n",
      "[INFO] Epoch: 23 , batch: 254 , training loss: 4.145432\n",
      "[INFO] Epoch: 23 , batch: 255 , training loss: 4.098162\n",
      "[INFO] Epoch: 23 , batch: 256 , training loss: 4.111516\n",
      "[INFO] Epoch: 23 , batch: 257 , training loss: 4.255339\n",
      "[INFO] Epoch: 23 , batch: 258 , training loss: 4.281931\n",
      "[INFO] Epoch: 23 , batch: 259 , training loss: 4.312622\n",
      "[INFO] Epoch: 23 , batch: 260 , training loss: 4.084181\n",
      "[INFO] Epoch: 23 , batch: 261 , training loss: 4.252058\n",
      "[INFO] Epoch: 23 , batch: 262 , training loss: 4.417248\n",
      "[INFO] Epoch: 23 , batch: 263 , training loss: 4.556468\n",
      "[INFO] Epoch: 23 , batch: 264 , training loss: 3.944391\n",
      "[INFO] Epoch: 23 , batch: 265 , training loss: 4.034378\n",
      "[INFO] Epoch: 23 , batch: 266 , training loss: 4.474204\n",
      "[INFO] Epoch: 23 , batch: 267 , training loss: 4.187746\n",
      "[INFO] Epoch: 23 , batch: 268 , training loss: 4.120320\n",
      "[INFO] Epoch: 23 , batch: 269 , training loss: 4.108673\n",
      "[INFO] Epoch: 23 , batch: 270 , training loss: 4.122694\n",
      "[INFO] Epoch: 23 , batch: 271 , training loss: 4.154296\n",
      "[INFO] Epoch: 23 , batch: 272 , training loss: 4.150581\n",
      "[INFO] Epoch: 23 , batch: 273 , training loss: 4.131124\n",
      "[INFO] Epoch: 23 , batch: 274 , training loss: 4.252359\n",
      "[INFO] Epoch: 23 , batch: 275 , training loss: 4.107891\n",
      "[INFO] Epoch: 23 , batch: 276 , training loss: 4.174203\n",
      "[INFO] Epoch: 23 , batch: 277 , training loss: 4.340616\n",
      "[INFO] Epoch: 23 , batch: 278 , training loss: 3.995983\n",
      "[INFO] Epoch: 23 , batch: 279 , training loss: 4.014315\n",
      "[INFO] Epoch: 23 , batch: 280 , training loss: 3.958783\n",
      "[INFO] Epoch: 23 , batch: 281 , training loss: 4.109117\n",
      "[INFO] Epoch: 23 , batch: 282 , training loss: 4.018894\n",
      "[INFO] Epoch: 23 , batch: 283 , training loss: 4.031551\n",
      "[INFO] Epoch: 23 , batch: 284 , training loss: 4.067917\n",
      "[INFO] Epoch: 23 , batch: 285 , training loss: 4.023637\n",
      "[INFO] Epoch: 23 , batch: 286 , training loss: 4.001966\n",
      "[INFO] Epoch: 23 , batch: 287 , training loss: 3.951580\n",
      "[INFO] Epoch: 23 , batch: 288 , training loss: 3.914779\n",
      "[INFO] Epoch: 23 , batch: 289 , training loss: 3.982598\n",
      "[INFO] Epoch: 23 , batch: 290 , training loss: 3.758980\n",
      "[INFO] Epoch: 23 , batch: 291 , training loss: 3.743717\n",
      "[INFO] Epoch: 23 , batch: 292 , training loss: 3.871159\n",
      "[INFO] Epoch: 23 , batch: 293 , training loss: 3.792336\n",
      "[INFO] Epoch: 23 , batch: 294 , training loss: 4.465850\n",
      "[INFO] Epoch: 23 , batch: 295 , training loss: 4.233182\n",
      "[INFO] Epoch: 23 , batch: 296 , training loss: 4.170129\n",
      "[INFO] Epoch: 23 , batch: 297 , training loss: 4.116678\n",
      "[INFO] Epoch: 23 , batch: 298 , training loss: 3.955020\n",
      "[INFO] Epoch: 23 , batch: 299 , training loss: 3.998091\n",
      "[INFO] Epoch: 23 , batch: 300 , training loss: 3.980526\n",
      "[INFO] Epoch: 23 , batch: 301 , training loss: 3.903052\n",
      "[INFO] Epoch: 23 , batch: 302 , training loss: 4.086356\n",
      "[INFO] Epoch: 23 , batch: 303 , training loss: 4.093715\n",
      "[INFO] Epoch: 23 , batch: 304 , training loss: 4.232952\n",
      "[INFO] Epoch: 23 , batch: 305 , training loss: 4.031584\n",
      "[INFO] Epoch: 23 , batch: 306 , training loss: 4.179577\n",
      "[INFO] Epoch: 23 , batch: 307 , training loss: 4.181884\n",
      "[INFO] Epoch: 23 , batch: 308 , training loss: 3.995308\n",
      "[INFO] Epoch: 23 , batch: 309 , training loss: 4.009312\n",
      "[INFO] Epoch: 23 , batch: 310 , training loss: 3.932746\n",
      "[INFO] Epoch: 23 , batch: 311 , training loss: 3.906451\n",
      "[INFO] Epoch: 23 , batch: 312 , training loss: 3.833243\n",
      "[INFO] Epoch: 23 , batch: 313 , training loss: 3.904400\n",
      "[INFO] Epoch: 23 , batch: 314 , training loss: 4.010239\n",
      "[INFO] Epoch: 23 , batch: 315 , training loss: 4.074775\n",
      "[INFO] Epoch: 23 , batch: 316 , training loss: 4.325044\n",
      "[INFO] Epoch: 23 , batch: 317 , training loss: 4.681058\n",
      "[INFO] Epoch: 23 , batch: 318 , training loss: 4.832473\n",
      "[INFO] Epoch: 23 , batch: 319 , training loss: 4.509554\n",
      "[INFO] Epoch: 23 , batch: 320 , training loss: 4.041541\n",
      "[INFO] Epoch: 23 , batch: 321 , training loss: 3.859673\n",
      "[INFO] Epoch: 23 , batch: 322 , training loss: 3.965773\n",
      "[INFO] Epoch: 23 , batch: 323 , training loss: 3.992413\n",
      "[INFO] Epoch: 23 , batch: 324 , training loss: 3.972731\n",
      "[INFO] Epoch: 23 , batch: 325 , training loss: 4.084421\n",
      "[INFO] Epoch: 23 , batch: 326 , training loss: 4.154704\n",
      "[INFO] Epoch: 23 , batch: 327 , training loss: 4.088061\n",
      "[INFO] Epoch: 23 , batch: 328 , training loss: 4.097737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 23 , batch: 329 , training loss: 3.971138\n",
      "[INFO] Epoch: 23 , batch: 330 , training loss: 3.990267\n",
      "[INFO] Epoch: 23 , batch: 331 , training loss: 4.140429\n",
      "[INFO] Epoch: 23 , batch: 332 , training loss: 3.989995\n",
      "[INFO] Epoch: 23 , batch: 333 , training loss: 3.963957\n",
      "[INFO] Epoch: 23 , batch: 334 , training loss: 3.974240\n",
      "[INFO] Epoch: 23 , batch: 335 , training loss: 4.103442\n",
      "[INFO] Epoch: 23 , batch: 336 , training loss: 4.113194\n",
      "[INFO] Epoch: 23 , batch: 337 , training loss: 4.158078\n",
      "[INFO] Epoch: 23 , batch: 338 , training loss: 4.375639\n",
      "[INFO] Epoch: 23 , batch: 339 , training loss: 4.175556\n",
      "[INFO] Epoch: 23 , batch: 340 , training loss: 4.357299\n",
      "[INFO] Epoch: 23 , batch: 341 , training loss: 4.104541\n",
      "[INFO] Epoch: 23 , batch: 342 , training loss: 3.902370\n",
      "[INFO] Epoch: 23 , batch: 343 , training loss: 3.970733\n",
      "[INFO] Epoch: 23 , batch: 344 , training loss: 3.829808\n",
      "[INFO] Epoch: 23 , batch: 345 , training loss: 3.984261\n",
      "[INFO] Epoch: 23 , batch: 346 , training loss: 4.012537\n",
      "[INFO] Epoch: 23 , batch: 347 , training loss: 3.925309\n",
      "[INFO] Epoch: 23 , batch: 348 , training loss: 4.045046\n",
      "[INFO] Epoch: 23 , batch: 349 , training loss: 4.125082\n",
      "[INFO] Epoch: 23 , batch: 350 , training loss: 3.981615\n",
      "[INFO] Epoch: 23 , batch: 351 , training loss: 4.064659\n",
      "[INFO] Epoch: 23 , batch: 352 , training loss: 4.071244\n",
      "[INFO] Epoch: 23 , batch: 353 , training loss: 4.056150\n",
      "[INFO] Epoch: 23 , batch: 354 , training loss: 4.134978\n",
      "[INFO] Epoch: 23 , batch: 355 , training loss: 4.128749\n",
      "[INFO] Epoch: 23 , batch: 356 , training loss: 4.001264\n",
      "[INFO] Epoch: 23 , batch: 357 , training loss: 4.088031\n",
      "[INFO] Epoch: 23 , batch: 358 , training loss: 3.992521\n",
      "[INFO] Epoch: 23 , batch: 359 , training loss: 4.001709\n",
      "[INFO] Epoch: 23 , batch: 360 , training loss: 4.093605\n",
      "[INFO] Epoch: 23 , batch: 361 , training loss: 4.061984\n",
      "[INFO] Epoch: 23 , batch: 362 , training loss: 4.166728\n",
      "[INFO] Epoch: 23 , batch: 363 , training loss: 4.045011\n",
      "[INFO] Epoch: 23 , batch: 364 , training loss: 4.125778\n",
      "[INFO] Epoch: 23 , batch: 365 , training loss: 4.002868\n",
      "[INFO] Epoch: 23 , batch: 366 , training loss: 4.115376\n",
      "[INFO] Epoch: 23 , batch: 367 , training loss: 4.159422\n",
      "[INFO] Epoch: 23 , batch: 368 , training loss: 4.578758\n",
      "[INFO] Epoch: 23 , batch: 369 , training loss: 4.268528\n",
      "[INFO] Epoch: 23 , batch: 370 , training loss: 4.021799\n",
      "[INFO] Epoch: 23 , batch: 371 , training loss: 4.433439\n",
      "[INFO] Epoch: 23 , batch: 372 , training loss: 4.719555\n",
      "[INFO] Epoch: 23 , batch: 373 , training loss: 4.782151\n",
      "[INFO] Epoch: 23 , batch: 374 , training loss: 4.855793\n",
      "[INFO] Epoch: 23 , batch: 375 , training loss: 4.846110\n",
      "[INFO] Epoch: 23 , batch: 376 , training loss: 4.736516\n",
      "[INFO] Epoch: 23 , batch: 377 , training loss: 4.494646\n",
      "[INFO] Epoch: 23 , batch: 378 , training loss: 4.577628\n",
      "[INFO] Epoch: 23 , batch: 379 , training loss: 4.556559\n",
      "[INFO] Epoch: 23 , batch: 380 , training loss: 4.692725\n",
      "[INFO] Epoch: 23 , batch: 381 , training loss: 4.406868\n",
      "[INFO] Epoch: 23 , batch: 382 , training loss: 4.715254\n",
      "[INFO] Epoch: 23 , batch: 383 , training loss: 4.714691\n",
      "[INFO] Epoch: 23 , batch: 384 , training loss: 4.681920\n",
      "[INFO] Epoch: 23 , batch: 385 , training loss: 4.377206\n",
      "[INFO] Epoch: 23 , batch: 386 , training loss: 4.632170\n",
      "[INFO] Epoch: 23 , batch: 387 , training loss: 4.596724\n",
      "[INFO] Epoch: 23 , batch: 388 , training loss: 4.402377\n",
      "[INFO] Epoch: 23 , batch: 389 , training loss: 4.231882\n",
      "[INFO] Epoch: 23 , batch: 390 , training loss: 4.248019\n",
      "[INFO] Epoch: 23 , batch: 391 , training loss: 4.254941\n",
      "[INFO] Epoch: 23 , batch: 392 , training loss: 4.625551\n",
      "[INFO] Epoch: 23 , batch: 393 , training loss: 4.532028\n",
      "[INFO] Epoch: 23 , batch: 394 , training loss: 4.620182\n",
      "[INFO] Epoch: 23 , batch: 395 , training loss: 4.429489\n",
      "[INFO] Epoch: 23 , batch: 396 , training loss: 4.234696\n",
      "[INFO] Epoch: 23 , batch: 397 , training loss: 4.376003\n",
      "[INFO] Epoch: 23 , batch: 398 , training loss: 4.263490\n",
      "[INFO] Epoch: 23 , batch: 399 , training loss: 4.336681\n",
      "[INFO] Epoch: 23 , batch: 400 , training loss: 4.299112\n",
      "[INFO] Epoch: 23 , batch: 401 , training loss: 4.734700\n",
      "[INFO] Epoch: 23 , batch: 402 , training loss: 4.446302\n",
      "[INFO] Epoch: 23 , batch: 403 , training loss: 4.282534\n",
      "[INFO] Epoch: 23 , batch: 404 , training loss: 4.440306\n",
      "[INFO] Epoch: 23 , batch: 405 , training loss: 4.510577\n",
      "[INFO] Epoch: 23 , batch: 406 , training loss: 4.415522\n",
      "[INFO] Epoch: 23 , batch: 407 , training loss: 4.451154\n",
      "[INFO] Epoch: 23 , batch: 408 , training loss: 4.381522\n",
      "[INFO] Epoch: 23 , batch: 409 , training loss: 4.425499\n",
      "[INFO] Epoch: 23 , batch: 410 , training loss: 4.481215\n",
      "[INFO] Epoch: 23 , batch: 411 , training loss: 4.658909\n",
      "[INFO] Epoch: 23 , batch: 412 , training loss: 4.477941\n",
      "[INFO] Epoch: 23 , batch: 413 , training loss: 4.349267\n",
      "[INFO] Epoch: 23 , batch: 414 , training loss: 4.389853\n",
      "[INFO] Epoch: 23 , batch: 415 , training loss: 4.425511\n",
      "[INFO] Epoch: 23 , batch: 416 , training loss: 4.505888\n",
      "[INFO] Epoch: 23 , batch: 417 , training loss: 4.404039\n",
      "[INFO] Epoch: 23 , batch: 418 , training loss: 4.449443\n",
      "[INFO] Epoch: 23 , batch: 419 , training loss: 4.420728\n",
      "[INFO] Epoch: 23 , batch: 420 , training loss: 4.391289\n",
      "[INFO] Epoch: 23 , batch: 421 , training loss: 4.388486\n",
      "[INFO] Epoch: 23 , batch: 422 , training loss: 4.228759\n",
      "[INFO] Epoch: 23 , batch: 423 , training loss: 4.439382\n",
      "[INFO] Epoch: 23 , batch: 424 , training loss: 4.616242\n",
      "[INFO] Epoch: 23 , batch: 425 , training loss: 4.479550\n",
      "[INFO] Epoch: 23 , batch: 426 , training loss: 4.206922\n",
      "[INFO] Epoch: 23 , batch: 427 , training loss: 4.469855\n",
      "[INFO] Epoch: 23 , batch: 428 , training loss: 4.319770\n",
      "[INFO] Epoch: 23 , batch: 429 , training loss: 4.233021\n",
      "[INFO] Epoch: 23 , batch: 430 , training loss: 4.461390\n",
      "[INFO] Epoch: 23 , batch: 431 , training loss: 4.061153\n",
      "[INFO] Epoch: 23 , batch: 432 , training loss: 4.129519\n",
      "[INFO] Epoch: 23 , batch: 433 , training loss: 4.144628\n",
      "[INFO] Epoch: 23 , batch: 434 , training loss: 4.042670\n",
      "[INFO] Epoch: 23 , batch: 435 , training loss: 4.391693\n",
      "[INFO] Epoch: 23 , batch: 436 , training loss: 4.440901\n",
      "[INFO] Epoch: 23 , batch: 437 , training loss: 4.222025\n",
      "[INFO] Epoch: 23 , batch: 438 , training loss: 4.079685\n",
      "[INFO] Epoch: 23 , batch: 439 , training loss: 4.316168\n",
      "[INFO] Epoch: 23 , batch: 440 , training loss: 4.446640\n",
      "[INFO] Epoch: 23 , batch: 441 , training loss: 4.512168\n",
      "[INFO] Epoch: 23 , batch: 442 , training loss: 4.282131\n",
      "[INFO] Epoch: 23 , batch: 443 , training loss: 4.503735\n",
      "[INFO] Epoch: 23 , batch: 444 , training loss: 4.073570\n",
      "[INFO] Epoch: 23 , batch: 445 , training loss: 3.994040\n",
      "[INFO] Epoch: 23 , batch: 446 , training loss: 3.922899\n",
      "[INFO] Epoch: 23 , batch: 447 , training loss: 4.117664\n",
      "[INFO] Epoch: 23 , batch: 448 , training loss: 4.229573\n",
      "[INFO] Epoch: 23 , batch: 449 , training loss: 4.609415\n",
      "[INFO] Epoch: 23 , batch: 450 , training loss: 4.682988\n",
      "[INFO] Epoch: 23 , batch: 451 , training loss: 4.594591\n",
      "[INFO] Epoch: 23 , batch: 452 , training loss: 4.405152\n",
      "[INFO] Epoch: 23 , batch: 453 , training loss: 4.165689\n",
      "[INFO] Epoch: 23 , batch: 454 , training loss: 4.297377\n",
      "[INFO] Epoch: 23 , batch: 455 , training loss: 4.370824\n",
      "[INFO] Epoch: 23 , batch: 456 , training loss: 4.354701\n",
      "[INFO] Epoch: 23 , batch: 457 , training loss: 4.440214\n",
      "[INFO] Epoch: 23 , batch: 458 , training loss: 4.178605\n",
      "[INFO] Epoch: 23 , batch: 459 , training loss: 4.156909\n",
      "[INFO] Epoch: 23 , batch: 460 , training loss: 4.274564\n",
      "[INFO] Epoch: 23 , batch: 461 , training loss: 4.239791\n",
      "[INFO] Epoch: 23 , batch: 462 , training loss: 4.290308\n",
      "[INFO] Epoch: 23 , batch: 463 , training loss: 4.211920\n",
      "[INFO] Epoch: 23 , batch: 464 , training loss: 4.382353\n",
      "[INFO] Epoch: 23 , batch: 465 , training loss: 4.319881\n",
      "[INFO] Epoch: 23 , batch: 466 , training loss: 4.409560\n",
      "[INFO] Epoch: 23 , batch: 467 , training loss: 4.395013\n",
      "[INFO] Epoch: 23 , batch: 468 , training loss: 4.348220\n",
      "[INFO] Epoch: 23 , batch: 469 , training loss: 4.389808\n",
      "[INFO] Epoch: 23 , batch: 470 , training loss: 4.196747\n",
      "[INFO] Epoch: 23 , batch: 471 , training loss: 4.297382\n",
      "[INFO] Epoch: 23 , batch: 472 , training loss: 4.361009\n",
      "[INFO] Epoch: 23 , batch: 473 , training loss: 4.266412\n",
      "[INFO] Epoch: 23 , batch: 474 , training loss: 4.067874\n",
      "[INFO] Epoch: 23 , batch: 475 , training loss: 3.950005\n",
      "[INFO] Epoch: 23 , batch: 476 , training loss: 4.341944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 23 , batch: 477 , training loss: 4.453319\n",
      "[INFO] Epoch: 23 , batch: 478 , training loss: 4.461040\n",
      "[INFO] Epoch: 23 , batch: 479 , training loss: 4.428461\n",
      "[INFO] Epoch: 23 , batch: 480 , training loss: 4.549666\n",
      "[INFO] Epoch: 23 , batch: 481 , training loss: 4.436676\n",
      "[INFO] Epoch: 23 , batch: 482 , training loss: 4.531229\n",
      "[INFO] Epoch: 23 , batch: 483 , training loss: 4.376649\n",
      "[INFO] Epoch: 23 , batch: 484 , training loss: 4.186439\n",
      "[INFO] Epoch: 23 , batch: 485 , training loss: 4.277959\n",
      "[INFO] Epoch: 23 , batch: 486 , training loss: 4.182585\n",
      "[INFO] Epoch: 23 , batch: 487 , training loss: 4.166673\n",
      "[INFO] Epoch: 23 , batch: 488 , training loss: 4.342189\n",
      "[INFO] Epoch: 23 , batch: 489 , training loss: 4.239495\n",
      "[INFO] Epoch: 23 , batch: 490 , training loss: 4.307450\n",
      "[INFO] Epoch: 23 , batch: 491 , training loss: 4.231020\n",
      "[INFO] Epoch: 23 , batch: 492 , training loss: 4.202366\n",
      "[INFO] Epoch: 23 , batch: 493 , training loss: 4.369240\n",
      "[INFO] Epoch: 23 , batch: 494 , training loss: 4.268000\n",
      "[INFO] Epoch: 23 , batch: 495 , training loss: 4.432893\n",
      "[INFO] Epoch: 23 , batch: 496 , training loss: 4.303064\n",
      "[INFO] Epoch: 23 , batch: 497 , training loss: 4.347434\n",
      "[INFO] Epoch: 23 , batch: 498 , training loss: 4.340489\n",
      "[INFO] Epoch: 23 , batch: 499 , training loss: 4.411129\n",
      "[INFO] Epoch: 23 , batch: 500 , training loss: 4.521625\n",
      "[INFO] Epoch: 23 , batch: 501 , training loss: 4.873777\n",
      "[INFO] Epoch: 23 , batch: 502 , training loss: 4.925905\n",
      "[INFO] Epoch: 23 , batch: 503 , training loss: 4.610209\n",
      "[INFO] Epoch: 23 , batch: 504 , training loss: 4.747776\n",
      "[INFO] Epoch: 23 , batch: 505 , training loss: 4.691628\n",
      "[INFO] Epoch: 23 , batch: 506 , training loss: 4.688587\n",
      "[INFO] Epoch: 23 , batch: 507 , training loss: 4.725819\n",
      "[INFO] Epoch: 23 , batch: 508 , training loss: 4.649033\n",
      "[INFO] Epoch: 23 , batch: 509 , training loss: 4.440243\n",
      "[INFO] Epoch: 23 , batch: 510 , training loss: 4.516585\n",
      "[INFO] Epoch: 23 , batch: 511 , training loss: 4.434544\n",
      "[INFO] Epoch: 23 , batch: 512 , training loss: 4.520988\n",
      "[INFO] Epoch: 23 , batch: 513 , training loss: 4.773592\n",
      "[INFO] Epoch: 23 , batch: 514 , training loss: 4.423417\n",
      "[INFO] Epoch: 23 , batch: 515 , training loss: 4.690504\n",
      "[INFO] Epoch: 23 , batch: 516 , training loss: 4.461042\n",
      "[INFO] Epoch: 23 , batch: 517 , training loss: 4.440699\n",
      "[INFO] Epoch: 23 , batch: 518 , training loss: 4.408449\n",
      "[INFO] Epoch: 23 , batch: 519 , training loss: 4.250449\n",
      "[INFO] Epoch: 23 , batch: 520 , training loss: 4.477984\n",
      "[INFO] Epoch: 23 , batch: 521 , training loss: 4.468868\n",
      "[INFO] Epoch: 23 , batch: 522 , training loss: 4.546029\n",
      "[INFO] Epoch: 23 , batch: 523 , training loss: 4.443962\n",
      "[INFO] Epoch: 23 , batch: 524 , training loss: 4.727678\n",
      "[INFO] Epoch: 23 , batch: 525 , training loss: 4.606958\n",
      "[INFO] Epoch: 23 , batch: 526 , training loss: 4.406334\n",
      "[INFO] Epoch: 23 , batch: 527 , training loss: 4.456642\n",
      "[INFO] Epoch: 23 , batch: 528 , training loss: 4.452961\n",
      "[INFO] Epoch: 23 , batch: 529 , training loss: 4.435160\n",
      "[INFO] Epoch: 23 , batch: 530 , training loss: 4.283934\n",
      "[INFO] Epoch: 23 , batch: 531 , training loss: 4.429002\n",
      "[INFO] Epoch: 23 , batch: 532 , training loss: 4.336672\n",
      "[INFO] Epoch: 23 , batch: 533 , training loss: 4.490343\n",
      "[INFO] Epoch: 23 , batch: 534 , training loss: 4.460268\n",
      "[INFO] Epoch: 23 , batch: 535 , training loss: 4.487971\n",
      "[INFO] Epoch: 23 , batch: 536 , training loss: 4.342616\n",
      "[INFO] Epoch: 23 , batch: 537 , training loss: 4.294683\n",
      "[INFO] Epoch: 23 , batch: 538 , training loss: 4.396401\n",
      "[INFO] Epoch: 23 , batch: 539 , training loss: 4.501588\n",
      "[INFO] Epoch: 23 , batch: 540 , training loss: 5.044662\n",
      "[INFO] Epoch: 23 , batch: 541 , training loss: 4.878608\n",
      "[INFO] Epoch: 23 , batch: 542 , training loss: 4.721840\n",
      "[INFO] Epoch: 24 , batch: 0 , training loss: 3.770389\n",
      "[INFO] Epoch: 24 , batch: 1 , training loss: 3.611594\n",
      "[INFO] Epoch: 24 , batch: 2 , training loss: 3.748890\n",
      "[INFO] Epoch: 24 , batch: 3 , training loss: 3.621891\n",
      "[INFO] Epoch: 24 , batch: 4 , training loss: 3.924626\n",
      "[INFO] Epoch: 24 , batch: 5 , training loss: 3.629877\n",
      "[INFO] Epoch: 24 , batch: 6 , training loss: 3.985128\n",
      "[INFO] Epoch: 24 , batch: 7 , training loss: 3.868814\n",
      "[INFO] Epoch: 24 , batch: 8 , training loss: 3.554945\n",
      "[INFO] Epoch: 24 , batch: 9 , training loss: 3.806920\n",
      "[INFO] Epoch: 24 , batch: 10 , training loss: 3.701648\n",
      "[INFO] Epoch: 24 , batch: 11 , training loss: 3.664147\n",
      "[INFO] Epoch: 24 , batch: 12 , training loss: 3.556838\n",
      "[INFO] Epoch: 24 , batch: 13 , training loss: 3.642265\n",
      "[INFO] Epoch: 24 , batch: 14 , training loss: 3.528351\n",
      "[INFO] Epoch: 24 , batch: 15 , training loss: 3.754270\n",
      "[INFO] Epoch: 24 , batch: 16 , training loss: 3.608349\n",
      "[INFO] Epoch: 24 , batch: 17 , training loss: 3.728530\n",
      "[INFO] Epoch: 24 , batch: 18 , training loss: 3.640363\n",
      "[INFO] Epoch: 24 , batch: 19 , training loss: 3.418780\n",
      "[INFO] Epoch: 24 , batch: 20 , training loss: 3.400795\n",
      "[INFO] Epoch: 24 , batch: 21 , training loss: 3.545795\n",
      "[INFO] Epoch: 24 , batch: 22 , training loss: 3.431645\n",
      "[INFO] Epoch: 24 , batch: 23 , training loss: 3.618125\n",
      "[INFO] Epoch: 24 , batch: 24 , training loss: 3.480128\n",
      "[INFO] Epoch: 24 , batch: 25 , training loss: 3.624437\n",
      "[INFO] Epoch: 24 , batch: 26 , training loss: 3.480898\n",
      "[INFO] Epoch: 24 , batch: 27 , training loss: 3.439407\n",
      "[INFO] Epoch: 24 , batch: 28 , training loss: 3.643436\n",
      "[INFO] Epoch: 24 , batch: 29 , training loss: 3.456499\n",
      "[INFO] Epoch: 24 , batch: 30 , training loss: 3.492875\n",
      "[INFO] Epoch: 24 , batch: 31 , training loss: 3.587159\n",
      "[INFO] Epoch: 24 , batch: 32 , training loss: 3.536092\n",
      "[INFO] Epoch: 24 , batch: 33 , training loss: 3.609459\n",
      "[INFO] Epoch: 24 , batch: 34 , training loss: 3.598775\n",
      "[INFO] Epoch: 24 , batch: 35 , training loss: 3.550786\n",
      "[INFO] Epoch: 24 , batch: 36 , training loss: 3.639623\n",
      "[INFO] Epoch: 24 , batch: 37 , training loss: 3.460578\n",
      "[INFO] Epoch: 24 , batch: 38 , training loss: 3.572128\n",
      "[INFO] Epoch: 24 , batch: 39 , training loss: 3.395169\n",
      "[INFO] Epoch: 24 , batch: 40 , training loss: 3.581068\n",
      "[INFO] Epoch: 24 , batch: 41 , training loss: 3.561649\n",
      "[INFO] Epoch: 24 , batch: 42 , training loss: 3.986268\n",
      "[INFO] Epoch: 24 , batch: 43 , training loss: 3.723068\n",
      "[INFO] Epoch: 24 , batch: 44 , training loss: 4.053889\n",
      "[INFO] Epoch: 24 , batch: 45 , training loss: 4.024965\n",
      "[INFO] Epoch: 24 , batch: 46 , training loss: 3.998861\n",
      "[INFO] Epoch: 24 , batch: 47 , training loss: 3.659967\n",
      "[INFO] Epoch: 24 , batch: 48 , training loss: 3.623009\n",
      "[INFO] Epoch: 24 , batch: 49 , training loss: 3.872209\n",
      "[INFO] Epoch: 24 , batch: 50 , training loss: 3.644489\n",
      "[INFO] Epoch: 24 , batch: 51 , training loss: 3.858152\n",
      "[INFO] Epoch: 24 , batch: 52 , training loss: 3.667567\n",
      "[INFO] Epoch: 24 , batch: 53 , training loss: 3.800758\n",
      "[INFO] Epoch: 24 , batch: 54 , training loss: 3.815839\n",
      "[INFO] Epoch: 24 , batch: 55 , training loss: 3.912577\n",
      "[INFO] Epoch: 24 , batch: 56 , training loss: 3.751500\n",
      "[INFO] Epoch: 24 , batch: 57 , training loss: 3.668400\n",
      "[INFO] Epoch: 24 , batch: 58 , training loss: 3.721920\n",
      "[INFO] Epoch: 24 , batch: 59 , training loss: 3.791144\n",
      "[INFO] Epoch: 24 , batch: 60 , training loss: 3.743866\n",
      "[INFO] Epoch: 24 , batch: 61 , training loss: 3.793807\n",
      "[INFO] Epoch: 24 , batch: 62 , training loss: 3.690144\n",
      "[INFO] Epoch: 24 , batch: 63 , training loss: 3.858866\n",
      "[INFO] Epoch: 24 , batch: 64 , training loss: 4.091415\n",
      "[INFO] Epoch: 24 , batch: 65 , training loss: 3.761821\n",
      "[INFO] Epoch: 24 , batch: 66 , training loss: 3.639814\n",
      "[INFO] Epoch: 24 , batch: 67 , training loss: 3.648105\n",
      "[INFO] Epoch: 24 , batch: 68 , training loss: 3.874327\n",
      "[INFO] Epoch: 24 , batch: 69 , training loss: 3.763586\n",
      "[INFO] Epoch: 24 , batch: 70 , training loss: 3.963244\n",
      "[INFO] Epoch: 24 , batch: 71 , training loss: 3.828270\n",
      "[INFO] Epoch: 24 , batch: 72 , training loss: 3.891322\n",
      "[INFO] Epoch: 24 , batch: 73 , training loss: 3.825277\n",
      "[INFO] Epoch: 24 , batch: 74 , training loss: 3.949319\n",
      "[INFO] Epoch: 24 , batch: 75 , training loss: 3.805020\n",
      "[INFO] Epoch: 24 , batch: 76 , training loss: 3.902489\n",
      "[INFO] Epoch: 24 , batch: 77 , training loss: 3.838268\n",
      "[INFO] Epoch: 24 , batch: 78 , training loss: 3.960445\n",
      "[INFO] Epoch: 24 , batch: 79 , training loss: 3.786154\n",
      "[INFO] Epoch: 24 , batch: 80 , training loss: 3.992830\n",
      "[INFO] Epoch: 24 , batch: 81 , training loss: 3.901058\n",
      "[INFO] Epoch: 24 , batch: 82 , training loss: 3.895440\n",
      "[INFO] Epoch: 24 , batch: 83 , training loss: 3.991269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 24 , batch: 84 , training loss: 3.944759\n",
      "[INFO] Epoch: 24 , batch: 85 , training loss: 4.024574\n",
      "[INFO] Epoch: 24 , batch: 86 , training loss: 3.979456\n",
      "[INFO] Epoch: 24 , batch: 87 , training loss: 3.909785\n",
      "[INFO] Epoch: 24 , batch: 88 , training loss: 4.063830\n",
      "[INFO] Epoch: 24 , batch: 89 , training loss: 3.850729\n",
      "[INFO] Epoch: 24 , batch: 90 , training loss: 3.960127\n",
      "[INFO] Epoch: 24 , batch: 91 , training loss: 3.859086\n",
      "[INFO] Epoch: 24 , batch: 92 , training loss: 3.897532\n",
      "[INFO] Epoch: 24 , batch: 93 , training loss: 3.989069\n",
      "[INFO] Epoch: 24 , batch: 94 , training loss: 4.120275\n",
      "[INFO] Epoch: 24 , batch: 95 , training loss: 3.936429\n",
      "[INFO] Epoch: 24 , batch: 96 , training loss: 3.885543\n",
      "[INFO] Epoch: 24 , batch: 97 , training loss: 3.831580\n",
      "[INFO] Epoch: 24 , batch: 98 , training loss: 3.791669\n",
      "[INFO] Epoch: 24 , batch: 99 , training loss: 3.900180\n",
      "[INFO] Epoch: 24 , batch: 100 , training loss: 3.759653\n",
      "[INFO] Epoch: 24 , batch: 101 , training loss: 3.811963\n",
      "[INFO] Epoch: 24 , batch: 102 , training loss: 3.960874\n",
      "[INFO] Epoch: 24 , batch: 103 , training loss: 3.745613\n",
      "[INFO] Epoch: 24 , batch: 104 , training loss: 3.689854\n",
      "[INFO] Epoch: 24 , batch: 105 , training loss: 3.956251\n",
      "[INFO] Epoch: 24 , batch: 106 , training loss: 3.982237\n",
      "[INFO] Epoch: 24 , batch: 107 , training loss: 3.812773\n",
      "[INFO] Epoch: 24 , batch: 108 , training loss: 3.766449\n",
      "[INFO] Epoch: 24 , batch: 109 , training loss: 3.696723\n",
      "[INFO] Epoch: 24 , batch: 110 , training loss: 3.888497\n",
      "[INFO] Epoch: 24 , batch: 111 , training loss: 3.955420\n",
      "[INFO] Epoch: 24 , batch: 112 , training loss: 3.887005\n",
      "[INFO] Epoch: 24 , batch: 113 , training loss: 3.883220\n",
      "[INFO] Epoch: 24 , batch: 114 , training loss: 3.856225\n",
      "[INFO] Epoch: 24 , batch: 115 , training loss: 3.851053\n",
      "[INFO] Epoch: 24 , batch: 116 , training loss: 3.765755\n",
      "[INFO] Epoch: 24 , batch: 117 , training loss: 3.985545\n",
      "[INFO] Epoch: 24 , batch: 118 , training loss: 3.939817\n",
      "[INFO] Epoch: 24 , batch: 119 , training loss: 4.123568\n",
      "[INFO] Epoch: 24 , batch: 120 , training loss: 4.075253\n",
      "[INFO] Epoch: 24 , batch: 121 , training loss: 3.951159\n",
      "[INFO] Epoch: 24 , batch: 122 , training loss: 3.834574\n",
      "[INFO] Epoch: 24 , batch: 123 , training loss: 3.844226\n",
      "[INFO] Epoch: 24 , batch: 124 , training loss: 3.958062\n",
      "[INFO] Epoch: 24 , batch: 125 , training loss: 3.761486\n",
      "[INFO] Epoch: 24 , batch: 126 , training loss: 3.802333\n",
      "[INFO] Epoch: 24 , batch: 127 , training loss: 3.786808\n",
      "[INFO] Epoch: 24 , batch: 128 , training loss: 3.936243\n",
      "[INFO] Epoch: 24 , batch: 129 , training loss: 3.886654\n",
      "[INFO] Epoch: 24 , batch: 130 , training loss: 3.876566\n",
      "[INFO] Epoch: 24 , batch: 131 , training loss: 3.903312\n",
      "[INFO] Epoch: 24 , batch: 132 , training loss: 3.892606\n",
      "[INFO] Epoch: 24 , batch: 133 , training loss: 3.865016\n",
      "[INFO] Epoch: 24 , batch: 134 , training loss: 3.628901\n",
      "[INFO] Epoch: 24 , batch: 135 , training loss: 3.689472\n",
      "[INFO] Epoch: 24 , batch: 136 , training loss: 3.991726\n",
      "[INFO] Epoch: 24 , batch: 137 , training loss: 3.896185\n",
      "[INFO] Epoch: 24 , batch: 138 , training loss: 3.940493\n",
      "[INFO] Epoch: 24 , batch: 139 , training loss: 4.486310\n",
      "[INFO] Epoch: 24 , batch: 140 , training loss: 4.314973\n",
      "[INFO] Epoch: 24 , batch: 141 , training loss: 4.084772\n",
      "[INFO] Epoch: 24 , batch: 142 , training loss: 3.779772\n",
      "[INFO] Epoch: 24 , batch: 143 , training loss: 3.946270\n",
      "[INFO] Epoch: 24 , batch: 144 , training loss: 3.756369\n",
      "[INFO] Epoch: 24 , batch: 145 , training loss: 3.842609\n",
      "[INFO] Epoch: 24 , batch: 146 , training loss: 4.055330\n",
      "[INFO] Epoch: 24 , batch: 147 , training loss: 3.704273\n",
      "[INFO] Epoch: 24 , batch: 148 , training loss: 3.697660\n",
      "[INFO] Epoch: 24 , batch: 149 , training loss: 3.779137\n",
      "[INFO] Epoch: 24 , batch: 150 , training loss: 4.037960\n",
      "[INFO] Epoch: 24 , batch: 151 , training loss: 3.857032\n",
      "[INFO] Epoch: 24 , batch: 152 , training loss: 3.889596\n",
      "[INFO] Epoch: 24 , batch: 153 , training loss: 3.877645\n",
      "[INFO] Epoch: 24 , batch: 154 , training loss: 3.998416\n",
      "[INFO] Epoch: 24 , batch: 155 , training loss: 4.191558\n",
      "[INFO] Epoch: 24 , batch: 156 , training loss: 3.949471\n",
      "[INFO] Epoch: 24 , batch: 157 , training loss: 3.923263\n",
      "[INFO] Epoch: 24 , batch: 158 , training loss: 4.021029\n",
      "[INFO] Epoch: 24 , batch: 159 , training loss: 3.962030\n",
      "[INFO] Epoch: 24 , batch: 160 , training loss: 4.194969\n",
      "[INFO] Epoch: 24 , batch: 161 , training loss: 4.309101\n",
      "[INFO] Epoch: 24 , batch: 162 , training loss: 4.263385\n",
      "[INFO] Epoch: 24 , batch: 163 , training loss: 4.457841\n",
      "[INFO] Epoch: 24 , batch: 164 , training loss: 4.375357\n",
      "[INFO] Epoch: 24 , batch: 165 , training loss: 4.323895\n",
      "[INFO] Epoch: 24 , batch: 166 , training loss: 4.198153\n",
      "[INFO] Epoch: 24 , batch: 167 , training loss: 4.363264\n",
      "[INFO] Epoch: 24 , batch: 168 , training loss: 3.933959\n",
      "[INFO] Epoch: 24 , batch: 169 , training loss: 3.941941\n",
      "[INFO] Epoch: 24 , batch: 170 , training loss: 4.089211\n",
      "[INFO] Epoch: 24 , batch: 171 , training loss: 3.540413\n",
      "[INFO] Epoch: 24 , batch: 172 , training loss: 3.756230\n",
      "[INFO] Epoch: 24 , batch: 173 , training loss: 4.078437\n",
      "[INFO] Epoch: 24 , batch: 174 , training loss: 4.496903\n",
      "[INFO] Epoch: 24 , batch: 175 , training loss: 4.837873\n",
      "[INFO] Epoch: 24 , batch: 176 , training loss: 4.511346\n",
      "[INFO] Epoch: 24 , batch: 177 , training loss: 4.119132\n",
      "[INFO] Epoch: 24 , batch: 178 , training loss: 4.114597\n",
      "[INFO] Epoch: 24 , batch: 179 , training loss: 4.175209\n",
      "[INFO] Epoch: 24 , batch: 180 , training loss: 4.108102\n",
      "[INFO] Epoch: 24 , batch: 181 , training loss: 4.376657\n",
      "[INFO] Epoch: 24 , batch: 182 , training loss: 4.325178\n",
      "[INFO] Epoch: 24 , batch: 183 , training loss: 4.297112\n",
      "[INFO] Epoch: 24 , batch: 184 , training loss: 4.185886\n",
      "[INFO] Epoch: 24 , batch: 185 , training loss: 4.159565\n",
      "[INFO] Epoch: 24 , batch: 186 , training loss: 4.311184\n",
      "[INFO] Epoch: 24 , batch: 187 , training loss: 4.401511\n",
      "[INFO] Epoch: 24 , batch: 188 , training loss: 4.387482\n",
      "[INFO] Epoch: 24 , batch: 189 , training loss: 4.293546\n",
      "[INFO] Epoch: 24 , batch: 190 , training loss: 4.319358\n",
      "[INFO] Epoch: 24 , batch: 191 , training loss: 4.407685\n",
      "[INFO] Epoch: 24 , batch: 192 , training loss: 4.264634\n",
      "[INFO] Epoch: 24 , batch: 193 , training loss: 4.367012\n",
      "[INFO] Epoch: 24 , batch: 194 , training loss: 4.310981\n",
      "[INFO] Epoch: 24 , batch: 195 , training loss: 4.240844\n",
      "[INFO] Epoch: 24 , batch: 196 , training loss: 4.090067\n",
      "[INFO] Epoch: 24 , batch: 197 , training loss: 4.166545\n",
      "[INFO] Epoch: 24 , batch: 198 , training loss: 4.097044\n",
      "[INFO] Epoch: 24 , batch: 199 , training loss: 4.229056\n",
      "[INFO] Epoch: 24 , batch: 200 , training loss: 4.143538\n",
      "[INFO] Epoch: 24 , batch: 201 , training loss: 4.049358\n",
      "[INFO] Epoch: 24 , batch: 202 , training loss: 4.047405\n",
      "[INFO] Epoch: 24 , batch: 203 , training loss: 4.169975\n",
      "[INFO] Epoch: 24 , batch: 204 , training loss: 4.246131\n",
      "[INFO] Epoch: 24 , batch: 205 , training loss: 3.854045\n",
      "[INFO] Epoch: 24 , batch: 206 , training loss: 3.768895\n",
      "[INFO] Epoch: 24 , batch: 207 , training loss: 3.782764\n",
      "[INFO] Epoch: 24 , batch: 208 , training loss: 4.091248\n",
      "[INFO] Epoch: 24 , batch: 209 , training loss: 4.062131\n",
      "[INFO] Epoch: 24 , batch: 210 , training loss: 4.076242\n",
      "[INFO] Epoch: 24 , batch: 211 , training loss: 4.065782\n",
      "[INFO] Epoch: 24 , batch: 212 , training loss: 4.163927\n",
      "[INFO] Epoch: 24 , batch: 213 , training loss: 4.121007\n",
      "[INFO] Epoch: 24 , batch: 214 , training loss: 4.194137\n",
      "[INFO] Epoch: 24 , batch: 215 , training loss: 4.426633\n",
      "[INFO] Epoch: 24 , batch: 216 , training loss: 4.136683\n",
      "[INFO] Epoch: 24 , batch: 217 , training loss: 4.055986\n",
      "[INFO] Epoch: 24 , batch: 218 , training loss: 4.058152\n",
      "[INFO] Epoch: 24 , batch: 219 , training loss: 4.172121\n",
      "[INFO] Epoch: 24 , batch: 220 , training loss: 3.991449\n",
      "[INFO] Epoch: 24 , batch: 221 , training loss: 3.999759\n",
      "[INFO] Epoch: 24 , batch: 222 , training loss: 4.138777\n",
      "[INFO] Epoch: 24 , batch: 223 , training loss: 4.236876\n",
      "[INFO] Epoch: 24 , batch: 224 , training loss: 4.277052\n",
      "[INFO] Epoch: 24 , batch: 225 , training loss: 4.164313\n",
      "[INFO] Epoch: 24 , batch: 226 , training loss: 4.275261\n",
      "[INFO] Epoch: 24 , batch: 227 , training loss: 4.255099\n",
      "[INFO] Epoch: 24 , batch: 228 , training loss: 4.282622\n",
      "[INFO] Epoch: 24 , batch: 229 , training loss: 4.163365\n",
      "[INFO] Epoch: 24 , batch: 230 , training loss: 4.025703\n",
      "[INFO] Epoch: 24 , batch: 231 , training loss: 3.887151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 24 , batch: 232 , training loss: 4.010140\n",
      "[INFO] Epoch: 24 , batch: 233 , training loss: 4.044862\n",
      "[INFO] Epoch: 24 , batch: 234 , training loss: 3.737292\n",
      "[INFO] Epoch: 24 , batch: 235 , training loss: 3.851796\n",
      "[INFO] Epoch: 24 , batch: 236 , training loss: 3.956000\n",
      "[INFO] Epoch: 24 , batch: 237 , training loss: 4.167506\n",
      "[INFO] Epoch: 24 , batch: 238 , training loss: 3.950345\n",
      "[INFO] Epoch: 24 , batch: 239 , training loss: 3.989734\n",
      "[INFO] Epoch: 24 , batch: 240 , training loss: 4.039443\n",
      "[INFO] Epoch: 24 , batch: 241 , training loss: 3.832359\n",
      "[INFO] Epoch: 24 , batch: 242 , training loss: 3.852528\n",
      "[INFO] Epoch: 24 , batch: 243 , training loss: 4.131690\n",
      "[INFO] Epoch: 24 , batch: 244 , training loss: 4.102104\n",
      "[INFO] Epoch: 24 , batch: 245 , training loss: 4.049997\n",
      "[INFO] Epoch: 24 , batch: 246 , training loss: 3.758390\n",
      "[INFO] Epoch: 24 , batch: 247 , training loss: 3.935492\n",
      "[INFO] Epoch: 24 , batch: 248 , training loss: 4.001241\n",
      "[INFO] Epoch: 24 , batch: 249 , training loss: 3.986485\n",
      "[INFO] Epoch: 24 , batch: 250 , training loss: 3.786856\n",
      "[INFO] Epoch: 24 , batch: 251 , training loss: 4.228231\n",
      "[INFO] Epoch: 24 , batch: 252 , training loss: 3.940503\n",
      "[INFO] Epoch: 24 , batch: 253 , training loss: 3.881204\n",
      "[INFO] Epoch: 24 , batch: 254 , training loss: 4.119012\n",
      "[INFO] Epoch: 24 , batch: 255 , training loss: 4.090826\n",
      "[INFO] Epoch: 24 , batch: 256 , training loss: 4.100822\n",
      "[INFO] Epoch: 24 , batch: 257 , training loss: 4.242397\n",
      "[INFO] Epoch: 24 , batch: 258 , training loss: 4.268559\n",
      "[INFO] Epoch: 24 , batch: 259 , training loss: 4.290800\n",
      "[INFO] Epoch: 24 , batch: 260 , training loss: 4.078164\n",
      "[INFO] Epoch: 24 , batch: 261 , training loss: 4.230662\n",
      "[INFO] Epoch: 24 , batch: 262 , training loss: 4.395822\n",
      "[INFO] Epoch: 24 , batch: 263 , training loss: 4.538575\n",
      "[INFO] Epoch: 24 , batch: 264 , training loss: 3.939389\n",
      "[INFO] Epoch: 24 , batch: 265 , training loss: 4.018373\n",
      "[INFO] Epoch: 24 , batch: 266 , training loss: 4.445607\n",
      "[INFO] Epoch: 24 , batch: 267 , training loss: 4.197002\n",
      "[INFO] Epoch: 24 , batch: 268 , training loss: 4.116737\n",
      "[INFO] Epoch: 24 , batch: 269 , training loss: 4.113801\n",
      "[INFO] Epoch: 24 , batch: 270 , training loss: 4.118979\n",
      "[INFO] Epoch: 24 , batch: 271 , training loss: 4.139449\n",
      "[INFO] Epoch: 24 , batch: 272 , training loss: 4.143808\n",
      "[INFO] Epoch: 24 , batch: 273 , training loss: 4.131124\n",
      "[INFO] Epoch: 24 , batch: 274 , training loss: 4.245822\n",
      "[INFO] Epoch: 24 , batch: 275 , training loss: 4.096344\n",
      "[INFO] Epoch: 24 , batch: 276 , training loss: 4.166686\n",
      "[INFO] Epoch: 24 , batch: 277 , training loss: 4.343039\n",
      "[INFO] Epoch: 24 , batch: 278 , training loss: 3.988808\n",
      "[INFO] Epoch: 24 , batch: 279 , training loss: 4.000988\n",
      "[INFO] Epoch: 24 , batch: 280 , training loss: 3.954349\n",
      "[INFO] Epoch: 24 , batch: 281 , training loss: 4.102335\n",
      "[INFO] Epoch: 24 , batch: 282 , training loss: 4.015850\n",
      "[INFO] Epoch: 24 , batch: 283 , training loss: 4.021582\n",
      "[INFO] Epoch: 24 , batch: 284 , training loss: 4.057394\n",
      "[INFO] Epoch: 24 , batch: 285 , training loss: 4.011618\n",
      "[INFO] Epoch: 24 , batch: 286 , training loss: 4.013996\n",
      "[INFO] Epoch: 24 , batch: 287 , training loss: 3.937993\n",
      "[INFO] Epoch: 24 , batch: 288 , training loss: 3.906193\n",
      "[INFO] Epoch: 24 , batch: 289 , training loss: 3.985853\n",
      "[INFO] Epoch: 24 , batch: 290 , training loss: 3.760538\n",
      "[INFO] Epoch: 24 , batch: 291 , training loss: 3.742143\n",
      "[INFO] Epoch: 24 , batch: 292 , training loss: 3.877670\n",
      "[INFO] Epoch: 24 , batch: 293 , training loss: 3.788063\n",
      "[INFO] Epoch: 24 , batch: 294 , training loss: 4.451742\n",
      "[INFO] Epoch: 24 , batch: 295 , training loss: 4.221669\n",
      "[INFO] Epoch: 24 , batch: 296 , training loss: 4.166939\n",
      "[INFO] Epoch: 24 , batch: 297 , training loss: 4.119402\n",
      "[INFO] Epoch: 24 , batch: 298 , training loss: 3.946965\n",
      "[INFO] Epoch: 24 , batch: 299 , training loss: 3.993469\n",
      "[INFO] Epoch: 24 , batch: 300 , training loss: 3.964590\n",
      "[INFO] Epoch: 24 , batch: 301 , training loss: 3.904047\n",
      "[INFO] Epoch: 24 , batch: 302 , training loss: 4.080416\n",
      "[INFO] Epoch: 24 , batch: 303 , training loss: 4.092687\n",
      "[INFO] Epoch: 24 , batch: 304 , training loss: 4.233201\n",
      "[INFO] Epoch: 24 , batch: 305 , training loss: 4.041443\n",
      "[INFO] Epoch: 24 , batch: 306 , training loss: 4.168571\n",
      "[INFO] Epoch: 24 , batch: 307 , training loss: 4.174742\n",
      "[INFO] Epoch: 24 , batch: 308 , training loss: 3.976868\n",
      "[INFO] Epoch: 24 , batch: 309 , training loss: 3.989069\n",
      "[INFO] Epoch: 24 , batch: 310 , training loss: 3.921095\n",
      "[INFO] Epoch: 24 , batch: 311 , training loss: 3.898647\n",
      "[INFO] Epoch: 24 , batch: 312 , training loss: 3.825085\n",
      "[INFO] Epoch: 24 , batch: 313 , training loss: 3.902562\n",
      "[INFO] Epoch: 24 , batch: 314 , training loss: 3.994323\n",
      "[INFO] Epoch: 24 , batch: 315 , training loss: 4.059846\n",
      "[INFO] Epoch: 24 , batch: 316 , training loss: 4.312870\n",
      "[INFO] Epoch: 24 , batch: 317 , training loss: 4.671138\n",
      "[INFO] Epoch: 24 , batch: 318 , training loss: 4.825038\n",
      "[INFO] Epoch: 24 , batch: 319 , training loss: 4.488575\n",
      "[INFO] Epoch: 24 , batch: 320 , training loss: 4.031500\n",
      "[INFO] Epoch: 24 , batch: 321 , training loss: 3.843482\n",
      "[INFO] Epoch: 24 , batch: 322 , training loss: 3.952123\n",
      "[INFO] Epoch: 24 , batch: 323 , training loss: 3.984985\n",
      "[INFO] Epoch: 24 , batch: 324 , training loss: 3.961623\n",
      "[INFO] Epoch: 24 , batch: 325 , training loss: 4.081370\n",
      "[INFO] Epoch: 24 , batch: 326 , training loss: 4.145347\n",
      "[INFO] Epoch: 24 , batch: 327 , training loss: 4.071875\n",
      "[INFO] Epoch: 24 , batch: 328 , training loss: 4.066060\n",
      "[INFO] Epoch: 24 , batch: 329 , training loss: 3.961094\n",
      "[INFO] Epoch: 24 , batch: 330 , training loss: 3.988801\n",
      "[INFO] Epoch: 24 , batch: 331 , training loss: 4.135408\n",
      "[INFO] Epoch: 24 , batch: 332 , training loss: 3.978934\n",
      "[INFO] Epoch: 24 , batch: 333 , training loss: 3.954672\n",
      "[INFO] Epoch: 24 , batch: 334 , training loss: 3.972384\n",
      "[INFO] Epoch: 24 , batch: 335 , training loss: 4.104532\n",
      "[INFO] Epoch: 24 , batch: 336 , training loss: 4.089830\n",
      "[INFO] Epoch: 24 , batch: 337 , training loss: 4.154872\n",
      "[INFO] Epoch: 24 , batch: 338 , training loss: 4.373235\n",
      "[INFO] Epoch: 24 , batch: 339 , training loss: 4.159956\n",
      "[INFO] Epoch: 24 , batch: 340 , training loss: 4.342059\n",
      "[INFO] Epoch: 24 , batch: 341 , training loss: 4.099702\n",
      "[INFO] Epoch: 24 , batch: 342 , training loss: 3.873559\n",
      "[INFO] Epoch: 24 , batch: 343 , training loss: 3.965780\n",
      "[INFO] Epoch: 24 , batch: 344 , training loss: 3.836148\n",
      "[INFO] Epoch: 24 , batch: 345 , training loss: 3.973502\n",
      "[INFO] Epoch: 24 , batch: 346 , training loss: 3.998169\n",
      "[INFO] Epoch: 24 , batch: 347 , training loss: 3.908145\n",
      "[INFO] Epoch: 24 , batch: 348 , training loss: 4.023840\n",
      "[INFO] Epoch: 24 , batch: 349 , training loss: 4.123458\n",
      "[INFO] Epoch: 24 , batch: 350 , training loss: 3.969527\n",
      "[INFO] Epoch: 24 , batch: 351 , training loss: 4.043849\n",
      "[INFO] Epoch: 24 , batch: 352 , training loss: 4.060540\n",
      "[INFO] Epoch: 24 , batch: 353 , training loss: 4.049468\n",
      "[INFO] Epoch: 24 , batch: 354 , training loss: 4.127773\n",
      "[INFO] Epoch: 24 , batch: 355 , training loss: 4.126223\n",
      "[INFO] Epoch: 24 , batch: 356 , training loss: 4.000877\n",
      "[INFO] Epoch: 24 , batch: 357 , training loss: 4.086939\n",
      "[INFO] Epoch: 24 , batch: 358 , training loss: 3.969686\n",
      "[INFO] Epoch: 24 , batch: 359 , training loss: 3.987031\n",
      "[INFO] Epoch: 24 , batch: 360 , training loss: 4.086534\n",
      "[INFO] Epoch: 24 , batch: 361 , training loss: 4.053215\n",
      "[INFO] Epoch: 24 , batch: 362 , training loss: 4.164895\n",
      "[INFO] Epoch: 24 , batch: 363 , training loss: 4.036182\n",
      "[INFO] Epoch: 24 , batch: 364 , training loss: 4.117419\n",
      "[INFO] Epoch: 24 , batch: 365 , training loss: 4.006491\n",
      "[INFO] Epoch: 24 , batch: 366 , training loss: 4.106866\n",
      "[INFO] Epoch: 24 , batch: 367 , training loss: 4.164713\n",
      "[INFO] Epoch: 24 , batch: 368 , training loss: 4.577318\n",
      "[INFO] Epoch: 24 , batch: 369 , training loss: 4.241939\n",
      "[INFO] Epoch: 24 , batch: 370 , training loss: 4.003145\n",
      "[INFO] Epoch: 24 , batch: 371 , training loss: 4.450899\n",
      "[INFO] Epoch: 24 , batch: 372 , training loss: 4.677703\n",
      "[INFO] Epoch: 24 , batch: 373 , training loss: 4.750046\n",
      "[INFO] Epoch: 24 , batch: 374 , training loss: 4.852187\n",
      "[INFO] Epoch: 24 , batch: 375 , training loss: 4.852644\n",
      "[INFO] Epoch: 24 , batch: 376 , training loss: 4.732941\n",
      "[INFO] Epoch: 24 , batch: 377 , training loss: 4.480338\n",
      "[INFO] Epoch: 24 , batch: 378 , training loss: 4.558288\n",
      "[INFO] Epoch: 24 , batch: 379 , training loss: 4.554089\n",
      "[INFO] Epoch: 24 , batch: 380 , training loss: 4.692980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 24 , batch: 381 , training loss: 4.390147\n",
      "[INFO] Epoch: 24 , batch: 382 , training loss: 4.722408\n",
      "[INFO] Epoch: 24 , batch: 383 , training loss: 4.705482\n",
      "[INFO] Epoch: 24 , batch: 384 , training loss: 4.674136\n",
      "[INFO] Epoch: 24 , batch: 385 , training loss: 4.353293\n",
      "[INFO] Epoch: 24 , batch: 386 , training loss: 4.603508\n",
      "[INFO] Epoch: 24 , batch: 387 , training loss: 4.571480\n",
      "[INFO] Epoch: 24 , batch: 388 , training loss: 4.391455\n",
      "[INFO] Epoch: 24 , batch: 389 , training loss: 4.197837\n",
      "[INFO] Epoch: 24 , batch: 390 , training loss: 4.233904\n",
      "[INFO] Epoch: 24 , batch: 391 , training loss: 4.245705\n",
      "[INFO] Epoch: 24 , batch: 392 , training loss: 4.625820\n",
      "[INFO] Epoch: 24 , batch: 393 , training loss: 4.514371\n",
      "[INFO] Epoch: 24 , batch: 394 , training loss: 4.604342\n",
      "[INFO] Epoch: 24 , batch: 395 , training loss: 4.410446\n",
      "[INFO] Epoch: 24 , batch: 396 , training loss: 4.234831\n",
      "[INFO] Epoch: 24 , batch: 397 , training loss: 4.384393\n",
      "[INFO] Epoch: 24 , batch: 398 , training loss: 4.237653\n",
      "[INFO] Epoch: 24 , batch: 399 , training loss: 4.328135\n",
      "[INFO] Epoch: 24 , batch: 400 , training loss: 4.288912\n",
      "[INFO] Epoch: 24 , batch: 401 , training loss: 4.722257\n",
      "[INFO] Epoch: 24 , batch: 402 , training loss: 4.434729\n",
      "[INFO] Epoch: 24 , batch: 403 , training loss: 4.267472\n",
      "[INFO] Epoch: 24 , batch: 404 , training loss: 4.426912\n",
      "[INFO] Epoch: 24 , batch: 405 , training loss: 4.496984\n",
      "[INFO] Epoch: 24 , batch: 406 , training loss: 4.402291\n",
      "[INFO] Epoch: 24 , batch: 407 , training loss: 4.447692\n",
      "[INFO] Epoch: 24 , batch: 408 , training loss: 4.387370\n",
      "[INFO] Epoch: 24 , batch: 409 , training loss: 4.415486\n",
      "[INFO] Epoch: 24 , batch: 410 , training loss: 4.478531\n",
      "[INFO] Epoch: 24 , batch: 411 , training loss: 4.651242\n",
      "[INFO] Epoch: 24 , batch: 412 , training loss: 4.463700\n",
      "[INFO] Epoch: 24 , batch: 413 , training loss: 4.339015\n",
      "[INFO] Epoch: 24 , batch: 414 , training loss: 4.385118\n",
      "[INFO] Epoch: 24 , batch: 415 , training loss: 4.417672\n",
      "[INFO] Epoch: 24 , batch: 416 , training loss: 4.501821\n",
      "[INFO] Epoch: 24 , batch: 417 , training loss: 4.399875\n",
      "[INFO] Epoch: 24 , batch: 418 , training loss: 4.460275\n",
      "[INFO] Epoch: 24 , batch: 419 , training loss: 4.424943\n",
      "[INFO] Epoch: 24 , batch: 420 , training loss: 4.385528\n",
      "[INFO] Epoch: 24 , batch: 421 , training loss: 4.381813\n",
      "[INFO] Epoch: 24 , batch: 422 , training loss: 4.225227\n",
      "[INFO] Epoch: 24 , batch: 423 , training loss: 4.452255\n",
      "[INFO] Epoch: 24 , batch: 424 , training loss: 4.619337\n",
      "[INFO] Epoch: 24 , batch: 425 , training loss: 4.480425\n",
      "[INFO] Epoch: 24 , batch: 426 , training loss: 4.203909\n",
      "[INFO] Epoch: 24 , batch: 427 , training loss: 4.475807\n",
      "[INFO] Epoch: 24 , batch: 428 , training loss: 4.323641\n",
      "[INFO] Epoch: 24 , batch: 429 , training loss: 4.224626\n",
      "[INFO] Epoch: 24 , batch: 430 , training loss: 4.456797\n",
      "[INFO] Epoch: 24 , batch: 431 , training loss: 4.053042\n",
      "[INFO] Epoch: 24 , batch: 432 , training loss: 4.114331\n",
      "[INFO] Epoch: 24 , batch: 433 , training loss: 4.134790\n",
      "[INFO] Epoch: 24 , batch: 434 , training loss: 4.041481\n",
      "[INFO] Epoch: 24 , batch: 435 , training loss: 4.381070\n",
      "[INFO] Epoch: 24 , batch: 436 , training loss: 4.437754\n",
      "[INFO] Epoch: 24 , batch: 437 , training loss: 4.211834\n",
      "[INFO] Epoch: 24 , batch: 438 , training loss: 4.083572\n",
      "[INFO] Epoch: 24 , batch: 439 , training loss: 4.309415\n",
      "[INFO] Epoch: 24 , batch: 440 , training loss: 4.443293\n",
      "[INFO] Epoch: 24 , batch: 441 , training loss: 4.521795\n",
      "[INFO] Epoch: 24 , batch: 442 , training loss: 4.273474\n",
      "[INFO] Epoch: 24 , batch: 443 , training loss: 4.502310\n",
      "[INFO] Epoch: 24 , batch: 444 , training loss: 4.059441\n",
      "[INFO] Epoch: 24 , batch: 445 , training loss: 3.974701\n",
      "[INFO] Epoch: 24 , batch: 446 , training loss: 3.924317\n",
      "[INFO] Epoch: 24 , batch: 447 , training loss: 4.119545\n",
      "[INFO] Epoch: 24 , batch: 448 , training loss: 4.222457\n",
      "[INFO] Epoch: 24 , batch: 449 , training loss: 4.632693\n",
      "[INFO] Epoch: 24 , batch: 450 , training loss: 4.686841\n",
      "[INFO] Epoch: 24 , batch: 451 , training loss: 4.570812\n",
      "[INFO] Epoch: 24 , batch: 452 , training loss: 4.395839\n",
      "[INFO] Epoch: 24 , batch: 453 , training loss: 4.178922\n",
      "[INFO] Epoch: 24 , batch: 454 , training loss: 4.300258\n",
      "[INFO] Epoch: 24 , batch: 455 , training loss: 4.360675\n",
      "[INFO] Epoch: 24 , batch: 456 , training loss: 4.346908\n",
      "[INFO] Epoch: 24 , batch: 457 , training loss: 4.423130\n",
      "[INFO] Epoch: 24 , batch: 458 , training loss: 4.181293\n",
      "[INFO] Epoch: 24 , batch: 459 , training loss: 4.145631\n",
      "[INFO] Epoch: 24 , batch: 460 , training loss: 4.269725\n",
      "[INFO] Epoch: 24 , batch: 461 , training loss: 4.231801\n",
      "[INFO] Epoch: 24 , batch: 462 , training loss: 4.277504\n",
      "[INFO] Epoch: 24 , batch: 463 , training loss: 4.201084\n",
      "[INFO] Epoch: 24 , batch: 464 , training loss: 4.379425\n",
      "[INFO] Epoch: 24 , batch: 465 , training loss: 4.316010\n",
      "[INFO] Epoch: 24 , batch: 466 , training loss: 4.421798\n",
      "[INFO] Epoch: 24 , batch: 467 , training loss: 4.390796\n",
      "[INFO] Epoch: 24 , batch: 468 , training loss: 4.341923\n",
      "[INFO] Epoch: 24 , batch: 469 , training loss: 4.376899\n",
      "[INFO] Epoch: 24 , batch: 470 , training loss: 4.178439\n",
      "[INFO] Epoch: 24 , batch: 471 , training loss: 4.304006\n",
      "[INFO] Epoch: 24 , batch: 472 , training loss: 4.350098\n",
      "[INFO] Epoch: 24 , batch: 473 , training loss: 4.261033\n",
      "[INFO] Epoch: 24 , batch: 474 , training loss: 4.042958\n",
      "[INFO] Epoch: 24 , batch: 475 , training loss: 3.936442\n",
      "[INFO] Epoch: 24 , batch: 476 , training loss: 4.341304\n",
      "[INFO] Epoch: 24 , batch: 477 , training loss: 4.432687\n",
      "[INFO] Epoch: 24 , batch: 478 , training loss: 4.448046\n",
      "[INFO] Epoch: 24 , batch: 479 , training loss: 4.411438\n",
      "[INFO] Epoch: 24 , batch: 480 , training loss: 4.539415\n",
      "[INFO] Epoch: 24 , batch: 481 , training loss: 4.435076\n",
      "[INFO] Epoch: 24 , batch: 482 , training loss: 4.527840\n",
      "[INFO] Epoch: 24 , batch: 483 , training loss: 4.367273\n",
      "[INFO] Epoch: 24 , batch: 484 , training loss: 4.181015\n",
      "[INFO] Epoch: 24 , batch: 485 , training loss: 4.278069\n",
      "[INFO] Epoch: 24 , batch: 486 , training loss: 4.165954\n",
      "[INFO] Epoch: 24 , batch: 487 , training loss: 4.165025\n",
      "[INFO] Epoch: 24 , batch: 488 , training loss: 4.335803\n",
      "[INFO] Epoch: 24 , batch: 489 , training loss: 4.236130\n",
      "[INFO] Epoch: 24 , batch: 490 , training loss: 4.297286\n",
      "[INFO] Epoch: 24 , batch: 491 , training loss: 4.231853\n",
      "[INFO] Epoch: 24 , batch: 492 , training loss: 4.199379\n",
      "[INFO] Epoch: 24 , batch: 493 , training loss: 4.355572\n",
      "[INFO] Epoch: 24 , batch: 494 , training loss: 4.265661\n",
      "[INFO] Epoch: 24 , batch: 495 , training loss: 4.422951\n",
      "[INFO] Epoch: 24 , batch: 496 , training loss: 4.298653\n",
      "[INFO] Epoch: 24 , batch: 497 , training loss: 4.347426\n",
      "[INFO] Epoch: 24 , batch: 498 , training loss: 4.323074\n",
      "[INFO] Epoch: 24 , batch: 499 , training loss: 4.398925\n",
      "[INFO] Epoch: 24 , batch: 500 , training loss: 4.513573\n",
      "[INFO] Epoch: 24 , batch: 501 , training loss: 4.878648\n",
      "[INFO] Epoch: 24 , batch: 502 , training loss: 4.902681\n",
      "[INFO] Epoch: 24 , batch: 503 , training loss: 4.584492\n",
      "[INFO] Epoch: 24 , batch: 504 , training loss: 4.733493\n",
      "[INFO] Epoch: 24 , batch: 505 , training loss: 4.688717\n",
      "[INFO] Epoch: 24 , batch: 506 , training loss: 4.656671\n",
      "[INFO] Epoch: 24 , batch: 507 , training loss: 4.715997\n",
      "[INFO] Epoch: 24 , batch: 508 , training loss: 4.617730\n",
      "[INFO] Epoch: 24 , batch: 509 , training loss: 4.428901\n",
      "[INFO] Epoch: 24 , batch: 510 , training loss: 4.487129\n",
      "[INFO] Epoch: 24 , batch: 511 , training loss: 4.420800\n",
      "[INFO] Epoch: 24 , batch: 512 , training loss: 4.510451\n",
      "[INFO] Epoch: 24 , batch: 513 , training loss: 4.745501\n",
      "[INFO] Epoch: 24 , batch: 514 , training loss: 4.411280\n",
      "[INFO] Epoch: 24 , batch: 515 , training loss: 4.676505\n",
      "[INFO] Epoch: 24 , batch: 516 , training loss: 4.464457\n",
      "[INFO] Epoch: 24 , batch: 517 , training loss: 4.411301\n",
      "[INFO] Epoch: 24 , batch: 518 , training loss: 4.389775\n",
      "[INFO] Epoch: 24 , batch: 519 , training loss: 4.231325\n",
      "[INFO] Epoch: 24 , batch: 520 , training loss: 4.472860\n",
      "[INFO] Epoch: 24 , batch: 521 , training loss: 4.451128\n",
      "[INFO] Epoch: 24 , batch: 522 , training loss: 4.534645\n",
      "[INFO] Epoch: 24 , batch: 523 , training loss: 4.426549\n",
      "[INFO] Epoch: 24 , batch: 524 , training loss: 4.725606\n",
      "[INFO] Epoch: 24 , batch: 525 , training loss: 4.605455\n",
      "[INFO] Epoch: 24 , batch: 526 , training loss: 4.390523\n",
      "[INFO] Epoch: 24 , batch: 527 , training loss: 4.446146\n",
      "[INFO] Epoch: 24 , batch: 528 , training loss: 4.447164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 24 , batch: 529 , training loss: 4.425150\n",
      "[INFO] Epoch: 24 , batch: 530 , training loss: 4.275629\n",
      "[INFO] Epoch: 24 , batch: 531 , training loss: 4.411546\n",
      "[INFO] Epoch: 24 , batch: 532 , training loss: 4.321266\n",
      "[INFO] Epoch: 24 , batch: 533 , training loss: 4.467074\n",
      "[INFO] Epoch: 24 , batch: 534 , training loss: 4.452284\n",
      "[INFO] Epoch: 24 , batch: 535 , training loss: 4.459654\n",
      "[INFO] Epoch: 24 , batch: 536 , training loss: 4.322453\n",
      "[INFO] Epoch: 24 , batch: 537 , training loss: 4.291317\n",
      "[INFO] Epoch: 24 , batch: 538 , training loss: 4.377922\n",
      "[INFO] Epoch: 24 , batch: 539 , training loss: 4.500072\n",
      "[INFO] Epoch: 24 , batch: 540 , training loss: 5.031326\n",
      "[INFO] Epoch: 24 , batch: 541 , training loss: 4.857283\n",
      "[INFO] Epoch: 24 , batch: 542 , training loss: 4.698066\n",
      "[INFO] Epoch: 25 , batch: 0 , training loss: 3.721474\n",
      "[INFO] Epoch: 25 , batch: 1 , training loss: 3.592722\n",
      "[INFO] Epoch: 25 , batch: 2 , training loss: 3.736460\n",
      "[INFO] Epoch: 25 , batch: 3 , training loss: 3.595634\n",
      "[INFO] Epoch: 25 , batch: 4 , training loss: 3.933194\n",
      "[INFO] Epoch: 25 , batch: 5 , training loss: 3.600847\n",
      "[INFO] Epoch: 25 , batch: 6 , training loss: 3.923022\n",
      "[INFO] Epoch: 25 , batch: 7 , training loss: 3.842593\n",
      "[INFO] Epoch: 25 , batch: 8 , training loss: 3.529865\n",
      "[INFO] Epoch: 25 , batch: 9 , training loss: 3.797989\n",
      "[INFO] Epoch: 25 , batch: 10 , training loss: 3.708438\n",
      "[INFO] Epoch: 25 , batch: 11 , training loss: 3.657440\n",
      "[INFO] Epoch: 25 , batch: 12 , training loss: 3.549473\n",
      "[INFO] Epoch: 25 , batch: 13 , training loss: 3.625980\n",
      "[INFO] Epoch: 25 , batch: 14 , training loss: 3.497972\n",
      "[INFO] Epoch: 25 , batch: 15 , training loss: 3.733881\n",
      "[INFO] Epoch: 25 , batch: 16 , training loss: 3.585430\n",
      "[INFO] Epoch: 25 , batch: 17 , training loss: 3.717013\n",
      "[INFO] Epoch: 25 , batch: 18 , training loss: 3.632997\n",
      "[INFO] Epoch: 25 , batch: 19 , training loss: 3.420657\n",
      "[INFO] Epoch: 25 , batch: 20 , training loss: 3.370400\n",
      "[INFO] Epoch: 25 , batch: 21 , training loss: 3.516150\n",
      "[INFO] Epoch: 25 , batch: 22 , training loss: 3.437637\n",
      "[INFO] Epoch: 25 , batch: 23 , training loss: 3.611122\n",
      "[INFO] Epoch: 25 , batch: 24 , training loss: 3.468942\n",
      "[INFO] Epoch: 25 , batch: 25 , training loss: 3.609266\n",
      "[INFO] Epoch: 25 , batch: 26 , training loss: 3.462223\n",
      "[INFO] Epoch: 25 , batch: 27 , training loss: 3.427663\n",
      "[INFO] Epoch: 25 , batch: 28 , training loss: 3.639577\n",
      "[INFO] Epoch: 25 , batch: 29 , training loss: 3.428900\n",
      "[INFO] Epoch: 25 , batch: 30 , training loss: 3.482414\n",
      "[INFO] Epoch: 25 , batch: 31 , training loss: 3.600266\n",
      "[INFO] Epoch: 25 , batch: 32 , training loss: 3.538652\n",
      "[INFO] Epoch: 25 , batch: 33 , training loss: 3.588109\n",
      "[INFO] Epoch: 25 , batch: 34 , training loss: 3.583291\n",
      "[INFO] Epoch: 25 , batch: 35 , training loss: 3.520327\n",
      "[INFO] Epoch: 25 , batch: 36 , training loss: 3.602475\n",
      "[INFO] Epoch: 25 , batch: 37 , training loss: 3.460583\n",
      "[INFO] Epoch: 25 , batch: 38 , training loss: 3.544366\n",
      "[INFO] Epoch: 25 , batch: 39 , training loss: 3.365046\n",
      "[INFO] Epoch: 25 , batch: 40 , training loss: 3.586708\n",
      "[INFO] Epoch: 25 , batch: 41 , training loss: 3.525868\n",
      "[INFO] Epoch: 25 , batch: 42 , training loss: 3.994117\n",
      "[INFO] Epoch: 25 , batch: 43 , training loss: 3.652889\n",
      "[INFO] Epoch: 25 , batch: 44 , training loss: 4.022032\n",
      "[INFO] Epoch: 25 , batch: 45 , training loss: 3.950015\n",
      "[INFO] Epoch: 25 , batch: 46 , training loss: 3.916806\n",
      "[INFO] Epoch: 25 , batch: 47 , training loss: 3.616272\n",
      "[INFO] Epoch: 25 , batch: 48 , training loss: 3.594189\n",
      "[INFO] Epoch: 25 , batch: 49 , training loss: 3.853201\n",
      "[INFO] Epoch: 25 , batch: 50 , training loss: 3.599246\n",
      "[INFO] Epoch: 25 , batch: 51 , training loss: 3.847364\n",
      "[INFO] Epoch: 25 , batch: 52 , training loss: 3.649544\n",
      "[INFO] Epoch: 25 , batch: 53 , training loss: 3.763059\n",
      "[INFO] Epoch: 25 , batch: 54 , training loss: 3.787603\n",
      "[INFO] Epoch: 25 , batch: 55 , training loss: 3.874893\n",
      "[INFO] Epoch: 25 , batch: 56 , training loss: 3.694986\n",
      "[INFO] Epoch: 25 , batch: 57 , training loss: 3.647816\n",
      "[INFO] Epoch: 25 , batch: 58 , training loss: 3.700036\n",
      "[INFO] Epoch: 25 , batch: 59 , training loss: 3.759248\n",
      "[INFO] Epoch: 25 , batch: 60 , training loss: 3.711421\n",
      "[INFO] Epoch: 25 , batch: 61 , training loss: 3.776129\n",
      "[INFO] Epoch: 25 , batch: 62 , training loss: 3.659914\n",
      "[INFO] Epoch: 25 , batch: 63 , training loss: 3.822526\n",
      "[INFO] Epoch: 25 , batch: 64 , training loss: 4.072570\n",
      "[INFO] Epoch: 25 , batch: 65 , training loss: 3.733958\n",
      "[INFO] Epoch: 25 , batch: 66 , training loss: 3.622611\n",
      "[INFO] Epoch: 25 , batch: 67 , training loss: 3.631433\n",
      "[INFO] Epoch: 25 , batch: 68 , training loss: 3.839023\n",
      "[INFO] Epoch: 25 , batch: 69 , training loss: 3.733423\n",
      "[INFO] Epoch: 25 , batch: 70 , training loss: 3.957396\n",
      "[INFO] Epoch: 25 , batch: 71 , training loss: 3.808487\n",
      "[INFO] Epoch: 25 , batch: 72 , training loss: 3.875627\n",
      "[INFO] Epoch: 25 , batch: 73 , training loss: 3.823977\n",
      "[INFO] Epoch: 25 , batch: 74 , training loss: 3.918804\n",
      "[INFO] Epoch: 25 , batch: 75 , training loss: 3.807269\n",
      "[INFO] Epoch: 25 , batch: 76 , training loss: 3.884854\n",
      "[INFO] Epoch: 25 , batch: 77 , training loss: 3.809213\n",
      "[INFO] Epoch: 25 , batch: 78 , training loss: 3.925443\n",
      "[INFO] Epoch: 25 , batch: 79 , training loss: 3.775620\n",
      "[INFO] Epoch: 25 , batch: 80 , training loss: 3.977897\n",
      "[INFO] Epoch: 25 , batch: 81 , training loss: 3.920303\n",
      "[INFO] Epoch: 25 , batch: 82 , training loss: 3.881673\n",
      "[INFO] Epoch: 25 , batch: 83 , training loss: 3.971277\n",
      "[INFO] Epoch: 25 , batch: 84 , training loss: 3.932386\n",
      "[INFO] Epoch: 25 , batch: 85 , training loss: 4.023888\n",
      "[INFO] Epoch: 25 , batch: 86 , training loss: 3.948844\n",
      "[INFO] Epoch: 25 , batch: 87 , training loss: 3.889906\n",
      "[INFO] Epoch: 25 , batch: 88 , training loss: 4.060102\n",
      "[INFO] Epoch: 25 , batch: 89 , training loss: 3.845584\n",
      "[INFO] Epoch: 25 , batch: 90 , training loss: 3.940888\n",
      "[INFO] Epoch: 25 , batch: 91 , training loss: 3.860301\n",
      "[INFO] Epoch: 25 , batch: 92 , training loss: 3.881920\n",
      "[INFO] Epoch: 25 , batch: 93 , training loss: 3.996728\n",
      "[INFO] Epoch: 25 , batch: 94 , training loss: 4.108099\n",
      "[INFO] Epoch: 25 , batch: 95 , training loss: 3.895089\n",
      "[INFO] Epoch: 25 , batch: 96 , training loss: 3.869736\n",
      "[INFO] Epoch: 25 , batch: 97 , training loss: 3.821197\n",
      "[INFO] Epoch: 25 , batch: 98 , training loss: 3.769196\n",
      "[INFO] Epoch: 25 , batch: 99 , training loss: 3.899992\n",
      "[INFO] Epoch: 25 , batch: 100 , training loss: 3.748648\n",
      "[INFO] Epoch: 25 , batch: 101 , training loss: 3.781600\n",
      "[INFO] Epoch: 25 , batch: 102 , training loss: 3.941031\n",
      "[INFO] Epoch: 25 , batch: 103 , training loss: 3.754683\n",
      "[INFO] Epoch: 25 , batch: 104 , training loss: 3.692008\n",
      "[INFO] Epoch: 25 , batch: 105 , training loss: 3.944685\n",
      "[INFO] Epoch: 25 , batch: 106 , training loss: 3.969364\n",
      "[INFO] Epoch: 25 , batch: 107 , training loss: 3.820252\n",
      "[INFO] Epoch: 25 , batch: 108 , training loss: 3.744649\n",
      "[INFO] Epoch: 25 , batch: 109 , training loss: 3.684450\n",
      "[INFO] Epoch: 25 , batch: 110 , training loss: 3.862452\n",
      "[INFO] Epoch: 25 , batch: 111 , training loss: 3.927190\n",
      "[INFO] Epoch: 25 , batch: 112 , training loss: 3.860747\n",
      "[INFO] Epoch: 25 , batch: 113 , training loss: 3.834311\n",
      "[INFO] Epoch: 25 , batch: 114 , training loss: 3.873638\n",
      "[INFO] Epoch: 25 , batch: 115 , training loss: 3.848204\n",
      "[INFO] Epoch: 25 , batch: 116 , training loss: 3.755321\n",
      "[INFO] Epoch: 25 , batch: 117 , training loss: 3.963964\n",
      "[INFO] Epoch: 25 , batch: 118 , training loss: 3.940192\n",
      "[INFO] Epoch: 25 , batch: 119 , training loss: 4.107864\n",
      "[INFO] Epoch: 25 , batch: 120 , training loss: 4.034190\n",
      "[INFO] Epoch: 25 , batch: 121 , training loss: 3.949148\n",
      "[INFO] Epoch: 25 , batch: 122 , training loss: 3.823526\n",
      "[INFO] Epoch: 25 , batch: 123 , training loss: 3.851725\n",
      "[INFO] Epoch: 25 , batch: 124 , training loss: 3.937902\n",
      "[INFO] Epoch: 25 , batch: 125 , training loss: 3.748492\n",
      "[INFO] Epoch: 25 , batch: 126 , training loss: 3.786909\n",
      "[INFO] Epoch: 25 , batch: 127 , training loss: 3.778741\n",
      "[INFO] Epoch: 25 , batch: 128 , training loss: 3.933568\n",
      "[INFO] Epoch: 25 , batch: 129 , training loss: 3.877424\n",
      "[INFO] Epoch: 25 , batch: 130 , training loss: 3.868492\n",
      "[INFO] Epoch: 25 , batch: 131 , training loss: 3.870428\n",
      "[INFO] Epoch: 25 , batch: 132 , training loss: 3.880857\n",
      "[INFO] Epoch: 25 , batch: 133 , training loss: 3.858408\n",
      "[INFO] Epoch: 25 , batch: 134 , training loss: 3.621724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 25 , batch: 135 , training loss: 3.687471\n",
      "[INFO] Epoch: 25 , batch: 136 , training loss: 3.995342\n",
      "[INFO] Epoch: 25 , batch: 137 , training loss: 3.894115\n",
      "[INFO] Epoch: 25 , batch: 138 , training loss: 3.942877\n",
      "[INFO] Epoch: 25 , batch: 139 , training loss: 4.504901\n",
      "[INFO] Epoch: 25 , batch: 140 , training loss: 4.264756\n",
      "[INFO] Epoch: 25 , batch: 141 , training loss: 4.043972\n",
      "[INFO] Epoch: 25 , batch: 142 , training loss: 3.774364\n",
      "[INFO] Epoch: 25 , batch: 143 , training loss: 3.910452\n",
      "[INFO] Epoch: 25 , batch: 144 , training loss: 3.765391\n",
      "[INFO] Epoch: 25 , batch: 145 , training loss: 3.828432\n",
      "[INFO] Epoch: 25 , batch: 146 , training loss: 4.033410\n",
      "[INFO] Epoch: 25 , batch: 147 , training loss: 3.696455\n",
      "[INFO] Epoch: 25 , batch: 148 , training loss: 3.682177\n",
      "[INFO] Epoch: 25 , batch: 149 , training loss: 3.758093\n",
      "[INFO] Epoch: 25 , batch: 150 , training loss: 4.019212\n",
      "[INFO] Epoch: 25 , batch: 151 , training loss: 3.852114\n",
      "[INFO] Epoch: 25 , batch: 152 , training loss: 3.884684\n",
      "[INFO] Epoch: 25 , batch: 153 , training loss: 3.879360\n",
      "[INFO] Epoch: 25 , batch: 154 , training loss: 4.001920\n",
      "[INFO] Epoch: 25 , batch: 155 , training loss: 4.173189\n",
      "[INFO] Epoch: 25 , batch: 156 , training loss: 3.954093\n",
      "[INFO] Epoch: 25 , batch: 157 , training loss: 3.916026\n",
      "[INFO] Epoch: 25 , batch: 158 , training loss: 4.040742\n",
      "[INFO] Epoch: 25 , batch: 159 , training loss: 3.990961\n",
      "[INFO] Epoch: 25 , batch: 160 , training loss: 4.208689\n",
      "[INFO] Epoch: 25 , batch: 161 , training loss: 4.316344\n",
      "[INFO] Epoch: 25 , batch: 162 , training loss: 4.265659\n",
      "[INFO] Epoch: 25 , batch: 163 , training loss: 4.461103\n",
      "[INFO] Epoch: 25 , batch: 164 , training loss: 4.364242\n",
      "[INFO] Epoch: 25 , batch: 165 , training loss: 4.321470\n",
      "[INFO] Epoch: 25 , batch: 166 , training loss: 4.193145\n",
      "[INFO] Epoch: 25 , batch: 167 , training loss: 4.320644\n",
      "[INFO] Epoch: 25 , batch: 168 , training loss: 3.978095\n",
      "[INFO] Epoch: 25 , batch: 169 , training loss: 3.918460\n",
      "[INFO] Epoch: 25 , batch: 170 , training loss: 4.111880\n",
      "[INFO] Epoch: 25 , batch: 171 , training loss: 3.515468\n",
      "[INFO] Epoch: 25 , batch: 172 , training loss: 3.778845\n",
      "[INFO] Epoch: 25 , batch: 173 , training loss: 4.051680\n",
      "[INFO] Epoch: 25 , batch: 174 , training loss: 4.504955\n",
      "[INFO] Epoch: 25 , batch: 175 , training loss: 4.818162\n",
      "[INFO] Epoch: 25 , batch: 176 , training loss: 4.536449\n",
      "[INFO] Epoch: 25 , batch: 177 , training loss: 4.124924\n",
      "[INFO] Epoch: 25 , batch: 178 , training loss: 4.111934\n",
      "[INFO] Epoch: 25 , batch: 179 , training loss: 4.154393\n",
      "[INFO] Epoch: 25 , batch: 180 , training loss: 4.111403\n",
      "[INFO] Epoch: 25 , batch: 181 , training loss: 4.393846\n",
      "[INFO] Epoch: 25 , batch: 182 , training loss: 4.345140\n",
      "[INFO] Epoch: 25 , batch: 183 , training loss: 4.283373\n",
      "[INFO] Epoch: 25 , batch: 184 , training loss: 4.204266\n",
      "[INFO] Epoch: 25 , batch: 185 , training loss: 4.138149\n",
      "[INFO] Epoch: 25 , batch: 186 , training loss: 4.289407\n",
      "[INFO] Epoch: 25 , batch: 187 , training loss: 4.387590\n",
      "[INFO] Epoch: 25 , batch: 188 , training loss: 4.382966\n",
      "[INFO] Epoch: 25 , batch: 189 , training loss: 4.309520\n",
      "[INFO] Epoch: 25 , batch: 190 , training loss: 4.322313\n",
      "[INFO] Epoch: 25 , batch: 191 , training loss: 4.407696\n",
      "[INFO] Epoch: 25 , batch: 192 , training loss: 4.246826\n",
      "[INFO] Epoch: 25 , batch: 193 , training loss: 4.362579\n",
      "[INFO] Epoch: 25 , batch: 194 , training loss: 4.315457\n",
      "[INFO] Epoch: 25 , batch: 195 , training loss: 4.251686\n",
      "[INFO] Epoch: 25 , batch: 196 , training loss: 4.093909\n",
      "[INFO] Epoch: 25 , batch: 197 , training loss: 4.165532\n",
      "[INFO] Epoch: 25 , batch: 198 , training loss: 4.096980\n",
      "[INFO] Epoch: 25 , batch: 199 , training loss: 4.221266\n",
      "[INFO] Epoch: 25 , batch: 200 , training loss: 4.140310\n",
      "[INFO] Epoch: 25 , batch: 201 , training loss: 4.051261\n",
      "[INFO] Epoch: 25 , batch: 202 , training loss: 4.037436\n",
      "[INFO] Epoch: 25 , batch: 203 , training loss: 4.155585\n",
      "[INFO] Epoch: 25 , batch: 204 , training loss: 4.237197\n",
      "[INFO] Epoch: 25 , batch: 205 , training loss: 3.844600\n",
      "[INFO] Epoch: 25 , batch: 206 , training loss: 3.776327\n",
      "[INFO] Epoch: 25 , batch: 207 , training loss: 3.781737\n",
      "[INFO] Epoch: 25 , batch: 208 , training loss: 4.105026\n",
      "[INFO] Epoch: 25 , batch: 209 , training loss: 4.051806\n",
      "[INFO] Epoch: 25 , batch: 210 , training loss: 4.086693\n",
      "[INFO] Epoch: 25 , batch: 211 , training loss: 4.060095\n",
      "[INFO] Epoch: 25 , batch: 212 , training loss: 4.173372\n",
      "[INFO] Epoch: 25 , batch: 213 , training loss: 4.128778\n",
      "[INFO] Epoch: 25 , batch: 214 , training loss: 4.202821\n",
      "[INFO] Epoch: 25 , batch: 215 , training loss: 4.418142\n",
      "[INFO] Epoch: 25 , batch: 216 , training loss: 4.121273\n",
      "[INFO] Epoch: 25 , batch: 217 , training loss: 4.069478\n",
      "[INFO] Epoch: 25 , batch: 218 , training loss: 4.071647\n",
      "[INFO] Epoch: 25 , batch: 219 , training loss: 4.150012\n",
      "[INFO] Epoch: 25 , batch: 220 , training loss: 3.987898\n",
      "[INFO] Epoch: 25 , batch: 221 , training loss: 3.998889\n",
      "[INFO] Epoch: 25 , batch: 222 , training loss: 4.129174\n",
      "[INFO] Epoch: 25 , batch: 223 , training loss: 4.245654\n",
      "[INFO] Epoch: 25 , batch: 224 , training loss: 4.295778\n",
      "[INFO] Epoch: 25 , batch: 225 , training loss: 4.164010\n",
      "[INFO] Epoch: 25 , batch: 226 , training loss: 4.301330\n",
      "[INFO] Epoch: 25 , batch: 227 , training loss: 4.269069\n",
      "[INFO] Epoch: 25 , batch: 228 , training loss: 4.275962\n",
      "[INFO] Epoch: 25 , batch: 229 , training loss: 4.183460\n",
      "[INFO] Epoch: 25 , batch: 230 , training loss: 4.009238\n",
      "[INFO] Epoch: 25 , batch: 231 , training loss: 3.895519\n",
      "[INFO] Epoch: 25 , batch: 232 , training loss: 4.008722\n",
      "[INFO] Epoch: 25 , batch: 233 , training loss: 4.040942\n",
      "[INFO] Epoch: 25 , batch: 234 , training loss: 3.743933\n",
      "[INFO] Epoch: 25 , batch: 235 , training loss: 3.865112\n",
      "[INFO] Epoch: 25 , batch: 236 , training loss: 3.959685\n",
      "[INFO] Epoch: 25 , batch: 237 , training loss: 4.185741\n",
      "[INFO] Epoch: 25 , batch: 238 , training loss: 3.932146\n",
      "[INFO] Epoch: 25 , batch: 239 , training loss: 3.982201\n",
      "[INFO] Epoch: 25 , batch: 240 , training loss: 4.046719\n",
      "[INFO] Epoch: 25 , batch: 241 , training loss: 3.838065\n",
      "[INFO] Epoch: 25 , batch: 242 , training loss: 3.847007\n",
      "[INFO] Epoch: 25 , batch: 243 , training loss: 4.123012\n",
      "[INFO] Epoch: 25 , batch: 244 , training loss: 4.098736\n",
      "[INFO] Epoch: 25 , batch: 245 , training loss: 4.066172\n",
      "[INFO] Epoch: 25 , batch: 246 , training loss: 3.772058\n",
      "[INFO] Epoch: 25 , batch: 247 , training loss: 3.936865\n",
      "[INFO] Epoch: 25 , batch: 248 , training loss: 3.997338\n",
      "[INFO] Epoch: 25 , batch: 249 , training loss: 3.990928\n",
      "[INFO] Epoch: 25 , batch: 250 , training loss: 3.782521\n",
      "[INFO] Epoch: 25 , batch: 251 , training loss: 4.237831\n",
      "[INFO] Epoch: 25 , batch: 252 , training loss: 3.942560\n",
      "[INFO] Epoch: 25 , batch: 253 , training loss: 3.868042\n",
      "[INFO] Epoch: 25 , batch: 254 , training loss: 4.120947\n",
      "[INFO] Epoch: 25 , batch: 255 , training loss: 4.107536\n",
      "[INFO] Epoch: 25 , batch: 256 , training loss: 4.108564\n",
      "[INFO] Epoch: 25 , batch: 257 , training loss: 4.241022\n",
      "[INFO] Epoch: 25 , batch: 258 , training loss: 4.255713\n",
      "[INFO] Epoch: 25 , batch: 259 , training loss: 4.287960\n",
      "[INFO] Epoch: 25 , batch: 260 , training loss: 4.079239\n",
      "[INFO] Epoch: 25 , batch: 261 , training loss: 4.240857\n",
      "[INFO] Epoch: 25 , batch: 262 , training loss: 4.407542\n",
      "[INFO] Epoch: 25 , batch: 263 , training loss: 4.550180\n",
      "[INFO] Epoch: 25 , batch: 264 , training loss: 3.927040\n",
      "[INFO] Epoch: 25 , batch: 265 , training loss: 4.029465\n",
      "[INFO] Epoch: 25 , batch: 266 , training loss: 4.444232\n",
      "[INFO] Epoch: 25 , batch: 267 , training loss: 4.183639\n",
      "[INFO] Epoch: 25 , batch: 268 , training loss: 4.118902\n",
      "[INFO] Epoch: 25 , batch: 269 , training loss: 4.100218\n",
      "[INFO] Epoch: 25 , batch: 270 , training loss: 4.107584\n",
      "[INFO] Epoch: 25 , batch: 271 , training loss: 4.147269\n",
      "[INFO] Epoch: 25 , batch: 272 , training loss: 4.134768\n",
      "[INFO] Epoch: 25 , batch: 273 , training loss: 4.142856\n",
      "[INFO] Epoch: 25 , batch: 274 , training loss: 4.229853\n",
      "[INFO] Epoch: 25 , batch: 275 , training loss: 4.097352\n",
      "[INFO] Epoch: 25 , batch: 276 , training loss: 4.166280\n",
      "[INFO] Epoch: 25 , batch: 277 , training loss: 4.343482\n",
      "[INFO] Epoch: 25 , batch: 278 , training loss: 3.982945\n",
      "[INFO] Epoch: 25 , batch: 279 , training loss: 4.006755\n",
      "[INFO] Epoch: 25 , batch: 280 , training loss: 3.980285\n",
      "[INFO] Epoch: 25 , batch: 281 , training loss: 4.105538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 25 , batch: 282 , training loss: 4.013026\n",
      "[INFO] Epoch: 25 , batch: 283 , training loss: 4.022388\n",
      "[INFO] Epoch: 25 , batch: 284 , training loss: 4.059412\n",
      "[INFO] Epoch: 25 , batch: 285 , training loss: 4.004461\n",
      "[INFO] Epoch: 25 , batch: 286 , training loss: 4.005943\n",
      "[INFO] Epoch: 25 , batch: 287 , training loss: 3.935000\n",
      "[INFO] Epoch: 25 , batch: 288 , training loss: 3.915066\n",
      "[INFO] Epoch: 25 , batch: 289 , training loss: 3.999575\n",
      "[INFO] Epoch: 25 , batch: 290 , training loss: 3.753947\n",
      "[INFO] Epoch: 25 , batch: 291 , training loss: 3.742879\n",
      "[INFO] Epoch: 25 , batch: 292 , training loss: 3.852324\n",
      "[INFO] Epoch: 25 , batch: 293 , training loss: 3.781166\n",
      "[INFO] Epoch: 25 , batch: 294 , training loss: 4.441320\n",
      "[INFO] Epoch: 25 , batch: 295 , training loss: 4.218027\n",
      "[INFO] Epoch: 25 , batch: 296 , training loss: 4.156416\n",
      "[INFO] Epoch: 25 , batch: 297 , training loss: 4.105439\n",
      "[INFO] Epoch: 25 , batch: 298 , training loss: 3.938979\n",
      "[INFO] Epoch: 25 , batch: 299 , training loss: 3.987428\n",
      "[INFO] Epoch: 25 , batch: 300 , training loss: 3.978574\n",
      "[INFO] Epoch: 25 , batch: 301 , training loss: 3.894317\n",
      "[INFO] Epoch: 25 , batch: 302 , training loss: 4.072858\n",
      "[INFO] Epoch: 25 , batch: 303 , training loss: 4.058270\n",
      "[INFO] Epoch: 25 , batch: 304 , training loss: 4.226163\n",
      "[INFO] Epoch: 25 , batch: 305 , training loss: 4.021399\n",
      "[INFO] Epoch: 25 , batch: 306 , training loss: 4.151688\n",
      "[INFO] Epoch: 25 , batch: 307 , training loss: 4.172335\n",
      "[INFO] Epoch: 25 , batch: 308 , training loss: 3.976292\n",
      "[INFO] Epoch: 25 , batch: 309 , training loss: 3.997145\n",
      "[INFO] Epoch: 25 , batch: 310 , training loss: 3.917918\n",
      "[INFO] Epoch: 25 , batch: 311 , training loss: 3.915787\n",
      "[INFO] Epoch: 25 , batch: 312 , training loss: 3.820498\n",
      "[INFO] Epoch: 25 , batch: 313 , training loss: 3.900715\n",
      "[INFO] Epoch: 25 , batch: 314 , training loss: 3.993661\n",
      "[INFO] Epoch: 25 , batch: 315 , training loss: 4.067568\n",
      "[INFO] Epoch: 25 , batch: 316 , training loss: 4.305047\n",
      "[INFO] Epoch: 25 , batch: 317 , training loss: 4.674168\n",
      "[INFO] Epoch: 25 , batch: 318 , training loss: 4.828823\n",
      "[INFO] Epoch: 25 , batch: 319 , training loss: 4.490762\n",
      "[INFO] Epoch: 25 , batch: 320 , training loss: 4.023543\n",
      "[INFO] Epoch: 25 , batch: 321 , training loss: 3.842793\n",
      "[INFO] Epoch: 25 , batch: 322 , training loss: 3.944413\n",
      "[INFO] Epoch: 25 , batch: 323 , training loss: 3.973315\n",
      "[INFO] Epoch: 25 , batch: 324 , training loss: 3.972932\n",
      "[INFO] Epoch: 25 , batch: 325 , training loss: 4.092052\n",
      "[INFO] Epoch: 25 , batch: 326 , training loss: 4.143885\n",
      "[INFO] Epoch: 25 , batch: 327 , training loss: 4.070746\n",
      "[INFO] Epoch: 25 , batch: 328 , training loss: 4.072385\n",
      "[INFO] Epoch: 25 , batch: 329 , training loss: 3.956282\n",
      "[INFO] Epoch: 25 , batch: 330 , training loss: 3.974229\n",
      "[INFO] Epoch: 25 , batch: 331 , training loss: 4.133233\n",
      "[INFO] Epoch: 25 , batch: 332 , training loss: 3.980557\n",
      "[INFO] Epoch: 25 , batch: 333 , training loss: 3.947057\n",
      "[INFO] Epoch: 25 , batch: 334 , training loss: 3.963042\n",
      "[INFO] Epoch: 25 , batch: 335 , training loss: 4.089542\n",
      "[INFO] Epoch: 25 , batch: 336 , training loss: 4.093610\n",
      "[INFO] Epoch: 25 , batch: 337 , training loss: 4.138374\n",
      "[INFO] Epoch: 25 , batch: 338 , training loss: 4.357269\n",
      "[INFO] Epoch: 25 , batch: 339 , training loss: 4.151060\n",
      "[INFO] Epoch: 25 , batch: 340 , training loss: 4.343016\n",
      "[INFO] Epoch: 25 , batch: 341 , training loss: 4.106768\n",
      "[INFO] Epoch: 25 , batch: 342 , training loss: 3.877211\n",
      "[INFO] Epoch: 25 , batch: 343 , training loss: 3.956088\n",
      "[INFO] Epoch: 25 , batch: 344 , training loss: 3.838336\n",
      "[INFO] Epoch: 25 , batch: 345 , training loss: 3.972852\n",
      "[INFO] Epoch: 25 , batch: 346 , training loss: 4.000896\n",
      "[INFO] Epoch: 25 , batch: 347 , training loss: 3.922704\n",
      "[INFO] Epoch: 25 , batch: 348 , training loss: 4.014559\n",
      "[INFO] Epoch: 25 , batch: 349 , training loss: 4.114074\n",
      "[INFO] Epoch: 25 , batch: 350 , training loss: 3.965175\n",
      "[INFO] Epoch: 25 , batch: 351 , training loss: 4.052558\n",
      "[INFO] Epoch: 25 , batch: 352 , training loss: 4.057607\n",
      "[INFO] Epoch: 25 , batch: 353 , training loss: 4.053858\n",
      "[INFO] Epoch: 25 , batch: 354 , training loss: 4.141912\n",
      "[INFO] Epoch: 25 , batch: 355 , training loss: 4.129614\n",
      "[INFO] Epoch: 25 , batch: 356 , training loss: 4.006678\n",
      "[INFO] Epoch: 25 , batch: 357 , training loss: 4.088716\n",
      "[INFO] Epoch: 25 , batch: 358 , training loss: 3.984641\n",
      "[INFO] Epoch: 25 , batch: 359 , training loss: 3.981002\n",
      "[INFO] Epoch: 25 , batch: 360 , training loss: 4.087983\n",
      "[INFO] Epoch: 25 , batch: 361 , training loss: 4.065005\n",
      "[INFO] Epoch: 25 , batch: 362 , training loss: 4.164378\n",
      "[INFO] Epoch: 25 , batch: 363 , training loss: 4.038730\n",
      "[INFO] Epoch: 25 , batch: 364 , training loss: 4.100918\n",
      "[INFO] Epoch: 25 , batch: 365 , training loss: 4.008574\n",
      "[INFO] Epoch: 25 , batch: 366 , training loss: 4.112988\n",
      "[INFO] Epoch: 25 , batch: 367 , training loss: 4.162378\n",
      "[INFO] Epoch: 25 , batch: 368 , training loss: 4.554940\n",
      "[INFO] Epoch: 25 , batch: 369 , training loss: 4.253808\n",
      "[INFO] Epoch: 25 , batch: 370 , training loss: 3.999414\n",
      "[INFO] Epoch: 25 , batch: 371 , training loss: 4.446259\n",
      "[INFO] Epoch: 25 , batch: 372 , training loss: 4.690582\n",
      "[INFO] Epoch: 25 , batch: 373 , training loss: 4.772370\n",
      "[INFO] Epoch: 25 , batch: 374 , training loss: 4.870761\n",
      "[INFO] Epoch: 25 , batch: 375 , training loss: 4.834215\n",
      "[INFO] Epoch: 25 , batch: 376 , training loss: 4.736131\n",
      "[INFO] Epoch: 25 , batch: 377 , training loss: 4.476963\n",
      "[INFO] Epoch: 25 , batch: 378 , training loss: 4.562892\n",
      "[INFO] Epoch: 25 , batch: 379 , training loss: 4.556483\n",
      "[INFO] Epoch: 25 , batch: 380 , training loss: 4.689108\n",
      "[INFO] Epoch: 25 , batch: 381 , training loss: 4.405464\n",
      "[INFO] Epoch: 25 , batch: 382 , training loss: 4.719052\n",
      "[INFO] Epoch: 25 , batch: 383 , training loss: 4.718319\n",
      "[INFO] Epoch: 25 , batch: 384 , training loss: 4.685003\n",
      "[INFO] Epoch: 25 , batch: 385 , training loss: 4.373329\n",
      "[INFO] Epoch: 25 , batch: 386 , training loss: 4.616998\n",
      "[INFO] Epoch: 25 , batch: 387 , training loss: 4.586124\n",
      "[INFO] Epoch: 25 , batch: 388 , training loss: 4.389939\n",
      "[INFO] Epoch: 25 , batch: 389 , training loss: 4.224321\n",
      "[INFO] Epoch: 25 , batch: 390 , training loss: 4.238620\n",
      "[INFO] Epoch: 25 , batch: 391 , training loss: 4.259010\n",
      "[INFO] Epoch: 25 , batch: 392 , training loss: 4.629290\n",
      "[INFO] Epoch: 25 , batch: 393 , training loss: 4.528832\n",
      "[INFO] Epoch: 25 , batch: 394 , training loss: 4.603885\n",
      "[INFO] Epoch: 25 , batch: 395 , training loss: 4.424851\n",
      "[INFO] Epoch: 25 , batch: 396 , training loss: 4.239504\n",
      "[INFO] Epoch: 25 , batch: 397 , training loss: 4.386041\n",
      "[INFO] Epoch: 25 , batch: 398 , training loss: 4.235847\n",
      "[INFO] Epoch: 25 , batch: 399 , training loss: 4.330488\n",
      "[INFO] Epoch: 25 , batch: 400 , training loss: 4.310280\n",
      "[INFO] Epoch: 25 , batch: 401 , training loss: 4.719426\n",
      "[INFO] Epoch: 25 , batch: 402 , training loss: 4.456662\n",
      "[INFO] Epoch: 25 , batch: 403 , training loss: 4.276204\n",
      "[INFO] Epoch: 25 , batch: 404 , training loss: 4.436106\n",
      "[INFO] Epoch: 25 , batch: 405 , training loss: 4.511141\n",
      "[INFO] Epoch: 25 , batch: 406 , training loss: 4.413288\n",
      "[INFO] Epoch: 25 , batch: 407 , training loss: 4.457311\n",
      "[INFO] Epoch: 25 , batch: 408 , training loss: 4.396646\n",
      "[INFO] Epoch: 25 , batch: 409 , training loss: 4.427016\n",
      "[INFO] Epoch: 25 , batch: 410 , training loss: 4.485166\n",
      "[INFO] Epoch: 25 , batch: 411 , training loss: 4.659425\n",
      "[INFO] Epoch: 25 , batch: 412 , training loss: 4.480097\n",
      "[INFO] Epoch: 25 , batch: 413 , training loss: 4.341164\n",
      "[INFO] Epoch: 25 , batch: 414 , training loss: 4.394091\n",
      "[INFO] Epoch: 25 , batch: 415 , training loss: 4.428756\n",
      "[INFO] Epoch: 25 , batch: 416 , training loss: 4.516110\n",
      "[INFO] Epoch: 25 , batch: 417 , training loss: 4.402190\n",
      "[INFO] Epoch: 25 , batch: 418 , training loss: 4.458728\n",
      "[INFO] Epoch: 25 , batch: 419 , training loss: 4.423573\n",
      "[INFO] Epoch: 25 , batch: 420 , training loss: 4.397053\n",
      "[INFO] Epoch: 25 , batch: 421 , training loss: 4.378451\n",
      "[INFO] Epoch: 25 , batch: 422 , training loss: 4.228346\n",
      "[INFO] Epoch: 25 , batch: 423 , training loss: 4.461834\n",
      "[INFO] Epoch: 25 , batch: 424 , training loss: 4.628198\n",
      "[INFO] Epoch: 25 , batch: 425 , training loss: 4.484501\n",
      "[INFO] Epoch: 25 , batch: 426 , training loss: 4.204697\n",
      "[INFO] Epoch: 25 , batch: 427 , training loss: 4.476550\n",
      "[INFO] Epoch: 25 , batch: 428 , training loss: 4.324553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 25 , batch: 429 , training loss: 4.233899\n",
      "[INFO] Epoch: 25 , batch: 430 , training loss: 4.474957\n",
      "[INFO] Epoch: 25 , batch: 431 , training loss: 4.060705\n",
      "[INFO] Epoch: 25 , batch: 432 , training loss: 4.114796\n",
      "[INFO] Epoch: 25 , batch: 433 , training loss: 4.149763\n",
      "[INFO] Epoch: 25 , batch: 434 , training loss: 4.036366\n",
      "[INFO] Epoch: 25 , batch: 435 , training loss: 4.394400\n",
      "[INFO] Epoch: 25 , batch: 436 , training loss: 4.450012\n",
      "[INFO] Epoch: 25 , batch: 437 , training loss: 4.218730\n",
      "[INFO] Epoch: 25 , batch: 438 , training loss: 4.080472\n",
      "[INFO] Epoch: 25 , batch: 439 , training loss: 4.316834\n",
      "[INFO] Epoch: 25 , batch: 440 , training loss: 4.448524\n",
      "[INFO] Epoch: 25 , batch: 441 , training loss: 4.504097\n",
      "[INFO] Epoch: 25 , batch: 442 , training loss: 4.282326\n",
      "[INFO] Epoch: 25 , batch: 443 , training loss: 4.496013\n",
      "[INFO] Epoch: 25 , batch: 444 , training loss: 4.082633\n",
      "[INFO] Epoch: 25 , batch: 445 , training loss: 3.988713\n",
      "[INFO] Epoch: 25 , batch: 446 , training loss: 3.922359\n",
      "[INFO] Epoch: 25 , batch: 447 , training loss: 4.119949\n",
      "[INFO] Epoch: 25 , batch: 448 , training loss: 4.229953\n",
      "[INFO] Epoch: 25 , batch: 449 , training loss: 4.616748\n",
      "[INFO] Epoch: 25 , batch: 450 , training loss: 4.677618\n",
      "[INFO] Epoch: 25 , batch: 451 , training loss: 4.577600\n",
      "[INFO] Epoch: 25 , batch: 452 , training loss: 4.402542\n",
      "[INFO] Epoch: 25 , batch: 453 , training loss: 4.177536\n",
      "[INFO] Epoch: 25 , batch: 454 , training loss: 4.313772\n",
      "[INFO] Epoch: 25 , batch: 455 , training loss: 4.368500\n",
      "[INFO] Epoch: 25 , batch: 456 , training loss: 4.352375\n",
      "[INFO] Epoch: 25 , batch: 457 , training loss: 4.431059\n",
      "[INFO] Epoch: 25 , batch: 458 , training loss: 4.181575\n",
      "[INFO] Epoch: 25 , batch: 459 , training loss: 4.151168\n",
      "[INFO] Epoch: 25 , batch: 460 , training loss: 4.259348\n",
      "[INFO] Epoch: 25 , batch: 461 , training loss: 4.247169\n",
      "[INFO] Epoch: 25 , batch: 462 , training loss: 4.275609\n",
      "[INFO] Epoch: 25 , batch: 463 , training loss: 4.217181\n",
      "[INFO] Epoch: 25 , batch: 464 , training loss: 4.383387\n",
      "[INFO] Epoch: 25 , batch: 465 , training loss: 4.313431\n",
      "[INFO] Epoch: 25 , batch: 466 , training loss: 4.426090\n",
      "[INFO] Epoch: 25 , batch: 467 , training loss: 4.398673\n",
      "[INFO] Epoch: 25 , batch: 468 , training loss: 4.344394\n",
      "[INFO] Epoch: 25 , batch: 469 , training loss: 4.389504\n",
      "[INFO] Epoch: 25 , batch: 470 , training loss: 4.197706\n",
      "[INFO] Epoch: 25 , batch: 471 , training loss: 4.315625\n",
      "[INFO] Epoch: 25 , batch: 472 , training loss: 4.373436\n",
      "[INFO] Epoch: 25 , batch: 473 , training loss: 4.265525\n",
      "[INFO] Epoch: 25 , batch: 474 , training loss: 4.055412\n",
      "[INFO] Epoch: 25 , batch: 475 , training loss: 3.945474\n",
      "[INFO] Epoch: 25 , batch: 476 , training loss: 4.340796\n",
      "[INFO] Epoch: 25 , batch: 477 , training loss: 4.441457\n",
      "[INFO] Epoch: 25 , batch: 478 , training loss: 4.471006\n",
      "[INFO] Epoch: 25 , batch: 479 , training loss: 4.427283\n",
      "[INFO] Epoch: 25 , batch: 480 , training loss: 4.548422\n",
      "[INFO] Epoch: 25 , batch: 481 , training loss: 4.436532\n",
      "[INFO] Epoch: 25 , batch: 482 , training loss: 4.531950\n",
      "[INFO] Epoch: 25 , batch: 483 , training loss: 4.374283\n",
      "[INFO] Epoch: 25 , batch: 484 , training loss: 4.188393\n",
      "[INFO] Epoch: 25 , batch: 485 , training loss: 4.275024\n",
      "[INFO] Epoch: 25 , batch: 486 , training loss: 4.180208\n",
      "[INFO] Epoch: 25 , batch: 487 , training loss: 4.169925\n",
      "[INFO] Epoch: 25 , batch: 488 , training loss: 4.337470\n",
      "[INFO] Epoch: 25 , batch: 489 , training loss: 4.242779\n",
      "[INFO] Epoch: 25 , batch: 490 , training loss: 4.301476\n",
      "[INFO] Epoch: 25 , batch: 491 , training loss: 4.231126\n",
      "[INFO] Epoch: 25 , batch: 492 , training loss: 4.192685\n",
      "[INFO] Epoch: 25 , batch: 493 , training loss: 4.370483\n",
      "[INFO] Epoch: 25 , batch: 494 , training loss: 4.262094\n",
      "[INFO] Epoch: 25 , batch: 495 , training loss: 4.425351\n",
      "[INFO] Epoch: 25 , batch: 496 , training loss: 4.295895\n",
      "[INFO] Epoch: 25 , batch: 497 , training loss: 4.355105\n",
      "[INFO] Epoch: 25 , batch: 498 , training loss: 4.332417\n",
      "[INFO] Epoch: 25 , batch: 499 , training loss: 4.398330\n",
      "[INFO] Epoch: 25 , batch: 500 , training loss: 4.522900\n",
      "[INFO] Epoch: 25 , batch: 501 , training loss: 4.871431\n",
      "[INFO] Epoch: 25 , batch: 502 , training loss: 4.900238\n",
      "[INFO] Epoch: 25 , batch: 503 , training loss: 4.590730\n",
      "[INFO] Epoch: 25 , batch: 504 , training loss: 4.743079\n",
      "[INFO] Epoch: 25 , batch: 505 , training loss: 4.682372\n",
      "[INFO] Epoch: 25 , batch: 506 , training loss: 4.650545\n",
      "[INFO] Epoch: 25 , batch: 507 , training loss: 4.709952\n",
      "[INFO] Epoch: 25 , batch: 508 , training loss: 4.619978\n",
      "[INFO] Epoch: 25 , batch: 509 , training loss: 4.436225\n",
      "[INFO] Epoch: 25 , batch: 510 , training loss: 4.499159\n",
      "[INFO] Epoch: 25 , batch: 511 , training loss: 4.435192\n",
      "[INFO] Epoch: 25 , batch: 512 , training loss: 4.513150\n",
      "[INFO] Epoch: 25 , batch: 513 , training loss: 4.762306\n",
      "[INFO] Epoch: 25 , batch: 514 , training loss: 4.409331\n",
      "[INFO] Epoch: 25 , batch: 515 , training loss: 4.681935\n",
      "[INFO] Epoch: 25 , batch: 516 , training loss: 4.458694\n",
      "[INFO] Epoch: 25 , batch: 517 , training loss: 4.424191\n",
      "[INFO] Epoch: 25 , batch: 518 , training loss: 4.381452\n",
      "[INFO] Epoch: 25 , batch: 519 , training loss: 4.231479\n",
      "[INFO] Epoch: 25 , batch: 520 , training loss: 4.471098\n",
      "[INFO] Epoch: 25 , batch: 521 , training loss: 4.454148\n",
      "[INFO] Epoch: 25 , batch: 522 , training loss: 4.532280\n",
      "[INFO] Epoch: 25 , batch: 523 , training loss: 4.436793\n",
      "[INFO] Epoch: 25 , batch: 524 , training loss: 4.733644\n",
      "[INFO] Epoch: 25 , batch: 525 , training loss: 4.605494\n",
      "[INFO] Epoch: 25 , batch: 526 , training loss: 4.404794\n",
      "[INFO] Epoch: 25 , batch: 527 , training loss: 4.449490\n",
      "[INFO] Epoch: 25 , batch: 528 , training loss: 4.453822\n",
      "[INFO] Epoch: 25 , batch: 529 , training loss: 4.429870\n",
      "[INFO] Epoch: 25 , batch: 530 , training loss: 4.267906\n",
      "[INFO] Epoch: 25 , batch: 531 , training loss: 4.410059\n",
      "[INFO] Epoch: 25 , batch: 532 , training loss: 4.330584\n",
      "[INFO] Epoch: 25 , batch: 533 , training loss: 4.471007\n",
      "[INFO] Epoch: 25 , batch: 534 , training loss: 4.453186\n",
      "[INFO] Epoch: 25 , batch: 535 , training loss: 4.463306\n",
      "[INFO] Epoch: 25 , batch: 536 , training loss: 4.326073\n",
      "[INFO] Epoch: 25 , batch: 537 , training loss: 4.287291\n",
      "[INFO] Epoch: 25 , batch: 538 , training loss: 4.384869\n",
      "[INFO] Epoch: 25 , batch: 539 , training loss: 4.494348\n",
      "[INFO] Epoch: 25 , batch: 540 , training loss: 5.040179\n",
      "[INFO] Epoch: 25 , batch: 541 , training loss: 4.840857\n",
      "[INFO] Epoch: 25 , batch: 542 , training loss: 4.729747\n",
      "[INFO] Epoch: 26 , batch: 0 , training loss: 3.768800\n",
      "[INFO] Epoch: 26 , batch: 1 , training loss: 3.593986\n",
      "[INFO] Epoch: 26 , batch: 2 , training loss: 3.743926\n",
      "[INFO] Epoch: 26 , batch: 3 , training loss: 3.613589\n",
      "[INFO] Epoch: 26 , batch: 4 , training loss: 3.906678\n",
      "[INFO] Epoch: 26 , batch: 5 , training loss: 3.659690\n",
      "[INFO] Epoch: 26 , batch: 6 , training loss: 3.964290\n",
      "[INFO] Epoch: 26 , batch: 7 , training loss: 3.891466\n",
      "[INFO] Epoch: 26 , batch: 8 , training loss: 3.547348\n",
      "[INFO] Epoch: 26 , batch: 9 , training loss: 3.825409\n",
      "[INFO] Epoch: 26 , batch: 10 , training loss: 3.735299\n",
      "[INFO] Epoch: 26 , batch: 11 , training loss: 3.677478\n",
      "[INFO] Epoch: 26 , batch: 12 , training loss: 3.582358\n",
      "[INFO] Epoch: 26 , batch: 13 , training loss: 3.632894\n",
      "[INFO] Epoch: 26 , batch: 14 , training loss: 3.507381\n",
      "[INFO] Epoch: 26 , batch: 15 , training loss: 3.767672\n",
      "[INFO] Epoch: 26 , batch: 16 , training loss: 3.611942\n",
      "[INFO] Epoch: 26 , batch: 17 , training loss: 3.735972\n",
      "[INFO] Epoch: 26 , batch: 18 , training loss: 3.661793\n",
      "[INFO] Epoch: 26 , batch: 19 , training loss: 3.428913\n",
      "[INFO] Epoch: 26 , batch: 20 , training loss: 3.402278\n",
      "[INFO] Epoch: 26 , batch: 21 , training loss: 3.538550\n",
      "[INFO] Epoch: 26 , batch: 22 , training loss: 3.478428\n",
      "[INFO] Epoch: 26 , batch: 23 , training loss: 3.641246\n",
      "[INFO] Epoch: 26 , batch: 24 , training loss: 3.494591\n",
      "[INFO] Epoch: 26 , batch: 25 , training loss: 3.610920\n",
      "[INFO] Epoch: 26 , batch: 26 , training loss: 3.484178\n",
      "[INFO] Epoch: 26 , batch: 27 , training loss: 3.459951\n",
      "[INFO] Epoch: 26 , batch: 28 , training loss: 3.668094\n",
      "[INFO] Epoch: 26 , batch: 29 , training loss: 3.462052\n",
      "[INFO] Epoch: 26 , batch: 30 , training loss: 3.482826\n",
      "[INFO] Epoch: 26 , batch: 31 , training loss: 3.605311\n",
      "[INFO] Epoch: 26 , batch: 32 , training loss: 3.585093\n",
      "[INFO] Epoch: 26 , batch: 33 , training loss: 3.619850\n",
      "[INFO] Epoch: 26 , batch: 34 , training loss: 3.600591\n",
      "[INFO] Epoch: 26 , batch: 35 , training loss: 3.510610\n",
      "[INFO] Epoch: 26 , batch: 36 , training loss: 3.639118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 26 , batch: 37 , training loss: 3.461385\n",
      "[INFO] Epoch: 26 , batch: 38 , training loss: 3.593058\n",
      "[INFO] Epoch: 26 , batch: 39 , training loss: 3.389765\n",
      "[INFO] Epoch: 26 , batch: 40 , training loss: 3.602305\n",
      "[INFO] Epoch: 26 , batch: 41 , training loss: 3.539196\n",
      "[INFO] Epoch: 26 , batch: 42 , training loss: 4.010694\n",
      "[INFO] Epoch: 26 , batch: 43 , training loss: 3.692513\n",
      "[INFO] Epoch: 26 , batch: 44 , training loss: 4.011323\n",
      "[INFO] Epoch: 26 , batch: 45 , training loss: 4.033426\n",
      "[INFO] Epoch: 26 , batch: 46 , training loss: 3.961721\n",
      "[INFO] Epoch: 26 , batch: 47 , training loss: 3.633937\n",
      "[INFO] Epoch: 26 , batch: 48 , training loss: 3.605417\n",
      "[INFO] Epoch: 26 , batch: 49 , training loss: 3.855531\n",
      "[INFO] Epoch: 26 , batch: 50 , training loss: 3.639123\n",
      "[INFO] Epoch: 26 , batch: 51 , training loss: 3.837364\n",
      "[INFO] Epoch: 26 , batch: 52 , training loss: 3.698908\n",
      "[INFO] Epoch: 26 , batch: 53 , training loss: 3.795459\n",
      "[INFO] Epoch: 26 , batch: 54 , training loss: 3.811100\n",
      "[INFO] Epoch: 26 , batch: 55 , training loss: 3.910135\n",
      "[INFO] Epoch: 26 , batch: 56 , training loss: 3.690087\n",
      "[INFO] Epoch: 26 , batch: 57 , training loss: 3.641254\n",
      "[INFO] Epoch: 26 , batch: 58 , training loss: 3.696199\n",
      "[INFO] Epoch: 26 , batch: 59 , training loss: 3.782957\n",
      "[INFO] Epoch: 26 , batch: 60 , training loss: 3.738129\n",
      "[INFO] Epoch: 26 , batch: 61 , training loss: 3.782957\n",
      "[INFO] Epoch: 26 , batch: 62 , training loss: 3.662628\n",
      "[INFO] Epoch: 26 , batch: 63 , training loss: 3.856389\n",
      "[INFO] Epoch: 26 , batch: 64 , training loss: 4.060427\n",
      "[INFO] Epoch: 26 , batch: 65 , training loss: 3.758909\n",
      "[INFO] Epoch: 26 , batch: 66 , training loss: 3.651667\n",
      "[INFO] Epoch: 26 , batch: 67 , training loss: 3.653857\n",
      "[INFO] Epoch: 26 , batch: 68 , training loss: 3.842963\n",
      "[INFO] Epoch: 26 , batch: 69 , training loss: 3.755898\n",
      "[INFO] Epoch: 26 , batch: 70 , training loss: 3.959699\n",
      "[INFO] Epoch: 26 , batch: 71 , training loss: 3.821535\n",
      "[INFO] Epoch: 26 , batch: 72 , training loss: 3.910890\n",
      "[INFO] Epoch: 26 , batch: 73 , training loss: 3.823539\n",
      "[INFO] Epoch: 26 , batch: 74 , training loss: 3.948072\n",
      "[INFO] Epoch: 26 , batch: 75 , training loss: 3.815601\n",
      "[INFO] Epoch: 26 , batch: 76 , training loss: 3.926895\n",
      "[INFO] Epoch: 26 , batch: 77 , training loss: 3.828951\n",
      "[INFO] Epoch: 26 , batch: 78 , training loss: 3.938403\n",
      "[INFO] Epoch: 26 , batch: 79 , training loss: 3.797054\n",
      "[INFO] Epoch: 26 , batch: 80 , training loss: 3.984585\n",
      "[INFO] Epoch: 26 , batch: 81 , training loss: 3.927846\n",
      "[INFO] Epoch: 26 , batch: 82 , training loss: 3.887053\n",
      "[INFO] Epoch: 26 , batch: 83 , training loss: 3.995302\n",
      "[INFO] Epoch: 26 , batch: 84 , training loss: 3.920846\n",
      "[INFO] Epoch: 26 , batch: 85 , training loss: 4.041523\n",
      "[INFO] Epoch: 26 , batch: 86 , training loss: 3.996874\n",
      "[INFO] Epoch: 26 , batch: 87 , training loss: 3.905444\n",
      "[INFO] Epoch: 26 , batch: 88 , training loss: 4.072253\n",
      "[INFO] Epoch: 26 , batch: 89 , training loss: 3.877330\n",
      "[INFO] Epoch: 26 , batch: 90 , training loss: 3.952983\n",
      "[INFO] Epoch: 26 , batch: 91 , training loss: 3.882765\n",
      "[INFO] Epoch: 26 , batch: 92 , training loss: 3.891460\n",
      "[INFO] Epoch: 26 , batch: 93 , training loss: 3.991386\n",
      "[INFO] Epoch: 26 , batch: 94 , training loss: 4.115476\n",
      "[INFO] Epoch: 26 , batch: 95 , training loss: 3.901927\n",
      "[INFO] Epoch: 26 , batch: 96 , training loss: 3.890862\n",
      "[INFO] Epoch: 26 , batch: 97 , training loss: 3.836123\n",
      "[INFO] Epoch: 26 , batch: 98 , training loss: 3.779852\n",
      "[INFO] Epoch: 26 , batch: 99 , training loss: 3.906287\n",
      "[INFO] Epoch: 26 , batch: 100 , training loss: 3.772155\n",
      "[INFO] Epoch: 26 , batch: 101 , training loss: 3.795710\n",
      "[INFO] Epoch: 26 , batch: 102 , training loss: 3.961065\n",
      "[INFO] Epoch: 26 , batch: 103 , training loss: 3.763261\n",
      "[INFO] Epoch: 26 , batch: 104 , training loss: 3.695403\n",
      "[INFO] Epoch: 26 , batch: 105 , training loss: 3.946679\n",
      "[INFO] Epoch: 26 , batch: 106 , training loss: 3.966507\n",
      "[INFO] Epoch: 26 , batch: 107 , training loss: 3.838962\n",
      "[INFO] Epoch: 26 , batch: 108 , training loss: 3.750117\n",
      "[INFO] Epoch: 26 , batch: 109 , training loss: 3.709322\n",
      "[INFO] Epoch: 26 , batch: 110 , training loss: 3.898134\n",
      "[INFO] Epoch: 26 , batch: 111 , training loss: 3.945590\n",
      "[INFO] Epoch: 26 , batch: 112 , training loss: 3.873034\n",
      "[INFO] Epoch: 26 , batch: 113 , training loss: 3.857055\n",
      "[INFO] Epoch: 26 , batch: 114 , training loss: 3.874424\n",
      "[INFO] Epoch: 26 , batch: 115 , training loss: 3.869863\n",
      "[INFO] Epoch: 26 , batch: 116 , training loss: 3.762266\n",
      "[INFO] Epoch: 26 , batch: 117 , training loss: 3.992792\n",
      "[INFO] Epoch: 26 , batch: 118 , training loss: 3.962142\n",
      "[INFO] Epoch: 26 , batch: 119 , training loss: 4.104918\n",
      "[INFO] Epoch: 26 , batch: 120 , training loss: 4.079519\n",
      "[INFO] Epoch: 26 , batch: 121 , training loss: 3.967108\n",
      "[INFO] Epoch: 26 , batch: 122 , training loss: 3.835287\n",
      "[INFO] Epoch: 26 , batch: 123 , training loss: 3.818746\n",
      "[INFO] Epoch: 26 , batch: 124 , training loss: 3.957963\n",
      "[INFO] Epoch: 26 , batch: 125 , training loss: 3.753491\n",
      "[INFO] Epoch: 26 , batch: 126 , training loss: 3.778624\n",
      "[INFO] Epoch: 26 , batch: 127 , training loss: 3.796724\n",
      "[INFO] Epoch: 26 , batch: 128 , training loss: 3.935050\n",
      "[INFO] Epoch: 26 , batch: 129 , training loss: 3.864546\n",
      "[INFO] Epoch: 26 , batch: 130 , training loss: 3.855697\n",
      "[INFO] Epoch: 26 , batch: 131 , training loss: 3.849817\n",
      "[INFO] Epoch: 26 , batch: 132 , training loss: 3.895506\n",
      "[INFO] Epoch: 26 , batch: 133 , training loss: 3.863854\n",
      "[INFO] Epoch: 26 , batch: 134 , training loss: 3.621594\n",
      "[INFO] Epoch: 26 , batch: 135 , training loss: 3.696794\n",
      "[INFO] Epoch: 26 , batch: 136 , training loss: 3.978245\n",
      "[INFO] Epoch: 26 , batch: 137 , training loss: 3.886704\n",
      "[INFO] Epoch: 26 , batch: 138 , training loss: 3.927217\n",
      "[INFO] Epoch: 26 , batch: 139 , training loss: 4.529023\n",
      "[INFO] Epoch: 26 , batch: 140 , training loss: 4.312932\n",
      "[INFO] Epoch: 26 , batch: 141 , training loss: 4.082659\n",
      "[INFO] Epoch: 26 , batch: 142 , training loss: 3.799210\n",
      "[INFO] Epoch: 26 , batch: 143 , training loss: 3.924895\n",
      "[INFO] Epoch: 26 , batch: 144 , training loss: 3.757244\n",
      "[INFO] Epoch: 26 , batch: 145 , training loss: 3.848663\n",
      "[INFO] Epoch: 26 , batch: 146 , training loss: 4.056282\n",
      "[INFO] Epoch: 26 , batch: 147 , training loss: 3.692312\n",
      "[INFO] Epoch: 26 , batch: 148 , training loss: 3.691007\n",
      "[INFO] Epoch: 26 , batch: 149 , training loss: 3.782783\n",
      "[INFO] Epoch: 26 , batch: 150 , training loss: 4.034291\n",
      "[INFO] Epoch: 26 , batch: 151 , training loss: 3.855255\n",
      "[INFO] Epoch: 26 , batch: 152 , training loss: 3.899760\n",
      "[INFO] Epoch: 26 , batch: 153 , training loss: 3.887916\n",
      "[INFO] Epoch: 26 , batch: 154 , training loss: 3.977900\n",
      "[INFO] Epoch: 26 , batch: 155 , training loss: 4.235191\n",
      "[INFO] Epoch: 26 , batch: 156 , training loss: 3.940657\n",
      "[INFO] Epoch: 26 , batch: 157 , training loss: 3.915085\n",
      "[INFO] Epoch: 26 , batch: 158 , training loss: 4.047126\n",
      "[INFO] Epoch: 26 , batch: 159 , training loss: 3.960315\n",
      "[INFO] Epoch: 26 , batch: 160 , training loss: 4.227539\n",
      "[INFO] Epoch: 26 , batch: 161 , training loss: 4.282279\n",
      "[INFO] Epoch: 26 , batch: 162 , training loss: 4.280061\n",
      "[INFO] Epoch: 26 , batch: 163 , training loss: 4.452587\n",
      "[INFO] Epoch: 26 , batch: 164 , training loss: 4.357658\n",
      "[INFO] Epoch: 26 , batch: 165 , training loss: 4.317267\n",
      "[INFO] Epoch: 26 , batch: 166 , training loss: 4.216311\n",
      "[INFO] Epoch: 26 , batch: 167 , training loss: 4.371454\n",
      "[INFO] Epoch: 26 , batch: 168 , training loss: 4.013987\n",
      "[INFO] Epoch: 26 , batch: 169 , training loss: 3.991807\n",
      "[INFO] Epoch: 26 , batch: 170 , training loss: 4.151583\n",
      "[INFO] Epoch: 26 , batch: 171 , training loss: 3.569602\n",
      "[INFO] Epoch: 26 , batch: 172 , training loss: 3.807960\n",
      "[INFO] Epoch: 26 , batch: 173 , training loss: 4.105083\n",
      "[INFO] Epoch: 26 , batch: 174 , training loss: 4.571874\n",
      "[INFO] Epoch: 26 , batch: 175 , training loss: 4.827245\n",
      "[INFO] Epoch: 26 , batch: 176 , training loss: 4.557547\n",
      "[INFO] Epoch: 26 , batch: 177 , training loss: 4.134554\n",
      "[INFO] Epoch: 26 , batch: 178 , training loss: 4.145224\n",
      "[INFO] Epoch: 26 , batch: 179 , training loss: 4.162602\n",
      "[INFO] Epoch: 26 , batch: 180 , training loss: 4.158032\n",
      "[INFO] Epoch: 26 , batch: 181 , training loss: 4.390228\n",
      "[INFO] Epoch: 26 , batch: 182 , training loss: 4.337740\n",
      "[INFO] Epoch: 26 , batch: 183 , training loss: 4.296926\n",
      "[INFO] Epoch: 26 , batch: 184 , training loss: 4.195543\n",
      "[INFO] Epoch: 26 , batch: 185 , training loss: 4.188854\n",
      "[INFO] Epoch: 26 , batch: 186 , training loss: 4.295258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 26 , batch: 187 , training loss: 4.395596\n",
      "[INFO] Epoch: 26 , batch: 188 , training loss: 4.390285\n",
      "[INFO] Epoch: 26 , batch: 189 , training loss: 4.316400\n",
      "[INFO] Epoch: 26 , batch: 190 , training loss: 4.341727\n",
      "[INFO] Epoch: 26 , batch: 191 , training loss: 4.440038\n",
      "[INFO] Epoch: 26 , batch: 192 , training loss: 4.273448\n",
      "[INFO] Epoch: 26 , batch: 193 , training loss: 4.393707\n",
      "[INFO] Epoch: 26 , batch: 194 , training loss: 4.325102\n",
      "[INFO] Epoch: 26 , batch: 195 , training loss: 4.265684\n",
      "[INFO] Epoch: 26 , batch: 196 , training loss: 4.100841\n",
      "[INFO] Epoch: 26 , batch: 197 , training loss: 4.192280\n",
      "[INFO] Epoch: 26 , batch: 198 , training loss: 4.123435\n",
      "[INFO] Epoch: 26 , batch: 199 , training loss: 4.243386\n",
      "[INFO] Epoch: 26 , batch: 200 , training loss: 4.145728\n",
      "[INFO] Epoch: 26 , batch: 201 , training loss: 4.054680\n",
      "[INFO] Epoch: 26 , batch: 202 , training loss: 4.055390\n",
      "[INFO] Epoch: 26 , batch: 203 , training loss: 4.176253\n",
      "[INFO] Epoch: 26 , batch: 204 , training loss: 4.251844\n",
      "[INFO] Epoch: 26 , batch: 205 , training loss: 3.867908\n",
      "[INFO] Epoch: 26 , batch: 206 , training loss: 3.772187\n",
      "[INFO] Epoch: 26 , batch: 207 , training loss: 3.781813\n",
      "[INFO] Epoch: 26 , batch: 208 , training loss: 4.102194\n",
      "[INFO] Epoch: 26 , batch: 209 , training loss: 4.059280\n",
      "[INFO] Epoch: 26 , batch: 210 , training loss: 4.087950\n",
      "[INFO] Epoch: 26 , batch: 211 , training loss: 4.072783\n",
      "[INFO] Epoch: 26 , batch: 212 , training loss: 4.168213\n",
      "[INFO] Epoch: 26 , batch: 213 , training loss: 4.144688\n",
      "[INFO] Epoch: 26 , batch: 214 , training loss: 4.204967\n",
      "[INFO] Epoch: 26 , batch: 215 , training loss: 4.419695\n",
      "[INFO] Epoch: 26 , batch: 216 , training loss: 4.147575\n",
      "[INFO] Epoch: 26 , batch: 217 , training loss: 4.056035\n",
      "[INFO] Epoch: 26 , batch: 218 , training loss: 4.047755\n",
      "[INFO] Epoch: 26 , batch: 219 , training loss: 4.169861\n",
      "[INFO] Epoch: 26 , batch: 220 , training loss: 3.992396\n",
      "[INFO] Epoch: 26 , batch: 221 , training loss: 4.011346\n",
      "[INFO] Epoch: 26 , batch: 222 , training loss: 4.139242\n",
      "[INFO] Epoch: 26 , batch: 223 , training loss: 4.249491\n",
      "[INFO] Epoch: 26 , batch: 224 , training loss: 4.275216\n",
      "[INFO] Epoch: 26 , batch: 225 , training loss: 4.170401\n",
      "[INFO] Epoch: 26 , batch: 226 , training loss: 4.282234\n",
      "[INFO] Epoch: 26 , batch: 227 , training loss: 4.260615\n",
      "[INFO] Epoch: 26 , batch: 228 , training loss: 4.288969\n",
      "[INFO] Epoch: 26 , batch: 229 , training loss: 4.174739\n",
      "[INFO] Epoch: 26 , batch: 230 , training loss: 4.023788\n",
      "[INFO] Epoch: 26 , batch: 231 , training loss: 3.890721\n",
      "[INFO] Epoch: 26 , batch: 232 , training loss: 4.038742\n",
      "[INFO] Epoch: 26 , batch: 233 , training loss: 4.056817\n",
      "[INFO] Epoch: 26 , batch: 234 , training loss: 3.750249\n",
      "[INFO] Epoch: 26 , batch: 235 , training loss: 3.850358\n",
      "[INFO] Epoch: 26 , batch: 236 , training loss: 3.954851\n",
      "[INFO] Epoch: 26 , batch: 237 , training loss: 4.179999\n",
      "[INFO] Epoch: 26 , batch: 238 , training loss: 3.946433\n",
      "[INFO] Epoch: 26 , batch: 239 , training loss: 3.995120\n",
      "[INFO] Epoch: 26 , batch: 240 , training loss: 4.050175\n",
      "[INFO] Epoch: 26 , batch: 241 , training loss: 3.845956\n",
      "[INFO] Epoch: 26 , batch: 242 , training loss: 3.862201\n",
      "[INFO] Epoch: 26 , batch: 243 , training loss: 4.141468\n",
      "[INFO] Epoch: 26 , batch: 244 , training loss: 4.116104\n",
      "[INFO] Epoch: 26 , batch: 245 , training loss: 4.074000\n",
      "[INFO] Epoch: 26 , batch: 246 , training loss: 3.772563\n",
      "[INFO] Epoch: 26 , batch: 247 , training loss: 3.940705\n",
      "[INFO] Epoch: 26 , batch: 248 , training loss: 4.001517\n",
      "[INFO] Epoch: 26 , batch: 249 , training loss: 3.987970\n",
      "[INFO] Epoch: 26 , batch: 250 , training loss: 3.797858\n",
      "[INFO] Epoch: 26 , batch: 251 , training loss: 4.241217\n",
      "[INFO] Epoch: 26 , batch: 252 , training loss: 3.954402\n",
      "[INFO] Epoch: 26 , batch: 253 , training loss: 3.882849\n",
      "[INFO] Epoch: 26 , batch: 254 , training loss: 4.144934\n",
      "[INFO] Epoch: 26 , batch: 255 , training loss: 4.096603\n",
      "[INFO] Epoch: 26 , batch: 256 , training loss: 4.106309\n",
      "[INFO] Epoch: 26 , batch: 257 , training loss: 4.258699\n",
      "[INFO] Epoch: 26 , batch: 258 , training loss: 4.277762\n",
      "[INFO] Epoch: 26 , batch: 259 , training loss: 4.313823\n",
      "[INFO] Epoch: 26 , batch: 260 , training loss: 4.084562\n",
      "[INFO] Epoch: 26 , batch: 261 , training loss: 4.241139\n",
      "[INFO] Epoch: 26 , batch: 262 , training loss: 4.425935\n",
      "[INFO] Epoch: 26 , batch: 263 , training loss: 4.566048\n",
      "[INFO] Epoch: 26 , batch: 264 , training loss: 3.945440\n",
      "[INFO] Epoch: 26 , batch: 265 , training loss: 4.024360\n",
      "[INFO] Epoch: 26 , batch: 266 , training loss: 4.460403\n",
      "[INFO] Epoch: 26 , batch: 267 , training loss: 4.188686\n",
      "[INFO] Epoch: 26 , batch: 268 , training loss: 4.130555\n",
      "[INFO] Epoch: 26 , batch: 269 , training loss: 4.109949\n",
      "[INFO] Epoch: 26 , batch: 270 , training loss: 4.119812\n",
      "[INFO] Epoch: 26 , batch: 271 , training loss: 4.162495\n",
      "[INFO] Epoch: 26 , batch: 272 , training loss: 4.150192\n",
      "[INFO] Epoch: 26 , batch: 273 , training loss: 4.150970\n",
      "[INFO] Epoch: 26 , batch: 274 , training loss: 4.235863\n",
      "[INFO] Epoch: 26 , batch: 275 , training loss: 4.116216\n",
      "[INFO] Epoch: 26 , batch: 276 , training loss: 4.191092\n",
      "[INFO] Epoch: 26 , batch: 277 , training loss: 4.344551\n",
      "[INFO] Epoch: 26 , batch: 278 , training loss: 4.003940\n",
      "[INFO] Epoch: 26 , batch: 279 , training loss: 4.024178\n",
      "[INFO] Epoch: 26 , batch: 280 , training loss: 3.988986\n",
      "[INFO] Epoch: 26 , batch: 281 , training loss: 4.110717\n",
      "[INFO] Epoch: 26 , batch: 282 , training loss: 3.998048\n",
      "[INFO] Epoch: 26 , batch: 283 , training loss: 4.028156\n",
      "[INFO] Epoch: 26 , batch: 284 , training loss: 4.067542\n",
      "[INFO] Epoch: 26 , batch: 285 , training loss: 4.015628\n",
      "[INFO] Epoch: 26 , batch: 286 , training loss: 4.008901\n",
      "[INFO] Epoch: 26 , batch: 287 , training loss: 3.940034\n",
      "[INFO] Epoch: 26 , batch: 288 , training loss: 3.916088\n",
      "[INFO] Epoch: 26 , batch: 289 , training loss: 3.994072\n",
      "[INFO] Epoch: 26 , batch: 290 , training loss: 3.759217\n",
      "[INFO] Epoch: 26 , batch: 291 , training loss: 3.741765\n",
      "[INFO] Epoch: 26 , batch: 292 , training loss: 3.876727\n",
      "[INFO] Epoch: 26 , batch: 293 , training loss: 3.783475\n",
      "[INFO] Epoch: 26 , batch: 294 , training loss: 4.459469\n",
      "[INFO] Epoch: 26 , batch: 295 , training loss: 4.222785\n",
      "[INFO] Epoch: 26 , batch: 296 , training loss: 4.172852\n",
      "[INFO] Epoch: 26 , batch: 297 , training loss: 4.119369\n",
      "[INFO] Epoch: 26 , batch: 298 , training loss: 3.947816\n",
      "[INFO] Epoch: 26 , batch: 299 , training loss: 4.002472\n",
      "[INFO] Epoch: 26 , batch: 300 , training loss: 3.980256\n",
      "[INFO] Epoch: 26 , batch: 301 , training loss: 3.905695\n",
      "[INFO] Epoch: 26 , batch: 302 , training loss: 4.068640\n",
      "[INFO] Epoch: 26 , batch: 303 , training loss: 4.076887\n",
      "[INFO] Epoch: 26 , batch: 304 , training loss: 4.227735\n",
      "[INFO] Epoch: 26 , batch: 305 , training loss: 4.039517\n",
      "[INFO] Epoch: 26 , batch: 306 , training loss: 4.155677\n",
      "[INFO] Epoch: 26 , batch: 307 , training loss: 4.175999\n",
      "[INFO] Epoch: 26 , batch: 308 , training loss: 3.982357\n",
      "[INFO] Epoch: 26 , batch: 309 , training loss: 3.991585\n",
      "[INFO] Epoch: 26 , batch: 310 , training loss: 3.924307\n",
      "[INFO] Epoch: 26 , batch: 311 , training loss: 3.908273\n",
      "[INFO] Epoch: 26 , batch: 312 , training loss: 3.835681\n",
      "[INFO] Epoch: 26 , batch: 313 , training loss: 3.899741\n",
      "[INFO] Epoch: 26 , batch: 314 , training loss: 4.000041\n",
      "[INFO] Epoch: 26 , batch: 315 , training loss: 4.069391\n",
      "[INFO] Epoch: 26 , batch: 316 , training loss: 4.330706\n",
      "[INFO] Epoch: 26 , batch: 317 , training loss: 4.696580\n",
      "[INFO] Epoch: 26 , batch: 318 , training loss: 4.800748\n",
      "[INFO] Epoch: 26 , batch: 319 , training loss: 4.507405\n",
      "[INFO] Epoch: 26 , batch: 320 , training loss: 4.042185\n",
      "[INFO] Epoch: 26 , batch: 321 , training loss: 3.849806\n",
      "[INFO] Epoch: 26 , batch: 322 , training loss: 3.954110\n",
      "[INFO] Epoch: 26 , batch: 323 , training loss: 3.981795\n",
      "[INFO] Epoch: 26 , batch: 324 , training loss: 3.967859\n",
      "[INFO] Epoch: 26 , batch: 325 , training loss: 4.100498\n",
      "[INFO] Epoch: 26 , batch: 326 , training loss: 4.147762\n",
      "[INFO] Epoch: 26 , batch: 327 , training loss: 4.072767\n",
      "[INFO] Epoch: 26 , batch: 328 , training loss: 4.080836\n",
      "[INFO] Epoch: 26 , batch: 329 , training loss: 3.976605\n",
      "[INFO] Epoch: 26 , batch: 330 , training loss: 3.979489\n",
      "[INFO] Epoch: 26 , batch: 331 , training loss: 4.126776\n",
      "[INFO] Epoch: 26 , batch: 332 , training loss: 3.989773\n",
      "[INFO] Epoch: 26 , batch: 333 , training loss: 3.951024\n",
      "[INFO] Epoch: 26 , batch: 334 , training loss: 3.964541\n",
      "[INFO] Epoch: 26 , batch: 335 , training loss: 4.086739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 26 , batch: 336 , training loss: 4.094990\n",
      "[INFO] Epoch: 26 , batch: 337 , training loss: 4.134779\n",
      "[INFO] Epoch: 26 , batch: 338 , training loss: 4.361238\n",
      "[INFO] Epoch: 26 , batch: 339 , training loss: 4.163194\n",
      "[INFO] Epoch: 26 , batch: 340 , training loss: 4.345295\n",
      "[INFO] Epoch: 26 , batch: 341 , training loss: 4.107158\n",
      "[INFO] Epoch: 26 , batch: 342 , training loss: 3.891339\n",
      "[INFO] Epoch: 26 , batch: 343 , training loss: 3.962852\n",
      "[INFO] Epoch: 26 , batch: 344 , training loss: 3.826101\n",
      "[INFO] Epoch: 26 , batch: 345 , training loss: 3.975273\n",
      "[INFO] Epoch: 26 , batch: 346 , training loss: 4.004902\n",
      "[INFO] Epoch: 26 , batch: 347 , training loss: 3.920117\n",
      "[INFO] Epoch: 26 , batch: 348 , training loss: 4.023603\n",
      "[INFO] Epoch: 26 , batch: 349 , training loss: 4.125698\n",
      "[INFO] Epoch: 26 , batch: 350 , training loss: 3.975901\n",
      "[INFO] Epoch: 26 , batch: 351 , training loss: 4.056698\n",
      "[INFO] Epoch: 26 , batch: 352 , training loss: 4.082647\n",
      "[INFO] Epoch: 26 , batch: 353 , training loss: 4.047173\n",
      "[INFO] Epoch: 26 , batch: 354 , training loss: 4.144889\n",
      "[INFO] Epoch: 26 , batch: 355 , training loss: 4.123470\n",
      "[INFO] Epoch: 26 , batch: 356 , training loss: 4.005575\n",
      "[INFO] Epoch: 26 , batch: 357 , training loss: 4.089346\n",
      "[INFO] Epoch: 26 , batch: 358 , training loss: 3.989130\n",
      "[INFO] Epoch: 26 , batch: 359 , training loss: 3.978833\n",
      "[INFO] Epoch: 26 , batch: 360 , training loss: 4.101629\n",
      "[INFO] Epoch: 26 , batch: 361 , training loss: 4.060582\n",
      "[INFO] Epoch: 26 , batch: 362 , training loss: 4.164908\n",
      "[INFO] Epoch: 26 , batch: 363 , training loss: 4.045143\n",
      "[INFO] Epoch: 26 , batch: 364 , training loss: 4.114592\n",
      "[INFO] Epoch: 26 , batch: 365 , training loss: 4.005835\n",
      "[INFO] Epoch: 26 , batch: 366 , training loss: 4.112844\n",
      "[INFO] Epoch: 26 , batch: 367 , training loss: 4.162393\n",
      "[INFO] Epoch: 26 , batch: 368 , training loss: 4.575617\n",
      "[INFO] Epoch: 26 , batch: 369 , training loss: 4.259068\n",
      "[INFO] Epoch: 26 , batch: 370 , training loss: 4.024934\n",
      "[INFO] Epoch: 26 , batch: 371 , training loss: 4.458792\n",
      "[INFO] Epoch: 26 , batch: 372 , training loss: 4.710907\n",
      "[INFO] Epoch: 26 , batch: 373 , training loss: 4.777508\n",
      "[INFO] Epoch: 26 , batch: 374 , training loss: 4.853747\n",
      "[INFO] Epoch: 26 , batch: 375 , training loss: 4.853684\n",
      "[INFO] Epoch: 26 , batch: 376 , training loss: 4.719717\n",
      "[INFO] Epoch: 26 , batch: 377 , training loss: 4.476689\n",
      "[INFO] Epoch: 26 , batch: 378 , training loss: 4.555429\n",
      "[INFO] Epoch: 26 , batch: 379 , training loss: 4.547090\n",
      "[INFO] Epoch: 26 , batch: 380 , training loss: 4.688631\n",
      "[INFO] Epoch: 26 , batch: 381 , training loss: 4.390159\n",
      "[INFO] Epoch: 26 , batch: 382 , training loss: 4.693152\n",
      "[INFO] Epoch: 26 , batch: 383 , training loss: 4.713288\n",
      "[INFO] Epoch: 26 , batch: 384 , training loss: 4.690593\n",
      "[INFO] Epoch: 26 , batch: 385 , training loss: 4.372252\n",
      "[INFO] Epoch: 26 , batch: 386 , training loss: 4.627630\n",
      "[INFO] Epoch: 26 , batch: 387 , training loss: 4.600406\n",
      "[INFO] Epoch: 26 , batch: 388 , training loss: 4.406069\n",
      "[INFO] Epoch: 26 , batch: 389 , training loss: 4.203231\n",
      "[INFO] Epoch: 26 , batch: 390 , training loss: 4.260711\n",
      "[INFO] Epoch: 26 , batch: 391 , training loss: 4.256805\n",
      "[INFO] Epoch: 26 , batch: 392 , training loss: 4.624050\n",
      "[INFO] Epoch: 26 , batch: 393 , training loss: 4.524955\n",
      "[INFO] Epoch: 26 , batch: 394 , training loss: 4.586117\n",
      "[INFO] Epoch: 26 , batch: 395 , training loss: 4.435333\n",
      "[INFO] Epoch: 26 , batch: 396 , training loss: 4.246676\n",
      "[INFO] Epoch: 26 , batch: 397 , training loss: 4.391350\n",
      "[INFO] Epoch: 26 , batch: 398 , training loss: 4.241744\n",
      "[INFO] Epoch: 26 , batch: 399 , training loss: 4.329198\n",
      "[INFO] Epoch: 26 , batch: 400 , training loss: 4.295991\n",
      "[INFO] Epoch: 26 , batch: 401 , training loss: 4.746485\n",
      "[INFO] Epoch: 26 , batch: 402 , training loss: 4.446645\n",
      "[INFO] Epoch: 26 , batch: 403 , training loss: 4.288610\n",
      "[INFO] Epoch: 26 , batch: 404 , training loss: 4.438167\n",
      "[INFO] Epoch: 26 , batch: 405 , training loss: 4.504444\n",
      "[INFO] Epoch: 26 , batch: 406 , training loss: 4.414741\n",
      "[INFO] Epoch: 26 , batch: 407 , training loss: 4.467379\n",
      "[INFO] Epoch: 26 , batch: 408 , training loss: 4.405308\n",
      "[INFO] Epoch: 26 , batch: 409 , training loss: 4.429100\n",
      "[INFO] Epoch: 26 , batch: 410 , training loss: 4.492607\n",
      "[INFO] Epoch: 26 , batch: 411 , training loss: 4.652699\n",
      "[INFO] Epoch: 26 , batch: 412 , training loss: 4.488735\n",
      "[INFO] Epoch: 26 , batch: 413 , training loss: 4.362562\n",
      "[INFO] Epoch: 26 , batch: 414 , training loss: 4.383034\n",
      "[INFO] Epoch: 26 , batch: 415 , training loss: 4.428833\n",
      "[INFO] Epoch: 26 , batch: 416 , training loss: 4.519210\n",
      "[INFO] Epoch: 26 , batch: 417 , training loss: 4.400444\n",
      "[INFO] Epoch: 26 , batch: 418 , training loss: 4.456943\n",
      "[INFO] Epoch: 26 , batch: 419 , training loss: 4.445242\n",
      "[INFO] Epoch: 26 , batch: 420 , training loss: 4.411405\n",
      "[INFO] Epoch: 26 , batch: 421 , training loss: 4.386879\n",
      "[INFO] Epoch: 26 , batch: 422 , training loss: 4.225600\n",
      "[INFO] Epoch: 26 , batch: 423 , training loss: 4.444411\n",
      "[INFO] Epoch: 26 , batch: 424 , training loss: 4.632288\n",
      "[INFO] Epoch: 26 , batch: 425 , training loss: 4.487053\n",
      "[INFO] Epoch: 26 , batch: 426 , training loss: 4.211855\n",
      "[INFO] Epoch: 26 , batch: 427 , training loss: 4.471261\n",
      "[INFO] Epoch: 26 , batch: 428 , training loss: 4.354265\n",
      "[INFO] Epoch: 26 , batch: 429 , training loss: 4.235335\n",
      "[INFO] Epoch: 26 , batch: 430 , training loss: 4.475360\n",
      "[INFO] Epoch: 26 , batch: 431 , training loss: 4.066960\n",
      "[INFO] Epoch: 26 , batch: 432 , training loss: 4.122734\n",
      "[INFO] Epoch: 26 , batch: 433 , training loss: 4.154093\n",
      "[INFO] Epoch: 26 , batch: 434 , training loss: 4.039941\n",
      "[INFO] Epoch: 26 , batch: 435 , training loss: 4.399718\n",
      "[INFO] Epoch: 26 , batch: 436 , training loss: 4.446773\n",
      "[INFO] Epoch: 26 , batch: 437 , training loss: 4.210831\n",
      "[INFO] Epoch: 26 , batch: 438 , training loss: 4.083720\n",
      "[INFO] Epoch: 26 , batch: 439 , training loss: 4.314131\n",
      "[INFO] Epoch: 26 , batch: 440 , training loss: 4.443710\n",
      "[INFO] Epoch: 26 , batch: 441 , training loss: 4.535097\n",
      "[INFO] Epoch: 26 , batch: 442 , training loss: 4.288534\n",
      "[INFO] Epoch: 26 , batch: 443 , training loss: 4.501490\n",
      "[INFO] Epoch: 26 , batch: 444 , training loss: 4.084837\n",
      "[INFO] Epoch: 26 , batch: 445 , training loss: 3.991941\n",
      "[INFO] Epoch: 26 , batch: 446 , training loss: 3.926008\n",
      "[INFO] Epoch: 26 , batch: 447 , training loss: 4.129708\n",
      "[INFO] Epoch: 26 , batch: 448 , training loss: 4.225689\n",
      "[INFO] Epoch: 26 , batch: 449 , training loss: 4.624650\n",
      "[INFO] Epoch: 26 , batch: 450 , training loss: 4.685580\n",
      "[INFO] Epoch: 26 , batch: 451 , training loss: 4.576408\n",
      "[INFO] Epoch: 26 , batch: 452 , training loss: 4.395126\n",
      "[INFO] Epoch: 26 , batch: 453 , training loss: 4.175927\n",
      "[INFO] Epoch: 26 , batch: 454 , training loss: 4.317076\n",
      "[INFO] Epoch: 26 , batch: 455 , training loss: 4.376790\n",
      "[INFO] Epoch: 26 , batch: 456 , training loss: 4.355202\n",
      "[INFO] Epoch: 26 , batch: 457 , training loss: 4.443710\n",
      "[INFO] Epoch: 26 , batch: 458 , training loss: 4.189401\n",
      "[INFO] Epoch: 26 , batch: 459 , training loss: 4.146978\n",
      "[INFO] Epoch: 26 , batch: 460 , training loss: 4.276806\n",
      "[INFO] Epoch: 26 , batch: 461 , training loss: 4.236216\n",
      "[INFO] Epoch: 26 , batch: 462 , training loss: 4.284799\n",
      "[INFO] Epoch: 26 , batch: 463 , training loss: 4.211998\n",
      "[INFO] Epoch: 26 , batch: 464 , training loss: 4.386704\n",
      "[INFO] Epoch: 26 , batch: 465 , training loss: 4.337641\n",
      "[INFO] Epoch: 26 , batch: 466 , training loss: 4.431108\n",
      "[INFO] Epoch: 26 , batch: 467 , training loss: 4.382576\n",
      "[INFO] Epoch: 26 , batch: 468 , training loss: 4.358874\n",
      "[INFO] Epoch: 26 , batch: 469 , training loss: 4.383232\n",
      "[INFO] Epoch: 26 , batch: 470 , training loss: 4.192530\n",
      "[INFO] Epoch: 26 , batch: 471 , training loss: 4.319690\n",
      "[INFO] Epoch: 26 , batch: 472 , training loss: 4.356920\n",
      "[INFO] Epoch: 26 , batch: 473 , training loss: 4.254547\n",
      "[INFO] Epoch: 26 , batch: 474 , training loss: 4.060695\n",
      "[INFO] Epoch: 26 , batch: 475 , training loss: 3.938582\n",
      "[INFO] Epoch: 26 , batch: 476 , training loss: 4.347379\n",
      "[INFO] Epoch: 26 , batch: 477 , training loss: 4.459812\n",
      "[INFO] Epoch: 26 , batch: 478 , training loss: 4.461513\n",
      "[INFO] Epoch: 26 , batch: 479 , training loss: 4.428740\n",
      "[INFO] Epoch: 26 , batch: 480 , training loss: 4.548615\n",
      "[INFO] Epoch: 26 , batch: 481 , training loss: 4.448794\n",
      "[INFO] Epoch: 26 , batch: 482 , training loss: 4.525359\n",
      "[INFO] Epoch: 26 , batch: 483 , training loss: 4.378117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 26 , batch: 484 , training loss: 4.175187\n",
      "[INFO] Epoch: 26 , batch: 485 , training loss: 4.279009\n",
      "[INFO] Epoch: 26 , batch: 486 , training loss: 4.182415\n",
      "[INFO] Epoch: 26 , batch: 487 , training loss: 4.158677\n",
      "[INFO] Epoch: 26 , batch: 488 , training loss: 4.334318\n",
      "[INFO] Epoch: 26 , batch: 489 , training loss: 4.250176\n",
      "[INFO] Epoch: 26 , batch: 490 , training loss: 4.303641\n",
      "[INFO] Epoch: 26 , batch: 491 , training loss: 4.248015\n",
      "[INFO] Epoch: 26 , batch: 492 , training loss: 4.190805\n",
      "[INFO] Epoch: 26 , batch: 493 , training loss: 4.364140\n",
      "[INFO] Epoch: 26 , batch: 494 , training loss: 4.260902\n",
      "[INFO] Epoch: 26 , batch: 495 , training loss: 4.425450\n",
      "[INFO] Epoch: 26 , batch: 496 , training loss: 4.304928\n",
      "[INFO] Epoch: 26 , batch: 497 , training loss: 4.350208\n",
      "[INFO] Epoch: 26 , batch: 498 , training loss: 4.327086\n",
      "[INFO] Epoch: 26 , batch: 499 , training loss: 4.402107\n",
      "[INFO] Epoch: 26 , batch: 500 , training loss: 4.559712\n",
      "[INFO] Epoch: 26 , batch: 501 , training loss: 4.880923\n",
      "[INFO] Epoch: 26 , batch: 502 , training loss: 4.926306\n",
      "[INFO] Epoch: 26 , batch: 503 , training loss: 4.580357\n",
      "[INFO] Epoch: 26 , batch: 504 , training loss: 4.711513\n",
      "[INFO] Epoch: 26 , batch: 505 , training loss: 4.679823\n",
      "[INFO] Epoch: 26 , batch: 506 , training loss: 4.641036\n",
      "[INFO] Epoch: 26 , batch: 507 , training loss: 4.708874\n",
      "[INFO] Epoch: 26 , batch: 508 , training loss: 4.612858\n",
      "[INFO] Epoch: 26 , batch: 509 , training loss: 4.434364\n",
      "[INFO] Epoch: 26 , batch: 510 , training loss: 4.498672\n",
      "[INFO] Epoch: 26 , batch: 511 , training loss: 4.427068\n",
      "[INFO] Epoch: 26 , batch: 512 , training loss: 4.520220\n",
      "[INFO] Epoch: 26 , batch: 513 , training loss: 4.768224\n",
      "[INFO] Epoch: 26 , batch: 514 , training loss: 4.408139\n",
      "[INFO] Epoch: 26 , batch: 515 , training loss: 4.677577\n",
      "[INFO] Epoch: 26 , batch: 516 , training loss: 4.454433\n",
      "[INFO] Epoch: 26 , batch: 517 , training loss: 4.417021\n",
      "[INFO] Epoch: 26 , batch: 518 , training loss: 4.386825\n",
      "[INFO] Epoch: 26 , batch: 519 , training loss: 4.238018\n",
      "[INFO] Epoch: 26 , batch: 520 , training loss: 4.473099\n",
      "[INFO] Epoch: 26 , batch: 521 , training loss: 4.460703\n",
      "[INFO] Epoch: 26 , batch: 522 , training loss: 4.540928\n",
      "[INFO] Epoch: 26 , batch: 523 , training loss: 4.439320\n",
      "[INFO] Epoch: 26 , batch: 524 , training loss: 4.728694\n",
      "[INFO] Epoch: 26 , batch: 525 , training loss: 4.609931\n",
      "[INFO] Epoch: 26 , batch: 526 , training loss: 4.409492\n",
      "[INFO] Epoch: 26 , batch: 527 , training loss: 4.450823\n",
      "[INFO] Epoch: 26 , batch: 528 , training loss: 4.440987\n",
      "[INFO] Epoch: 26 , batch: 529 , training loss: 4.421089\n",
      "[INFO] Epoch: 26 , batch: 530 , training loss: 4.265018\n",
      "[INFO] Epoch: 26 , batch: 531 , training loss: 4.419466\n",
      "[INFO] Epoch: 26 , batch: 532 , training loss: 4.324685\n",
      "[INFO] Epoch: 26 , batch: 533 , training loss: 4.468593\n",
      "[INFO] Epoch: 26 , batch: 534 , training loss: 4.459935\n",
      "[INFO] Epoch: 26 , batch: 535 , training loss: 4.474953\n",
      "[INFO] Epoch: 26 , batch: 536 , training loss: 4.325649\n",
      "[INFO] Epoch: 26 , batch: 537 , training loss: 4.291263\n",
      "[INFO] Epoch: 26 , batch: 538 , training loss: 4.367981\n",
      "[INFO] Epoch: 26 , batch: 539 , training loss: 4.486738\n",
      "[INFO] Epoch: 26 , batch: 540 , training loss: 5.015973\n",
      "[INFO] Epoch: 26 , batch: 541 , training loss: 4.860617\n",
      "[INFO] Epoch: 26 , batch: 542 , training loss: 4.712377\n",
      "[INFO] Epoch: 27 , batch: 0 , training loss: 3.758994\n",
      "[INFO] Epoch: 27 , batch: 1 , training loss: 3.609537\n",
      "[INFO] Epoch: 27 , batch: 2 , training loss: 3.766934\n",
      "[INFO] Epoch: 27 , batch: 3 , training loss: 3.612863\n",
      "[INFO] Epoch: 27 , batch: 4 , training loss: 3.916499\n",
      "[INFO] Epoch: 27 , batch: 5 , training loss: 3.612966\n",
      "[INFO] Epoch: 27 , batch: 6 , training loss: 3.973498\n",
      "[INFO] Epoch: 27 , batch: 7 , training loss: 3.908962\n",
      "[INFO] Epoch: 27 , batch: 8 , training loss: 3.562912\n",
      "[INFO] Epoch: 27 , batch: 9 , training loss: 3.822335\n",
      "[INFO] Epoch: 27 , batch: 10 , training loss: 3.755985\n",
      "[INFO] Epoch: 27 , batch: 11 , training loss: 3.705931\n",
      "[INFO] Epoch: 27 , batch: 12 , training loss: 3.587965\n",
      "[INFO] Epoch: 27 , batch: 13 , training loss: 3.631415\n",
      "[INFO] Epoch: 27 , batch: 14 , training loss: 3.525351\n",
      "[INFO] Epoch: 27 , batch: 15 , training loss: 3.746533\n",
      "[INFO] Epoch: 27 , batch: 16 , training loss: 3.607368\n",
      "[INFO] Epoch: 27 , batch: 17 , training loss: 3.718886\n",
      "[INFO] Epoch: 27 , batch: 18 , training loss: 3.664264\n",
      "[INFO] Epoch: 27 , batch: 19 , training loss: 3.429991\n",
      "[INFO] Epoch: 27 , batch: 20 , training loss: 3.415760\n",
      "[INFO] Epoch: 27 , batch: 21 , training loss: 3.541856\n",
      "[INFO] Epoch: 27 , batch: 22 , training loss: 3.471778\n",
      "[INFO] Epoch: 27 , batch: 23 , training loss: 3.642024\n",
      "[INFO] Epoch: 27 , batch: 24 , training loss: 3.491701\n",
      "[INFO] Epoch: 27 , batch: 25 , training loss: 3.615222\n",
      "[INFO] Epoch: 27 , batch: 26 , training loss: 3.456570\n",
      "[INFO] Epoch: 27 , batch: 27 , training loss: 3.447620\n",
      "[INFO] Epoch: 27 , batch: 28 , training loss: 3.648010\n",
      "[INFO] Epoch: 27 , batch: 29 , training loss: 3.458786\n",
      "[INFO] Epoch: 27 , batch: 30 , training loss: 3.486696\n",
      "[INFO] Epoch: 27 , batch: 31 , training loss: 3.585983\n",
      "[INFO] Epoch: 27 , batch: 32 , training loss: 3.553231\n",
      "[INFO] Epoch: 27 , batch: 33 , training loss: 3.573559\n",
      "[INFO] Epoch: 27 , batch: 34 , training loss: 3.581764\n",
      "[INFO] Epoch: 27 , batch: 35 , training loss: 3.540466\n",
      "[INFO] Epoch: 27 , batch: 36 , training loss: 3.626483\n",
      "[INFO] Epoch: 27 , batch: 37 , training loss: 3.476471\n",
      "[INFO] Epoch: 27 , batch: 38 , training loss: 3.569895\n",
      "[INFO] Epoch: 27 , batch: 39 , training loss: 3.373838\n",
      "[INFO] Epoch: 27 , batch: 40 , training loss: 3.564965\n",
      "[INFO] Epoch: 27 , batch: 41 , training loss: 3.535112\n",
      "[INFO] Epoch: 27 , batch: 42 , training loss: 3.958869\n",
      "[INFO] Epoch: 27 , batch: 43 , training loss: 3.684691\n",
      "[INFO] Epoch: 27 , batch: 44 , training loss: 4.030289\n",
      "[INFO] Epoch: 27 , batch: 45 , training loss: 3.936540\n",
      "[INFO] Epoch: 27 , batch: 46 , training loss: 3.938701\n",
      "[INFO] Epoch: 27 , batch: 47 , training loss: 3.621275\n",
      "[INFO] Epoch: 27 , batch: 48 , training loss: 3.575085\n",
      "[INFO] Epoch: 27 , batch: 49 , training loss: 3.859655\n",
      "[INFO] Epoch: 27 , batch: 50 , training loss: 3.581737\n",
      "[INFO] Epoch: 27 , batch: 51 , training loss: 3.839603\n",
      "[INFO] Epoch: 27 , batch: 52 , training loss: 3.685114\n",
      "[INFO] Epoch: 27 , batch: 53 , training loss: 3.767347\n",
      "[INFO] Epoch: 27 , batch: 54 , training loss: 3.784807\n",
      "[INFO] Epoch: 27 , batch: 55 , training loss: 3.883619\n",
      "[INFO] Epoch: 27 , batch: 56 , training loss: 3.693913\n",
      "[INFO] Epoch: 27 , batch: 57 , training loss: 3.612060\n",
      "[INFO] Epoch: 27 , batch: 58 , training loss: 3.681127\n",
      "[INFO] Epoch: 27 , batch: 59 , training loss: 3.768891\n",
      "[INFO] Epoch: 27 , batch: 60 , training loss: 3.741583\n",
      "[INFO] Epoch: 27 , batch: 61 , training loss: 3.787051\n",
      "[INFO] Epoch: 27 , batch: 62 , training loss: 3.651129\n",
      "[INFO] Epoch: 27 , batch: 63 , training loss: 3.826705\n",
      "[INFO] Epoch: 27 , batch: 64 , training loss: 4.076649\n",
      "[INFO] Epoch: 27 , batch: 65 , training loss: 3.717574\n",
      "[INFO] Epoch: 27 , batch: 66 , training loss: 3.618982\n",
      "[INFO] Epoch: 27 , batch: 67 , training loss: 3.626295\n",
      "[INFO] Epoch: 27 , batch: 68 , training loss: 3.833268\n",
      "[INFO] Epoch: 27 , batch: 69 , training loss: 3.747456\n",
      "[INFO] Epoch: 27 , batch: 70 , training loss: 3.978218\n",
      "[INFO] Epoch: 27 , batch: 71 , training loss: 3.799463\n",
      "[INFO] Epoch: 27 , batch: 72 , training loss: 3.881751\n",
      "[INFO] Epoch: 27 , batch: 73 , training loss: 3.808939\n",
      "[INFO] Epoch: 27 , batch: 74 , training loss: 3.904449\n",
      "[INFO] Epoch: 27 , batch: 75 , training loss: 3.795544\n",
      "[INFO] Epoch: 27 , batch: 76 , training loss: 3.878207\n",
      "[INFO] Epoch: 27 , batch: 77 , training loss: 3.831161\n",
      "[INFO] Epoch: 27 , batch: 78 , training loss: 3.917176\n",
      "[INFO] Epoch: 27 , batch: 79 , training loss: 3.779747\n",
      "[INFO] Epoch: 27 , batch: 80 , training loss: 3.960511\n",
      "[INFO] Epoch: 27 , batch: 81 , training loss: 3.897921\n",
      "[INFO] Epoch: 27 , batch: 82 , training loss: 3.881529\n",
      "[INFO] Epoch: 27 , batch: 83 , training loss: 3.989353\n",
      "[INFO] Epoch: 27 , batch: 84 , training loss: 3.893913\n",
      "[INFO] Epoch: 27 , batch: 85 , training loss: 4.033983\n",
      "[INFO] Epoch: 27 , batch: 86 , training loss: 3.950197\n",
      "[INFO] Epoch: 27 , batch: 87 , training loss: 3.891050\n",
      "[INFO] Epoch: 27 , batch: 88 , training loss: 4.045663\n",
      "[INFO] Epoch: 27 , batch: 89 , training loss: 3.847771\n",
      "[INFO] Epoch: 27 , batch: 90 , training loss: 3.939418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 27 , batch: 91 , training loss: 3.874388\n",
      "[INFO] Epoch: 27 , batch: 92 , training loss: 3.892311\n",
      "[INFO] Epoch: 27 , batch: 93 , training loss: 3.972687\n",
      "[INFO] Epoch: 27 , batch: 94 , training loss: 4.107631\n",
      "[INFO] Epoch: 27 , batch: 95 , training loss: 3.894866\n",
      "[INFO] Epoch: 27 , batch: 96 , training loss: 3.858117\n",
      "[INFO] Epoch: 27 , batch: 97 , training loss: 3.805985\n",
      "[INFO] Epoch: 27 , batch: 98 , training loss: 3.766831\n",
      "[INFO] Epoch: 27 , batch: 99 , training loss: 3.906191\n",
      "[INFO] Epoch: 27 , batch: 100 , training loss: 3.764836\n",
      "[INFO] Epoch: 27 , batch: 101 , training loss: 3.790497\n",
      "[INFO] Epoch: 27 , batch: 102 , training loss: 3.932867\n",
      "[INFO] Epoch: 27 , batch: 103 , training loss: 3.735064\n",
      "[INFO] Epoch: 27 , batch: 104 , training loss: 3.668921\n",
      "[INFO] Epoch: 27 , batch: 105 , training loss: 3.946862\n",
      "[INFO] Epoch: 27 , batch: 106 , training loss: 3.940094\n",
      "[INFO] Epoch: 27 , batch: 107 , training loss: 3.807212\n",
      "[INFO] Epoch: 27 , batch: 108 , training loss: 3.735431\n",
      "[INFO] Epoch: 27 , batch: 109 , training loss: 3.700860\n",
      "[INFO] Epoch: 27 , batch: 110 , training loss: 3.851628\n",
      "[INFO] Epoch: 27 , batch: 111 , training loss: 3.944759\n",
      "[INFO] Epoch: 27 , batch: 112 , training loss: 3.846739\n",
      "[INFO] Epoch: 27 , batch: 113 , training loss: 3.847700\n",
      "[INFO] Epoch: 27 , batch: 114 , training loss: 3.852603\n",
      "[INFO] Epoch: 27 , batch: 115 , training loss: 3.843974\n",
      "[INFO] Epoch: 27 , batch: 116 , training loss: 3.754065\n",
      "[INFO] Epoch: 27 , batch: 117 , training loss: 3.976862\n",
      "[INFO] Epoch: 27 , batch: 118 , training loss: 3.932372\n",
      "[INFO] Epoch: 27 , batch: 119 , training loss: 4.092625\n",
      "[INFO] Epoch: 27 , batch: 120 , training loss: 4.059992\n",
      "[INFO] Epoch: 27 , batch: 121 , training loss: 3.956708\n",
      "[INFO] Epoch: 27 , batch: 122 , training loss: 3.821664\n",
      "[INFO] Epoch: 27 , batch: 123 , training loss: 3.823617\n",
      "[INFO] Epoch: 27 , batch: 124 , training loss: 3.935660\n",
      "[INFO] Epoch: 27 , batch: 125 , training loss: 3.749320\n",
      "[INFO] Epoch: 27 , batch: 126 , training loss: 3.771345\n",
      "[INFO] Epoch: 27 , batch: 127 , training loss: 3.775547\n",
      "[INFO] Epoch: 27 , batch: 128 , training loss: 3.934071\n",
      "[INFO] Epoch: 27 , batch: 129 , training loss: 3.872750\n",
      "[INFO] Epoch: 27 , batch: 130 , training loss: 3.849400\n",
      "[INFO] Epoch: 27 , batch: 131 , training loss: 3.860695\n",
      "[INFO] Epoch: 27 , batch: 132 , training loss: 3.871907\n",
      "[INFO] Epoch: 27 , batch: 133 , training loss: 3.858153\n",
      "[INFO] Epoch: 27 , batch: 134 , training loss: 3.618054\n",
      "[INFO] Epoch: 27 , batch: 135 , training loss: 3.677810\n",
      "[INFO] Epoch: 27 , batch: 136 , training loss: 3.995146\n",
      "[INFO] Epoch: 27 , batch: 137 , training loss: 3.901628\n",
      "[INFO] Epoch: 27 , batch: 138 , training loss: 3.946610\n",
      "[INFO] Epoch: 27 , batch: 139 , training loss: 4.486439\n",
      "[INFO] Epoch: 27 , batch: 140 , training loss: 4.253130\n",
      "[INFO] Epoch: 27 , batch: 141 , training loss: 4.050874\n",
      "[INFO] Epoch: 27 , batch: 142 , training loss: 3.775210\n",
      "[INFO] Epoch: 27 , batch: 143 , training loss: 3.929840\n",
      "[INFO] Epoch: 27 , batch: 144 , training loss: 3.774015\n",
      "[INFO] Epoch: 27 , batch: 145 , training loss: 3.824430\n",
      "[INFO] Epoch: 27 , batch: 146 , training loss: 4.035014\n",
      "[INFO] Epoch: 27 , batch: 147 , training loss: 3.685862\n",
      "[INFO] Epoch: 27 , batch: 148 , training loss: 3.668624\n",
      "[INFO] Epoch: 27 , batch: 149 , training loss: 3.764739\n",
      "[INFO] Epoch: 27 , batch: 150 , training loss: 4.012794\n",
      "[INFO] Epoch: 27 , batch: 151 , training loss: 3.858290\n",
      "[INFO] Epoch: 27 , batch: 152 , training loss: 3.855483\n",
      "[INFO] Epoch: 27 , batch: 153 , training loss: 3.875454\n",
      "[INFO] Epoch: 27 , batch: 154 , training loss: 3.971151\n",
      "[INFO] Epoch: 27 , batch: 155 , training loss: 4.189706\n",
      "[INFO] Epoch: 27 , batch: 156 , training loss: 3.917705\n",
      "[INFO] Epoch: 27 , batch: 157 , training loss: 3.899765\n",
      "[INFO] Epoch: 27 , batch: 158 , training loss: 4.037583\n",
      "[INFO] Epoch: 27 , batch: 159 , training loss: 3.920537\n",
      "[INFO] Epoch: 27 , batch: 160 , training loss: 4.185931\n",
      "[INFO] Epoch: 27 , batch: 161 , training loss: 4.254240\n",
      "[INFO] Epoch: 27 , batch: 162 , training loss: 4.236284\n",
      "[INFO] Epoch: 27 , batch: 163 , training loss: 4.459222\n",
      "[INFO] Epoch: 27 , batch: 164 , training loss: 4.361113\n",
      "[INFO] Epoch: 27 , batch: 165 , training loss: 4.296373\n",
      "[INFO] Epoch: 27 , batch: 166 , training loss: 4.149487\n",
      "[INFO] Epoch: 27 , batch: 167 , training loss: 4.332122\n",
      "[INFO] Epoch: 27 , batch: 168 , training loss: 3.951281\n",
      "[INFO] Epoch: 27 , batch: 169 , training loss: 3.918743\n",
      "[INFO] Epoch: 27 , batch: 170 , training loss: 4.100063\n",
      "[INFO] Epoch: 27 , batch: 171 , training loss: 3.537364\n",
      "[INFO] Epoch: 27 , batch: 172 , training loss: 3.761460\n",
      "[INFO] Epoch: 27 , batch: 173 , training loss: 4.059978\n",
      "[INFO] Epoch: 27 , batch: 174 , training loss: 4.522467\n",
      "[INFO] Epoch: 27 , batch: 175 , training loss: 4.786133\n",
      "[INFO] Epoch: 27 , batch: 176 , training loss: 4.505629\n",
      "[INFO] Epoch: 27 , batch: 177 , training loss: 4.087211\n",
      "[INFO] Epoch: 27 , batch: 178 , training loss: 4.088762\n",
      "[INFO] Epoch: 27 , batch: 179 , training loss: 4.153367\n",
      "[INFO] Epoch: 27 , batch: 180 , training loss: 4.097450\n",
      "[INFO] Epoch: 27 , batch: 181 , training loss: 4.384499\n",
      "[INFO] Epoch: 27 , batch: 182 , training loss: 4.326633\n",
      "[INFO] Epoch: 27 , batch: 183 , training loss: 4.263554\n",
      "[INFO] Epoch: 27 , batch: 184 , training loss: 4.168281\n",
      "[INFO] Epoch: 27 , batch: 185 , training loss: 4.161465\n",
      "[INFO] Epoch: 27 , batch: 186 , training loss: 4.283666\n",
      "[INFO] Epoch: 27 , batch: 187 , training loss: 4.386184\n",
      "[INFO] Epoch: 27 , batch: 188 , training loss: 4.379287\n",
      "[INFO] Epoch: 27 , batch: 189 , training loss: 4.276659\n",
      "[INFO] Epoch: 27 , batch: 190 , training loss: 4.311425\n",
      "[INFO] Epoch: 27 , batch: 191 , training loss: 4.413648\n",
      "[INFO] Epoch: 27 , batch: 192 , training loss: 4.246170\n",
      "[INFO] Epoch: 27 , batch: 193 , training loss: 4.366524\n",
      "[INFO] Epoch: 27 , batch: 194 , training loss: 4.320894\n",
      "[INFO] Epoch: 27 , batch: 195 , training loss: 4.254261\n",
      "[INFO] Epoch: 27 , batch: 196 , training loss: 4.091977\n",
      "[INFO] Epoch: 27 , batch: 197 , training loss: 4.168359\n",
      "[INFO] Epoch: 27 , batch: 198 , training loss: 4.077376\n",
      "[INFO] Epoch: 27 , batch: 199 , training loss: 4.202602\n",
      "[INFO] Epoch: 27 , batch: 200 , training loss: 4.146101\n",
      "[INFO] Epoch: 27 , batch: 201 , training loss: 4.041793\n",
      "[INFO] Epoch: 27 , batch: 202 , training loss: 4.046284\n",
      "[INFO] Epoch: 27 , batch: 203 , training loss: 4.157518\n",
      "[INFO] Epoch: 27 , batch: 204 , training loss: 4.242711\n",
      "[INFO] Epoch: 27 , batch: 205 , training loss: 3.846284\n",
      "[INFO] Epoch: 27 , batch: 206 , training loss: 3.775067\n",
      "[INFO] Epoch: 27 , batch: 207 , training loss: 3.781504\n",
      "[INFO] Epoch: 27 , batch: 208 , training loss: 4.097360\n",
      "[INFO] Epoch: 27 , batch: 209 , training loss: 4.053494\n",
      "[INFO] Epoch: 27 , batch: 210 , training loss: 4.082296\n",
      "[INFO] Epoch: 27 , batch: 211 , training loss: 4.060724\n",
      "[INFO] Epoch: 27 , batch: 212 , training loss: 4.175348\n",
      "[INFO] Epoch: 27 , batch: 213 , training loss: 4.124800\n",
      "[INFO] Epoch: 27 , batch: 214 , training loss: 4.209404\n",
      "[INFO] Epoch: 27 , batch: 215 , training loss: 4.405570\n",
      "[INFO] Epoch: 27 , batch: 216 , training loss: 4.126908\n",
      "[INFO] Epoch: 27 , batch: 217 , training loss: 4.045137\n",
      "[INFO] Epoch: 27 , batch: 218 , training loss: 4.054080\n",
      "[INFO] Epoch: 27 , batch: 219 , training loss: 4.150508\n",
      "[INFO] Epoch: 27 , batch: 220 , training loss: 3.990553\n",
      "[INFO] Epoch: 27 , batch: 221 , training loss: 3.989066\n",
      "[INFO] Epoch: 27 , batch: 222 , training loss: 4.143677\n",
      "[INFO] Epoch: 27 , batch: 223 , training loss: 4.236498\n",
      "[INFO] Epoch: 27 , batch: 224 , training loss: 4.279156\n",
      "[INFO] Epoch: 27 , batch: 225 , training loss: 4.153478\n",
      "[INFO] Epoch: 27 , batch: 226 , training loss: 4.302053\n",
      "[INFO] Epoch: 27 , batch: 227 , training loss: 4.263982\n",
      "[INFO] Epoch: 27 , batch: 228 , training loss: 4.294223\n",
      "[INFO] Epoch: 27 , batch: 229 , training loss: 4.151774\n",
      "[INFO] Epoch: 27 , batch: 230 , training loss: 4.022703\n",
      "[INFO] Epoch: 27 , batch: 231 , training loss: 3.880526\n",
      "[INFO] Epoch: 27 , batch: 232 , training loss: 4.023721\n",
      "[INFO] Epoch: 27 , batch: 233 , training loss: 4.036939\n",
      "[INFO] Epoch: 27 , batch: 234 , training loss: 3.719422\n",
      "[INFO] Epoch: 27 , batch: 235 , training loss: 3.855318\n",
      "[INFO] Epoch: 27 , batch: 236 , training loss: 3.955270\n",
      "[INFO] Epoch: 27 , batch: 237 , training loss: 4.172061\n",
      "[INFO] Epoch: 27 , batch: 238 , training loss: 3.939343\n",
      "[INFO] Epoch: 27 , batch: 239 , training loss: 3.979059\n",
      "[INFO] Epoch: 27 , batch: 240 , training loss: 4.027905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 27 , batch: 241 , training loss: 3.836147\n",
      "[INFO] Epoch: 27 , batch: 242 , training loss: 3.835633\n",
      "[INFO] Epoch: 27 , batch: 243 , training loss: 4.138285\n",
      "[INFO] Epoch: 27 , batch: 244 , training loss: 4.094340\n",
      "[INFO] Epoch: 27 , batch: 245 , training loss: 4.064151\n",
      "[INFO] Epoch: 27 , batch: 246 , training loss: 3.751456\n",
      "[INFO] Epoch: 27 , batch: 247 , training loss: 3.911524\n",
      "[INFO] Epoch: 27 , batch: 248 , training loss: 3.993950\n",
      "[INFO] Epoch: 27 , batch: 249 , training loss: 3.972898\n",
      "[INFO] Epoch: 27 , batch: 250 , training loss: 3.771893\n",
      "[INFO] Epoch: 27 , batch: 251 , training loss: 4.236329\n",
      "[INFO] Epoch: 27 , batch: 252 , training loss: 3.948969\n",
      "[INFO] Epoch: 27 , batch: 253 , training loss: 3.855164\n",
      "[INFO] Epoch: 27 , batch: 254 , training loss: 4.114938\n",
      "[INFO] Epoch: 27 , batch: 255 , training loss: 4.096144\n",
      "[INFO] Epoch: 27 , batch: 256 , training loss: 4.093459\n",
      "[INFO] Epoch: 27 , batch: 257 , training loss: 4.236507\n",
      "[INFO] Epoch: 27 , batch: 258 , training loss: 4.253505\n",
      "[INFO] Epoch: 27 , batch: 259 , training loss: 4.289171\n",
      "[INFO] Epoch: 27 , batch: 260 , training loss: 4.078731\n",
      "[INFO] Epoch: 27 , batch: 261 , training loss: 4.231341\n",
      "[INFO] Epoch: 27 , batch: 262 , training loss: 4.389650\n",
      "[INFO] Epoch: 27 , batch: 263 , training loss: 4.540255\n",
      "[INFO] Epoch: 27 , batch: 264 , training loss: 3.936915\n",
      "[INFO] Epoch: 27 , batch: 265 , training loss: 4.027140\n",
      "[INFO] Epoch: 27 , batch: 266 , training loss: 4.434254\n",
      "[INFO] Epoch: 27 , batch: 267 , training loss: 4.179649\n",
      "[INFO] Epoch: 27 , batch: 268 , training loss: 4.092843\n",
      "[INFO] Epoch: 27 , batch: 269 , training loss: 4.095853\n",
      "[INFO] Epoch: 27 , batch: 270 , training loss: 4.122141\n",
      "[INFO] Epoch: 27 , batch: 271 , training loss: 4.126776\n",
      "[INFO] Epoch: 27 , batch: 272 , training loss: 4.137506\n",
      "[INFO] Epoch: 27 , batch: 273 , training loss: 4.113699\n",
      "[INFO] Epoch: 27 , batch: 274 , training loss: 4.219339\n",
      "[INFO] Epoch: 27 , batch: 275 , training loss: 4.087431\n",
      "[INFO] Epoch: 27 , batch: 276 , training loss: 4.164075\n",
      "[INFO] Epoch: 27 , batch: 277 , training loss: 4.336704\n",
      "[INFO] Epoch: 27 , batch: 278 , training loss: 3.985359\n",
      "[INFO] Epoch: 27 , batch: 279 , training loss: 4.006510\n",
      "[INFO] Epoch: 27 , batch: 280 , training loss: 3.975431\n",
      "[INFO] Epoch: 27 , batch: 281 , training loss: 4.093195\n",
      "[INFO] Epoch: 27 , batch: 282 , training loss: 3.999487\n",
      "[INFO] Epoch: 27 , batch: 283 , training loss: 4.026527\n",
      "[INFO] Epoch: 27 , batch: 284 , training loss: 4.056272\n",
      "[INFO] Epoch: 27 , batch: 285 , training loss: 3.997077\n",
      "[INFO] Epoch: 27 , batch: 286 , training loss: 4.008173\n",
      "[INFO] Epoch: 27 , batch: 287 , training loss: 3.945752\n",
      "[INFO] Epoch: 27 , batch: 288 , training loss: 3.899239\n",
      "[INFO] Epoch: 27 , batch: 289 , training loss: 3.985854\n",
      "[INFO] Epoch: 27 , batch: 290 , training loss: 3.737510\n",
      "[INFO] Epoch: 27 , batch: 291 , training loss: 3.740787\n",
      "[INFO] Epoch: 27 , batch: 292 , training loss: 3.848519\n",
      "[INFO] Epoch: 27 , batch: 293 , training loss: 3.761099\n",
      "[INFO] Epoch: 27 , batch: 294 , training loss: 4.446139\n",
      "[INFO] Epoch: 27 , batch: 295 , training loss: 4.219940\n",
      "[INFO] Epoch: 27 , batch: 296 , training loss: 4.146832\n",
      "[INFO] Epoch: 27 , batch: 297 , training loss: 4.095834\n",
      "[INFO] Epoch: 27 , batch: 298 , training loss: 3.933523\n",
      "[INFO] Epoch: 27 , batch: 299 , training loss: 3.989088\n",
      "[INFO] Epoch: 27 , batch: 300 , training loss: 3.959708\n",
      "[INFO] Epoch: 27 , batch: 301 , training loss: 3.895027\n",
      "[INFO] Epoch: 27 , batch: 302 , training loss: 4.063065\n",
      "[INFO] Epoch: 27 , batch: 303 , training loss: 4.062819\n",
      "[INFO] Epoch: 27 , batch: 304 , training loss: 4.208739\n",
      "[INFO] Epoch: 27 , batch: 305 , training loss: 4.022707\n",
      "[INFO] Epoch: 27 , batch: 306 , training loss: 4.158611\n",
      "[INFO] Epoch: 27 , batch: 307 , training loss: 4.169191\n",
      "[INFO] Epoch: 27 , batch: 308 , training loss: 3.968551\n",
      "[INFO] Epoch: 27 , batch: 309 , training loss: 3.992618\n",
      "[INFO] Epoch: 27 , batch: 310 , training loss: 3.902250\n",
      "[INFO] Epoch: 27 , batch: 311 , training loss: 3.909312\n",
      "[INFO] Epoch: 27 , batch: 312 , training loss: 3.826048\n",
      "[INFO] Epoch: 27 , batch: 313 , training loss: 3.905596\n",
      "[INFO] Epoch: 27 , batch: 314 , training loss: 3.983207\n",
      "[INFO] Epoch: 27 , batch: 315 , training loss: 4.044793\n",
      "[INFO] Epoch: 27 , batch: 316 , training loss: 4.308419\n",
      "[INFO] Epoch: 27 , batch: 317 , training loss: 4.643934\n",
      "[INFO] Epoch: 27 , batch: 318 , training loss: 4.794635\n",
      "[INFO] Epoch: 27 , batch: 319 , training loss: 4.492733\n",
      "[INFO] Epoch: 27 , batch: 320 , training loss: 4.024027\n",
      "[INFO] Epoch: 27 , batch: 321 , training loss: 3.835603\n",
      "[INFO] Epoch: 27 , batch: 322 , training loss: 3.940222\n",
      "[INFO] Epoch: 27 , batch: 323 , training loss: 3.966443\n",
      "[INFO] Epoch: 27 , batch: 324 , training loss: 3.936052\n",
      "[INFO] Epoch: 27 , batch: 325 , training loss: 4.089773\n",
      "[INFO] Epoch: 27 , batch: 326 , training loss: 4.144278\n",
      "[INFO] Epoch: 27 , batch: 327 , training loss: 4.054293\n",
      "[INFO] Epoch: 27 , batch: 328 , training loss: 4.062300\n",
      "[INFO] Epoch: 27 , batch: 329 , training loss: 3.961071\n",
      "[INFO] Epoch: 27 , batch: 330 , training loss: 3.962407\n",
      "[INFO] Epoch: 27 , batch: 331 , training loss: 4.125998\n",
      "[INFO] Epoch: 27 , batch: 332 , training loss: 3.979403\n",
      "[INFO] Epoch: 27 , batch: 333 , training loss: 3.946852\n",
      "[INFO] Epoch: 27 , batch: 334 , training loss: 3.960016\n",
      "[INFO] Epoch: 27 , batch: 335 , training loss: 4.081748\n",
      "[INFO] Epoch: 27 , batch: 336 , training loss: 4.088473\n",
      "[INFO] Epoch: 27 , batch: 337 , training loss: 4.126659\n",
      "[INFO] Epoch: 27 , batch: 338 , training loss: 4.349586\n",
      "[INFO] Epoch: 27 , batch: 339 , training loss: 4.151772\n",
      "[INFO] Epoch: 27 , batch: 340 , training loss: 4.325162\n",
      "[INFO] Epoch: 27 , batch: 341 , training loss: 4.094116\n",
      "[INFO] Epoch: 27 , batch: 342 , training loss: 3.867608\n",
      "[INFO] Epoch: 27 , batch: 343 , training loss: 3.949121\n",
      "[INFO] Epoch: 27 , batch: 344 , training loss: 3.818141\n",
      "[INFO] Epoch: 27 , batch: 345 , training loss: 3.963315\n",
      "[INFO] Epoch: 27 , batch: 346 , training loss: 4.012288\n",
      "[INFO] Epoch: 27 , batch: 347 , training loss: 3.902471\n",
      "[INFO] Epoch: 27 , batch: 348 , training loss: 4.018942\n",
      "[INFO] Epoch: 27 , batch: 349 , training loss: 4.100945\n",
      "[INFO] Epoch: 27 , batch: 350 , training loss: 3.958494\n",
      "[INFO] Epoch: 27 , batch: 351 , training loss: 4.041317\n",
      "[INFO] Epoch: 27 , batch: 352 , training loss: 4.051057\n",
      "[INFO] Epoch: 27 , batch: 353 , training loss: 4.047159\n",
      "[INFO] Epoch: 27 , batch: 354 , training loss: 4.128389\n",
      "[INFO] Epoch: 27 , batch: 355 , training loss: 4.128594\n",
      "[INFO] Epoch: 27 , batch: 356 , training loss: 3.988792\n",
      "[INFO] Epoch: 27 , batch: 357 , training loss: 4.073777\n",
      "[INFO] Epoch: 27 , batch: 358 , training loss: 3.972078\n",
      "[INFO] Epoch: 27 , batch: 359 , training loss: 3.972655\n",
      "[INFO] Epoch: 27 , batch: 360 , training loss: 4.089625\n",
      "[INFO] Epoch: 27 , batch: 361 , training loss: 4.047743\n",
      "[INFO] Epoch: 27 , batch: 362 , training loss: 4.162905\n",
      "[INFO] Epoch: 27 , batch: 363 , training loss: 4.029775\n",
      "[INFO] Epoch: 27 , batch: 364 , training loss: 4.103863\n",
      "[INFO] Epoch: 27 , batch: 365 , training loss: 3.997936\n",
      "[INFO] Epoch: 27 , batch: 366 , training loss: 4.104520\n",
      "[INFO] Epoch: 27 , batch: 367 , training loss: 4.147681\n",
      "[INFO] Epoch: 27 , batch: 368 , training loss: 4.546341\n",
      "[INFO] Epoch: 27 , batch: 369 , training loss: 4.239768\n",
      "[INFO] Epoch: 27 , batch: 370 , training loss: 3.993664\n",
      "[INFO] Epoch: 27 , batch: 371 , training loss: 4.443585\n",
      "[INFO] Epoch: 27 , batch: 372 , training loss: 4.675384\n",
      "[INFO] Epoch: 27 , batch: 373 , training loss: 4.739918\n",
      "[INFO] Epoch: 27 , batch: 374 , training loss: 4.840573\n",
      "[INFO] Epoch: 27 , batch: 375 , training loss: 4.822464\n",
      "[INFO] Epoch: 27 , batch: 376 , training loss: 4.716307\n",
      "[INFO] Epoch: 27 , batch: 377 , training loss: 4.463936\n",
      "[INFO] Epoch: 27 , batch: 378 , training loss: 4.556839\n",
      "[INFO] Epoch: 27 , batch: 379 , training loss: 4.527039\n",
      "[INFO] Epoch: 27 , batch: 380 , training loss: 4.691161\n",
      "[INFO] Epoch: 27 , batch: 381 , training loss: 4.387913\n",
      "[INFO] Epoch: 27 , batch: 382 , training loss: 4.711308\n",
      "[INFO] Epoch: 27 , batch: 383 , training loss: 4.706140\n",
      "[INFO] Epoch: 27 , batch: 384 , training loss: 4.674188\n",
      "[INFO] Epoch: 27 , batch: 385 , training loss: 4.331338\n",
      "[INFO] Epoch: 27 , batch: 386 , training loss: 4.604965\n",
      "[INFO] Epoch: 27 , batch: 387 , training loss: 4.549672\n",
      "[INFO] Epoch: 27 , batch: 388 , training loss: 4.380331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 27 , batch: 389 , training loss: 4.203480\n",
      "[INFO] Epoch: 27 , batch: 390 , training loss: 4.234868\n",
      "[INFO] Epoch: 27 , batch: 391 , training loss: 4.239055\n",
      "[INFO] Epoch: 27 , batch: 392 , training loss: 4.610844\n",
      "[INFO] Epoch: 27 , batch: 393 , training loss: 4.512908\n",
      "[INFO] Epoch: 27 , batch: 394 , training loss: 4.595227\n",
      "[INFO] Epoch: 27 , batch: 395 , training loss: 4.402434\n",
      "[INFO] Epoch: 27 , batch: 396 , training loss: 4.226198\n",
      "[INFO] Epoch: 27 , batch: 397 , training loss: 4.375900\n",
      "[INFO] Epoch: 27 , batch: 398 , training loss: 4.224113\n",
      "[INFO] Epoch: 27 , batch: 399 , training loss: 4.307574\n",
      "[INFO] Epoch: 27 , batch: 400 , training loss: 4.278355\n",
      "[INFO] Epoch: 27 , batch: 401 , training loss: 4.726309\n",
      "[INFO] Epoch: 27 , batch: 402 , training loss: 4.427322\n",
      "[INFO] Epoch: 27 , batch: 403 , training loss: 4.263171\n",
      "[INFO] Epoch: 27 , batch: 404 , training loss: 4.416500\n",
      "[INFO] Epoch: 27 , batch: 405 , training loss: 4.498553\n",
      "[INFO] Epoch: 27 , batch: 406 , training loss: 4.410758\n",
      "[INFO] Epoch: 27 , batch: 407 , training loss: 4.442008\n",
      "[INFO] Epoch: 27 , batch: 408 , training loss: 4.375560\n",
      "[INFO] Epoch: 27 , batch: 409 , training loss: 4.406218\n",
      "[INFO] Epoch: 27 , batch: 410 , training loss: 4.478952\n",
      "[INFO] Epoch: 27 , batch: 411 , training loss: 4.632387\n",
      "[INFO] Epoch: 27 , batch: 412 , training loss: 4.461683\n",
      "[INFO] Epoch: 27 , batch: 413 , training loss: 4.338681\n",
      "[INFO] Epoch: 27 , batch: 414 , training loss: 4.366638\n",
      "[INFO] Epoch: 27 , batch: 415 , training loss: 4.414811\n",
      "[INFO] Epoch: 27 , batch: 416 , training loss: 4.497283\n",
      "[INFO] Epoch: 27 , batch: 417 , training loss: 4.386959\n",
      "[INFO] Epoch: 27 , batch: 418 , training loss: 4.447525\n",
      "[INFO] Epoch: 27 , batch: 419 , training loss: 4.419622\n",
      "[INFO] Epoch: 27 , batch: 420 , training loss: 4.408179\n",
      "[INFO] Epoch: 27 , batch: 421 , training loss: 4.370305\n",
      "[INFO] Epoch: 27 , batch: 422 , training loss: 4.212473\n",
      "[INFO] Epoch: 27 , batch: 423 , training loss: 4.448128\n",
      "[INFO] Epoch: 27 , batch: 424 , training loss: 4.609891\n",
      "[INFO] Epoch: 27 , batch: 425 , training loss: 4.472957\n",
      "[INFO] Epoch: 27 , batch: 426 , training loss: 4.195917\n",
      "[INFO] Epoch: 27 , batch: 427 , training loss: 4.446993\n",
      "[INFO] Epoch: 27 , batch: 428 , training loss: 4.333370\n",
      "[INFO] Epoch: 27 , batch: 429 , training loss: 4.227957\n",
      "[INFO] Epoch: 27 , batch: 430 , training loss: 4.436519\n",
      "[INFO] Epoch: 27 , batch: 431 , training loss: 4.051666\n",
      "[INFO] Epoch: 27 , batch: 432 , training loss: 4.108430\n",
      "[INFO] Epoch: 27 , batch: 433 , training loss: 4.135701\n",
      "[INFO] Epoch: 27 , batch: 434 , training loss: 4.022192\n",
      "[INFO] Epoch: 27 , batch: 435 , training loss: 4.376152\n",
      "[INFO] Epoch: 27 , batch: 436 , training loss: 4.428185\n",
      "[INFO] Epoch: 27 , batch: 437 , training loss: 4.200979\n",
      "[INFO] Epoch: 27 , batch: 438 , training loss: 4.071274\n",
      "[INFO] Epoch: 27 , batch: 439 , training loss: 4.304053\n",
      "[INFO] Epoch: 27 , batch: 440 , training loss: 4.424352\n",
      "[INFO] Epoch: 27 , batch: 441 , training loss: 4.508252\n",
      "[INFO] Epoch: 27 , batch: 442 , training loss: 4.274978\n",
      "[INFO] Epoch: 27 , batch: 443 , training loss: 4.491606\n",
      "[INFO] Epoch: 27 , batch: 444 , training loss: 4.060949\n",
      "[INFO] Epoch: 27 , batch: 445 , training loss: 3.976706\n",
      "[INFO] Epoch: 27 , batch: 446 , training loss: 3.910724\n",
      "[INFO] Epoch: 27 , batch: 447 , training loss: 4.108420\n",
      "[INFO] Epoch: 27 , batch: 448 , training loss: 4.216990\n",
      "[INFO] Epoch: 27 , batch: 449 , training loss: 4.612822\n",
      "[INFO] Epoch: 27 , batch: 450 , training loss: 4.676092\n",
      "[INFO] Epoch: 27 , batch: 451 , training loss: 4.559608\n",
      "[INFO] Epoch: 27 , batch: 452 , training loss: 4.374393\n",
      "[INFO] Epoch: 27 , batch: 453 , training loss: 4.157624\n",
      "[INFO] Epoch: 27 , batch: 454 , training loss: 4.287140\n",
      "[INFO] Epoch: 27 , batch: 455 , training loss: 4.371023\n",
      "[INFO] Epoch: 27 , batch: 456 , training loss: 4.346404\n",
      "[INFO] Epoch: 27 , batch: 457 , training loss: 4.436234\n",
      "[INFO] Epoch: 27 , batch: 458 , training loss: 4.183179\n",
      "[INFO] Epoch: 27 , batch: 459 , training loss: 4.151453\n",
      "[INFO] Epoch: 27 , batch: 460 , training loss: 4.267681\n",
      "[INFO] Epoch: 27 , batch: 461 , training loss: 4.215605\n",
      "[INFO] Epoch: 27 , batch: 462 , training loss: 4.263620\n",
      "[INFO] Epoch: 27 , batch: 463 , training loss: 4.203413\n",
      "[INFO] Epoch: 27 , batch: 464 , training loss: 4.376525\n",
      "[INFO] Epoch: 27 , batch: 465 , training loss: 4.327128\n",
      "[INFO] Epoch: 27 , batch: 466 , training loss: 4.425949\n",
      "[INFO] Epoch: 27 , batch: 467 , training loss: 4.385648\n",
      "[INFO] Epoch: 27 , batch: 468 , training loss: 4.345510\n",
      "[INFO] Epoch: 27 , batch: 469 , training loss: 4.361700\n",
      "[INFO] Epoch: 27 , batch: 470 , training loss: 4.183887\n",
      "[INFO] Epoch: 27 , batch: 471 , training loss: 4.306211\n",
      "[INFO] Epoch: 27 , batch: 472 , training loss: 4.347719\n",
      "[INFO] Epoch: 27 , batch: 473 , training loss: 4.258985\n",
      "[INFO] Epoch: 27 , batch: 474 , training loss: 4.034813\n",
      "[INFO] Epoch: 27 , batch: 475 , training loss: 3.938138\n",
      "[INFO] Epoch: 27 , batch: 476 , training loss: 4.330546\n",
      "[INFO] Epoch: 27 , batch: 477 , training loss: 4.443227\n",
      "[INFO] Epoch: 27 , batch: 478 , training loss: 4.449371\n",
      "[INFO] Epoch: 27 , batch: 479 , training loss: 4.409036\n",
      "[INFO] Epoch: 27 , batch: 480 , training loss: 4.540328\n",
      "[INFO] Epoch: 27 , batch: 481 , training loss: 4.435287\n",
      "[INFO] Epoch: 27 , batch: 482 , training loss: 4.514498\n",
      "[INFO] Epoch: 27 , batch: 483 , training loss: 4.366533\n",
      "[INFO] Epoch: 27 , batch: 484 , training loss: 4.165118\n",
      "[INFO] Epoch: 27 , batch: 485 , training loss: 4.256780\n",
      "[INFO] Epoch: 27 , batch: 486 , training loss: 4.173712\n",
      "[INFO] Epoch: 27 , batch: 487 , training loss: 4.152248\n",
      "[INFO] Epoch: 27 , batch: 488 , training loss: 4.328754\n",
      "[INFO] Epoch: 27 , batch: 489 , training loss: 4.250721\n",
      "[INFO] Epoch: 27 , batch: 490 , training loss: 4.288440\n",
      "[INFO] Epoch: 27 , batch: 491 , training loss: 4.218905\n",
      "[INFO] Epoch: 27 , batch: 492 , training loss: 4.189048\n",
      "[INFO] Epoch: 27 , batch: 493 , training loss: 4.350887\n",
      "[INFO] Epoch: 27 , batch: 494 , training loss: 4.260621\n",
      "[INFO] Epoch: 27 , batch: 495 , training loss: 4.426411\n",
      "[INFO] Epoch: 27 , batch: 496 , training loss: 4.308765\n",
      "[INFO] Epoch: 27 , batch: 497 , training loss: 4.342322\n",
      "[INFO] Epoch: 27 , batch: 498 , training loss: 4.322465\n",
      "[INFO] Epoch: 27 , batch: 499 , training loss: 4.393369\n",
      "[INFO] Epoch: 27 , batch: 500 , training loss: 4.519211\n",
      "[INFO] Epoch: 27 , batch: 501 , training loss: 4.856190\n",
      "[INFO] Epoch: 27 , batch: 502 , training loss: 4.884486\n",
      "[INFO] Epoch: 27 , batch: 503 , training loss: 4.553658\n",
      "[INFO] Epoch: 27 , batch: 504 , training loss: 4.694596\n",
      "[INFO] Epoch: 27 , batch: 505 , training loss: 4.661250\n",
      "[INFO] Epoch: 27 , batch: 506 , training loss: 4.645110\n",
      "[INFO] Epoch: 27 , batch: 507 , training loss: 4.712296\n",
      "[INFO] Epoch: 27 , batch: 508 , training loss: 4.599889\n",
      "[INFO] Epoch: 27 , batch: 509 , training loss: 4.420744\n",
      "[INFO] Epoch: 27 , batch: 510 , training loss: 4.483566\n",
      "[INFO] Epoch: 27 , batch: 511 , training loss: 4.406196\n",
      "[INFO] Epoch: 27 , batch: 512 , training loss: 4.519854\n",
      "[INFO] Epoch: 27 , batch: 513 , training loss: 4.771531\n",
      "[INFO] Epoch: 27 , batch: 514 , training loss: 4.403685\n",
      "[INFO] Epoch: 27 , batch: 515 , training loss: 4.655243\n",
      "[INFO] Epoch: 27 , batch: 516 , training loss: 4.455135\n",
      "[INFO] Epoch: 27 , batch: 517 , training loss: 4.409291\n",
      "[INFO] Epoch: 27 , batch: 518 , training loss: 4.383933\n",
      "[INFO] Epoch: 27 , batch: 519 , training loss: 4.232430\n",
      "[INFO] Epoch: 27 , batch: 520 , training loss: 4.463963\n",
      "[INFO] Epoch: 27 , batch: 521 , training loss: 4.445756\n",
      "[INFO] Epoch: 27 , batch: 522 , training loss: 4.521926\n",
      "[INFO] Epoch: 27 , batch: 523 , training loss: 4.431867\n",
      "[INFO] Epoch: 27 , batch: 524 , training loss: 4.713411\n",
      "[INFO] Epoch: 27 , batch: 525 , training loss: 4.598669\n",
      "[INFO] Epoch: 27 , batch: 526 , training loss: 4.394281\n",
      "[INFO] Epoch: 27 , batch: 527 , training loss: 4.440914\n",
      "[INFO] Epoch: 27 , batch: 528 , training loss: 4.449785\n",
      "[INFO] Epoch: 27 , batch: 529 , training loss: 4.428514\n",
      "[INFO] Epoch: 27 , batch: 530 , training loss: 4.256431\n",
      "[INFO] Epoch: 27 , batch: 531 , training loss: 4.407212\n",
      "[INFO] Epoch: 27 , batch: 532 , training loss: 4.322380\n",
      "[INFO] Epoch: 27 , batch: 533 , training loss: 4.464308\n",
      "[INFO] Epoch: 27 , batch: 534 , training loss: 4.454153\n",
      "[INFO] Epoch: 27 , batch: 535 , training loss: 4.467017\n",
      "[INFO] Epoch: 27 , batch: 536 , training loss: 4.314340\n",
      "[INFO] Epoch: 27 , batch: 537 , training loss: 4.293612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 27 , batch: 538 , training loss: 4.368348\n",
      "[INFO] Epoch: 27 , batch: 539 , training loss: 4.479885\n",
      "[INFO] Epoch: 27 , batch: 540 , training loss: 5.008130\n",
      "[INFO] Epoch: 27 , batch: 541 , training loss: 4.802159\n",
      "[INFO] Epoch: 27 , batch: 542 , training loss: 4.704616\n",
      "[INFO] Epoch: 28 , batch: 0 , training loss: 3.707824\n",
      "[INFO] Epoch: 28 , batch: 1 , training loss: 3.553886\n",
      "[INFO] Epoch: 28 , batch: 2 , training loss: 3.739687\n",
      "[INFO] Epoch: 28 , batch: 3 , training loss: 3.579066\n",
      "[INFO] Epoch: 28 , batch: 4 , training loss: 3.889253\n",
      "[INFO] Epoch: 28 , batch: 5 , training loss: 3.578440\n",
      "[INFO] Epoch: 28 , batch: 6 , training loss: 3.913880\n",
      "[INFO] Epoch: 28 , batch: 7 , training loss: 3.854306\n",
      "[INFO] Epoch: 28 , batch: 8 , training loss: 3.547594\n",
      "[INFO] Epoch: 28 , batch: 9 , training loss: 3.825483\n",
      "[INFO] Epoch: 28 , batch: 10 , training loss: 3.755595\n",
      "[INFO] Epoch: 28 , batch: 11 , training loss: 3.682013\n",
      "[INFO] Epoch: 28 , batch: 12 , training loss: 3.584034\n",
      "[INFO] Epoch: 28 , batch: 13 , training loss: 3.617446\n",
      "[INFO] Epoch: 28 , batch: 14 , training loss: 3.496202\n",
      "[INFO] Epoch: 28 , batch: 15 , training loss: 3.739393\n",
      "[INFO] Epoch: 28 , batch: 16 , training loss: 3.593110\n",
      "[INFO] Epoch: 28 , batch: 17 , training loss: 3.736882\n",
      "[INFO] Epoch: 28 , batch: 18 , training loss: 3.660764\n",
      "[INFO] Epoch: 28 , batch: 19 , training loss: 3.430031\n",
      "[INFO] Epoch: 28 , batch: 20 , training loss: 3.390433\n",
      "[INFO] Epoch: 28 , batch: 21 , training loss: 3.527736\n",
      "[INFO] Epoch: 28 , batch: 22 , training loss: 3.437745\n",
      "[INFO] Epoch: 28 , batch: 23 , training loss: 3.606272\n",
      "[INFO] Epoch: 28 , batch: 24 , training loss: 3.449877\n",
      "[INFO] Epoch: 28 , batch: 25 , training loss: 3.579884\n",
      "[INFO] Epoch: 28 , batch: 26 , training loss: 3.455167\n",
      "[INFO] Epoch: 28 , batch: 27 , training loss: 3.436697\n",
      "[INFO] Epoch: 28 , batch: 28 , training loss: 3.622641\n",
      "[INFO] Epoch: 28 , batch: 29 , training loss: 3.456290\n",
      "[INFO] Epoch: 28 , batch: 30 , training loss: 3.470693\n",
      "[INFO] Epoch: 28 , batch: 31 , training loss: 3.589250\n",
      "[INFO] Epoch: 28 , batch: 32 , training loss: 3.524587\n",
      "[INFO] Epoch: 28 , batch: 33 , training loss: 3.571008\n",
      "[INFO] Epoch: 28 , batch: 34 , training loss: 3.525565\n",
      "[INFO] Epoch: 28 , batch: 35 , training loss: 3.495200\n",
      "[INFO] Epoch: 28 , batch: 36 , training loss: 3.621115\n",
      "[INFO] Epoch: 28 , batch: 37 , training loss: 3.449760\n",
      "[INFO] Epoch: 28 , batch: 38 , training loss: 3.561711\n",
      "[INFO] Epoch: 28 , batch: 39 , training loss: 3.362533\n",
      "[INFO] Epoch: 28 , batch: 40 , training loss: 3.578019\n",
      "[INFO] Epoch: 28 , batch: 41 , training loss: 3.517725\n",
      "[INFO] Epoch: 28 , batch: 42 , training loss: 3.951903\n",
      "[INFO] Epoch: 28 , batch: 43 , training loss: 3.677567\n",
      "[INFO] Epoch: 28 , batch: 44 , training loss: 4.031250\n",
      "[INFO] Epoch: 28 , batch: 45 , training loss: 3.935906\n",
      "[INFO] Epoch: 28 , batch: 46 , training loss: 3.903191\n",
      "[INFO] Epoch: 28 , batch: 47 , training loss: 3.581365\n",
      "[INFO] Epoch: 28 , batch: 48 , training loss: 3.535028\n",
      "[INFO] Epoch: 28 , batch: 49 , training loss: 3.824529\n",
      "[INFO] Epoch: 28 , batch: 50 , training loss: 3.550458\n",
      "[INFO] Epoch: 28 , batch: 51 , training loss: 3.805700\n",
      "[INFO] Epoch: 28 , batch: 52 , training loss: 3.671544\n",
      "[INFO] Epoch: 28 , batch: 53 , training loss: 3.745736\n",
      "[INFO] Epoch: 28 , batch: 54 , training loss: 3.787302\n",
      "[INFO] Epoch: 28 , batch: 55 , training loss: 3.868025\n",
      "[INFO] Epoch: 28 , batch: 56 , training loss: 3.672193\n",
      "[INFO] Epoch: 28 , batch: 57 , training loss: 3.627747\n",
      "[INFO] Epoch: 28 , batch: 58 , training loss: 3.671754\n",
      "[INFO] Epoch: 28 , batch: 59 , training loss: 3.748732\n",
      "[INFO] Epoch: 28 , batch: 60 , training loss: 3.725872\n",
      "[INFO] Epoch: 28 , batch: 61 , training loss: 3.764466\n",
      "[INFO] Epoch: 28 , batch: 62 , training loss: 3.650234\n",
      "[INFO] Epoch: 28 , batch: 63 , training loss: 3.801749\n",
      "[INFO] Epoch: 28 , batch: 64 , training loss: 4.056239\n",
      "[INFO] Epoch: 28 , batch: 65 , training loss: 3.717047\n",
      "[INFO] Epoch: 28 , batch: 66 , training loss: 3.603015\n",
      "[INFO] Epoch: 28 , batch: 67 , training loss: 3.626785\n",
      "[INFO] Epoch: 28 , batch: 68 , training loss: 3.778582\n",
      "[INFO] Epoch: 28 , batch: 69 , training loss: 3.725762\n",
      "[INFO] Epoch: 28 , batch: 70 , training loss: 3.943886\n",
      "[INFO] Epoch: 28 , batch: 71 , training loss: 3.788434\n",
      "[INFO] Epoch: 28 , batch: 72 , training loss: 3.852755\n",
      "[INFO] Epoch: 28 , batch: 73 , training loss: 3.790940\n",
      "[INFO] Epoch: 28 , batch: 74 , training loss: 3.890275\n",
      "[INFO] Epoch: 28 , batch: 75 , training loss: 3.774275\n",
      "[INFO] Epoch: 28 , batch: 76 , training loss: 3.856904\n",
      "[INFO] Epoch: 28 , batch: 77 , training loss: 3.826704\n",
      "[INFO] Epoch: 28 , batch: 78 , training loss: 3.909461\n",
      "[INFO] Epoch: 28 , batch: 79 , training loss: 3.760898\n",
      "[INFO] Epoch: 28 , batch: 80 , training loss: 3.957686\n",
      "[INFO] Epoch: 28 , batch: 81 , training loss: 3.889680\n",
      "[INFO] Epoch: 28 , batch: 82 , training loss: 3.885774\n",
      "[INFO] Epoch: 28 , batch: 83 , training loss: 3.959729\n",
      "[INFO] Epoch: 28 , batch: 84 , training loss: 3.892767\n",
      "[INFO] Epoch: 28 , batch: 85 , training loss: 3.989080\n",
      "[INFO] Epoch: 28 , batch: 86 , training loss: 3.925592\n",
      "[INFO] Epoch: 28 , batch: 87 , training loss: 3.884779\n",
      "[INFO] Epoch: 28 , batch: 88 , training loss: 4.037285\n",
      "[INFO] Epoch: 28 , batch: 89 , training loss: 3.833377\n",
      "[INFO] Epoch: 28 , batch: 90 , training loss: 3.948794\n",
      "[INFO] Epoch: 28 , batch: 91 , training loss: 3.836761\n",
      "[INFO] Epoch: 28 , batch: 92 , training loss: 3.867700\n",
      "[INFO] Epoch: 28 , batch: 93 , training loss: 3.975545\n",
      "[INFO] Epoch: 28 , batch: 94 , training loss: 4.085423\n",
      "[INFO] Epoch: 28 , batch: 95 , training loss: 3.898395\n",
      "[INFO] Epoch: 28 , batch: 96 , training loss: 3.857450\n",
      "[INFO] Epoch: 28 , batch: 97 , training loss: 3.804000\n",
      "[INFO] Epoch: 28 , batch: 98 , training loss: 3.762659\n",
      "[INFO] Epoch: 28 , batch: 99 , training loss: 3.860562\n",
      "[INFO] Epoch: 28 , batch: 100 , training loss: 3.758880\n",
      "[INFO] Epoch: 28 , batch: 101 , training loss: 3.773038\n",
      "[INFO] Epoch: 28 , batch: 102 , training loss: 3.942835\n",
      "[INFO] Epoch: 28 , batch: 103 , training loss: 3.724620\n",
      "[INFO] Epoch: 28 , batch: 104 , training loss: 3.684237\n",
      "[INFO] Epoch: 28 , batch: 105 , training loss: 3.913121\n",
      "[INFO] Epoch: 28 , batch: 106 , training loss: 3.945354\n",
      "[INFO] Epoch: 28 , batch: 107 , training loss: 3.786838\n",
      "[INFO] Epoch: 28 , batch: 108 , training loss: 3.747281\n",
      "[INFO] Epoch: 28 , batch: 109 , training loss: 3.682428\n",
      "[INFO] Epoch: 28 , batch: 110 , training loss: 3.849567\n",
      "[INFO] Epoch: 28 , batch: 111 , training loss: 3.933127\n",
      "[INFO] Epoch: 28 , batch: 112 , training loss: 3.824690\n",
      "[INFO] Epoch: 28 , batch: 113 , training loss: 3.855882\n",
      "[INFO] Epoch: 28 , batch: 114 , training loss: 3.834430\n",
      "[INFO] Epoch: 28 , batch: 115 , training loss: 3.833632\n",
      "[INFO] Epoch: 28 , batch: 116 , training loss: 3.739255\n",
      "[INFO] Epoch: 28 , batch: 117 , training loss: 3.954214\n",
      "[INFO] Epoch: 28 , batch: 118 , training loss: 3.930048\n",
      "[INFO] Epoch: 28 , batch: 119 , training loss: 4.077600\n",
      "[INFO] Epoch: 28 , batch: 120 , training loss: 4.047512\n",
      "[INFO] Epoch: 28 , batch: 121 , training loss: 3.939443\n",
      "[INFO] Epoch: 28 , batch: 122 , training loss: 3.820346\n",
      "[INFO] Epoch: 28 , batch: 123 , training loss: 3.837644\n",
      "[INFO] Epoch: 28 , batch: 124 , training loss: 3.950041\n",
      "[INFO] Epoch: 28 , batch: 125 , training loss: 3.725683\n",
      "[INFO] Epoch: 28 , batch: 126 , training loss: 3.766024\n",
      "[INFO] Epoch: 28 , batch: 127 , training loss: 3.770590\n",
      "[INFO] Epoch: 28 , batch: 128 , training loss: 3.934502\n",
      "[INFO] Epoch: 28 , batch: 129 , training loss: 3.864367\n",
      "[INFO] Epoch: 28 , batch: 130 , training loss: 3.851386\n",
      "[INFO] Epoch: 28 , batch: 131 , training loss: 3.850687\n",
      "[INFO] Epoch: 28 , batch: 132 , training loss: 3.857615\n",
      "[INFO] Epoch: 28 , batch: 133 , training loss: 3.834874\n",
      "[INFO] Epoch: 28 , batch: 134 , training loss: 3.596760\n",
      "[INFO] Epoch: 28 , batch: 135 , training loss: 3.683826\n",
      "[INFO] Epoch: 28 , batch: 136 , training loss: 3.966112\n",
      "[INFO] Epoch: 28 , batch: 137 , training loss: 3.882739\n",
      "[INFO] Epoch: 28 , batch: 138 , training loss: 3.923472\n",
      "[INFO] Epoch: 28 , batch: 139 , training loss: 4.498633\n",
      "[INFO] Epoch: 28 , batch: 140 , training loss: 4.259075\n",
      "[INFO] Epoch: 28 , batch: 141 , training loss: 4.049185\n",
      "[INFO] Epoch: 28 , batch: 142 , training loss: 3.769646\n",
      "[INFO] Epoch: 28 , batch: 143 , training loss: 3.908336\n",
      "[INFO] Epoch: 28 , batch: 144 , training loss: 3.739704\n",
      "[INFO] Epoch: 28 , batch: 145 , training loss: 3.814112\n",
      "[INFO] Epoch: 28 , batch: 146 , training loss: 4.018116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 28 , batch: 147 , training loss: 3.691416\n",
      "[INFO] Epoch: 28 , batch: 148 , training loss: 3.658468\n",
      "[INFO] Epoch: 28 , batch: 149 , training loss: 3.746636\n",
      "[INFO] Epoch: 28 , batch: 150 , training loss: 3.996250\n",
      "[INFO] Epoch: 28 , batch: 151 , training loss: 3.841345\n",
      "[INFO] Epoch: 28 , batch: 152 , training loss: 3.848560\n",
      "[INFO] Epoch: 28 , batch: 153 , training loss: 3.846806\n",
      "[INFO] Epoch: 28 , batch: 154 , training loss: 3.944804\n",
      "[INFO] Epoch: 28 , batch: 155 , training loss: 4.172819\n",
      "[INFO] Epoch: 28 , batch: 156 , training loss: 3.906294\n",
      "[INFO] Epoch: 28 , batch: 157 , training loss: 3.865064\n",
      "[INFO] Epoch: 28 , batch: 158 , training loss: 3.988085\n",
      "[INFO] Epoch: 28 , batch: 159 , training loss: 3.923815\n",
      "[INFO] Epoch: 28 , batch: 160 , training loss: 4.152191\n",
      "[INFO] Epoch: 28 , batch: 161 , training loss: 4.237443\n",
      "[INFO] Epoch: 28 , batch: 162 , training loss: 4.201120\n",
      "[INFO] Epoch: 28 , batch: 163 , training loss: 4.410922\n",
      "[INFO] Epoch: 28 , batch: 164 , training loss: 4.346354\n",
      "[INFO] Epoch: 28 , batch: 165 , training loss: 4.273855\n",
      "[INFO] Epoch: 28 , batch: 166 , training loss: 4.116080\n",
      "[INFO] Epoch: 28 , batch: 167 , training loss: 4.239371\n",
      "[INFO] Epoch: 28 , batch: 168 , training loss: 3.878629\n",
      "[INFO] Epoch: 28 , batch: 169 , training loss: 3.833477\n",
      "[INFO] Epoch: 28 , batch: 170 , training loss: 4.047999\n",
      "[INFO] Epoch: 28 , batch: 171 , training loss: 3.452698\n",
      "[INFO] Epoch: 28 , batch: 172 , training loss: 3.707185\n",
      "[INFO] Epoch: 28 , batch: 173 , training loss: 3.999734\n",
      "[INFO] Epoch: 28 , batch: 174 , training loss: 4.487439\n",
      "[INFO] Epoch: 28 , batch: 175 , training loss: 4.768082\n",
      "[INFO] Epoch: 28 , batch: 176 , training loss: 4.432436\n",
      "[INFO] Epoch: 28 , batch: 177 , training loss: 4.077467\n",
      "[INFO] Epoch: 28 , batch: 178 , training loss: 4.088745\n",
      "[INFO] Epoch: 28 , batch: 179 , training loss: 4.165297\n",
      "[INFO] Epoch: 28 , batch: 180 , training loss: 4.067858\n",
      "[INFO] Epoch: 28 , batch: 181 , training loss: 4.358377\n",
      "[INFO] Epoch: 28 , batch: 182 , training loss: 4.300432\n",
      "[INFO] Epoch: 28 , batch: 183 , training loss: 4.262096\n",
      "[INFO] Epoch: 28 , batch: 184 , training loss: 4.153799\n",
      "[INFO] Epoch: 28 , batch: 185 , training loss: 4.125477\n",
      "[INFO] Epoch: 28 , batch: 186 , training loss: 4.259099\n",
      "[INFO] Epoch: 28 , batch: 187 , training loss: 4.359618\n",
      "[INFO] Epoch: 28 , batch: 188 , training loss: 4.345232\n",
      "[INFO] Epoch: 28 , batch: 189 , training loss: 4.264318\n",
      "[INFO] Epoch: 28 , batch: 190 , training loss: 4.288036\n",
      "[INFO] Epoch: 28 , batch: 191 , training loss: 4.391652\n",
      "[INFO] Epoch: 28 , batch: 192 , training loss: 4.249017\n",
      "[INFO] Epoch: 28 , batch: 193 , training loss: 4.348042\n",
      "[INFO] Epoch: 28 , batch: 194 , training loss: 4.309656\n",
      "[INFO] Epoch: 28 , batch: 195 , training loss: 4.217928\n",
      "[INFO] Epoch: 28 , batch: 196 , training loss: 4.073442\n",
      "[INFO] Epoch: 28 , batch: 197 , training loss: 4.166526\n",
      "[INFO] Epoch: 28 , batch: 198 , training loss: 4.085162\n",
      "[INFO] Epoch: 28 , batch: 199 , training loss: 4.191220\n",
      "[INFO] Epoch: 28 , batch: 200 , training loss: 4.134382\n",
      "[INFO] Epoch: 28 , batch: 201 , training loss: 4.023172\n",
      "[INFO] Epoch: 28 , batch: 202 , training loss: 4.034463\n",
      "[INFO] Epoch: 28 , batch: 203 , training loss: 4.134898\n",
      "[INFO] Epoch: 28 , batch: 204 , training loss: 4.235169\n",
      "[INFO] Epoch: 28 , batch: 205 , training loss: 3.846559\n",
      "[INFO] Epoch: 28 , batch: 206 , training loss: 3.764089\n",
      "[INFO] Epoch: 28 , batch: 207 , training loss: 3.766354\n",
      "[INFO] Epoch: 28 , batch: 208 , training loss: 4.078532\n",
      "[INFO] Epoch: 28 , batch: 209 , training loss: 4.024304\n",
      "[INFO] Epoch: 28 , batch: 210 , training loss: 4.076097\n",
      "[INFO] Epoch: 28 , batch: 211 , training loss: 4.040690\n",
      "[INFO] Epoch: 28 , batch: 212 , training loss: 4.152939\n",
      "[INFO] Epoch: 28 , batch: 213 , training loss: 4.130278\n",
      "[INFO] Epoch: 28 , batch: 214 , training loss: 4.206433\n",
      "[INFO] Epoch: 28 , batch: 215 , training loss: 4.402564\n",
      "[INFO] Epoch: 28 , batch: 216 , training loss: 4.120123\n",
      "[INFO] Epoch: 28 , batch: 217 , training loss: 4.042131\n",
      "[INFO] Epoch: 28 , batch: 218 , training loss: 4.034722\n",
      "[INFO] Epoch: 28 , batch: 219 , training loss: 4.145148\n",
      "[INFO] Epoch: 28 , batch: 220 , training loss: 3.966777\n",
      "[INFO] Epoch: 28 , batch: 221 , training loss: 4.008486\n",
      "[INFO] Epoch: 28 , batch: 222 , training loss: 4.136003\n",
      "[INFO] Epoch: 28 , batch: 223 , training loss: 4.230085\n",
      "[INFO] Epoch: 28 , batch: 224 , training loss: 4.271598\n",
      "[INFO] Epoch: 28 , batch: 225 , training loss: 4.160006\n",
      "[INFO] Epoch: 28 , batch: 226 , training loss: 4.278008\n",
      "[INFO] Epoch: 28 , batch: 227 , training loss: 4.254266\n",
      "[INFO] Epoch: 28 , batch: 228 , training loss: 4.260788\n",
      "[INFO] Epoch: 28 , batch: 229 , training loss: 4.159214\n",
      "[INFO] Epoch: 28 , batch: 230 , training loss: 4.000558\n",
      "[INFO] Epoch: 28 , batch: 231 , training loss: 3.865591\n",
      "[INFO] Epoch: 28 , batch: 232 , training loss: 4.012752\n",
      "[INFO] Epoch: 28 , batch: 233 , training loss: 4.038460\n",
      "[INFO] Epoch: 28 , batch: 234 , training loss: 3.714226\n",
      "[INFO] Epoch: 28 , batch: 235 , training loss: 3.833079\n",
      "[INFO] Epoch: 28 , batch: 236 , training loss: 3.941429\n",
      "[INFO] Epoch: 28 , batch: 237 , training loss: 4.163287\n",
      "[INFO] Epoch: 28 , batch: 238 , training loss: 3.939723\n",
      "[INFO] Epoch: 28 , batch: 239 , training loss: 3.969738\n",
      "[INFO] Epoch: 28 , batch: 240 , training loss: 4.017294\n",
      "[INFO] Epoch: 28 , batch: 241 , training loss: 3.824557\n",
      "[INFO] Epoch: 28 , batch: 242 , training loss: 3.846713\n",
      "[INFO] Epoch: 28 , batch: 243 , training loss: 4.091907\n",
      "[INFO] Epoch: 28 , batch: 244 , training loss: 4.084710\n",
      "[INFO] Epoch: 28 , batch: 245 , training loss: 4.066781\n",
      "[INFO] Epoch: 28 , batch: 246 , training loss: 3.751557\n",
      "[INFO] Epoch: 28 , batch: 247 , training loss: 3.913024\n",
      "[INFO] Epoch: 28 , batch: 248 , training loss: 3.998167\n",
      "[INFO] Epoch: 28 , batch: 249 , training loss: 3.968611\n",
      "[INFO] Epoch: 28 , batch: 250 , training loss: 3.775284\n",
      "[INFO] Epoch: 28 , batch: 251 , training loss: 4.240085\n",
      "[INFO] Epoch: 28 , batch: 252 , training loss: 3.925091\n",
      "[INFO] Epoch: 28 , batch: 253 , training loss: 3.840806\n",
      "[INFO] Epoch: 28 , batch: 254 , training loss: 4.087796\n",
      "[INFO] Epoch: 28 , batch: 255 , training loss: 4.070185\n",
      "[INFO] Epoch: 28 , batch: 256 , training loss: 4.086053\n",
      "[INFO] Epoch: 28 , batch: 257 , training loss: 4.230078\n",
      "[INFO] Epoch: 28 , batch: 258 , training loss: 4.238858\n",
      "[INFO] Epoch: 28 , batch: 259 , training loss: 4.284571\n",
      "[INFO] Epoch: 28 , batch: 260 , training loss: 4.056508\n",
      "[INFO] Epoch: 28 , batch: 261 , training loss: 4.213799\n",
      "[INFO] Epoch: 28 , batch: 262 , training loss: 4.379509\n",
      "[INFO] Epoch: 28 , batch: 263 , training loss: 4.541984\n",
      "[INFO] Epoch: 28 , batch: 264 , training loss: 3.926855\n",
      "[INFO] Epoch: 28 , batch: 265 , training loss: 4.017264\n",
      "[INFO] Epoch: 28 , batch: 266 , training loss: 4.422671\n",
      "[INFO] Epoch: 28 , batch: 267 , training loss: 4.174374\n",
      "[INFO] Epoch: 28 , batch: 268 , training loss: 4.090513\n",
      "[INFO] Epoch: 28 , batch: 269 , training loss: 4.090010\n",
      "[INFO] Epoch: 28 , batch: 270 , training loss: 4.099678\n",
      "[INFO] Epoch: 28 , batch: 271 , training loss: 4.117675\n",
      "[INFO] Epoch: 28 , batch: 272 , training loss: 4.128429\n",
      "[INFO] Epoch: 28 , batch: 273 , training loss: 4.113339\n",
      "[INFO] Epoch: 28 , batch: 274 , training loss: 4.212168\n",
      "[INFO] Epoch: 28 , batch: 275 , training loss: 4.097078\n",
      "[INFO] Epoch: 28 , batch: 276 , training loss: 4.158640\n",
      "[INFO] Epoch: 28 , batch: 277 , training loss: 4.334507\n",
      "[INFO] Epoch: 28 , batch: 278 , training loss: 3.969905\n",
      "[INFO] Epoch: 28 , batch: 279 , training loss: 3.992205\n",
      "[INFO] Epoch: 28 , batch: 280 , training loss: 3.968219\n",
      "[INFO] Epoch: 28 , batch: 281 , training loss: 4.089566\n",
      "[INFO] Epoch: 28 , batch: 282 , training loss: 3.996469\n",
      "[INFO] Epoch: 28 , batch: 283 , training loss: 3.998551\n",
      "[INFO] Epoch: 28 , batch: 284 , training loss: 4.045849\n",
      "[INFO] Epoch: 28 , batch: 285 , training loss: 3.984487\n",
      "[INFO] Epoch: 28 , batch: 286 , training loss: 3.999552\n",
      "[INFO] Epoch: 28 , batch: 287 , training loss: 3.944294\n",
      "[INFO] Epoch: 28 , batch: 288 , training loss: 3.894056\n",
      "[INFO] Epoch: 28 , batch: 289 , training loss: 3.978448\n",
      "[INFO] Epoch: 28 , batch: 290 , training loss: 3.741632\n",
      "[INFO] Epoch: 28 , batch: 291 , training loss: 3.738481\n",
      "[INFO] Epoch: 28 , batch: 292 , training loss: 3.841009\n",
      "[INFO] Epoch: 28 , batch: 293 , training loss: 3.758718\n",
      "[INFO] Epoch: 28 , batch: 294 , training loss: 4.442913\n",
      "[INFO] Epoch: 28 , batch: 295 , training loss: 4.216735\n",
      "[INFO] Epoch: 28 , batch: 296 , training loss: 4.141024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 28 , batch: 297 , training loss: 4.098068\n",
      "[INFO] Epoch: 28 , batch: 298 , training loss: 3.932641\n",
      "[INFO] Epoch: 28 , batch: 299 , training loss: 3.983800\n",
      "[INFO] Epoch: 28 , batch: 300 , training loss: 3.964660\n",
      "[INFO] Epoch: 28 , batch: 301 , training loss: 3.889271\n",
      "[INFO] Epoch: 28 , batch: 302 , training loss: 4.058980\n",
      "[INFO] Epoch: 28 , batch: 303 , training loss: 4.057097\n",
      "[INFO] Epoch: 28 , batch: 304 , training loss: 4.201685\n",
      "[INFO] Epoch: 28 , batch: 305 , training loss: 4.029254\n",
      "[INFO] Epoch: 28 , batch: 306 , training loss: 4.143852\n",
      "[INFO] Epoch: 28 , batch: 307 , training loss: 4.148160\n",
      "[INFO] Epoch: 28 , batch: 308 , training loss: 3.965286\n",
      "[INFO] Epoch: 28 , batch: 309 , training loss: 3.987472\n",
      "[INFO] Epoch: 28 , batch: 310 , training loss: 3.905204\n",
      "[INFO] Epoch: 28 , batch: 311 , training loss: 3.904468\n",
      "[INFO] Epoch: 28 , batch: 312 , training loss: 3.829044\n",
      "[INFO] Epoch: 28 , batch: 313 , training loss: 3.880349\n",
      "[INFO] Epoch: 28 , batch: 314 , training loss: 3.982759\n",
      "[INFO] Epoch: 28 , batch: 315 , training loss: 4.059006\n",
      "[INFO] Epoch: 28 , batch: 316 , training loss: 4.289477\n",
      "[INFO] Epoch: 28 , batch: 317 , training loss: 4.634544\n",
      "[INFO] Epoch: 28 , batch: 318 , training loss: 4.782961\n",
      "[INFO] Epoch: 28 , batch: 319 , training loss: 4.471334\n",
      "[INFO] Epoch: 28 , batch: 320 , training loss: 4.008181\n",
      "[INFO] Epoch: 28 , batch: 321 , training loss: 3.826391\n",
      "[INFO] Epoch: 28 , batch: 322 , training loss: 3.939198\n",
      "[INFO] Epoch: 28 , batch: 323 , training loss: 3.969444\n",
      "[INFO] Epoch: 28 , batch: 324 , training loss: 3.946677\n",
      "[INFO] Epoch: 28 , batch: 325 , training loss: 4.077117\n",
      "[INFO] Epoch: 28 , batch: 326 , training loss: 4.124820\n",
      "[INFO] Epoch: 28 , batch: 327 , training loss: 4.057685\n",
      "[INFO] Epoch: 28 , batch: 328 , training loss: 4.061141\n",
      "[INFO] Epoch: 28 , batch: 329 , training loss: 3.955369\n",
      "[INFO] Epoch: 28 , batch: 330 , training loss: 3.960825\n",
      "[INFO] Epoch: 28 , batch: 331 , training loss: 4.109026\n",
      "[INFO] Epoch: 28 , batch: 332 , training loss: 3.980837\n",
      "[INFO] Epoch: 28 , batch: 333 , training loss: 3.941382\n",
      "[INFO] Epoch: 28 , batch: 334 , training loss: 3.955989\n",
      "[INFO] Epoch: 28 , batch: 335 , training loss: 4.067114\n",
      "[INFO] Epoch: 28 , batch: 336 , training loss: 4.076620\n",
      "[INFO] Epoch: 28 , batch: 337 , training loss: 4.128903\n",
      "[INFO] Epoch: 28 , batch: 338 , training loss: 4.347908\n",
      "[INFO] Epoch: 28 , batch: 339 , training loss: 4.149198\n",
      "[INFO] Epoch: 28 , batch: 340 , training loss: 4.327040\n",
      "[INFO] Epoch: 28 , batch: 341 , training loss: 4.095236\n",
      "[INFO] Epoch: 28 , batch: 342 , training loss: 3.873262\n",
      "[INFO] Epoch: 28 , batch: 343 , training loss: 3.948887\n",
      "[INFO] Epoch: 28 , batch: 344 , training loss: 3.817303\n",
      "[INFO] Epoch: 28 , batch: 345 , training loss: 3.968305\n",
      "[INFO] Epoch: 28 , batch: 346 , training loss: 3.999855\n",
      "[INFO] Epoch: 28 , batch: 347 , training loss: 3.894785\n",
      "[INFO] Epoch: 28 , batch: 348 , training loss: 3.992778\n",
      "[INFO] Epoch: 28 , batch: 349 , training loss: 4.089092\n",
      "[INFO] Epoch: 28 , batch: 350 , training loss: 3.942859\n",
      "[INFO] Epoch: 28 , batch: 351 , training loss: 4.041450\n",
      "[INFO] Epoch: 28 , batch: 352 , training loss: 4.050858\n",
      "[INFO] Epoch: 28 , batch: 353 , training loss: 4.029167\n",
      "[INFO] Epoch: 28 , batch: 354 , training loss: 4.115232\n",
      "[INFO] Epoch: 28 , batch: 355 , training loss: 4.111955\n",
      "[INFO] Epoch: 28 , batch: 356 , training loss: 3.992592\n",
      "[INFO] Epoch: 28 , batch: 357 , training loss: 4.063496\n",
      "[INFO] Epoch: 28 , batch: 358 , training loss: 3.956577\n",
      "[INFO] Epoch: 28 , batch: 359 , training loss: 3.953191\n",
      "[INFO] Epoch: 28 , batch: 360 , training loss: 4.074553\n",
      "[INFO] Epoch: 28 , batch: 361 , training loss: 4.038779\n",
      "[INFO] Epoch: 28 , batch: 362 , training loss: 4.156973\n",
      "[INFO] Epoch: 28 , batch: 363 , training loss: 4.020432\n",
      "[INFO] Epoch: 28 , batch: 364 , training loss: 4.099099\n",
      "[INFO] Epoch: 28 , batch: 365 , training loss: 4.000941\n",
      "[INFO] Epoch: 28 , batch: 366 , training loss: 4.100089\n",
      "[INFO] Epoch: 28 , batch: 367 , training loss: 4.143764\n",
      "[INFO] Epoch: 28 , batch: 368 , training loss: 4.534515\n",
      "[INFO] Epoch: 28 , batch: 369 , training loss: 4.222210\n",
      "[INFO] Epoch: 28 , batch: 370 , training loss: 3.998575\n",
      "[INFO] Epoch: 28 , batch: 371 , training loss: 4.445646\n",
      "[INFO] Epoch: 28 , batch: 372 , training loss: 4.665836\n",
      "[INFO] Epoch: 28 , batch: 373 , training loss: 4.698319\n",
      "[INFO] Epoch: 28 , batch: 374 , training loss: 4.817119\n",
      "[INFO] Epoch: 28 , batch: 375 , training loss: 4.795725\n",
      "[INFO] Epoch: 28 , batch: 376 , training loss: 4.709848\n",
      "[INFO] Epoch: 28 , batch: 377 , training loss: 4.444216\n",
      "[INFO] Epoch: 28 , batch: 378 , training loss: 4.547632\n",
      "[INFO] Epoch: 28 , batch: 379 , training loss: 4.519976\n",
      "[INFO] Epoch: 28 , batch: 380 , training loss: 4.690899\n",
      "[INFO] Epoch: 28 , batch: 381 , training loss: 4.386635\n",
      "[INFO] Epoch: 28 , batch: 382 , training loss: 4.693890\n",
      "[INFO] Epoch: 28 , batch: 383 , training loss: 4.678373\n",
      "[INFO] Epoch: 28 , batch: 384 , training loss: 4.664557\n",
      "[INFO] Epoch: 28 , batch: 385 , training loss: 4.331755\n",
      "[INFO] Epoch: 28 , batch: 386 , training loss: 4.596598\n",
      "[INFO] Epoch: 28 , batch: 387 , training loss: 4.543702\n",
      "[INFO] Epoch: 28 , batch: 388 , training loss: 4.378354\n",
      "[INFO] Epoch: 28 , batch: 389 , training loss: 4.185688\n",
      "[INFO] Epoch: 28 , batch: 390 , training loss: 4.214138\n",
      "[INFO] Epoch: 28 , batch: 391 , training loss: 4.226391\n",
      "[INFO] Epoch: 28 , batch: 392 , training loss: 4.602187\n",
      "[INFO] Epoch: 28 , batch: 393 , training loss: 4.499184\n",
      "[INFO] Epoch: 28 , batch: 394 , training loss: 4.582763\n",
      "[INFO] Epoch: 28 , batch: 395 , training loss: 4.402928\n",
      "[INFO] Epoch: 28 , batch: 396 , training loss: 4.221764\n",
      "[INFO] Epoch: 28 , batch: 397 , training loss: 4.365357\n",
      "[INFO] Epoch: 28 , batch: 398 , training loss: 4.228325\n",
      "[INFO] Epoch: 28 , batch: 399 , training loss: 4.308085\n",
      "[INFO] Epoch: 28 , batch: 400 , training loss: 4.275977\n",
      "[INFO] Epoch: 28 , batch: 401 , training loss: 4.713480\n",
      "[INFO] Epoch: 28 , batch: 402 , training loss: 4.433626\n",
      "[INFO] Epoch: 28 , batch: 403 , training loss: 4.250413\n",
      "[INFO] Epoch: 28 , batch: 404 , training loss: 4.418462\n",
      "[INFO] Epoch: 28 , batch: 405 , training loss: 4.492913\n",
      "[INFO] Epoch: 28 , batch: 406 , training loss: 4.378434\n",
      "[INFO] Epoch: 28 , batch: 407 , training loss: 4.436130\n",
      "[INFO] Epoch: 28 , batch: 408 , training loss: 4.368788\n",
      "[INFO] Epoch: 28 , batch: 409 , training loss: 4.397390\n",
      "[INFO] Epoch: 28 , batch: 410 , training loss: 4.483680\n",
      "[INFO] Epoch: 28 , batch: 411 , training loss: 4.625904\n",
      "[INFO] Epoch: 28 , batch: 412 , training loss: 4.462116\n",
      "[INFO] Epoch: 28 , batch: 413 , training loss: 4.327711\n",
      "[INFO] Epoch: 28 , batch: 414 , training loss: 4.366082\n",
      "[INFO] Epoch: 28 , batch: 415 , training loss: 4.402828\n",
      "[INFO] Epoch: 28 , batch: 416 , training loss: 4.496860\n",
      "[INFO] Epoch: 28 , batch: 417 , training loss: 4.383437\n",
      "[INFO] Epoch: 28 , batch: 418 , training loss: 4.442560\n",
      "[INFO] Epoch: 28 , batch: 419 , training loss: 4.409708\n",
      "[INFO] Epoch: 28 , batch: 420 , training loss: 4.394347\n",
      "[INFO] Epoch: 28 , batch: 421 , training loss: 4.370864\n",
      "[INFO] Epoch: 28 , batch: 422 , training loss: 4.208979\n",
      "[INFO] Epoch: 28 , batch: 423 , training loss: 4.450783\n",
      "[INFO] Epoch: 28 , batch: 424 , training loss: 4.609460\n",
      "[INFO] Epoch: 28 , batch: 425 , training loss: 4.469662\n",
      "[INFO] Epoch: 28 , batch: 426 , training loss: 4.202810\n",
      "[INFO] Epoch: 28 , batch: 427 , training loss: 4.462193\n",
      "[INFO] Epoch: 28 , batch: 428 , training loss: 4.317160\n",
      "[INFO] Epoch: 28 , batch: 429 , training loss: 4.213950\n",
      "[INFO] Epoch: 28 , batch: 430 , training loss: 4.441744\n",
      "[INFO] Epoch: 28 , batch: 431 , training loss: 4.064407\n",
      "[INFO] Epoch: 28 , batch: 432 , training loss: 4.120580\n",
      "[INFO] Epoch: 28 , batch: 433 , training loss: 4.131626\n",
      "[INFO] Epoch: 28 , batch: 434 , training loss: 4.025961\n",
      "[INFO] Epoch: 28 , batch: 435 , training loss: 4.377360\n",
      "[INFO] Epoch: 28 , batch: 436 , training loss: 4.416142\n",
      "[INFO] Epoch: 28 , batch: 437 , training loss: 4.211401\n",
      "[INFO] Epoch: 28 , batch: 438 , training loss: 4.080934\n",
      "[INFO] Epoch: 28 , batch: 439 , training loss: 4.312393\n",
      "[INFO] Epoch: 28 , batch: 440 , training loss: 4.430831\n",
      "[INFO] Epoch: 28 , batch: 441 , training loss: 4.514976\n",
      "[INFO] Epoch: 28 , batch: 442 , training loss: 4.272259\n",
      "[INFO] Epoch: 28 , batch: 443 , training loss: 4.484242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 28 , batch: 444 , training loss: 4.063177\n",
      "[INFO] Epoch: 28 , batch: 445 , training loss: 3.990301\n",
      "[INFO] Epoch: 28 , batch: 446 , training loss: 3.911928\n",
      "[INFO] Epoch: 28 , batch: 447 , training loss: 4.115348\n",
      "[INFO] Epoch: 28 , batch: 448 , training loss: 4.225512\n",
      "[INFO] Epoch: 28 , batch: 449 , training loss: 4.599518\n",
      "[INFO] Epoch: 28 , batch: 450 , training loss: 4.666244\n",
      "[INFO] Epoch: 28 , batch: 451 , training loss: 4.564692\n",
      "[INFO] Epoch: 28 , batch: 452 , training loss: 4.381107\n",
      "[INFO] Epoch: 28 , batch: 453 , training loss: 4.159682\n",
      "[INFO] Epoch: 28 , batch: 454 , training loss: 4.287174\n",
      "[INFO] Epoch: 28 , batch: 455 , training loss: 4.375058\n",
      "[INFO] Epoch: 28 , batch: 456 , training loss: 4.349357\n",
      "[INFO] Epoch: 28 , batch: 457 , training loss: 4.415935\n",
      "[INFO] Epoch: 28 , batch: 458 , training loss: 4.181505\n",
      "[INFO] Epoch: 28 , batch: 459 , training loss: 4.139352\n",
      "[INFO] Epoch: 28 , batch: 460 , training loss: 4.269990\n",
      "[INFO] Epoch: 28 , batch: 461 , training loss: 4.230156\n",
      "[INFO] Epoch: 28 , batch: 462 , training loss: 4.273375\n",
      "[INFO] Epoch: 28 , batch: 463 , training loss: 4.203508\n",
      "[INFO] Epoch: 28 , batch: 464 , training loss: 4.370626\n",
      "[INFO] Epoch: 28 , batch: 465 , training loss: 4.309347\n",
      "[INFO] Epoch: 28 , batch: 466 , training loss: 4.428296\n",
      "[INFO] Epoch: 28 , batch: 467 , training loss: 4.363925\n",
      "[INFO] Epoch: 28 , batch: 468 , training loss: 4.341258\n",
      "[INFO] Epoch: 28 , batch: 469 , training loss: 4.361768\n",
      "[INFO] Epoch: 28 , batch: 470 , training loss: 4.168249\n",
      "[INFO] Epoch: 28 , batch: 471 , training loss: 4.309021\n",
      "[INFO] Epoch: 28 , batch: 472 , training loss: 4.350379\n",
      "[INFO] Epoch: 28 , batch: 473 , training loss: 4.256492\n",
      "[INFO] Epoch: 28 , batch: 474 , training loss: 4.040448\n",
      "[INFO] Epoch: 28 , batch: 475 , training loss: 3.941778\n",
      "[INFO] Epoch: 28 , batch: 476 , training loss: 4.340915\n",
      "[INFO] Epoch: 28 , batch: 477 , training loss: 4.426227\n",
      "[INFO] Epoch: 28 , batch: 478 , training loss: 4.438368\n",
      "[INFO] Epoch: 28 , batch: 479 , training loss: 4.412111\n",
      "[INFO] Epoch: 28 , batch: 480 , training loss: 4.533160\n",
      "[INFO] Epoch: 28 , batch: 481 , training loss: 4.441773\n",
      "[INFO] Epoch: 28 , batch: 482 , training loss: 4.523559\n",
      "[INFO] Epoch: 28 , batch: 483 , training loss: 4.371104\n",
      "[INFO] Epoch: 28 , batch: 484 , training loss: 4.183197\n",
      "[INFO] Epoch: 28 , batch: 485 , training loss: 4.276207\n",
      "[INFO] Epoch: 28 , batch: 486 , training loss: 4.168307\n",
      "[INFO] Epoch: 28 , batch: 487 , training loss: 4.145993\n",
      "[INFO] Epoch: 28 , batch: 488 , training loss: 4.327207\n",
      "[INFO] Epoch: 28 , batch: 489 , training loss: 4.224333\n",
      "[INFO] Epoch: 28 , batch: 490 , training loss: 4.297818\n",
      "[INFO] Epoch: 28 , batch: 491 , training loss: 4.220618\n",
      "[INFO] Epoch: 28 , batch: 492 , training loss: 4.183595\n",
      "[INFO] Epoch: 28 , batch: 493 , training loss: 4.354065\n",
      "[INFO] Epoch: 28 , batch: 494 , training loss: 4.258999\n",
      "[INFO] Epoch: 28 , batch: 495 , training loss: 4.430652\n",
      "[INFO] Epoch: 28 , batch: 496 , training loss: 4.299729\n",
      "[INFO] Epoch: 28 , batch: 497 , training loss: 4.333497\n",
      "[INFO] Epoch: 28 , batch: 498 , training loss: 4.309447\n",
      "[INFO] Epoch: 28 , batch: 499 , training loss: 4.386581\n",
      "[INFO] Epoch: 28 , batch: 500 , training loss: 4.516609\n",
      "[INFO] Epoch: 28 , batch: 501 , training loss: 4.855761\n",
      "[INFO] Epoch: 28 , batch: 502 , training loss: 4.876494\n",
      "[INFO] Epoch: 28 , batch: 503 , training loss: 4.552743\n",
      "[INFO] Epoch: 28 , batch: 504 , training loss: 4.689559\n",
      "[INFO] Epoch: 28 , batch: 505 , training loss: 4.657089\n",
      "[INFO] Epoch: 28 , batch: 506 , training loss: 4.639400\n",
      "[INFO] Epoch: 28 , batch: 507 , training loss: 4.722301\n",
      "[INFO] Epoch: 28 , batch: 508 , training loss: 4.601574\n",
      "[INFO] Epoch: 28 , batch: 509 , training loss: 4.412738\n",
      "[INFO] Epoch: 28 , batch: 510 , training loss: 4.483233\n",
      "[INFO] Epoch: 28 , batch: 511 , training loss: 4.411151\n",
      "[INFO] Epoch: 28 , batch: 512 , training loss: 4.505663\n",
      "[INFO] Epoch: 28 , batch: 513 , training loss: 4.750243\n",
      "[INFO] Epoch: 28 , batch: 514 , training loss: 4.388421\n",
      "[INFO] Epoch: 28 , batch: 515 , training loss: 4.668311\n",
      "[INFO] Epoch: 28 , batch: 516 , training loss: 4.449897\n",
      "[INFO] Epoch: 28 , batch: 517 , training loss: 4.403540\n",
      "[INFO] Epoch: 28 , batch: 518 , training loss: 4.383416\n",
      "[INFO] Epoch: 28 , batch: 519 , training loss: 4.223969\n",
      "[INFO] Epoch: 28 , batch: 520 , training loss: 4.461009\n",
      "[INFO] Epoch: 28 , batch: 521 , training loss: 4.439909\n",
      "[INFO] Epoch: 28 , batch: 522 , training loss: 4.535590\n",
      "[INFO] Epoch: 28 , batch: 523 , training loss: 4.422895\n",
      "[INFO] Epoch: 28 , batch: 524 , training loss: 4.716677\n",
      "[INFO] Epoch: 28 , batch: 525 , training loss: 4.592021\n",
      "[INFO] Epoch: 28 , batch: 526 , training loss: 4.396198\n",
      "[INFO] Epoch: 28 , batch: 527 , training loss: 4.435582\n",
      "[INFO] Epoch: 28 , batch: 528 , training loss: 4.433631\n",
      "[INFO] Epoch: 28 , batch: 529 , training loss: 4.412222\n",
      "[INFO] Epoch: 28 , batch: 530 , training loss: 4.256217\n",
      "[INFO] Epoch: 28 , batch: 531 , training loss: 4.417516\n",
      "[INFO] Epoch: 28 , batch: 532 , training loss: 4.314289\n",
      "[INFO] Epoch: 28 , batch: 533 , training loss: 4.456712\n",
      "[INFO] Epoch: 28 , batch: 534 , training loss: 4.445915\n",
      "[INFO] Epoch: 28 , batch: 535 , training loss: 4.459520\n",
      "[INFO] Epoch: 28 , batch: 536 , training loss: 4.302614\n",
      "[INFO] Epoch: 28 , batch: 537 , training loss: 4.291213\n",
      "[INFO] Epoch: 28 , batch: 538 , training loss: 4.364531\n",
      "[INFO] Epoch: 28 , batch: 539 , training loss: 4.479198\n",
      "[INFO] Epoch: 28 , batch: 540 , training loss: 5.014850\n",
      "[INFO] Epoch: 28 , batch: 541 , training loss: 4.794622\n",
      "[INFO] Epoch: 28 , batch: 542 , training loss: 4.694984\n",
      "[INFO] Epoch: 29 , batch: 0 , training loss: 3.694453\n",
      "[INFO] Epoch: 29 , batch: 1 , training loss: 3.534554\n",
      "[INFO] Epoch: 29 , batch: 2 , training loss: 3.700966\n",
      "[INFO] Epoch: 29 , batch: 3 , training loss: 3.580433\n",
      "[INFO] Epoch: 29 , batch: 4 , training loss: 3.881824\n",
      "[INFO] Epoch: 29 , batch: 5 , training loss: 3.560048\n",
      "[INFO] Epoch: 29 , batch: 6 , training loss: 3.877639\n",
      "[INFO] Epoch: 29 , batch: 7 , training loss: 3.839603\n",
      "[INFO] Epoch: 29 , batch: 8 , training loss: 3.528742\n",
      "[INFO] Epoch: 29 , batch: 9 , training loss: 3.797579\n",
      "[INFO] Epoch: 29 , batch: 10 , training loss: 3.720729\n",
      "[INFO] Epoch: 29 , batch: 11 , training loss: 3.672104\n",
      "[INFO] Epoch: 29 , batch: 12 , training loss: 3.568961\n",
      "[INFO] Epoch: 29 , batch: 13 , training loss: 3.625710\n",
      "[INFO] Epoch: 29 , batch: 14 , training loss: 3.509794\n",
      "[INFO] Epoch: 29 , batch: 15 , training loss: 3.732619\n",
      "[INFO] Epoch: 29 , batch: 16 , training loss: 3.604834\n",
      "[INFO] Epoch: 29 , batch: 17 , training loss: 3.715277\n",
      "[INFO] Epoch: 29 , batch: 18 , training loss: 3.650621\n",
      "[INFO] Epoch: 29 , batch: 19 , training loss: 3.424327\n",
      "[INFO] Epoch: 29 , batch: 20 , training loss: 3.389494\n",
      "[INFO] Epoch: 29 , batch: 21 , training loss: 3.525631\n",
      "[INFO] Epoch: 29 , batch: 22 , training loss: 3.427196\n",
      "[INFO] Epoch: 29 , batch: 23 , training loss: 3.602511\n",
      "[INFO] Epoch: 29 , batch: 24 , training loss: 3.481224\n",
      "[INFO] Epoch: 29 , batch: 25 , training loss: 3.594522\n",
      "[INFO] Epoch: 29 , batch: 26 , training loss: 3.445094\n",
      "[INFO] Epoch: 29 , batch: 27 , training loss: 3.433284\n",
      "[INFO] Epoch: 29 , batch: 28 , training loss: 3.634606\n",
      "[INFO] Epoch: 29 , batch: 29 , training loss: 3.439690\n",
      "[INFO] Epoch: 29 , batch: 30 , training loss: 3.454191\n",
      "[INFO] Epoch: 29 , batch: 31 , training loss: 3.610320\n",
      "[INFO] Epoch: 29 , batch: 32 , training loss: 3.547350\n",
      "[INFO] Epoch: 29 , batch: 33 , training loss: 3.571940\n",
      "[INFO] Epoch: 29 , batch: 34 , training loss: 3.565758\n",
      "[INFO] Epoch: 29 , batch: 35 , training loss: 3.501511\n",
      "[INFO] Epoch: 29 , batch: 36 , training loss: 3.614643\n",
      "[INFO] Epoch: 29 , batch: 37 , training loss: 3.457503\n",
      "[INFO] Epoch: 29 , batch: 38 , training loss: 3.562756\n",
      "[INFO] Epoch: 29 , batch: 39 , training loss: 3.388337\n",
      "[INFO] Epoch: 29 , batch: 40 , training loss: 3.588358\n",
      "[INFO] Epoch: 29 , batch: 41 , training loss: 3.510285\n",
      "[INFO] Epoch: 29 , batch: 42 , training loss: 3.958638\n",
      "[INFO] Epoch: 29 , batch: 43 , training loss: 3.680515\n",
      "[INFO] Epoch: 29 , batch: 44 , training loss: 4.041483\n",
      "[INFO] Epoch: 29 , batch: 45 , training loss: 3.994780\n",
      "[INFO] Epoch: 29 , batch: 46 , training loss: 3.931166\n",
      "[INFO] Epoch: 29 , batch: 47 , training loss: 3.633800\n",
      "[INFO] Epoch: 29 , batch: 48 , training loss: 3.568802\n",
      "[INFO] Epoch: 29 , batch: 49 , training loss: 3.842111\n",
      "[INFO] Epoch: 29 , batch: 50 , training loss: 3.615985\n",
      "[INFO] Epoch: 29 , batch: 51 , training loss: 3.840860\n",
      "[INFO] Epoch: 29 , batch: 52 , training loss: 3.671629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 29 , batch: 53 , training loss: 3.754205\n",
      "[INFO] Epoch: 29 , batch: 54 , training loss: 3.781967\n",
      "[INFO] Epoch: 29 , batch: 55 , training loss: 3.894359\n",
      "[INFO] Epoch: 29 , batch: 56 , training loss: 3.674630\n",
      "[INFO] Epoch: 29 , batch: 57 , training loss: 3.625951\n",
      "[INFO] Epoch: 29 , batch: 58 , training loss: 3.706628\n",
      "[INFO] Epoch: 29 , batch: 59 , training loss: 3.783237\n",
      "[INFO] Epoch: 29 , batch: 60 , training loss: 3.720018\n",
      "[INFO] Epoch: 29 , batch: 61 , training loss: 3.774481\n",
      "[INFO] Epoch: 29 , batch: 62 , training loss: 3.664495\n",
      "[INFO] Epoch: 29 , batch: 63 , training loss: 3.834695\n",
      "[INFO] Epoch: 29 , batch: 64 , training loss: 4.058432\n",
      "[INFO] Epoch: 29 , batch: 65 , training loss: 3.735541\n",
      "[INFO] Epoch: 29 , batch: 66 , training loss: 3.643976\n",
      "[INFO] Epoch: 29 , batch: 67 , training loss: 3.662307\n",
      "[INFO] Epoch: 29 , batch: 68 , training loss: 3.816628\n",
      "[INFO] Epoch: 29 , batch: 69 , training loss: 3.742759\n",
      "[INFO] Epoch: 29 , batch: 70 , training loss: 3.967427\n",
      "[INFO] Epoch: 29 , batch: 71 , training loss: 3.805111\n",
      "[INFO] Epoch: 29 , batch: 72 , training loss: 3.849316\n",
      "[INFO] Epoch: 29 , batch: 73 , training loss: 3.831161\n",
      "[INFO] Epoch: 29 , batch: 74 , training loss: 3.952604\n",
      "[INFO] Epoch: 29 , batch: 75 , training loss: 3.797100\n",
      "[INFO] Epoch: 29 , batch: 76 , training loss: 3.902523\n",
      "[INFO] Epoch: 29 , batch: 77 , training loss: 3.836879\n",
      "[INFO] Epoch: 29 , batch: 78 , training loss: 3.937496\n",
      "[INFO] Epoch: 29 , batch: 79 , training loss: 3.799650\n",
      "[INFO] Epoch: 29 , batch: 80 , training loss: 3.964804\n",
      "[INFO] Epoch: 29 , batch: 81 , training loss: 3.916351\n",
      "[INFO] Epoch: 29 , batch: 82 , training loss: 3.872550\n",
      "[INFO] Epoch: 29 , batch: 83 , training loss: 3.965591\n",
      "[INFO] Epoch: 29 , batch: 84 , training loss: 3.916865\n",
      "[INFO] Epoch: 29 , batch: 85 , training loss: 4.018548\n",
      "[INFO] Epoch: 29 , batch: 86 , training loss: 3.939726\n",
      "[INFO] Epoch: 29 , batch: 87 , training loss: 3.922547\n",
      "[INFO] Epoch: 29 , batch: 88 , training loss: 4.048185\n",
      "[INFO] Epoch: 29 , batch: 89 , training loss: 3.843402\n",
      "[INFO] Epoch: 29 , batch: 90 , training loss: 3.979392\n",
      "[INFO] Epoch: 29 , batch: 91 , training loss: 3.846424\n",
      "[INFO] Epoch: 29 , batch: 92 , training loss: 3.871303\n",
      "[INFO] Epoch: 29 , batch: 93 , training loss: 3.973904\n",
      "[INFO] Epoch: 29 , batch: 94 , training loss: 4.139536\n",
      "[INFO] Epoch: 29 , batch: 95 , training loss: 3.894842\n",
      "[INFO] Epoch: 29 , batch: 96 , training loss: 3.858483\n",
      "[INFO] Epoch: 29 , batch: 97 , training loss: 3.807578\n",
      "[INFO] Epoch: 29 , batch: 98 , training loss: 3.763757\n",
      "[INFO] Epoch: 29 , batch: 99 , training loss: 3.887653\n",
      "[INFO] Epoch: 29 , batch: 100 , training loss: 3.740696\n",
      "[INFO] Epoch: 29 , batch: 101 , training loss: 3.788702\n",
      "[INFO] Epoch: 29 , batch: 102 , training loss: 3.933648\n",
      "[INFO] Epoch: 29 , batch: 103 , training loss: 3.734598\n",
      "[INFO] Epoch: 29 , batch: 104 , training loss: 3.683011\n",
      "[INFO] Epoch: 29 , batch: 105 , training loss: 3.945518\n",
      "[INFO] Epoch: 29 , batch: 106 , training loss: 3.973653\n",
      "[INFO] Epoch: 29 , batch: 107 , training loss: 3.801996\n",
      "[INFO] Epoch: 29 , batch: 108 , training loss: 3.757316\n",
      "[INFO] Epoch: 29 , batch: 109 , training loss: 3.691984\n",
      "[INFO] Epoch: 29 , batch: 110 , training loss: 3.860304\n",
      "[INFO] Epoch: 29 , batch: 111 , training loss: 3.929495\n",
      "[INFO] Epoch: 29 , batch: 112 , training loss: 3.848816\n",
      "[INFO] Epoch: 29 , batch: 113 , training loss: 3.854892\n",
      "[INFO] Epoch: 29 , batch: 114 , training loss: 3.835743\n",
      "[INFO] Epoch: 29 , batch: 115 , training loss: 3.826062\n",
      "[INFO] Epoch: 29 , batch: 116 , training loss: 3.767225\n",
      "[INFO] Epoch: 29 , batch: 117 , training loss: 3.985738\n",
      "[INFO] Epoch: 29 , batch: 118 , training loss: 3.958088\n",
      "[INFO] Epoch: 29 , batch: 119 , training loss: 4.063931\n",
      "[INFO] Epoch: 29 , batch: 120 , training loss: 4.042270\n",
      "[INFO] Epoch: 29 , batch: 121 , training loss: 3.932938\n",
      "[INFO] Epoch: 29 , batch: 122 , training loss: 3.855228\n",
      "[INFO] Epoch: 29 , batch: 123 , training loss: 3.843782\n",
      "[INFO] Epoch: 29 , batch: 124 , training loss: 3.955132\n",
      "[INFO] Epoch: 29 , batch: 125 , training loss: 3.742346\n",
      "[INFO] Epoch: 29 , batch: 126 , training loss: 3.767752\n",
      "[INFO] Epoch: 29 , batch: 127 , training loss: 3.777410\n",
      "[INFO] Epoch: 29 , batch: 128 , training loss: 3.936421\n",
      "[INFO] Epoch: 29 , batch: 129 , training loss: 3.873294\n",
      "[INFO] Epoch: 29 , batch: 130 , training loss: 3.869028\n",
      "[INFO] Epoch: 29 , batch: 131 , training loss: 3.867035\n",
      "[INFO] Epoch: 29 , batch: 132 , training loss: 3.894620\n",
      "[INFO] Epoch: 29 , batch: 133 , training loss: 3.857772\n",
      "[INFO] Epoch: 29 , batch: 134 , training loss: 3.605089\n",
      "[INFO] Epoch: 29 , batch: 135 , training loss: 3.679868\n",
      "[INFO] Epoch: 29 , batch: 136 , training loss: 3.982678\n",
      "[INFO] Epoch: 29 , batch: 137 , training loss: 3.867223\n",
      "[INFO] Epoch: 29 , batch: 138 , training loss: 3.943780\n",
      "[INFO] Epoch: 29 , batch: 139 , training loss: 4.525509\n",
      "[INFO] Epoch: 29 , batch: 140 , training loss: 4.318846\n",
      "[INFO] Epoch: 29 , batch: 141 , training loss: 4.073124\n",
      "[INFO] Epoch: 29 , batch: 142 , training loss: 3.793602\n",
      "[INFO] Epoch: 29 , batch: 143 , training loss: 3.894639\n",
      "[INFO] Epoch: 29 , batch: 144 , training loss: 3.764107\n",
      "[INFO] Epoch: 29 , batch: 145 , training loss: 3.848859\n",
      "[INFO] Epoch: 29 , batch: 146 , training loss: 4.025228\n",
      "[INFO] Epoch: 29 , batch: 147 , training loss: 3.678546\n",
      "[INFO] Epoch: 29 , batch: 148 , training loss: 3.680443\n",
      "[INFO] Epoch: 29 , batch: 149 , training loss: 3.772886\n",
      "[INFO] Epoch: 29 , batch: 150 , training loss: 4.019174\n",
      "[INFO] Epoch: 29 , batch: 151 , training loss: 3.867878\n",
      "[INFO] Epoch: 29 , batch: 152 , training loss: 3.880495\n",
      "[INFO] Epoch: 29 , batch: 153 , training loss: 3.871960\n",
      "[INFO] Epoch: 29 , batch: 154 , training loss: 3.964534\n",
      "[INFO] Epoch: 29 , batch: 155 , training loss: 4.218970\n",
      "[INFO] Epoch: 29 , batch: 156 , training loss: 3.938370\n",
      "[INFO] Epoch: 29 , batch: 157 , training loss: 3.925758\n",
      "[INFO] Epoch: 29 , batch: 158 , training loss: 4.011012\n",
      "[INFO] Epoch: 29 , batch: 159 , training loss: 3.963655\n",
      "[INFO] Epoch: 29 , batch: 160 , training loss: 4.191749\n",
      "[INFO] Epoch: 29 , batch: 161 , training loss: 4.282740\n",
      "[INFO] Epoch: 29 , batch: 162 , training loss: 4.234133\n",
      "[INFO] Epoch: 29 , batch: 163 , training loss: 4.425416\n",
      "[INFO] Epoch: 29 , batch: 164 , training loss: 4.349576\n",
      "[INFO] Epoch: 29 , batch: 165 , training loss: 4.287236\n",
      "[INFO] Epoch: 29 , batch: 166 , training loss: 4.131021\n",
      "[INFO] Epoch: 29 , batch: 167 , training loss: 4.320466\n",
      "[INFO] Epoch: 29 , batch: 168 , training loss: 3.925501\n",
      "[INFO] Epoch: 29 , batch: 169 , training loss: 3.920541\n",
      "[INFO] Epoch: 29 , batch: 170 , training loss: 4.089512\n",
      "[INFO] Epoch: 29 , batch: 171 , training loss: 3.519769\n",
      "[INFO] Epoch: 29 , batch: 172 , training loss: 3.744989\n",
      "[INFO] Epoch: 29 , batch: 173 , training loss: 4.057496\n",
      "[INFO] Epoch: 29 , batch: 174 , training loss: 4.530940\n",
      "[INFO] Epoch: 29 , batch: 175 , training loss: 4.785291\n",
      "[INFO] Epoch: 29 , batch: 176 , training loss: 4.488585\n",
      "[INFO] Epoch: 29 , batch: 177 , training loss: 4.093906\n",
      "[INFO] Epoch: 29 , batch: 178 , training loss: 4.099963\n",
      "[INFO] Epoch: 29 , batch: 179 , training loss: 4.143580\n",
      "[INFO] Epoch: 29 , batch: 180 , training loss: 4.106491\n",
      "[INFO] Epoch: 29 , batch: 181 , training loss: 4.392878\n",
      "[INFO] Epoch: 29 , batch: 182 , training loss: 4.352160\n",
      "[INFO] Epoch: 29 , batch: 183 , training loss: 4.284338\n",
      "[INFO] Epoch: 29 , batch: 184 , training loss: 4.175646\n",
      "[INFO] Epoch: 29 , batch: 185 , training loss: 4.150054\n",
      "[INFO] Epoch: 29 , batch: 186 , training loss: 4.293514\n",
      "[INFO] Epoch: 29 , batch: 187 , training loss: 4.372826\n",
      "[INFO] Epoch: 29 , batch: 188 , training loss: 4.400899\n",
      "[INFO] Epoch: 29 , batch: 189 , training loss: 4.294738\n",
      "[INFO] Epoch: 29 , batch: 190 , training loss: 4.317867\n",
      "[INFO] Epoch: 29 , batch: 191 , training loss: 4.413822\n",
      "[INFO] Epoch: 29 , batch: 192 , training loss: 4.269318\n",
      "[INFO] Epoch: 29 , batch: 193 , training loss: 4.354426\n",
      "[INFO] Epoch: 29 , batch: 194 , training loss: 4.310021\n",
      "[INFO] Epoch: 29 , batch: 195 , training loss: 4.236168\n",
      "[INFO] Epoch: 29 , batch: 196 , training loss: 4.100455\n",
      "[INFO] Epoch: 29 , batch: 197 , training loss: 4.188274\n",
      "[INFO] Epoch: 29 , batch: 198 , training loss: 4.095323\n",
      "[INFO] Epoch: 29 , batch: 199 , training loss: 4.223815\n",
      "[INFO] Epoch: 29 , batch: 200 , training loss: 4.150757\n",
      "[INFO] Epoch: 29 , batch: 201 , training loss: 4.028425\n",
      "[INFO] Epoch: 29 , batch: 202 , training loss: 4.037554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 29 , batch: 203 , training loss: 4.163035\n",
      "[INFO] Epoch: 29 , batch: 204 , training loss: 4.251701\n",
      "[INFO] Epoch: 29 , batch: 205 , training loss: 3.866499\n",
      "[INFO] Epoch: 29 , batch: 206 , training loss: 3.766389\n",
      "[INFO] Epoch: 29 , batch: 207 , training loss: 3.784950\n",
      "[INFO] Epoch: 29 , batch: 208 , training loss: 4.086721\n",
      "[INFO] Epoch: 29 , batch: 209 , training loss: 4.048507\n",
      "[INFO] Epoch: 29 , batch: 210 , training loss: 4.073872\n",
      "[INFO] Epoch: 29 , batch: 211 , training loss: 4.066878\n",
      "[INFO] Epoch: 29 , batch: 212 , training loss: 4.171097\n",
      "[INFO] Epoch: 29 , batch: 213 , training loss: 4.123671\n",
      "[INFO] Epoch: 29 , batch: 214 , training loss: 4.208890\n",
      "[INFO] Epoch: 29 , batch: 215 , training loss: 4.393486\n",
      "[INFO] Epoch: 29 , batch: 216 , training loss: 4.117163\n",
      "[INFO] Epoch: 29 , batch: 217 , training loss: 4.059631\n",
      "[INFO] Epoch: 29 , batch: 218 , training loss: 4.056572\n",
      "[INFO] Epoch: 29 , batch: 219 , training loss: 4.153337\n",
      "[INFO] Epoch: 29 , batch: 220 , training loss: 3.976161\n",
      "[INFO] Epoch: 29 , batch: 221 , training loss: 4.002960\n",
      "[INFO] Epoch: 29 , batch: 222 , training loss: 4.123818\n",
      "[INFO] Epoch: 29 , batch: 223 , training loss: 4.231914\n",
      "[INFO] Epoch: 29 , batch: 224 , training loss: 4.287537\n",
      "[INFO] Epoch: 29 , batch: 225 , training loss: 4.155372\n",
      "[INFO] Epoch: 29 , batch: 226 , training loss: 4.299497\n",
      "[INFO] Epoch: 29 , batch: 227 , training loss: 4.272586\n",
      "[INFO] Epoch: 29 , batch: 228 , training loss: 4.287305\n",
      "[INFO] Epoch: 29 , batch: 229 , training loss: 4.169312\n",
      "[INFO] Epoch: 29 , batch: 230 , training loss: 4.012181\n",
      "[INFO] Epoch: 29 , batch: 231 , training loss: 3.874736\n",
      "[INFO] Epoch: 29 , batch: 232 , training loss: 4.015986\n",
      "[INFO] Epoch: 29 , batch: 233 , training loss: 4.041798\n",
      "[INFO] Epoch: 29 , batch: 234 , training loss: 3.720525\n",
      "[INFO] Epoch: 29 , batch: 235 , training loss: 3.854490\n",
      "[INFO] Epoch: 29 , batch: 236 , training loss: 3.945196\n",
      "[INFO] Epoch: 29 , batch: 237 , training loss: 4.168464\n",
      "[INFO] Epoch: 29 , batch: 238 , training loss: 3.949275\n",
      "[INFO] Epoch: 29 , batch: 239 , training loss: 3.975738\n",
      "[INFO] Epoch: 29 , batch: 240 , training loss: 4.027872\n",
      "[INFO] Epoch: 29 , batch: 241 , training loss: 3.830302\n",
      "[INFO] Epoch: 29 , batch: 242 , training loss: 3.855580\n",
      "[INFO] Epoch: 29 , batch: 243 , training loss: 4.120467\n",
      "[INFO] Epoch: 29 , batch: 244 , training loss: 4.091037\n",
      "[INFO] Epoch: 29 , batch: 245 , training loss: 4.058992\n",
      "[INFO] Epoch: 29 , batch: 246 , training loss: 3.733579\n",
      "[INFO] Epoch: 29 , batch: 247 , training loss: 3.907232\n",
      "[INFO] Epoch: 29 , batch: 248 , training loss: 3.988832\n",
      "[INFO] Epoch: 29 , batch: 249 , training loss: 3.969714\n",
      "[INFO] Epoch: 29 , batch: 250 , training loss: 3.784426\n",
      "[INFO] Epoch: 29 , batch: 251 , training loss: 4.223625\n",
      "[INFO] Epoch: 29 , batch: 252 , training loss: 3.939309\n",
      "[INFO] Epoch: 29 , batch: 253 , training loss: 3.838279\n",
      "[INFO] Epoch: 29 , batch: 254 , training loss: 4.107385\n",
      "[INFO] Epoch: 29 , batch: 255 , training loss: 4.097751\n",
      "[INFO] Epoch: 29 , batch: 256 , training loss: 4.099693\n",
      "[INFO] Epoch: 29 , batch: 257 , training loss: 4.242037\n",
      "[INFO] Epoch: 29 , batch: 258 , training loss: 4.256651\n",
      "[INFO] Epoch: 29 , batch: 259 , training loss: 4.307907\n",
      "[INFO] Epoch: 29 , batch: 260 , training loss: 4.078513\n",
      "[INFO] Epoch: 29 , batch: 261 , training loss: 4.225633\n",
      "[INFO] Epoch: 29 , batch: 262 , training loss: 4.376222\n",
      "[INFO] Epoch: 29 , batch: 263 , training loss: 4.547772\n",
      "[INFO] Epoch: 29 , batch: 264 , training loss: 3.934562\n",
      "[INFO] Epoch: 29 , batch: 265 , training loss: 4.023192\n",
      "[INFO] Epoch: 29 , batch: 266 , training loss: 4.441501\n",
      "[INFO] Epoch: 29 , batch: 267 , training loss: 4.176311\n",
      "[INFO] Epoch: 29 , batch: 268 , training loss: 4.103864\n",
      "[INFO] Epoch: 29 , batch: 269 , training loss: 4.079563\n",
      "[INFO] Epoch: 29 , batch: 270 , training loss: 4.110655\n",
      "[INFO] Epoch: 29 , batch: 271 , training loss: 4.144421\n",
      "[INFO] Epoch: 29 , batch: 272 , training loss: 4.129930\n",
      "[INFO] Epoch: 29 , batch: 273 , training loss: 4.134413\n",
      "[INFO] Epoch: 29 , batch: 274 , training loss: 4.221599\n",
      "[INFO] Epoch: 29 , batch: 275 , training loss: 4.085301\n",
      "[INFO] Epoch: 29 , batch: 276 , training loss: 4.159368\n",
      "[INFO] Epoch: 29 , batch: 277 , training loss: 4.310552\n",
      "[INFO] Epoch: 29 , batch: 278 , training loss: 3.982327\n",
      "[INFO] Epoch: 29 , batch: 279 , training loss: 3.987614\n",
      "[INFO] Epoch: 29 , batch: 280 , training loss: 3.973600\n",
      "[INFO] Epoch: 29 , batch: 281 , training loss: 4.105052\n",
      "[INFO] Epoch: 29 , batch: 282 , training loss: 3.985340\n",
      "[INFO] Epoch: 29 , batch: 283 , training loss: 4.016754\n",
      "[INFO] Epoch: 29 , batch: 284 , training loss: 4.041553\n",
      "[INFO] Epoch: 29 , batch: 285 , training loss: 3.988120\n",
      "[INFO] Epoch: 29 , batch: 286 , training loss: 4.005203\n",
      "[INFO] Epoch: 29 , batch: 287 , training loss: 3.952703\n",
      "[INFO] Epoch: 29 , batch: 288 , training loss: 3.902347\n",
      "[INFO] Epoch: 29 , batch: 289 , training loss: 3.973876\n",
      "[INFO] Epoch: 29 , batch: 290 , training loss: 3.744209\n",
      "[INFO] Epoch: 29 , batch: 291 , training loss: 3.732743\n",
      "[INFO] Epoch: 29 , batch: 292 , training loss: 3.861303\n",
      "[INFO] Epoch: 29 , batch: 293 , training loss: 3.773843\n",
      "[INFO] Epoch: 29 , batch: 294 , training loss: 4.439081\n",
      "[INFO] Epoch: 29 , batch: 295 , training loss: 4.229855\n",
      "[INFO] Epoch: 29 , batch: 296 , training loss: 4.146303\n",
      "[INFO] Epoch: 29 , batch: 297 , training loss: 4.098074\n",
      "[INFO] Epoch: 29 , batch: 298 , training loss: 3.933989\n",
      "[INFO] Epoch: 29 , batch: 299 , training loss: 3.984298\n",
      "[INFO] Epoch: 29 , batch: 300 , training loss: 3.976459\n",
      "[INFO] Epoch: 29 , batch: 301 , training loss: 3.889356\n",
      "[INFO] Epoch: 29 , batch: 302 , training loss: 4.067077\n",
      "[INFO] Epoch: 29 , batch: 303 , training loss: 4.066791\n",
      "[INFO] Epoch: 29 , batch: 304 , training loss: 4.211129\n",
      "[INFO] Epoch: 29 , batch: 305 , training loss: 4.037491\n",
      "[INFO] Epoch: 29 , batch: 306 , training loss: 4.151152\n",
      "[INFO] Epoch: 29 , batch: 307 , training loss: 4.163559\n",
      "[INFO] Epoch: 29 , batch: 308 , training loss: 3.974778\n",
      "[INFO] Epoch: 29 , batch: 309 , training loss: 3.996389\n",
      "[INFO] Epoch: 29 , batch: 310 , training loss: 3.917223\n",
      "[INFO] Epoch: 29 , batch: 311 , training loss: 3.896105\n",
      "[INFO] Epoch: 29 , batch: 312 , training loss: 3.828758\n",
      "[INFO] Epoch: 29 , batch: 313 , training loss: 3.911110\n",
      "[INFO] Epoch: 29 , batch: 314 , training loss: 3.974287\n",
      "[INFO] Epoch: 29 , batch: 315 , training loss: 4.067299\n",
      "[INFO] Epoch: 29 , batch: 316 , training loss: 4.307862\n",
      "[INFO] Epoch: 29 , batch: 317 , training loss: 4.643860\n",
      "[INFO] Epoch: 29 , batch: 318 , training loss: 4.788627\n",
      "[INFO] Epoch: 29 , batch: 319 , training loss: 4.483364\n",
      "[INFO] Epoch: 29 , batch: 320 , training loss: 4.030284\n",
      "[INFO] Epoch: 29 , batch: 321 , training loss: 3.833375\n",
      "[INFO] Epoch: 29 , batch: 322 , training loss: 3.949771\n",
      "[INFO] Epoch: 29 , batch: 323 , training loss: 3.992653\n",
      "[INFO] Epoch: 29 , batch: 324 , training loss: 3.952520\n",
      "[INFO] Epoch: 29 , batch: 325 , training loss: 4.077162\n",
      "[INFO] Epoch: 29 , batch: 326 , training loss: 4.143466\n",
      "[INFO] Epoch: 29 , batch: 327 , training loss: 4.062344\n",
      "[INFO] Epoch: 29 , batch: 328 , training loss: 4.053733\n",
      "[INFO] Epoch: 29 , batch: 329 , training loss: 3.966935\n",
      "[INFO] Epoch: 29 , batch: 330 , training loss: 3.969311\n",
      "[INFO] Epoch: 29 , batch: 331 , training loss: 4.125850\n",
      "[INFO] Epoch: 29 , batch: 332 , training loss: 3.987464\n",
      "[INFO] Epoch: 29 , batch: 333 , training loss: 3.943619\n",
      "[INFO] Epoch: 29 , batch: 334 , training loss: 3.962993\n",
      "[INFO] Epoch: 29 , batch: 335 , training loss: 4.099953\n",
      "[INFO] Epoch: 29 , batch: 336 , training loss: 4.085299\n",
      "[INFO] Epoch: 29 , batch: 337 , training loss: 4.140275\n",
      "[INFO] Epoch: 29 , batch: 338 , training loss: 4.347578\n",
      "[INFO] Epoch: 29 , batch: 339 , training loss: 4.160056\n",
      "[INFO] Epoch: 29 , batch: 340 , training loss: 4.335914\n",
      "[INFO] Epoch: 29 , batch: 341 , training loss: 4.092975\n",
      "[INFO] Epoch: 29 , batch: 342 , training loss: 3.898540\n",
      "[INFO] Epoch: 29 , batch: 343 , training loss: 3.970556\n",
      "[INFO] Epoch: 29 , batch: 344 , training loss: 3.832278\n",
      "[INFO] Epoch: 29 , batch: 345 , training loss: 3.975317\n",
      "[INFO] Epoch: 29 , batch: 346 , training loss: 4.018252\n",
      "[INFO] Epoch: 29 , batch: 347 , training loss: 3.921612\n",
      "[INFO] Epoch: 29 , batch: 348 , training loss: 4.016029\n",
      "[INFO] Epoch: 29 , batch: 349 , training loss: 4.144541\n",
      "[INFO] Epoch: 29 , batch: 350 , training loss: 3.967514\n",
      "[INFO] Epoch: 29 , batch: 351 , training loss: 4.054773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 29 , batch: 352 , training loss: 4.077241\n",
      "[INFO] Epoch: 29 , batch: 353 , training loss: 4.039816\n",
      "[INFO] Epoch: 29 , batch: 354 , training loss: 4.138001\n",
      "[INFO] Epoch: 29 , batch: 355 , training loss: 4.139802\n",
      "[INFO] Epoch: 29 , batch: 356 , training loss: 4.014264\n",
      "[INFO] Epoch: 29 , batch: 357 , training loss: 4.087939\n",
      "[INFO] Epoch: 29 , batch: 358 , training loss: 3.982230\n",
      "[INFO] Epoch: 29 , batch: 359 , training loss: 3.980338\n",
      "[INFO] Epoch: 29 , batch: 360 , training loss: 4.107587\n",
      "[INFO] Epoch: 29 , batch: 361 , training loss: 4.069265\n",
      "[INFO] Epoch: 29 , batch: 362 , training loss: 4.164564\n",
      "[INFO] Epoch: 29 , batch: 363 , training loss: 4.043937\n",
      "[INFO] Epoch: 29 , batch: 364 , training loss: 4.119786\n",
      "[INFO] Epoch: 29 , batch: 365 , training loss: 4.005493\n",
      "[INFO] Epoch: 29 , batch: 366 , training loss: 4.135037\n",
      "[INFO] Epoch: 29 , batch: 367 , training loss: 4.170794\n",
      "[INFO] Epoch: 29 , batch: 368 , training loss: 4.572831\n",
      "[INFO] Epoch: 29 , batch: 369 , training loss: 4.260986\n",
      "[INFO] Epoch: 29 , batch: 370 , training loss: 4.032470\n",
      "[INFO] Epoch: 29 , batch: 371 , training loss: 4.477630\n",
      "[INFO] Epoch: 29 , batch: 372 , training loss: 4.694547\n",
      "[INFO] Epoch: 29 , batch: 373 , training loss: 4.757160\n",
      "[INFO] Epoch: 29 , batch: 374 , training loss: 4.866408\n",
      "[INFO] Epoch: 29 , batch: 375 , training loss: 4.819998\n",
      "[INFO] Epoch: 29 , batch: 376 , training loss: 4.755529\n",
      "[INFO] Epoch: 29 , batch: 377 , training loss: 4.499496\n",
      "[INFO] Epoch: 29 , batch: 378 , training loss: 4.580148\n",
      "[INFO] Epoch: 29 , batch: 379 , training loss: 4.562758\n",
      "[INFO] Epoch: 29 , batch: 380 , training loss: 4.709445\n",
      "[INFO] Epoch: 29 , batch: 381 , training loss: 4.419261\n",
      "[INFO] Epoch: 29 , batch: 382 , training loss: 4.711079\n",
      "[INFO] Epoch: 29 , batch: 383 , training loss: 4.720613\n",
      "[INFO] Epoch: 29 , batch: 384 , training loss: 4.702967\n",
      "[INFO] Epoch: 29 , batch: 385 , training loss: 4.359074\n",
      "[INFO] Epoch: 29 , batch: 386 , training loss: 4.613780\n",
      "[INFO] Epoch: 29 , batch: 387 , training loss: 4.584676\n",
      "[INFO] Epoch: 29 , batch: 388 , training loss: 4.403240\n",
      "[INFO] Epoch: 29 , batch: 389 , training loss: 4.217275\n",
      "[INFO] Epoch: 29 , batch: 390 , training loss: 4.234979\n",
      "[INFO] Epoch: 29 , batch: 391 , training loss: 4.264510\n",
      "[INFO] Epoch: 29 , batch: 392 , training loss: 4.620733\n",
      "[INFO] Epoch: 29 , batch: 393 , training loss: 4.522734\n",
      "[INFO] Epoch: 29 , batch: 394 , training loss: 4.593114\n",
      "[INFO] Epoch: 29 , batch: 395 , training loss: 4.412831\n",
      "[INFO] Epoch: 29 , batch: 396 , training loss: 4.252912\n",
      "[INFO] Epoch: 29 , batch: 397 , training loss: 4.383648\n",
      "[INFO] Epoch: 29 , batch: 398 , training loss: 4.230888\n",
      "[INFO] Epoch: 29 , batch: 399 , training loss: 4.319204\n",
      "[INFO] Epoch: 29 , batch: 400 , training loss: 4.305721\n",
      "[INFO] Epoch: 29 , batch: 401 , training loss: 4.717572\n",
      "[INFO] Epoch: 29 , batch: 402 , training loss: 4.431524\n",
      "[INFO] Epoch: 29 , batch: 403 , training loss: 4.263495\n",
      "[INFO] Epoch: 29 , batch: 404 , training loss: 4.416097\n",
      "[INFO] Epoch: 29 , batch: 405 , training loss: 4.498117\n",
      "[INFO] Epoch: 29 , batch: 406 , training loss: 4.393411\n",
      "[INFO] Epoch: 29 , batch: 407 , training loss: 4.454392\n",
      "[INFO] Epoch: 29 , batch: 408 , training loss: 4.397662\n",
      "[INFO] Epoch: 29 , batch: 409 , training loss: 4.411617\n",
      "[INFO] Epoch: 29 , batch: 410 , training loss: 4.480722\n",
      "[INFO] Epoch: 29 , batch: 411 , training loss: 4.642031\n",
      "[INFO] Epoch: 29 , batch: 412 , training loss: 4.466121\n",
      "[INFO] Epoch: 29 , batch: 413 , training loss: 4.345868\n",
      "[INFO] Epoch: 29 , batch: 414 , training loss: 4.374076\n",
      "[INFO] Epoch: 29 , batch: 415 , training loss: 4.420711\n",
      "[INFO] Epoch: 29 , batch: 416 , training loss: 4.507488\n",
      "[INFO] Epoch: 29 , batch: 417 , training loss: 4.402336\n",
      "[INFO] Epoch: 29 , batch: 418 , training loss: 4.447433\n",
      "[INFO] Epoch: 29 , batch: 419 , training loss: 4.427164\n",
      "[INFO] Epoch: 29 , batch: 420 , training loss: 4.398348\n",
      "[INFO] Epoch: 29 , batch: 421 , training loss: 4.362298\n",
      "[INFO] Epoch: 29 , batch: 422 , training loss: 4.220425\n",
      "[INFO] Epoch: 29 , batch: 423 , training loss: 4.458617\n",
      "[INFO] Epoch: 29 , batch: 424 , training loss: 4.634546\n",
      "[INFO] Epoch: 29 , batch: 425 , training loss: 4.478339\n",
      "[INFO] Epoch: 29 , batch: 426 , training loss: 4.217215\n",
      "[INFO] Epoch: 29 , batch: 427 , training loss: 4.468011\n",
      "[INFO] Epoch: 29 , batch: 428 , training loss: 4.315151\n",
      "[INFO] Epoch: 29 , batch: 429 , training loss: 4.228912\n",
      "[INFO] Epoch: 29 , batch: 430 , training loss: 4.461721\n",
      "[INFO] Epoch: 29 , batch: 431 , training loss: 4.067845\n",
      "[INFO] Epoch: 29 , batch: 432 , training loss: 4.118635\n",
      "[INFO] Epoch: 29 , batch: 433 , training loss: 4.145750\n",
      "[INFO] Epoch: 29 , batch: 434 , training loss: 4.045292\n",
      "[INFO] Epoch: 29 , batch: 435 , training loss: 4.394441\n",
      "[INFO] Epoch: 29 , batch: 436 , training loss: 4.434592\n",
      "[INFO] Epoch: 29 , batch: 437 , training loss: 4.219380\n",
      "[INFO] Epoch: 29 , batch: 438 , training loss: 4.094946\n",
      "[INFO] Epoch: 29 , batch: 439 , training loss: 4.324946\n",
      "[INFO] Epoch: 29 , batch: 440 , training loss: 4.445409\n",
      "[INFO] Epoch: 29 , batch: 441 , training loss: 4.533639\n",
      "[INFO] Epoch: 29 , batch: 442 , training loss: 4.290942\n",
      "[INFO] Epoch: 29 , batch: 443 , training loss: 4.493103\n",
      "[INFO] Epoch: 29 , batch: 444 , training loss: 4.070624\n",
      "[INFO] Epoch: 29 , batch: 445 , training loss: 3.995075\n",
      "[INFO] Epoch: 29 , batch: 446 , training loss: 3.912948\n",
      "[INFO] Epoch: 29 , batch: 447 , training loss: 4.109485\n",
      "[INFO] Epoch: 29 , batch: 448 , training loss: 4.242805\n",
      "[INFO] Epoch: 29 , batch: 449 , training loss: 4.626248\n",
      "[INFO] Epoch: 29 , batch: 450 , training loss: 4.687358\n",
      "[INFO] Epoch: 29 , batch: 451 , training loss: 4.560576\n",
      "[INFO] Epoch: 29 , batch: 452 , training loss: 4.397809\n",
      "[INFO] Epoch: 29 , batch: 453 , training loss: 4.165794\n",
      "[INFO] Epoch: 29 , batch: 454 , training loss: 4.315160\n",
      "[INFO] Epoch: 29 , batch: 455 , training loss: 4.370704\n",
      "[INFO] Epoch: 29 , batch: 456 , training loss: 4.355649\n",
      "[INFO] Epoch: 29 , batch: 457 , training loss: 4.423120\n",
      "[INFO] Epoch: 29 , batch: 458 , training loss: 4.178515\n",
      "[INFO] Epoch: 29 , batch: 459 , training loss: 4.152887\n",
      "[INFO] Epoch: 29 , batch: 460 , training loss: 4.275627\n",
      "[INFO] Epoch: 29 , batch: 461 , training loss: 4.246993\n",
      "[INFO] Epoch: 29 , batch: 462 , training loss: 4.292527\n",
      "[INFO] Epoch: 29 , batch: 463 , training loss: 4.197566\n",
      "[INFO] Epoch: 29 , batch: 464 , training loss: 4.397179\n",
      "[INFO] Epoch: 29 , batch: 465 , training loss: 4.322162\n",
      "[INFO] Epoch: 29 , batch: 466 , training loss: 4.417036\n",
      "[INFO] Epoch: 29 , batch: 467 , training loss: 4.380510\n",
      "[INFO] Epoch: 29 , batch: 468 , training loss: 4.338959\n",
      "[INFO] Epoch: 29 , batch: 469 , training loss: 4.379202\n",
      "[INFO] Epoch: 29 , batch: 470 , training loss: 4.189980\n",
      "[INFO] Epoch: 29 , batch: 471 , training loss: 4.313695\n",
      "[INFO] Epoch: 29 , batch: 472 , training loss: 4.364935\n",
      "[INFO] Epoch: 29 , batch: 473 , training loss: 4.261559\n",
      "[INFO] Epoch: 29 , batch: 474 , training loss: 4.049860\n",
      "[INFO] Epoch: 29 , batch: 475 , training loss: 3.951453\n",
      "[INFO] Epoch: 29 , batch: 476 , training loss: 4.348245\n",
      "[INFO] Epoch: 29 , batch: 477 , training loss: 4.437099\n",
      "[INFO] Epoch: 29 , batch: 478 , training loss: 4.454582\n",
      "[INFO] Epoch: 29 , batch: 479 , training loss: 4.430156\n",
      "[INFO] Epoch: 29 , batch: 480 , training loss: 4.541938\n",
      "[INFO] Epoch: 29 , batch: 481 , training loss: 4.437103\n",
      "[INFO] Epoch: 29 , batch: 482 , training loss: 4.540666\n",
      "[INFO] Epoch: 29 , batch: 483 , training loss: 4.381615\n",
      "[INFO] Epoch: 29 , batch: 484 , training loss: 4.190858\n",
      "[INFO] Epoch: 29 , batch: 485 , training loss: 4.266369\n",
      "[INFO] Epoch: 29 , batch: 486 , training loss: 4.174119\n",
      "[INFO] Epoch: 29 , batch: 487 , training loss: 4.160936\n",
      "[INFO] Epoch: 29 , batch: 488 , training loss: 4.336654\n",
      "[INFO] Epoch: 29 , batch: 489 , training loss: 4.246787\n",
      "[INFO] Epoch: 29 , batch: 490 , training loss: 4.308640\n",
      "[INFO] Epoch: 29 , batch: 491 , training loss: 4.228014\n",
      "[INFO] Epoch: 29 , batch: 492 , training loss: 4.188142\n",
      "[INFO] Epoch: 29 , batch: 493 , training loss: 4.359321\n",
      "[INFO] Epoch: 29 , batch: 494 , training loss: 4.264230\n",
      "[INFO] Epoch: 29 , batch: 495 , training loss: 4.440370\n",
      "[INFO] Epoch: 29 , batch: 496 , training loss: 4.302902\n",
      "[INFO] Epoch: 29 , batch: 497 , training loss: 4.352021\n",
      "[INFO] Epoch: 29 , batch: 498 , training loss: 4.317321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 29 , batch: 499 , training loss: 4.396112\n",
      "[INFO] Epoch: 29 , batch: 500 , training loss: 4.504742\n",
      "[INFO] Epoch: 29 , batch: 501 , training loss: 4.861145\n",
      "[INFO] Epoch: 29 , batch: 502 , training loss: 4.893322\n",
      "[INFO] Epoch: 29 , batch: 503 , training loss: 4.575397\n",
      "[INFO] Epoch: 29 , batch: 504 , training loss: 4.709233\n",
      "[INFO] Epoch: 29 , batch: 505 , training loss: 4.668076\n",
      "[INFO] Epoch: 29 , batch: 506 , training loss: 4.644381\n",
      "[INFO] Epoch: 29 , batch: 507 , training loss: 4.713638\n",
      "[INFO] Epoch: 29 , batch: 508 , training loss: 4.615243\n",
      "[INFO] Epoch: 29 , batch: 509 , training loss: 4.423703\n",
      "[INFO] Epoch: 29 , batch: 510 , training loss: 4.492406\n",
      "[INFO] Epoch: 29 , batch: 511 , training loss: 4.434916\n",
      "[INFO] Epoch: 29 , batch: 512 , training loss: 4.498894\n",
      "[INFO] Epoch: 29 , batch: 513 , training loss: 4.757397\n",
      "[INFO] Epoch: 29 , batch: 514 , training loss: 4.404157\n",
      "[INFO] Epoch: 29 , batch: 515 , training loss: 4.672114\n",
      "[INFO] Epoch: 29 , batch: 516 , training loss: 4.446519\n",
      "[INFO] Epoch: 29 , batch: 517 , training loss: 4.427871\n",
      "[INFO] Epoch: 29 , batch: 518 , training loss: 4.390200\n",
      "[INFO] Epoch: 29 , batch: 519 , training loss: 4.224565\n",
      "[INFO] Epoch: 29 , batch: 520 , training loss: 4.465573\n",
      "[INFO] Epoch: 29 , batch: 521 , training loss: 4.448355\n",
      "[INFO] Epoch: 29 , batch: 522 , training loss: 4.534933\n",
      "[INFO] Epoch: 29 , batch: 523 , training loss: 4.443946\n",
      "[INFO] Epoch: 29 , batch: 524 , training loss: 4.721143\n",
      "[INFO] Epoch: 29 , batch: 525 , training loss: 4.604599\n",
      "[INFO] Epoch: 29 , batch: 526 , training loss: 4.376734\n",
      "[INFO] Epoch: 29 , batch: 527 , training loss: 4.429961\n",
      "[INFO] Epoch: 29 , batch: 528 , training loss: 4.456856\n",
      "[INFO] Epoch: 29 , batch: 529 , training loss: 4.426867\n",
      "[INFO] Epoch: 29 , batch: 530 , training loss: 4.265464\n",
      "[INFO] Epoch: 29 , batch: 531 , training loss: 4.414647\n",
      "[INFO] Epoch: 29 , batch: 532 , training loss: 4.332970\n",
      "[INFO] Epoch: 29 , batch: 533 , training loss: 4.469308\n",
      "[INFO] Epoch: 29 , batch: 534 , training loss: 4.462963\n",
      "[INFO] Epoch: 29 , batch: 535 , training loss: 4.458196\n",
      "[INFO] Epoch: 29 , batch: 536 , training loss: 4.313228\n",
      "[INFO] Epoch: 29 , batch: 537 , training loss: 4.300847\n",
      "[INFO] Epoch: 29 , batch: 538 , training loss: 4.376622\n",
      "[INFO] Epoch: 29 , batch: 539 , training loss: 4.487505\n",
      "[INFO] Epoch: 29 , batch: 540 , training loss: 5.005365\n",
      "[INFO] Epoch: 29 , batch: 541 , training loss: 4.833320\n",
      "[INFO] Epoch: 29 , batch: 542 , training loss: 4.707431\n",
      "[INFO] Epoch: 30 , batch: 0 , training loss: 3.755722\n",
      "[INFO] Epoch: 30 , batch: 1 , training loss: 3.555990\n",
      "[INFO] Epoch: 30 , batch: 2 , training loss: 3.727802\n",
      "[INFO] Epoch: 30 , batch: 3 , training loss: 3.598153\n",
      "[INFO] Epoch: 30 , batch: 4 , training loss: 3.872288\n",
      "[INFO] Epoch: 30 , batch: 5 , training loss: 3.587695\n",
      "[INFO] Epoch: 30 , batch: 6 , training loss: 3.949260\n",
      "[INFO] Epoch: 30 , batch: 7 , training loss: 3.823886\n",
      "[INFO] Epoch: 30 , batch: 8 , training loss: 3.556097\n",
      "[INFO] Epoch: 30 , batch: 9 , training loss: 3.807044\n",
      "[INFO] Epoch: 30 , batch: 10 , training loss: 3.726084\n",
      "[INFO] Epoch: 30 , batch: 11 , training loss: 3.670407\n",
      "[INFO] Epoch: 30 , batch: 12 , training loss: 3.579622\n",
      "[INFO] Epoch: 30 , batch: 13 , training loss: 3.616135\n",
      "[INFO] Epoch: 30 , batch: 14 , training loss: 3.509618\n",
      "[INFO] Epoch: 30 , batch: 15 , training loss: 3.759004\n",
      "[INFO] Epoch: 30 , batch: 16 , training loss: 3.599212\n",
      "[INFO] Epoch: 30 , batch: 17 , training loss: 3.745435\n",
      "[INFO] Epoch: 30 , batch: 18 , training loss: 3.671872\n",
      "[INFO] Epoch: 30 , batch: 19 , training loss: 3.464799\n",
      "[INFO] Epoch: 30 , batch: 20 , training loss: 3.407895\n",
      "[INFO] Epoch: 30 , batch: 21 , training loss: 3.533343\n",
      "[INFO] Epoch: 30 , batch: 22 , training loss: 3.442269\n",
      "[INFO] Epoch: 30 , batch: 23 , training loss: 3.600895\n",
      "[INFO] Epoch: 30 , batch: 24 , training loss: 3.499562\n",
      "[INFO] Epoch: 30 , batch: 25 , training loss: 3.611498\n",
      "[INFO] Epoch: 30 , batch: 26 , training loss: 3.478754\n",
      "[INFO] Epoch: 30 , batch: 27 , training loss: 3.444584\n",
      "[INFO] Epoch: 30 , batch: 28 , training loss: 3.650664\n",
      "[INFO] Epoch: 30 , batch: 29 , training loss: 3.448961\n",
      "[INFO] Epoch: 30 , batch: 30 , training loss: 3.471412\n",
      "[INFO] Epoch: 30 , batch: 31 , training loss: 3.603446\n",
      "[INFO] Epoch: 30 , batch: 32 , training loss: 3.537823\n",
      "[INFO] Epoch: 30 , batch: 33 , training loss: 3.589681\n",
      "[INFO] Epoch: 30 , batch: 34 , training loss: 3.586078\n",
      "[INFO] Epoch: 30 , batch: 35 , training loss: 3.503266\n",
      "[INFO] Epoch: 30 , batch: 36 , training loss: 3.631322\n",
      "[INFO] Epoch: 30 , batch: 37 , training loss: 3.478977\n",
      "[INFO] Epoch: 30 , batch: 38 , training loss: 3.537134\n",
      "[INFO] Epoch: 30 , batch: 39 , training loss: 3.350243\n",
      "[INFO] Epoch: 30 , batch: 40 , training loss: 3.586718\n",
      "[INFO] Epoch: 30 , batch: 41 , training loss: 3.532175\n",
      "[INFO] Epoch: 30 , batch: 42 , training loss: 3.988823\n",
      "[INFO] Epoch: 30 , batch: 43 , training loss: 3.734836\n",
      "[INFO] Epoch: 30 , batch: 44 , training loss: 4.084024\n",
      "[INFO] Epoch: 30 , batch: 45 , training loss: 3.993638\n",
      "[INFO] Epoch: 30 , batch: 46 , training loss: 3.983844\n",
      "[INFO] Epoch: 30 , batch: 47 , training loss: 3.606740\n",
      "[INFO] Epoch: 30 , batch: 48 , training loss: 3.581939\n",
      "[INFO] Epoch: 30 , batch: 49 , training loss: 3.823678\n",
      "[INFO] Epoch: 30 , batch: 50 , training loss: 3.572995\n",
      "[INFO] Epoch: 30 , batch: 51 , training loss: 3.841892\n",
      "[INFO] Epoch: 30 , batch: 52 , training loss: 3.669501\n",
      "[INFO] Epoch: 30 , batch: 53 , training loss: 3.758999\n",
      "[INFO] Epoch: 30 , batch: 54 , training loss: 3.796427\n",
      "[INFO] Epoch: 30 , batch: 55 , training loss: 3.875556\n",
      "[INFO] Epoch: 30 , batch: 56 , training loss: 3.667983\n",
      "[INFO] Epoch: 30 , batch: 57 , training loss: 3.624274\n",
      "[INFO] Epoch: 30 , batch: 58 , training loss: 3.690727\n",
      "[INFO] Epoch: 30 , batch: 59 , training loss: 3.786974\n",
      "[INFO] Epoch: 30 , batch: 60 , training loss: 3.709519\n",
      "[INFO] Epoch: 30 , batch: 61 , training loss: 3.798905\n",
      "[INFO] Epoch: 30 , batch: 62 , training loss: 3.649952\n",
      "[INFO] Epoch: 30 , batch: 63 , training loss: 3.852540\n",
      "[INFO] Epoch: 30 , batch: 64 , training loss: 4.061909\n",
      "[INFO] Epoch: 30 , batch: 65 , training loss: 3.725284\n",
      "[INFO] Epoch: 30 , batch: 66 , training loss: 3.608399\n",
      "[INFO] Epoch: 30 , batch: 67 , training loss: 3.639563\n",
      "[INFO] Epoch: 30 , batch: 68 , training loss: 3.787711\n",
      "[INFO] Epoch: 30 , batch: 69 , training loss: 3.708479\n",
      "[INFO] Epoch: 30 , batch: 70 , training loss: 3.965458\n",
      "[INFO] Epoch: 30 , batch: 71 , training loss: 3.786456\n",
      "[INFO] Epoch: 30 , batch: 72 , training loss: 3.860217\n",
      "[INFO] Epoch: 30 , batch: 73 , training loss: 3.818208\n",
      "[INFO] Epoch: 30 , batch: 74 , training loss: 3.939381\n",
      "[INFO] Epoch: 30 , batch: 75 , training loss: 3.772844\n",
      "[INFO] Epoch: 30 , batch: 76 , training loss: 3.854300\n",
      "[INFO] Epoch: 30 , batch: 77 , training loss: 3.843372\n",
      "[INFO] Epoch: 30 , batch: 78 , training loss: 3.921420\n",
      "[INFO] Epoch: 30 , batch: 79 , training loss: 3.776240\n",
      "[INFO] Epoch: 30 , batch: 80 , training loss: 3.971444\n",
      "[INFO] Epoch: 30 , batch: 81 , training loss: 3.875727\n",
      "[INFO] Epoch: 30 , batch: 82 , training loss: 3.873658\n",
      "[INFO] Epoch: 30 , batch: 83 , training loss: 3.966686\n",
      "[INFO] Epoch: 30 , batch: 84 , training loss: 3.911586\n",
      "[INFO] Epoch: 30 , batch: 85 , training loss: 4.022525\n",
      "[INFO] Epoch: 30 , batch: 86 , training loss: 3.955703\n",
      "[INFO] Epoch: 30 , batch: 87 , training loss: 3.898592\n",
      "[INFO] Epoch: 30 , batch: 88 , training loss: 4.029006\n",
      "[INFO] Epoch: 30 , batch: 89 , training loss: 3.830980\n",
      "[INFO] Epoch: 30 , batch: 90 , training loss: 3.949475\n",
      "[INFO] Epoch: 30 , batch: 91 , training loss: 3.846358\n",
      "[INFO] Epoch: 30 , batch: 92 , training loss: 3.863253\n",
      "[INFO] Epoch: 30 , batch: 93 , training loss: 3.971457\n",
      "[INFO] Epoch: 30 , batch: 94 , training loss: 4.124431\n",
      "[INFO] Epoch: 30 , batch: 95 , training loss: 3.867910\n",
      "[INFO] Epoch: 30 , batch: 96 , training loss: 3.857860\n",
      "[INFO] Epoch: 30 , batch: 97 , training loss: 3.803004\n",
      "[INFO] Epoch: 30 , batch: 98 , training loss: 3.750387\n",
      "[INFO] Epoch: 30 , batch: 99 , training loss: 3.859644\n",
      "[INFO] Epoch: 30 , batch: 100 , training loss: 3.741331\n",
      "[INFO] Epoch: 30 , batch: 101 , training loss: 3.767238\n",
      "[INFO] Epoch: 30 , batch: 102 , training loss: 3.943512\n",
      "[INFO] Epoch: 30 , batch: 103 , training loss: 3.734984\n",
      "[INFO] Epoch: 30 , batch: 104 , training loss: 3.670965\n",
      "[INFO] Epoch: 30 , batch: 105 , training loss: 3.950967\n",
      "[INFO] Epoch: 30 , batch: 106 , training loss: 3.942797\n",
      "[INFO] Epoch: 30 , batch: 107 , training loss: 3.772366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 30 , batch: 108 , training loss: 3.730068\n",
      "[INFO] Epoch: 30 , batch: 109 , training loss: 3.676386\n",
      "[INFO] Epoch: 30 , batch: 110 , training loss: 3.863442\n",
      "[INFO] Epoch: 30 , batch: 111 , training loss: 3.931866\n",
      "[INFO] Epoch: 30 , batch: 112 , training loss: 3.838618\n",
      "[INFO] Epoch: 30 , batch: 113 , training loss: 3.846818\n",
      "[INFO] Epoch: 30 , batch: 114 , training loss: 3.837720\n",
      "[INFO] Epoch: 30 , batch: 115 , training loss: 3.834764\n",
      "[INFO] Epoch: 30 , batch: 116 , training loss: 3.732548\n",
      "[INFO] Epoch: 30 , batch: 117 , training loss: 3.976096\n",
      "[INFO] Epoch: 30 , batch: 118 , training loss: 3.940775\n",
      "[INFO] Epoch: 30 , batch: 119 , training loss: 4.056094\n",
      "[INFO] Epoch: 30 , batch: 120 , training loss: 4.040836\n",
      "[INFO] Epoch: 30 , batch: 121 , training loss: 3.919471\n",
      "[INFO] Epoch: 30 , batch: 122 , training loss: 3.823340\n",
      "[INFO] Epoch: 30 , batch: 123 , training loss: 3.831686\n",
      "[INFO] Epoch: 30 , batch: 124 , training loss: 3.926027\n",
      "[INFO] Epoch: 30 , batch: 125 , training loss: 3.756664\n",
      "[INFO] Epoch: 30 , batch: 126 , training loss: 3.769004\n",
      "[INFO] Epoch: 30 , batch: 127 , training loss: 3.773184\n",
      "[INFO] Epoch: 30 , batch: 128 , training loss: 3.932672\n",
      "[INFO] Epoch: 30 , batch: 129 , training loss: 3.868107\n",
      "[INFO] Epoch: 30 , batch: 130 , training loss: 3.843538\n",
      "[INFO] Epoch: 30 , batch: 131 , training loss: 3.837944\n",
      "[INFO] Epoch: 30 , batch: 132 , training loss: 3.870501\n",
      "[INFO] Epoch: 30 , batch: 133 , training loss: 3.861722\n",
      "[INFO] Epoch: 30 , batch: 134 , training loss: 3.604696\n",
      "[INFO] Epoch: 30 , batch: 135 , training loss: 3.681201\n",
      "[INFO] Epoch: 30 , batch: 136 , training loss: 3.974756\n",
      "[INFO] Epoch: 30 , batch: 137 , training loss: 3.895294\n",
      "[INFO] Epoch: 30 , batch: 138 , training loss: 3.942983\n",
      "[INFO] Epoch: 30 , batch: 139 , training loss: 4.475242\n",
      "[INFO] Epoch: 30 , batch: 140 , training loss: 4.280995\n",
      "[INFO] Epoch: 30 , batch: 141 , training loss: 4.038744\n",
      "[INFO] Epoch: 30 , batch: 142 , training loss: 3.790226\n",
      "[INFO] Epoch: 30 , batch: 143 , training loss: 3.918536\n",
      "[INFO] Epoch: 30 , batch: 144 , training loss: 3.760454\n",
      "[INFO] Epoch: 30 , batch: 145 , training loss: 3.837835\n",
      "[INFO] Epoch: 30 , batch: 146 , training loss: 4.038709\n",
      "[INFO] Epoch: 30 , batch: 147 , training loss: 3.669137\n",
      "[INFO] Epoch: 30 , batch: 148 , training loss: 3.647867\n",
      "[INFO] Epoch: 30 , batch: 149 , training loss: 3.736876\n",
      "[INFO] Epoch: 30 , batch: 150 , training loss: 3.994367\n",
      "[INFO] Epoch: 30 , batch: 151 , training loss: 3.868916\n",
      "[INFO] Epoch: 30 , batch: 152 , training loss: 3.849649\n",
      "[INFO] Epoch: 30 , batch: 153 , training loss: 3.858356\n",
      "[INFO] Epoch: 30 , batch: 154 , training loss: 3.971457\n",
      "[INFO] Epoch: 30 , batch: 155 , training loss: 4.178921\n",
      "[INFO] Epoch: 30 , batch: 156 , training loss: 3.932352\n",
      "[INFO] Epoch: 30 , batch: 157 , training loss: 3.877828\n",
      "[INFO] Epoch: 30 , batch: 158 , training loss: 3.984300\n",
      "[INFO] Epoch: 30 , batch: 159 , training loss: 3.937617\n",
      "[INFO] Epoch: 30 , batch: 160 , training loss: 4.213974\n",
      "[INFO] Epoch: 30 , batch: 161 , training loss: 4.248435\n",
      "[INFO] Epoch: 30 , batch: 162 , training loss: 4.211646\n",
      "[INFO] Epoch: 30 , batch: 163 , training loss: 4.428705\n",
      "[INFO] Epoch: 30 , batch: 164 , training loss: 4.350482\n",
      "[INFO] Epoch: 30 , batch: 165 , training loss: 4.277706\n",
      "[INFO] Epoch: 30 , batch: 166 , training loss: 4.156763\n",
      "[INFO] Epoch: 30 , batch: 167 , training loss: 4.294019\n",
      "[INFO] Epoch: 30 , batch: 168 , training loss: 3.890188\n",
      "[INFO] Epoch: 30 , batch: 169 , training loss: 3.922087\n",
      "[INFO] Epoch: 30 , batch: 170 , training loss: 4.045626\n",
      "[INFO] Epoch: 30 , batch: 171 , training loss: 3.502310\n",
      "[INFO] Epoch: 30 , batch: 172 , training loss: 3.737903\n",
      "[INFO] Epoch: 30 , batch: 173 , training loss: 4.051186\n",
      "[INFO] Epoch: 30 , batch: 174 , training loss: 4.509232\n",
      "[INFO] Epoch: 30 , batch: 175 , training loss: 4.774960\n",
      "[INFO] Epoch: 30 , batch: 176 , training loss: 4.493178\n",
      "[INFO] Epoch: 30 , batch: 177 , training loss: 4.109733\n",
      "[INFO] Epoch: 30 , batch: 178 , training loss: 4.120678\n",
      "[INFO] Epoch: 30 , batch: 179 , training loss: 4.145258\n",
      "[INFO] Epoch: 30 , batch: 180 , training loss: 4.091500\n",
      "[INFO] Epoch: 30 , batch: 181 , training loss: 4.410669\n",
      "[INFO] Epoch: 30 , batch: 182 , training loss: 4.325243\n",
      "[INFO] Epoch: 30 , batch: 183 , training loss: 4.293929\n",
      "[INFO] Epoch: 30 , batch: 184 , training loss: 4.201958\n",
      "[INFO] Epoch: 30 , batch: 185 , training loss: 4.140674\n",
      "[INFO] Epoch: 30 , batch: 186 , training loss: 4.283780\n",
      "[INFO] Epoch: 30 , batch: 187 , training loss: 4.364748\n",
      "[INFO] Epoch: 30 , batch: 188 , training loss: 4.373746\n",
      "[INFO] Epoch: 30 , batch: 189 , training loss: 4.320550\n",
      "[INFO] Epoch: 30 , batch: 190 , training loss: 4.331678\n",
      "[INFO] Epoch: 30 , batch: 191 , training loss: 4.441286\n",
      "[INFO] Epoch: 30 , batch: 192 , training loss: 4.271992\n",
      "[INFO] Epoch: 30 , batch: 193 , training loss: 4.390174\n",
      "[INFO] Epoch: 30 , batch: 194 , training loss: 4.340280\n",
      "[INFO] Epoch: 30 , batch: 195 , training loss: 4.283778\n",
      "[INFO] Epoch: 30 , batch: 196 , training loss: 4.105777\n",
      "[INFO] Epoch: 30 , batch: 197 , training loss: 4.209313\n",
      "[INFO] Epoch: 30 , batch: 198 , training loss: 4.121657\n",
      "[INFO] Epoch: 30 , batch: 199 , training loss: 4.246753\n",
      "[INFO] Epoch: 30 , batch: 200 , training loss: 4.158692\n",
      "[INFO] Epoch: 30 , batch: 201 , training loss: 4.048647\n",
      "[INFO] Epoch: 30 , batch: 202 , training loss: 4.062027\n",
      "[INFO] Epoch: 30 , batch: 203 , training loss: 4.154586\n",
      "[INFO] Epoch: 30 , batch: 204 , training loss: 4.263175\n",
      "[INFO] Epoch: 30 , batch: 205 , training loss: 3.870258\n",
      "[INFO] Epoch: 30 , batch: 206 , training loss: 3.798876\n",
      "[INFO] Epoch: 30 , batch: 207 , training loss: 3.811357\n",
      "[INFO] Epoch: 30 , batch: 208 , training loss: 4.114621\n",
      "[INFO] Epoch: 30 , batch: 209 , training loss: 4.077646\n",
      "[INFO] Epoch: 30 , batch: 210 , training loss: 4.095327\n",
      "[INFO] Epoch: 30 , batch: 211 , training loss: 4.095787\n",
      "[INFO] Epoch: 30 , batch: 212 , training loss: 4.201632\n",
      "[INFO] Epoch: 30 , batch: 213 , training loss: 4.153408\n",
      "[INFO] Epoch: 30 , batch: 214 , training loss: 4.212659\n",
      "[INFO] Epoch: 30 , batch: 215 , training loss: 4.431386\n",
      "[INFO] Epoch: 30 , batch: 216 , training loss: 4.163142\n",
      "[INFO] Epoch: 30 , batch: 217 , training loss: 4.062459\n",
      "[INFO] Epoch: 30 , batch: 218 , training loss: 4.065553\n",
      "[INFO] Epoch: 30 , batch: 219 , training loss: 4.179372\n",
      "[INFO] Epoch: 30 , batch: 220 , training loss: 3.979791\n",
      "[INFO] Epoch: 30 , batch: 221 , training loss: 4.003658\n",
      "[INFO] Epoch: 30 , batch: 222 , training loss: 4.168970\n",
      "[INFO] Epoch: 30 , batch: 223 , training loss: 4.259974\n",
      "[INFO] Epoch: 30 , batch: 224 , training loss: 4.298898\n",
      "[INFO] Epoch: 30 , batch: 225 , training loss: 4.182364\n",
      "[INFO] Epoch: 30 , batch: 226 , training loss: 4.321856\n",
      "[INFO] Epoch: 30 , batch: 227 , training loss: 4.269106\n",
      "[INFO] Epoch: 30 , batch: 228 , training loss: 4.292810\n",
      "[INFO] Epoch: 30 , batch: 229 , training loss: 4.173965\n",
      "[INFO] Epoch: 30 , batch: 230 , training loss: 4.033478\n",
      "[INFO] Epoch: 30 , batch: 231 , training loss: 3.882929\n",
      "[INFO] Epoch: 30 , batch: 232 , training loss: 4.038413\n",
      "[INFO] Epoch: 30 , batch: 233 , training loss: 4.040591\n",
      "[INFO] Epoch: 30 , batch: 234 , training loss: 3.742064\n",
      "[INFO] Epoch: 30 , batch: 235 , training loss: 3.857886\n",
      "[INFO] Epoch: 30 , batch: 236 , training loss: 3.955638\n",
      "[INFO] Epoch: 30 , batch: 237 , training loss: 4.190932\n",
      "[INFO] Epoch: 30 , batch: 238 , training loss: 3.945809\n",
      "[INFO] Epoch: 30 , batch: 239 , training loss: 4.002004\n",
      "[INFO] Epoch: 30 , batch: 240 , training loss: 4.041125\n",
      "[INFO] Epoch: 30 , batch: 241 , training loss: 3.865896\n",
      "[INFO] Epoch: 30 , batch: 242 , training loss: 3.863333\n",
      "[INFO] Epoch: 30 , batch: 243 , training loss: 4.147972\n",
      "[INFO] Epoch: 30 , batch: 244 , training loss: 4.108422\n",
      "[INFO] Epoch: 30 , batch: 245 , training loss: 4.061823\n",
      "[INFO] Epoch: 30 , batch: 246 , training loss: 3.772064\n",
      "[INFO] Epoch: 30 , batch: 247 , training loss: 3.934550\n",
      "[INFO] Epoch: 30 , batch: 248 , training loss: 3.994977\n",
      "[INFO] Epoch: 30 , batch: 249 , training loss: 3.980663\n",
      "[INFO] Epoch: 30 , batch: 250 , training loss: 3.773770\n",
      "[INFO] Epoch: 30 , batch: 251 , training loss: 4.251171\n",
      "[INFO] Epoch: 30 , batch: 252 , training loss: 3.954785\n",
      "[INFO] Epoch: 30 , batch: 253 , training loss: 3.846673\n",
      "[INFO] Epoch: 30 , batch: 254 , training loss: 4.123353\n",
      "[INFO] Epoch: 30 , batch: 255 , training loss: 4.098438\n",
      "[INFO] Epoch: 30 , batch: 256 , training loss: 4.117238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 30 , batch: 257 , training loss: 4.257106\n",
      "[INFO] Epoch: 30 , batch: 258 , training loss: 4.268387\n",
      "[INFO] Epoch: 30 , batch: 259 , training loss: 4.318858\n",
      "[INFO] Epoch: 30 , batch: 260 , training loss: 4.084566\n",
      "[INFO] Epoch: 30 , batch: 261 , training loss: 4.249260\n",
      "[INFO] Epoch: 30 , batch: 262 , training loss: 4.399971\n",
      "[INFO] Epoch: 30 , batch: 263 , training loss: 4.600791\n",
      "[INFO] Epoch: 30 , batch: 264 , training loss: 3.927152\n",
      "[INFO] Epoch: 30 , batch: 265 , training loss: 4.024628\n",
      "[INFO] Epoch: 30 , batch: 266 , training loss: 4.482367\n",
      "[INFO] Epoch: 30 , batch: 267 , training loss: 4.207991\n",
      "[INFO] Epoch: 30 , batch: 268 , training loss: 4.167774\n",
      "[INFO] Epoch: 30 , batch: 269 , training loss: 4.163245\n",
      "[INFO] Epoch: 30 , batch: 270 , training loss: 4.176980\n",
      "[INFO] Epoch: 30 , batch: 271 , training loss: 4.167940\n",
      "[INFO] Epoch: 30 , batch: 272 , training loss: 4.161218\n",
      "[INFO] Epoch: 30 , batch: 273 , training loss: 4.167451\n",
      "[INFO] Epoch: 30 , batch: 274 , training loss: 4.275217\n",
      "[INFO] Epoch: 30 , batch: 275 , training loss: 4.109193\n",
      "[INFO] Epoch: 30 , batch: 276 , training loss: 4.170774\n",
      "[INFO] Epoch: 30 , batch: 277 , training loss: 4.339169\n",
      "[INFO] Epoch: 30 , batch: 278 , training loss: 3.989377\n",
      "[INFO] Epoch: 30 , batch: 279 , training loss: 4.013353\n",
      "[INFO] Epoch: 30 , batch: 280 , training loss: 3.984948\n",
      "[INFO] Epoch: 30 , batch: 281 , training loss: 4.127295\n",
      "[INFO] Epoch: 30 , batch: 282 , training loss: 4.006422\n",
      "[INFO] Epoch: 30 , batch: 283 , training loss: 4.019575\n",
      "[INFO] Epoch: 30 , batch: 284 , training loss: 4.067755\n",
      "[INFO] Epoch: 30 , batch: 285 , training loss: 4.009466\n",
      "[INFO] Epoch: 30 , batch: 286 , training loss: 4.009987\n",
      "[INFO] Epoch: 30 , batch: 287 , training loss: 3.969140\n",
      "[INFO] Epoch: 30 , batch: 288 , training loss: 3.916980\n",
      "[INFO] Epoch: 30 , batch: 289 , training loss: 4.002225\n",
      "[INFO] Epoch: 30 , batch: 290 , training loss: 3.771082\n",
      "[INFO] Epoch: 30 , batch: 291 , training loss: 3.756307\n",
      "[INFO] Epoch: 30 , batch: 292 , training loss: 3.872733\n",
      "[INFO] Epoch: 30 , batch: 293 , training loss: 3.782078\n",
      "[INFO] Epoch: 30 , batch: 294 , training loss: 4.458998\n",
      "[INFO] Epoch: 30 , batch: 295 , training loss: 4.236407\n",
      "[INFO] Epoch: 30 , batch: 296 , training loss: 4.176112\n",
      "[INFO] Epoch: 30 , batch: 297 , training loss: 4.111104\n",
      "[INFO] Epoch: 30 , batch: 298 , training loss: 3.961005\n",
      "[INFO] Epoch: 30 , batch: 299 , training loss: 4.009238\n",
      "[INFO] Epoch: 30 , batch: 300 , training loss: 3.993487\n",
      "[INFO] Epoch: 30 , batch: 301 , training loss: 3.908466\n",
      "[INFO] Epoch: 30 , batch: 302 , training loss: 4.084400\n",
      "[INFO] Epoch: 30 , batch: 303 , training loss: 4.091467\n",
      "[INFO] Epoch: 30 , batch: 304 , training loss: 4.215267\n",
      "[INFO] Epoch: 30 , batch: 305 , training loss: 4.056412\n",
      "[INFO] Epoch: 30 , batch: 306 , training loss: 4.154763\n",
      "[INFO] Epoch: 30 , batch: 307 , training loss: 4.179980\n",
      "[INFO] Epoch: 30 , batch: 308 , training loss: 3.989338\n",
      "[INFO] Epoch: 30 , batch: 309 , training loss: 4.010335\n",
      "[INFO] Epoch: 30 , batch: 310 , training loss: 3.940354\n",
      "[INFO] Epoch: 30 , batch: 311 , training loss: 3.911920\n",
      "[INFO] Epoch: 30 , batch: 312 , training loss: 3.833241\n",
      "[INFO] Epoch: 30 , batch: 313 , training loss: 3.907550\n",
      "[INFO] Epoch: 30 , batch: 314 , training loss: 3.994313\n",
      "[INFO] Epoch: 30 , batch: 315 , training loss: 4.075234\n",
      "[INFO] Epoch: 30 , batch: 316 , training loss: 4.326348\n",
      "[INFO] Epoch: 30 , batch: 317 , training loss: 4.677469\n",
      "[INFO] Epoch: 30 , batch: 318 , training loss: 4.805792\n",
      "[INFO] Epoch: 30 , batch: 319 , training loss: 4.497034\n",
      "[INFO] Epoch: 30 , batch: 320 , training loss: 4.042603\n",
      "[INFO] Epoch: 30 , batch: 321 , training loss: 3.841230\n",
      "[INFO] Epoch: 30 , batch: 322 , training loss: 3.956661\n",
      "[INFO] Epoch: 30 , batch: 323 , training loss: 3.994963\n",
      "[INFO] Epoch: 30 , batch: 324 , training loss: 3.966791\n",
      "[INFO] Epoch: 30 , batch: 325 , training loss: 4.105291\n",
      "[INFO] Epoch: 30 , batch: 326 , training loss: 4.153072\n",
      "[INFO] Epoch: 30 , batch: 327 , training loss: 4.081322\n",
      "[INFO] Epoch: 30 , batch: 328 , training loss: 4.061576\n",
      "[INFO] Epoch: 30 , batch: 329 , training loss: 3.986481\n",
      "[INFO] Epoch: 30 , batch: 330 , training loss: 3.972859\n",
      "[INFO] Epoch: 30 , batch: 331 , training loss: 4.135689\n",
      "[INFO] Epoch: 30 , batch: 332 , training loss: 3.985451\n",
      "[INFO] Epoch: 30 , batch: 333 , training loss: 3.971424\n",
      "[INFO] Epoch: 30 , batch: 334 , training loss: 3.975464\n",
      "[INFO] Epoch: 30 , batch: 335 , training loss: 4.093408\n",
      "[INFO] Epoch: 30 , batch: 336 , training loss: 4.096432\n",
      "[INFO] Epoch: 30 , batch: 337 , training loss: 4.140186\n",
      "[INFO] Epoch: 30 , batch: 338 , training loss: 4.350647\n",
      "[INFO] Epoch: 30 , batch: 339 , training loss: 4.159235\n",
      "[INFO] Epoch: 30 , batch: 340 , training loss: 4.336699\n",
      "[INFO] Epoch: 30 , batch: 341 , training loss: 4.100883\n",
      "[INFO] Epoch: 30 , batch: 342 , training loss: 3.890723\n",
      "[INFO] Epoch: 30 , batch: 343 , training loss: 3.974149\n",
      "[INFO] Epoch: 30 , batch: 344 , training loss: 3.838751\n",
      "[INFO] Epoch: 30 , batch: 345 , training loss: 3.968629\n",
      "[INFO] Epoch: 30 , batch: 346 , training loss: 4.022859\n",
      "[INFO] Epoch: 30 , batch: 347 , training loss: 3.929519\n",
      "[INFO] Epoch: 30 , batch: 348 , training loss: 4.030515\n",
      "[INFO] Epoch: 30 , batch: 349 , training loss: 4.125808\n",
      "[INFO] Epoch: 30 , batch: 350 , training loss: 3.959426\n",
      "[INFO] Epoch: 30 , batch: 351 , training loss: 4.056382\n",
      "[INFO] Epoch: 30 , batch: 352 , training loss: 4.075195\n",
      "[INFO] Epoch: 30 , batch: 353 , training loss: 4.050229\n",
      "[INFO] Epoch: 30 , batch: 354 , training loss: 4.151680\n",
      "[INFO] Epoch: 30 , batch: 355 , training loss: 4.124655\n",
      "[INFO] Epoch: 30 , batch: 356 , training loss: 4.001994\n",
      "[INFO] Epoch: 30 , batch: 357 , training loss: 4.079439\n",
      "[INFO] Epoch: 30 , batch: 358 , training loss: 3.978953\n",
      "[INFO] Epoch: 30 , batch: 359 , training loss: 4.003920\n",
      "[INFO] Epoch: 30 , batch: 360 , training loss: 4.096025\n",
      "[INFO] Epoch: 30 , batch: 361 , training loss: 4.055792\n",
      "[INFO] Epoch: 30 , batch: 362 , training loss: 4.153487\n",
      "[INFO] Epoch: 30 , batch: 363 , training loss: 4.051932\n",
      "[INFO] Epoch: 30 , batch: 364 , training loss: 4.126115\n",
      "[INFO] Epoch: 30 , batch: 365 , training loss: 4.009736\n",
      "[INFO] Epoch: 30 , batch: 366 , training loss: 4.146180\n",
      "[INFO] Epoch: 30 , batch: 367 , training loss: 4.154063\n",
      "[INFO] Epoch: 30 , batch: 368 , training loss: 4.558626\n",
      "[INFO] Epoch: 30 , batch: 369 , training loss: 4.268023\n",
      "[INFO] Epoch: 30 , batch: 370 , training loss: 4.024699\n",
      "[INFO] Epoch: 30 , batch: 371 , training loss: 4.450415\n",
      "[INFO] Epoch: 30 , batch: 372 , training loss: 4.670638\n",
      "[INFO] Epoch: 30 , batch: 373 , training loss: 4.745070\n",
      "[INFO] Epoch: 30 , batch: 374 , training loss: 4.855363\n",
      "[INFO] Epoch: 30 , batch: 375 , training loss: 4.814452\n",
      "[INFO] Epoch: 30 , batch: 376 , training loss: 4.710834\n",
      "[INFO] Epoch: 30 , batch: 377 , training loss: 4.476792\n",
      "[INFO] Epoch: 30 , batch: 378 , training loss: 4.585186\n",
      "[INFO] Epoch: 30 , batch: 379 , training loss: 4.553533\n",
      "[INFO] Epoch: 30 , batch: 380 , training loss: 4.697103\n",
      "[INFO] Epoch: 30 , batch: 381 , training loss: 4.412120\n",
      "[INFO] Epoch: 30 , batch: 382 , training loss: 4.697509\n",
      "[INFO] Epoch: 30 , batch: 383 , training loss: 4.716737\n",
      "[INFO] Epoch: 30 , batch: 384 , training loss: 4.742099\n",
      "[INFO] Epoch: 30 , batch: 385 , training loss: 4.370474\n",
      "[INFO] Epoch: 30 , batch: 386 , training loss: 4.648436\n",
      "[INFO] Epoch: 30 , batch: 387 , training loss: 4.587948\n",
      "[INFO] Epoch: 30 , batch: 388 , training loss: 4.406020\n",
      "[INFO] Epoch: 30 , batch: 389 , training loss: 4.227339\n",
      "[INFO] Epoch: 30 , batch: 390 , training loss: 4.254406\n",
      "[INFO] Epoch: 30 , batch: 391 , training loss: 4.265488\n",
      "[INFO] Epoch: 30 , batch: 392 , training loss: 4.629290\n",
      "[INFO] Epoch: 30 , batch: 393 , training loss: 4.517590\n",
      "[INFO] Epoch: 30 , batch: 394 , training loss: 4.608905\n",
      "[INFO] Epoch: 30 , batch: 395 , training loss: 4.416757\n",
      "[INFO] Epoch: 30 , batch: 396 , training loss: 4.257170\n",
      "[INFO] Epoch: 30 , batch: 397 , training loss: 4.387456\n",
      "[INFO] Epoch: 30 , batch: 398 , training loss: 4.245162\n",
      "[INFO] Epoch: 30 , batch: 399 , training loss: 4.333658\n",
      "[INFO] Epoch: 30 , batch: 400 , training loss: 4.290173\n",
      "[INFO] Epoch: 30 , batch: 401 , training loss: 4.725798\n",
      "[INFO] Epoch: 30 , batch: 402 , training loss: 4.459499\n",
      "[INFO] Epoch: 30 , batch: 403 , training loss: 4.274678\n",
      "[INFO] Epoch: 30 , batch: 404 , training loss: 4.440672\n",
      "[INFO] Epoch: 30 , batch: 405 , training loss: 4.508804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 30 , batch: 406 , training loss: 4.413384\n",
      "[INFO] Epoch: 30 , batch: 407 , training loss: 4.436965\n",
      "[INFO] Epoch: 30 , batch: 408 , training loss: 4.413614\n",
      "[INFO] Epoch: 30 , batch: 409 , training loss: 4.413433\n",
      "[INFO] Epoch: 30 , batch: 410 , training loss: 4.492724\n",
      "[INFO] Epoch: 30 , batch: 411 , training loss: 4.654103\n",
      "[INFO] Epoch: 30 , batch: 412 , training loss: 4.474668\n",
      "[INFO] Epoch: 30 , batch: 413 , training loss: 4.348626\n",
      "[INFO] Epoch: 30 , batch: 414 , training loss: 4.394446\n",
      "[INFO] Epoch: 30 , batch: 415 , training loss: 4.427817\n",
      "[INFO] Epoch: 30 , batch: 416 , training loss: 4.500898\n",
      "[INFO] Epoch: 30 , batch: 417 , training loss: 4.411229\n",
      "[INFO] Epoch: 30 , batch: 418 , training loss: 4.475513\n",
      "[INFO] Epoch: 30 , batch: 419 , training loss: 4.433797\n",
      "[INFO] Epoch: 30 , batch: 420 , training loss: 4.408134\n",
      "[INFO] Epoch: 30 , batch: 421 , training loss: 4.389451\n",
      "[INFO] Epoch: 30 , batch: 422 , training loss: 4.234305\n",
      "[INFO] Epoch: 30 , batch: 423 , training loss: 4.456751\n",
      "[INFO] Epoch: 30 , batch: 424 , training loss: 4.627560\n",
      "[INFO] Epoch: 30 , batch: 425 , training loss: 4.491851\n",
      "[INFO] Epoch: 30 , batch: 426 , training loss: 4.235102\n",
      "[INFO] Epoch: 30 , batch: 427 , training loss: 4.482625\n",
      "[INFO] Epoch: 30 , batch: 428 , training loss: 4.333522\n",
      "[INFO] Epoch: 30 , batch: 429 , training loss: 4.254300\n",
      "[INFO] Epoch: 30 , batch: 430 , training loss: 4.478518\n",
      "[INFO] Epoch: 30 , batch: 431 , training loss: 4.076052\n",
      "[INFO] Epoch: 30 , batch: 432 , training loss: 4.128640\n",
      "[INFO] Epoch: 30 , batch: 433 , training loss: 4.166487\n",
      "[INFO] Epoch: 30 , batch: 434 , training loss: 4.053925\n",
      "[INFO] Epoch: 30 , batch: 435 , training loss: 4.402768\n",
      "[INFO] Epoch: 30 , batch: 436 , training loss: 4.441351\n",
      "[INFO] Epoch: 30 , batch: 437 , training loss: 4.228823\n",
      "[INFO] Epoch: 30 , batch: 438 , training loss: 4.081442\n",
      "[INFO] Epoch: 30 , batch: 439 , training loss: 4.327158\n",
      "[INFO] Epoch: 30 , batch: 440 , training loss: 4.460987\n",
      "[INFO] Epoch: 30 , batch: 441 , training loss: 4.544704\n",
      "[INFO] Epoch: 30 , batch: 442 , training loss: 4.284589\n",
      "[INFO] Epoch: 30 , batch: 443 , training loss: 4.510310\n",
      "[INFO] Epoch: 30 , batch: 444 , training loss: 4.104377\n",
      "[INFO] Epoch: 30 , batch: 445 , training loss: 4.006010\n",
      "[INFO] Epoch: 30 , batch: 446 , training loss: 3.914616\n",
      "[INFO] Epoch: 30 , batch: 447 , training loss: 4.120711\n",
      "[INFO] Epoch: 30 , batch: 448 , training loss: 4.244432\n",
      "[INFO] Epoch: 30 , batch: 449 , training loss: 4.618180\n",
      "[INFO] Epoch: 30 , batch: 450 , training loss: 4.699269\n",
      "[INFO] Epoch: 30 , batch: 451 , training loss: 4.563237\n",
      "[INFO] Epoch: 30 , batch: 452 , training loss: 4.400521\n",
      "[INFO] Epoch: 30 , batch: 453 , training loss: 4.170200\n",
      "[INFO] Epoch: 30 , batch: 454 , training loss: 4.323306\n",
      "[INFO] Epoch: 30 , batch: 455 , training loss: 4.385060\n",
      "[INFO] Epoch: 30 , batch: 456 , training loss: 4.363688\n",
      "[INFO] Epoch: 30 , batch: 457 , training loss: 4.437152\n",
      "[INFO] Epoch: 30 , batch: 458 , training loss: 4.183058\n",
      "[INFO] Epoch: 30 , batch: 459 , training loss: 4.161781\n",
      "[INFO] Epoch: 30 , batch: 460 , training loss: 4.276874\n",
      "[INFO] Epoch: 30 , batch: 461 , training loss: 4.236858\n",
      "[INFO] Epoch: 30 , batch: 462 , training loss: 4.288311\n",
      "[INFO] Epoch: 30 , batch: 463 , training loss: 4.209831\n",
      "[INFO] Epoch: 30 , batch: 464 , training loss: 4.383900\n",
      "[INFO] Epoch: 30 , batch: 465 , training loss: 4.324703\n",
      "[INFO] Epoch: 30 , batch: 466 , training loss: 4.429308\n",
      "[INFO] Epoch: 30 , batch: 467 , training loss: 4.395450\n",
      "[INFO] Epoch: 30 , batch: 468 , training loss: 4.345909\n",
      "[INFO] Epoch: 30 , batch: 469 , training loss: 4.378788\n",
      "[INFO] Epoch: 30 , batch: 470 , training loss: 4.187155\n",
      "[INFO] Epoch: 30 , batch: 471 , training loss: 4.314307\n",
      "[INFO] Epoch: 30 , batch: 472 , training loss: 4.368741\n",
      "[INFO] Epoch: 30 , batch: 473 , training loss: 4.272482\n",
      "[INFO] Epoch: 30 , batch: 474 , training loss: 4.061886\n",
      "[INFO] Epoch: 30 , batch: 475 , training loss: 3.944853\n",
      "[INFO] Epoch: 30 , batch: 476 , training loss: 4.348896\n",
      "[INFO] Epoch: 30 , batch: 477 , training loss: 4.441604\n",
      "[INFO] Epoch: 30 , batch: 478 , training loss: 4.450811\n",
      "[INFO] Epoch: 30 , batch: 479 , training loss: 4.410275\n",
      "[INFO] Epoch: 30 , batch: 480 , training loss: 4.552757\n",
      "[INFO] Epoch: 30 , batch: 481 , training loss: 4.432773\n",
      "[INFO] Epoch: 30 , batch: 482 , training loss: 4.544398\n",
      "[INFO] Epoch: 30 , batch: 483 , training loss: 4.383194\n",
      "[INFO] Epoch: 30 , batch: 484 , training loss: 4.196603\n",
      "[INFO] Epoch: 30 , batch: 485 , training loss: 4.277706\n",
      "[INFO] Epoch: 30 , batch: 486 , training loss: 4.182457\n",
      "[INFO] Epoch: 30 , batch: 487 , training loss: 4.169871\n",
      "[INFO] Epoch: 30 , batch: 488 , training loss: 4.337579\n",
      "[INFO] Epoch: 30 , batch: 489 , training loss: 4.242984\n",
      "[INFO] Epoch: 30 , batch: 490 , training loss: 4.313367\n",
      "[INFO] Epoch: 30 , batch: 491 , training loss: 4.240191\n",
      "[INFO] Epoch: 30 , batch: 492 , training loss: 4.203575\n",
      "[INFO] Epoch: 30 , batch: 493 , training loss: 4.363886\n",
      "[INFO] Epoch: 30 , batch: 494 , training loss: 4.269859\n",
      "[INFO] Epoch: 30 , batch: 495 , training loss: 4.445564\n",
      "[INFO] Epoch: 30 , batch: 496 , training loss: 4.302906\n",
      "[INFO] Epoch: 30 , batch: 497 , training loss: 4.343746\n",
      "[INFO] Epoch: 30 , batch: 498 , training loss: 4.322943\n",
      "[INFO] Epoch: 30 , batch: 499 , training loss: 4.398126\n",
      "[INFO] Epoch: 30 , batch: 500 , training loss: 4.532133\n",
      "[INFO] Epoch: 30 , batch: 501 , training loss: 4.895018\n",
      "[INFO] Epoch: 30 , batch: 502 , training loss: 4.907292\n",
      "[INFO] Epoch: 30 , batch: 503 , training loss: 4.561717\n",
      "[INFO] Epoch: 30 , batch: 504 , training loss: 4.716745\n",
      "[INFO] Epoch: 30 , batch: 505 , training loss: 4.680140\n",
      "[INFO] Epoch: 30 , batch: 506 , training loss: 4.637767\n",
      "[INFO] Epoch: 30 , batch: 507 , training loss: 4.707811\n",
      "[INFO] Epoch: 30 , batch: 508 , training loss: 4.619637\n",
      "[INFO] Epoch: 30 , batch: 509 , training loss: 4.421916\n",
      "[INFO] Epoch: 30 , batch: 510 , training loss: 4.498329\n",
      "[INFO] Epoch: 30 , batch: 511 , training loss: 4.438175\n",
      "[INFO] Epoch: 30 , batch: 512 , training loss: 4.514979\n",
      "[INFO] Epoch: 30 , batch: 513 , training loss: 4.772052\n",
      "[INFO] Epoch: 30 , batch: 514 , training loss: 4.403841\n",
      "[INFO] Epoch: 30 , batch: 515 , training loss: 4.672794\n",
      "[INFO] Epoch: 30 , batch: 516 , training loss: 4.451956\n",
      "[INFO] Epoch: 30 , batch: 517 , training loss: 4.424466\n",
      "[INFO] Epoch: 30 , batch: 518 , training loss: 4.394256\n",
      "[INFO] Epoch: 30 , batch: 519 , training loss: 4.236368\n",
      "[INFO] Epoch: 30 , batch: 520 , training loss: 4.460736\n",
      "[INFO] Epoch: 30 , batch: 521 , training loss: 4.454879\n",
      "[INFO] Epoch: 30 , batch: 522 , training loss: 4.520485\n",
      "[INFO] Epoch: 30 , batch: 523 , training loss: 4.435407\n",
      "[INFO] Epoch: 30 , batch: 524 , training loss: 4.722990\n",
      "[INFO] Epoch: 30 , batch: 525 , training loss: 4.594615\n",
      "[INFO] Epoch: 30 , batch: 526 , training loss: 4.391413\n",
      "[INFO] Epoch: 30 , batch: 527 , training loss: 4.435696\n",
      "[INFO] Epoch: 30 , batch: 528 , training loss: 4.458551\n",
      "[INFO] Epoch: 30 , batch: 529 , training loss: 4.426675\n",
      "[INFO] Epoch: 30 , batch: 530 , training loss: 4.273602\n",
      "[INFO] Epoch: 30 , batch: 531 , training loss: 4.423691\n",
      "[INFO] Epoch: 30 , batch: 532 , training loss: 4.320397\n",
      "[INFO] Epoch: 30 , batch: 533 , training loss: 4.467454\n",
      "[INFO] Epoch: 30 , batch: 534 , training loss: 4.467439\n",
      "[INFO] Epoch: 30 , batch: 535 , training loss: 4.461155\n",
      "[INFO] Epoch: 30 , batch: 536 , training loss: 4.320394\n",
      "[INFO] Epoch: 30 , batch: 537 , training loss: 4.299867\n",
      "[INFO] Epoch: 30 , batch: 538 , training loss: 4.378861\n",
      "[INFO] Epoch: 30 , batch: 539 , training loss: 4.485436\n",
      "[INFO] Epoch: 30 , batch: 540 , training loss: 4.996265\n",
      "[INFO] Epoch: 30 , batch: 541 , training loss: 4.824650\n",
      "[INFO] Epoch: 30 , batch: 542 , training loss: 4.683446\n",
      "[INFO] Epoch: 31 , batch: 0 , training loss: 3.773823\n",
      "[INFO] Epoch: 31 , batch: 1 , training loss: 3.588262\n",
      "[INFO] Epoch: 31 , batch: 2 , training loss: 3.759402\n",
      "[INFO] Epoch: 31 , batch: 3 , training loss: 3.609174\n",
      "[INFO] Epoch: 31 , batch: 4 , training loss: 3.903887\n",
      "[INFO] Epoch: 31 , batch: 5 , training loss: 3.620911\n",
      "[INFO] Epoch: 31 , batch: 6 , training loss: 4.095963\n",
      "[INFO] Epoch: 31 , batch: 7 , training loss: 3.848935\n",
      "[INFO] Epoch: 31 , batch: 8 , training loss: 3.547016\n",
      "[INFO] Epoch: 31 , batch: 9 , training loss: 3.823939\n",
      "[INFO] Epoch: 31 , batch: 10 , training loss: 3.735084\n",
      "[INFO] Epoch: 31 , batch: 11 , training loss: 3.677753\n",
      "[INFO] Epoch: 31 , batch: 12 , training loss: 3.597636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 31 , batch: 13 , training loss: 3.614684\n",
      "[INFO] Epoch: 31 , batch: 14 , training loss: 3.523128\n",
      "[INFO] Epoch: 31 , batch: 15 , training loss: 3.752621\n",
      "[INFO] Epoch: 31 , batch: 16 , training loss: 3.614572\n",
      "[INFO] Epoch: 31 , batch: 17 , training loss: 3.742305\n",
      "[INFO] Epoch: 31 , batch: 18 , training loss: 3.680145\n",
      "[INFO] Epoch: 31 , batch: 19 , training loss: 3.453664\n",
      "[INFO] Epoch: 31 , batch: 20 , training loss: 3.408818\n",
      "[INFO] Epoch: 31 , batch: 21 , training loss: 3.560168\n",
      "[INFO] Epoch: 31 , batch: 22 , training loss: 3.454225\n",
      "[INFO] Epoch: 31 , batch: 23 , training loss: 3.644733\n",
      "[INFO] Epoch: 31 , batch: 24 , training loss: 3.499389\n",
      "[INFO] Epoch: 31 , batch: 25 , training loss: 3.577964\n",
      "[INFO] Epoch: 31 , batch: 26 , training loss: 3.474581\n",
      "[INFO] Epoch: 31 , batch: 27 , training loss: 3.443059\n",
      "[INFO] Epoch: 31 , batch: 28 , training loss: 3.635289\n",
      "[INFO] Epoch: 31 , batch: 29 , training loss: 3.468446\n",
      "[INFO] Epoch: 31 , batch: 30 , training loss: 3.465871\n",
      "[INFO] Epoch: 31 , batch: 31 , training loss: 3.608063\n",
      "[INFO] Epoch: 31 , batch: 32 , training loss: 3.560144\n",
      "[INFO] Epoch: 31 , batch: 33 , training loss: 3.615821\n",
      "[INFO] Epoch: 31 , batch: 34 , training loss: 3.562083\n",
      "[INFO] Epoch: 31 , batch: 35 , training loss: 3.547249\n",
      "[INFO] Epoch: 31 , batch: 36 , training loss: 3.632070\n",
      "[INFO] Epoch: 31 , batch: 37 , training loss: 3.467996\n",
      "[INFO] Epoch: 31 , batch: 38 , training loss: 3.570377\n",
      "[INFO] Epoch: 31 , batch: 39 , training loss: 3.377581\n",
      "[INFO] Epoch: 31 , batch: 40 , training loss: 3.581914\n",
      "[INFO] Epoch: 31 , batch: 41 , training loss: 3.525282\n",
      "[INFO] Epoch: 31 , batch: 42 , training loss: 4.058193\n",
      "[INFO] Epoch: 31 , batch: 43 , training loss: 3.771097\n",
      "[INFO] Epoch: 31 , batch: 44 , training loss: 4.061660\n",
      "[INFO] Epoch: 31 , batch: 45 , training loss: 4.024373\n",
      "[INFO] Epoch: 31 , batch: 46 , training loss: 4.008829\n",
      "[INFO] Epoch: 31 , batch: 47 , training loss: 3.606544\n",
      "[INFO] Epoch: 31 , batch: 48 , training loss: 3.600959\n",
      "[INFO] Epoch: 31 , batch: 49 , training loss: 3.861088\n",
      "[INFO] Epoch: 31 , batch: 50 , training loss: 3.575202\n",
      "[INFO] Epoch: 31 , batch: 51 , training loss: 3.866157\n",
      "[INFO] Epoch: 31 , batch: 52 , training loss: 3.664082\n",
      "[INFO] Epoch: 31 , batch: 53 , training loss: 3.757068\n",
      "[INFO] Epoch: 31 , batch: 54 , training loss: 3.782996\n",
      "[INFO] Epoch: 31 , batch: 55 , training loss: 3.905500\n",
      "[INFO] Epoch: 31 , batch: 56 , training loss: 3.671227\n",
      "[INFO] Epoch: 31 , batch: 57 , training loss: 3.613559\n",
      "[INFO] Epoch: 31 , batch: 58 , training loss: 3.682405\n",
      "[INFO] Epoch: 31 , batch: 59 , training loss: 3.757179\n",
      "[INFO] Epoch: 31 , batch: 60 , training loss: 3.697252\n",
      "[INFO] Epoch: 31 , batch: 61 , training loss: 3.782870\n",
      "[INFO] Epoch: 31 , batch: 62 , training loss: 3.652171\n",
      "[INFO] Epoch: 31 , batch: 63 , training loss: 3.849549\n",
      "[INFO] Epoch: 31 , batch: 64 , training loss: 4.080747\n",
      "[INFO] Epoch: 31 , batch: 65 , training loss: 3.751065\n",
      "[INFO] Epoch: 31 , batch: 66 , training loss: 3.630954\n",
      "[INFO] Epoch: 31 , batch: 67 , training loss: 3.636713\n",
      "[INFO] Epoch: 31 , batch: 68 , training loss: 3.804094\n",
      "[INFO] Epoch: 31 , batch: 69 , training loss: 3.725789\n",
      "[INFO] Epoch: 31 , batch: 70 , training loss: 3.945526\n",
      "[INFO] Epoch: 31 , batch: 71 , training loss: 3.792922\n",
      "[INFO] Epoch: 31 , batch: 72 , training loss: 3.857642\n",
      "[INFO] Epoch: 31 , batch: 73 , training loss: 3.813743\n",
      "[INFO] Epoch: 31 , batch: 74 , training loss: 3.905260\n",
      "[INFO] Epoch: 31 , batch: 75 , training loss: 3.774157\n",
      "[INFO] Epoch: 31 , batch: 76 , training loss: 3.882457\n",
      "[INFO] Epoch: 31 , batch: 77 , training loss: 3.814877\n",
      "[INFO] Epoch: 31 , batch: 78 , training loss: 3.896323\n",
      "[INFO] Epoch: 31 , batch: 79 , training loss: 3.772806\n",
      "[INFO] Epoch: 31 , batch: 80 , training loss: 3.934164\n",
      "[INFO] Epoch: 31 , batch: 81 , training loss: 3.902370\n",
      "[INFO] Epoch: 31 , batch: 82 , training loss: 3.882114\n",
      "[INFO] Epoch: 31 , batch: 83 , training loss: 3.955870\n",
      "[INFO] Epoch: 31 , batch: 84 , training loss: 3.897602\n",
      "[INFO] Epoch: 31 , batch: 85 , training loss: 4.032207\n",
      "[INFO] Epoch: 31 , batch: 86 , training loss: 3.918892\n",
      "[INFO] Epoch: 31 , batch: 87 , training loss: 3.920768\n",
      "[INFO] Epoch: 31 , batch: 88 , training loss: 4.018246\n",
      "[INFO] Epoch: 31 , batch: 89 , training loss: 3.851688\n",
      "[INFO] Epoch: 31 , batch: 90 , training loss: 3.947515\n",
      "[INFO] Epoch: 31 , batch: 91 , training loss: 3.832830\n",
      "[INFO] Epoch: 31 , batch: 92 , training loss: 3.907261\n",
      "[INFO] Epoch: 31 , batch: 93 , training loss: 3.968386\n",
      "[INFO] Epoch: 31 , batch: 94 , training loss: 4.110851\n",
      "[INFO] Epoch: 31 , batch: 95 , training loss: 3.880008\n",
      "[INFO] Epoch: 31 , batch: 96 , training loss: 3.832059\n",
      "[INFO] Epoch: 31 , batch: 97 , training loss: 3.800481\n",
      "[INFO] Epoch: 31 , batch: 98 , training loss: 3.753824\n",
      "[INFO] Epoch: 31 , batch: 99 , training loss: 3.867289\n",
      "[INFO] Epoch: 31 , batch: 100 , training loss: 3.751920\n",
      "[INFO] Epoch: 31 , batch: 101 , training loss: 3.799345\n",
      "[INFO] Epoch: 31 , batch: 102 , training loss: 3.948327\n",
      "[INFO] Epoch: 31 , batch: 103 , training loss: 3.735322\n",
      "[INFO] Epoch: 31 , batch: 104 , training loss: 3.688432\n",
      "[INFO] Epoch: 31 , batch: 105 , training loss: 3.931381\n",
      "[INFO] Epoch: 31 , batch: 106 , training loss: 3.953907\n",
      "[INFO] Epoch: 31 , batch: 107 , training loss: 3.782511\n",
      "[INFO] Epoch: 31 , batch: 108 , training loss: 3.727562\n",
      "[INFO] Epoch: 31 , batch: 109 , training loss: 3.684660\n",
      "[INFO] Epoch: 31 , batch: 110 , training loss: 3.851214\n",
      "[INFO] Epoch: 31 , batch: 111 , training loss: 3.926635\n",
      "[INFO] Epoch: 31 , batch: 112 , training loss: 3.839332\n",
      "[INFO] Epoch: 31 , batch: 113 , training loss: 3.843411\n",
      "[INFO] Epoch: 31 , batch: 114 , training loss: 3.821018\n",
      "[INFO] Epoch: 31 , batch: 115 , training loss: 3.834062\n",
      "[INFO] Epoch: 31 , batch: 116 , training loss: 3.757465\n",
      "[INFO] Epoch: 31 , batch: 117 , training loss: 3.953951\n",
      "[INFO] Epoch: 31 , batch: 118 , training loss: 3.918684\n",
      "[INFO] Epoch: 31 , batch: 119 , training loss: 4.073124\n",
      "[INFO] Epoch: 31 , batch: 120 , training loss: 4.038213\n",
      "[INFO] Epoch: 31 , batch: 121 , training loss: 3.913423\n",
      "[INFO] Epoch: 31 , batch: 122 , training loss: 3.810483\n",
      "[INFO] Epoch: 31 , batch: 123 , training loss: 3.832180\n",
      "[INFO] Epoch: 31 , batch: 124 , training loss: 3.937048\n",
      "[INFO] Epoch: 31 , batch: 125 , training loss: 3.748293\n",
      "[INFO] Epoch: 31 , batch: 126 , training loss: 3.757391\n",
      "[INFO] Epoch: 31 , batch: 127 , training loss: 3.756228\n",
      "[INFO] Epoch: 31 , batch: 128 , training loss: 3.926409\n",
      "[INFO] Epoch: 31 , batch: 129 , training loss: 3.862595\n",
      "[INFO] Epoch: 31 , batch: 130 , training loss: 3.848132\n",
      "[INFO] Epoch: 31 , batch: 131 , training loss: 3.848483\n",
      "[INFO] Epoch: 31 , batch: 132 , training loss: 3.863204\n",
      "[INFO] Epoch: 31 , batch: 133 , training loss: 3.852289\n",
      "[INFO] Epoch: 31 , batch: 134 , training loss: 3.611286\n",
      "[INFO] Epoch: 31 , batch: 135 , training loss: 3.683764\n",
      "[INFO] Epoch: 31 , batch: 136 , training loss: 3.960946\n",
      "[INFO] Epoch: 31 , batch: 137 , training loss: 3.878525\n",
      "[INFO] Epoch: 31 , batch: 138 , training loss: 3.928911\n",
      "[INFO] Epoch: 31 , batch: 139 , training loss: 4.464136\n",
      "[INFO] Epoch: 31 , batch: 140 , training loss: 4.271091\n",
      "[INFO] Epoch: 31 , batch: 141 , training loss: 4.060196\n",
      "[INFO] Epoch: 31 , batch: 142 , training loss: 3.768690\n",
      "[INFO] Epoch: 31 , batch: 143 , training loss: 3.885379\n",
      "[INFO] Epoch: 31 , batch: 144 , training loss: 3.730814\n",
      "[INFO] Epoch: 31 , batch: 145 , training loss: 3.807089\n",
      "[INFO] Epoch: 31 , batch: 146 , training loss: 4.015504\n",
      "[INFO] Epoch: 31 , batch: 147 , training loss: 3.677155\n",
      "[INFO] Epoch: 31 , batch: 148 , training loss: 3.647700\n",
      "[INFO] Epoch: 31 , batch: 149 , training loss: 3.736796\n",
      "[INFO] Epoch: 31 , batch: 150 , training loss: 3.973543\n",
      "[INFO] Epoch: 31 , batch: 151 , training loss: 3.867132\n",
      "[INFO] Epoch: 31 , batch: 152 , training loss: 3.846646\n",
      "[INFO] Epoch: 31 , batch: 153 , training loss: 3.859143\n",
      "[INFO] Epoch: 31 , batch: 154 , training loss: 3.961867\n",
      "[INFO] Epoch: 31 , batch: 155 , training loss: 4.172277\n",
      "[INFO] Epoch: 31 , batch: 156 , training loss: 3.913620\n",
      "[INFO] Epoch: 31 , batch: 157 , training loss: 3.888670\n",
      "[INFO] Epoch: 31 , batch: 158 , training loss: 3.997260\n",
      "[INFO] Epoch: 31 , batch: 159 , training loss: 3.951509\n",
      "[INFO] Epoch: 31 , batch: 160 , training loss: 4.140972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 31 , batch: 161 , training loss: 4.239607\n",
      "[INFO] Epoch: 31 , batch: 162 , training loss: 4.187311\n",
      "[INFO] Epoch: 31 , batch: 163 , training loss: 4.380391\n",
      "[INFO] Epoch: 31 , batch: 164 , training loss: 4.332430\n",
      "[INFO] Epoch: 31 , batch: 165 , training loss: 4.271721\n",
      "[INFO] Epoch: 31 , batch: 166 , training loss: 4.124715\n",
      "[INFO] Epoch: 31 , batch: 167 , training loss: 4.290110\n",
      "[INFO] Epoch: 31 , batch: 168 , training loss: 3.927490\n",
      "[INFO] Epoch: 31 , batch: 169 , training loss: 3.922436\n",
      "[INFO] Epoch: 31 , batch: 170 , training loss: 4.066874\n",
      "[INFO] Epoch: 31 , batch: 171 , training loss: 3.503700\n",
      "[INFO] Epoch: 31 , batch: 172 , training loss: 3.737561\n",
      "[INFO] Epoch: 31 , batch: 173 , training loss: 4.042913\n",
      "[INFO] Epoch: 31 , batch: 174 , training loss: 4.453381\n",
      "[INFO] Epoch: 31 , batch: 175 , training loss: 4.772542\n",
      "[INFO] Epoch: 31 , batch: 176 , training loss: 4.486831\n",
      "[INFO] Epoch: 31 , batch: 177 , training loss: 4.087817\n",
      "[INFO] Epoch: 31 , batch: 178 , training loss: 4.072181\n",
      "[INFO] Epoch: 31 , batch: 179 , training loss: 4.138968\n",
      "[INFO] Epoch: 31 , batch: 180 , training loss: 4.096327\n",
      "[INFO] Epoch: 31 , batch: 181 , training loss: 4.357753\n",
      "[INFO] Epoch: 31 , batch: 182 , training loss: 4.324141\n",
      "[INFO] Epoch: 31 , batch: 183 , training loss: 4.282605\n",
      "[INFO] Epoch: 31 , batch: 184 , training loss: 4.155838\n",
      "[INFO] Epoch: 31 , batch: 185 , training loss: 4.139216\n",
      "[INFO] Epoch: 31 , batch: 186 , training loss: 4.263936\n",
      "[INFO] Epoch: 31 , batch: 187 , training loss: 4.345576\n",
      "[INFO] Epoch: 31 , batch: 188 , training loss: 4.354544\n",
      "[INFO] Epoch: 31 , batch: 189 , training loss: 4.273073\n",
      "[INFO] Epoch: 31 , batch: 190 , training loss: 4.305186\n",
      "[INFO] Epoch: 31 , batch: 191 , training loss: 4.423464\n",
      "[INFO] Epoch: 31 , batch: 192 , training loss: 4.259533\n",
      "[INFO] Epoch: 31 , batch: 193 , training loss: 4.351498\n",
      "[INFO] Epoch: 31 , batch: 194 , training loss: 4.309637\n",
      "[INFO] Epoch: 31 , batch: 195 , training loss: 4.245108\n",
      "[INFO] Epoch: 31 , batch: 196 , training loss: 4.085216\n",
      "[INFO] Epoch: 31 , batch: 197 , training loss: 4.196580\n",
      "[INFO] Epoch: 31 , batch: 198 , training loss: 4.113138\n",
      "[INFO] Epoch: 31 , batch: 199 , training loss: 4.253028\n",
      "[INFO] Epoch: 31 , batch: 200 , training loss: 4.173432\n",
      "[INFO] Epoch: 31 , batch: 201 , training loss: 4.074160\n",
      "[INFO] Epoch: 31 , batch: 202 , training loss: 4.072223\n",
      "[INFO] Epoch: 31 , batch: 203 , training loss: 4.193539\n",
      "[INFO] Epoch: 31 , batch: 204 , training loss: 4.304764\n",
      "[INFO] Epoch: 31 , batch: 205 , training loss: 3.894768\n",
      "[INFO] Epoch: 31 , batch: 206 , training loss: 3.822542\n",
      "[INFO] Epoch: 31 , batch: 207 , training loss: 3.831058\n",
      "[INFO] Epoch: 31 , batch: 208 , training loss: 4.163137\n",
      "[INFO] Epoch: 31 , batch: 209 , training loss: 4.122145\n",
      "[INFO] Epoch: 31 , batch: 210 , training loss: 4.132137\n",
      "[INFO] Epoch: 31 , batch: 211 , training loss: 4.122038\n",
      "[INFO] Epoch: 31 , batch: 212 , training loss: 4.228841\n",
      "[INFO] Epoch: 31 , batch: 213 , training loss: 4.204462\n",
      "[INFO] Epoch: 31 , batch: 214 , training loss: 4.274339\n",
      "[INFO] Epoch: 31 , batch: 215 , training loss: 4.493537\n",
      "[INFO] Epoch: 31 , batch: 216 , training loss: 4.213611\n",
      "[INFO] Epoch: 31 , batch: 217 , training loss: 4.144119\n",
      "[INFO] Epoch: 31 , batch: 218 , training loss: 4.102338\n",
      "[INFO] Epoch: 31 , batch: 219 , training loss: 4.215856\n",
      "[INFO] Epoch: 31 , batch: 220 , training loss: 4.053877\n",
      "[INFO] Epoch: 31 , batch: 221 , training loss: 4.056954\n",
      "[INFO] Epoch: 31 , batch: 222 , training loss: 4.187825\n",
      "[INFO] Epoch: 31 , batch: 223 , training loss: 4.281403\n",
      "[INFO] Epoch: 31 , batch: 224 , training loss: 4.341013\n",
      "[INFO] Epoch: 31 , batch: 225 , training loss: 4.217594\n",
      "[INFO] Epoch: 31 , batch: 226 , training loss: 4.362712\n",
      "[INFO] Epoch: 31 , batch: 227 , training loss: 4.329424\n",
      "[INFO] Epoch: 31 , batch: 228 , training loss: 4.339903\n",
      "[INFO] Epoch: 31 , batch: 229 , training loss: 4.235972\n",
      "[INFO] Epoch: 31 , batch: 230 , training loss: 4.073247\n",
      "[INFO] Epoch: 31 , batch: 231 , training loss: 3.930350\n",
      "[INFO] Epoch: 31 , batch: 232 , training loss: 4.063643\n",
      "[INFO] Epoch: 31 , batch: 233 , training loss: 4.103163\n",
      "[INFO] Epoch: 31 , batch: 234 , training loss: 3.798106\n",
      "[INFO] Epoch: 31 , batch: 235 , training loss: 3.911900\n",
      "[INFO] Epoch: 31 , batch: 236 , training loss: 4.030628\n",
      "[INFO] Epoch: 31 , batch: 237 , training loss: 4.224782\n",
      "[INFO] Epoch: 31 , batch: 238 , training loss: 3.983480\n",
      "[INFO] Epoch: 31 , batch: 239 , training loss: 4.040713\n",
      "[INFO] Epoch: 31 , batch: 240 , training loss: 4.083660\n",
      "[INFO] Epoch: 31 , batch: 241 , training loss: 3.883653\n",
      "[INFO] Epoch: 31 , batch: 242 , training loss: 3.893022\n",
      "[INFO] Epoch: 31 , batch: 243 , training loss: 4.192592\n",
      "[INFO] Epoch: 31 , batch: 244 , training loss: 4.146300\n",
      "[INFO] Epoch: 31 , batch: 245 , training loss: 4.105688\n",
      "[INFO] Epoch: 31 , batch: 246 , training loss: 3.815016\n",
      "[INFO] Epoch: 31 , batch: 247 , training loss: 3.978862\n",
      "[INFO] Epoch: 31 , batch: 248 , training loss: 4.046208\n",
      "[INFO] Epoch: 31 , batch: 249 , training loss: 4.017670\n",
      "[INFO] Epoch: 31 , batch: 250 , training loss: 3.825867\n",
      "[INFO] Epoch: 31 , batch: 251 , training loss: 4.286093\n",
      "[INFO] Epoch: 31 , batch: 252 , training loss: 3.982024\n",
      "[INFO] Epoch: 31 , batch: 253 , training loss: 3.901233\n",
      "[INFO] Epoch: 31 , batch: 254 , training loss: 4.185419\n",
      "[INFO] Epoch: 31 , batch: 255 , training loss: 4.143959\n",
      "[INFO] Epoch: 31 , batch: 256 , training loss: 4.138298\n",
      "[INFO] Epoch: 31 , batch: 257 , training loss: 4.299561\n",
      "[INFO] Epoch: 31 , batch: 258 , training loss: 4.321023\n",
      "[INFO] Epoch: 31 , batch: 259 , training loss: 4.341973\n",
      "[INFO] Epoch: 31 , batch: 260 , training loss: 4.122746\n",
      "[INFO] Epoch: 31 , batch: 261 , training loss: 4.294322\n",
      "[INFO] Epoch: 31 , batch: 262 , training loss: 4.431231\n",
      "[INFO] Epoch: 31 , batch: 263 , training loss: 4.601447\n",
      "[INFO] Epoch: 31 , batch: 264 , training loss: 3.959632\n",
      "[INFO] Epoch: 31 , batch: 265 , training loss: 4.049269\n",
      "[INFO] Epoch: 31 , batch: 266 , training loss: 4.505240\n",
      "[INFO] Epoch: 31 , batch: 267 , training loss: 4.217167\n",
      "[INFO] Epoch: 31 , batch: 268 , training loss: 4.146109\n",
      "[INFO] Epoch: 31 , batch: 269 , training loss: 4.131728\n",
      "[INFO] Epoch: 31 , batch: 270 , training loss: 4.153950\n",
      "[INFO] Epoch: 31 , batch: 271 , training loss: 4.173985\n",
      "[INFO] Epoch: 31 , batch: 272 , training loss: 4.173294\n",
      "[INFO] Epoch: 31 , batch: 273 , training loss: 4.194428\n",
      "[INFO] Epoch: 31 , batch: 274 , training loss: 4.274398\n",
      "[INFO] Epoch: 31 , batch: 275 , training loss: 4.117292\n",
      "[INFO] Epoch: 31 , batch: 276 , training loss: 4.191365\n",
      "[INFO] Epoch: 31 , batch: 277 , training loss: 4.381826\n",
      "[INFO] Epoch: 31 , batch: 278 , training loss: 4.013323\n",
      "[INFO] Epoch: 31 , batch: 279 , training loss: 4.014606\n",
      "[INFO] Epoch: 31 , batch: 280 , training loss: 4.008202\n",
      "[INFO] Epoch: 31 , batch: 281 , training loss: 4.133642\n",
      "[INFO] Epoch: 31 , batch: 282 , training loss: 4.048786\n",
      "[INFO] Epoch: 31 , batch: 283 , training loss: 4.055089\n",
      "[INFO] Epoch: 31 , batch: 284 , training loss: 4.093119\n",
      "[INFO] Epoch: 31 , batch: 285 , training loss: 4.029985\n",
      "[INFO] Epoch: 31 , batch: 286 , training loss: 4.028843\n",
      "[INFO] Epoch: 31 , batch: 287 , training loss: 3.973095\n",
      "[INFO] Epoch: 31 , batch: 288 , training loss: 3.951015\n",
      "[INFO] Epoch: 31 , batch: 289 , training loss: 4.023776\n",
      "[INFO] Epoch: 31 , batch: 290 , training loss: 3.785738\n",
      "[INFO] Epoch: 31 , batch: 291 , training loss: 3.776965\n",
      "[INFO] Epoch: 31 , batch: 292 , training loss: 3.879167\n",
      "[INFO] Epoch: 31 , batch: 293 , training loss: 3.816912\n",
      "[INFO] Epoch: 31 , batch: 294 , training loss: 4.477689\n",
      "[INFO] Epoch: 31 , batch: 295 , training loss: 4.266346\n",
      "[INFO] Epoch: 31 , batch: 296 , training loss: 4.191551\n",
      "[INFO] Epoch: 31 , batch: 297 , training loss: 4.138848\n",
      "[INFO] Epoch: 31 , batch: 298 , training loss: 3.979662\n",
      "[INFO] Epoch: 31 , batch: 299 , training loss: 4.014820\n",
      "[INFO] Epoch: 31 , batch: 300 , training loss: 3.990858\n",
      "[INFO] Epoch: 31 , batch: 301 , training loss: 3.926260\n",
      "[INFO] Epoch: 31 , batch: 302 , training loss: 4.098408\n",
      "[INFO] Epoch: 31 , batch: 303 , training loss: 4.098937\n",
      "[INFO] Epoch: 31 , batch: 304 , training loss: 4.246684\n",
      "[INFO] Epoch: 31 , batch: 305 , training loss: 4.079375\n",
      "[INFO] Epoch: 31 , batch: 306 , training loss: 4.194552\n",
      "[INFO] Epoch: 31 , batch: 307 , training loss: 4.195768\n",
      "[INFO] Epoch: 31 , batch: 308 , training loss: 4.005597\n",
      "[INFO] Epoch: 31 , batch: 309 , training loss: 4.029493\n",
      "[INFO] Epoch: 31 , batch: 310 , training loss: 3.933731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 31 , batch: 311 , training loss: 3.930899\n",
      "[INFO] Epoch: 31 , batch: 312 , training loss: 3.849577\n",
      "[INFO] Epoch: 31 , batch: 313 , training loss: 3.939219\n",
      "[INFO] Epoch: 31 , batch: 314 , training loss: 4.013552\n",
      "[INFO] Epoch: 31 , batch: 315 , training loss: 4.086435\n",
      "[INFO] Epoch: 31 , batch: 316 , training loss: 4.341573\n",
      "[INFO] Epoch: 31 , batch: 317 , training loss: 4.720028\n",
      "[INFO] Epoch: 31 , batch: 318 , training loss: 4.837283\n",
      "[INFO] Epoch: 31 , batch: 319 , training loss: 4.526546\n",
      "[INFO] Epoch: 31 , batch: 320 , training loss: 4.045371\n",
      "[INFO] Epoch: 31 , batch: 321 , training loss: 3.863171\n",
      "[INFO] Epoch: 31 , batch: 322 , training loss: 3.973578\n",
      "[INFO] Epoch: 31 , batch: 323 , training loss: 4.004231\n",
      "[INFO] Epoch: 31 , batch: 324 , training loss: 3.991296\n",
      "[INFO] Epoch: 31 , batch: 325 , training loss: 4.112117\n",
      "[INFO] Epoch: 31 , batch: 326 , training loss: 4.170215\n",
      "[INFO] Epoch: 31 , batch: 327 , training loss: 4.091886\n",
      "[INFO] Epoch: 31 , batch: 328 , training loss: 4.094742\n",
      "[INFO] Epoch: 31 , batch: 329 , training loss: 3.990801\n",
      "[INFO] Epoch: 31 , batch: 330 , training loss: 3.975480\n",
      "[INFO] Epoch: 31 , batch: 331 , training loss: 4.155894\n",
      "[INFO] Epoch: 31 , batch: 332 , training loss: 4.003997\n",
      "[INFO] Epoch: 31 , batch: 333 , training loss: 3.952057\n",
      "[INFO] Epoch: 31 , batch: 334 , training loss: 3.992756\n",
      "[INFO] Epoch: 31 , batch: 335 , training loss: 4.108052\n",
      "[INFO] Epoch: 31 , batch: 336 , training loss: 4.120923\n",
      "[INFO] Epoch: 31 , batch: 337 , training loss: 4.163736\n",
      "[INFO] Epoch: 31 , batch: 338 , training loss: 4.377395\n",
      "[INFO] Epoch: 31 , batch: 339 , training loss: 4.210755\n",
      "[INFO] Epoch: 31 , batch: 340 , training loss: 4.374411\n",
      "[INFO] Epoch: 31 , batch: 341 , training loss: 4.143562\n",
      "[INFO] Epoch: 31 , batch: 342 , training loss: 3.927627\n",
      "[INFO] Epoch: 31 , batch: 343 , training loss: 3.998501\n",
      "[INFO] Epoch: 31 , batch: 344 , training loss: 3.870835\n",
      "[INFO] Epoch: 31 , batch: 345 , training loss: 3.990697\n",
      "[INFO] Epoch: 31 , batch: 346 , training loss: 4.053929\n",
      "[INFO] Epoch: 31 , batch: 347 , training loss: 3.938523\n",
      "[INFO] Epoch: 31 , batch: 348 , training loss: 4.063867\n",
      "[INFO] Epoch: 31 , batch: 349 , training loss: 4.140632\n",
      "[INFO] Epoch: 31 , batch: 350 , training loss: 3.993619\n",
      "[INFO] Epoch: 31 , batch: 351 , training loss: 4.073973\n",
      "[INFO] Epoch: 31 , batch: 352 , training loss: 4.086580\n",
      "[INFO] Epoch: 31 , batch: 353 , training loss: 4.066869\n",
      "[INFO] Epoch: 31 , batch: 354 , training loss: 4.156640\n",
      "[INFO] Epoch: 31 , batch: 355 , training loss: 4.134565\n",
      "[INFO] Epoch: 31 , batch: 356 , training loss: 4.026300\n",
      "[INFO] Epoch: 31 , batch: 357 , training loss: 4.103640\n",
      "[INFO] Epoch: 31 , batch: 358 , training loss: 4.005755\n",
      "[INFO] Epoch: 31 , batch: 359 , training loss: 4.018281\n",
      "[INFO] Epoch: 31 , batch: 360 , training loss: 4.094179\n",
      "[INFO] Epoch: 31 , batch: 361 , training loss: 4.072651\n",
      "[INFO] Epoch: 31 , batch: 362 , training loss: 4.169843\n",
      "[INFO] Epoch: 31 , batch: 363 , training loss: 4.066659\n",
      "[INFO] Epoch: 31 , batch: 364 , training loss: 4.144047\n",
      "[INFO] Epoch: 31 , batch: 365 , training loss: 4.012491\n",
      "[INFO] Epoch: 31 , batch: 366 , training loss: 4.161009\n",
      "[INFO] Epoch: 31 , batch: 367 , training loss: 4.187209\n",
      "[INFO] Epoch: 31 , batch: 368 , training loss: 4.596103\n",
      "[INFO] Epoch: 31 , batch: 369 , training loss: 4.278499\n",
      "[INFO] Epoch: 31 , batch: 370 , training loss: 4.030587\n",
      "[INFO] Epoch: 31 , batch: 371 , training loss: 4.494476\n",
      "[INFO] Epoch: 31 , batch: 372 , training loss: 4.758305\n",
      "[INFO] Epoch: 31 , batch: 373 , training loss: 4.775340\n",
      "[INFO] Epoch: 31 , batch: 374 , training loss: 4.930856\n",
      "[INFO] Epoch: 31 , batch: 375 , training loss: 4.871841\n",
      "[INFO] Epoch: 31 , batch: 376 , training loss: 4.760130\n",
      "[INFO] Epoch: 31 , batch: 377 , training loss: 4.488499\n",
      "[INFO] Epoch: 31 , batch: 378 , training loss: 4.575219\n",
      "[INFO] Epoch: 31 , batch: 379 , training loss: 4.554015\n",
      "[INFO] Epoch: 31 , batch: 380 , training loss: 4.731008\n",
      "[INFO] Epoch: 31 , batch: 381 , training loss: 4.430680\n",
      "[INFO] Epoch: 31 , batch: 382 , training loss: 4.734023\n",
      "[INFO] Epoch: 31 , batch: 383 , training loss: 4.756731\n",
      "[INFO] Epoch: 31 , batch: 384 , training loss: 4.729786\n",
      "[INFO] Epoch: 31 , batch: 385 , training loss: 4.400794\n",
      "[INFO] Epoch: 31 , batch: 386 , training loss: 4.644939\n",
      "[INFO] Epoch: 31 , batch: 387 , training loss: 4.600814\n",
      "[INFO] Epoch: 31 , batch: 388 , training loss: 4.418448\n",
      "[INFO] Epoch: 31 , batch: 389 , training loss: 4.236897\n",
      "[INFO] Epoch: 31 , batch: 390 , training loss: 4.263641\n",
      "[INFO] Epoch: 31 , batch: 391 , training loss: 4.283013\n",
      "[INFO] Epoch: 31 , batch: 392 , training loss: 4.634224\n",
      "[INFO] Epoch: 31 , batch: 393 , training loss: 4.538336\n",
      "[INFO] Epoch: 31 , batch: 394 , training loss: 4.633094\n",
      "[INFO] Epoch: 31 , batch: 395 , training loss: 4.443164\n",
      "[INFO] Epoch: 31 , batch: 396 , training loss: 4.265884\n",
      "[INFO] Epoch: 31 , batch: 397 , training loss: 4.402075\n",
      "[INFO] Epoch: 31 , batch: 398 , training loss: 4.278584\n",
      "[INFO] Epoch: 31 , batch: 399 , training loss: 4.336582\n",
      "[INFO] Epoch: 31 , batch: 400 , training loss: 4.301226\n",
      "[INFO] Epoch: 31 , batch: 401 , training loss: 4.735978\n",
      "[INFO] Epoch: 31 , batch: 402 , training loss: 4.454978\n",
      "[INFO] Epoch: 31 , batch: 403 , training loss: 4.273946\n",
      "[INFO] Epoch: 31 , batch: 404 , training loss: 4.460299\n",
      "[INFO] Epoch: 31 , batch: 405 , training loss: 4.516304\n",
      "[INFO] Epoch: 31 , batch: 406 , training loss: 4.409009\n",
      "[INFO] Epoch: 31 , batch: 407 , training loss: 4.454334\n",
      "[INFO] Epoch: 31 , batch: 408 , training loss: 4.407922\n",
      "[INFO] Epoch: 31 , batch: 409 , training loss: 4.428693\n",
      "[INFO] Epoch: 31 , batch: 410 , training loss: 4.488008\n",
      "[INFO] Epoch: 31 , batch: 411 , training loss: 4.664578\n",
      "[INFO] Epoch: 31 , batch: 412 , training loss: 4.499657\n",
      "[INFO] Epoch: 31 , batch: 413 , training loss: 4.370090\n",
      "[INFO] Epoch: 31 , batch: 414 , training loss: 4.401096\n",
      "[INFO] Epoch: 31 , batch: 415 , training loss: 4.448603\n",
      "[INFO] Epoch: 31 , batch: 416 , training loss: 4.517292\n",
      "[INFO] Epoch: 31 , batch: 417 , training loss: 4.409796\n",
      "[INFO] Epoch: 31 , batch: 418 , training loss: 4.493930\n",
      "[INFO] Epoch: 31 , batch: 419 , training loss: 4.440697\n",
      "[INFO] Epoch: 31 , batch: 420 , training loss: 4.425764\n",
      "[INFO] Epoch: 31 , batch: 421 , training loss: 4.385676\n",
      "[INFO] Epoch: 31 , batch: 422 , training loss: 4.233190\n",
      "[INFO] Epoch: 31 , batch: 423 , training loss: 4.464782\n",
      "[INFO] Epoch: 31 , batch: 424 , training loss: 4.653600\n",
      "[INFO] Epoch: 31 , batch: 425 , training loss: 4.486102\n",
      "[INFO] Epoch: 31 , batch: 426 , training loss: 4.235990\n",
      "[INFO] Epoch: 31 , batch: 427 , training loss: 4.485571\n",
      "[INFO] Epoch: 31 , batch: 428 , training loss: 4.355243\n",
      "[INFO] Epoch: 31 , batch: 429 , training loss: 4.247898\n",
      "[INFO] Epoch: 31 , batch: 430 , training loss: 4.476639\n",
      "[INFO] Epoch: 31 , batch: 431 , training loss: 4.080895\n",
      "[INFO] Epoch: 31 , batch: 432 , training loss: 4.135475\n",
      "[INFO] Epoch: 31 , batch: 433 , training loss: 4.176118\n",
      "[INFO] Epoch: 31 , batch: 434 , training loss: 4.056716\n",
      "[INFO] Epoch: 31 , batch: 435 , training loss: 4.393726\n",
      "[INFO] Epoch: 31 , batch: 436 , training loss: 4.445851\n",
      "[INFO] Epoch: 31 , batch: 437 , training loss: 4.234067\n",
      "[INFO] Epoch: 31 , batch: 438 , training loss: 4.104840\n",
      "[INFO] Epoch: 31 , batch: 439 , training loss: 4.324923\n",
      "[INFO] Epoch: 31 , batch: 440 , training loss: 4.446636\n",
      "[INFO] Epoch: 31 , batch: 441 , training loss: 4.534862\n",
      "[INFO] Epoch: 31 , batch: 442 , training loss: 4.304876\n",
      "[INFO] Epoch: 31 , batch: 443 , training loss: 4.512386\n",
      "[INFO] Epoch: 31 , batch: 444 , training loss: 4.102638\n",
      "[INFO] Epoch: 31 , batch: 445 , training loss: 3.999853\n",
      "[INFO] Epoch: 31 , batch: 446 , training loss: 3.928275\n",
      "[INFO] Epoch: 31 , batch: 447 , training loss: 4.129967\n",
      "[INFO] Epoch: 31 , batch: 448 , training loss: 4.258917\n",
      "[INFO] Epoch: 31 , batch: 449 , training loss: 4.619518\n",
      "[INFO] Epoch: 31 , batch: 450 , training loss: 4.689377\n",
      "[INFO] Epoch: 31 , batch: 451 , training loss: 4.591090\n",
      "[INFO] Epoch: 31 , batch: 452 , training loss: 4.414008\n",
      "[INFO] Epoch: 31 , batch: 453 , training loss: 4.175538\n",
      "[INFO] Epoch: 31 , batch: 454 , training loss: 4.330244\n",
      "[INFO] Epoch: 31 , batch: 455 , training loss: 4.395822\n",
      "[INFO] Epoch: 31 , batch: 456 , training loss: 4.362691\n",
      "[INFO] Epoch: 31 , batch: 457 , training loss: 4.447950\n",
      "[INFO] Epoch: 31 , batch: 458 , training loss: 4.181979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 31 , batch: 459 , training loss: 4.154434\n",
      "[INFO] Epoch: 31 , batch: 460 , training loss: 4.268861\n",
      "[INFO] Epoch: 31 , batch: 461 , training loss: 4.244044\n",
      "[INFO] Epoch: 31 , batch: 462 , training loss: 4.294610\n",
      "[INFO] Epoch: 31 , batch: 463 , training loss: 4.204081\n",
      "[INFO] Epoch: 31 , batch: 464 , training loss: 4.383041\n",
      "[INFO] Epoch: 31 , batch: 465 , training loss: 4.345393\n",
      "[INFO] Epoch: 31 , batch: 466 , training loss: 4.444959\n",
      "[INFO] Epoch: 31 , batch: 467 , training loss: 4.397836\n",
      "[INFO] Epoch: 31 , batch: 468 , training loss: 4.358703\n",
      "[INFO] Epoch: 31 , batch: 469 , training loss: 4.376443\n",
      "[INFO] Epoch: 31 , batch: 470 , training loss: 4.201272\n",
      "[INFO] Epoch: 31 , batch: 471 , training loss: 4.321908\n",
      "[INFO] Epoch: 31 , batch: 472 , training loss: 4.372076\n",
      "[INFO] Epoch: 31 , batch: 473 , training loss: 4.288494\n",
      "[INFO] Epoch: 31 , batch: 474 , training loss: 4.065963\n",
      "[INFO] Epoch: 31 , batch: 475 , training loss: 3.950462\n",
      "[INFO] Epoch: 31 , batch: 476 , training loss: 4.349174\n",
      "[INFO] Epoch: 31 , batch: 477 , training loss: 4.459284\n",
      "[INFO] Epoch: 31 , batch: 478 , training loss: 4.463986\n",
      "[INFO] Epoch: 31 , batch: 479 , training loss: 4.428481\n",
      "[INFO] Epoch: 31 , batch: 480 , training loss: 4.565680\n",
      "[INFO] Epoch: 31 , batch: 481 , training loss: 4.439196\n",
      "[INFO] Epoch: 31 , batch: 482 , training loss: 4.563523\n",
      "[INFO] Epoch: 31 , batch: 483 , training loss: 4.379645\n",
      "[INFO] Epoch: 31 , batch: 484 , training loss: 4.186249\n",
      "[INFO] Epoch: 31 , batch: 485 , training loss: 4.279789\n",
      "[INFO] Epoch: 31 , batch: 486 , training loss: 4.184297\n",
      "[INFO] Epoch: 31 , batch: 487 , training loss: 4.168939\n",
      "[INFO] Epoch: 31 , batch: 488 , training loss: 4.360921\n",
      "[INFO] Epoch: 31 , batch: 489 , training loss: 4.245968\n",
      "[INFO] Epoch: 31 , batch: 490 , training loss: 4.310945\n",
      "[INFO] Epoch: 31 , batch: 491 , training loss: 4.227684\n",
      "[INFO] Epoch: 31 , batch: 492 , training loss: 4.216462\n",
      "[INFO] Epoch: 31 , batch: 493 , training loss: 4.375954\n",
      "[INFO] Epoch: 31 , batch: 494 , training loss: 4.270543\n",
      "[INFO] Epoch: 31 , batch: 495 , training loss: 4.448164\n",
      "[INFO] Epoch: 31 , batch: 496 , training loss: 4.305001\n",
      "[INFO] Epoch: 31 , batch: 497 , training loss: 4.353997\n",
      "[INFO] Epoch: 31 , batch: 498 , training loss: 4.332305\n",
      "[INFO] Epoch: 31 , batch: 499 , training loss: 4.420323\n",
      "[INFO] Epoch: 31 , batch: 500 , training loss: 4.550778\n",
      "[INFO] Epoch: 31 , batch: 501 , training loss: 4.882314\n",
      "[INFO] Epoch: 31 , batch: 502 , training loss: 4.918648\n",
      "[INFO] Epoch: 31 , batch: 503 , training loss: 4.577834\n",
      "[INFO] Epoch: 31 , batch: 504 , training loss: 4.736875\n",
      "[INFO] Epoch: 31 , batch: 505 , training loss: 4.683492\n",
      "[INFO] Epoch: 31 , batch: 506 , training loss: 4.655792\n",
      "[INFO] Epoch: 31 , batch: 507 , training loss: 4.713156\n",
      "[INFO] Epoch: 31 , batch: 508 , training loss: 4.623239\n",
      "[INFO] Epoch: 31 , batch: 509 , training loss: 4.418404\n",
      "[INFO] Epoch: 31 , batch: 510 , training loss: 4.507603\n",
      "[INFO] Epoch: 31 , batch: 511 , training loss: 4.441726\n",
      "[INFO] Epoch: 31 , batch: 512 , training loss: 4.507260\n",
      "[INFO] Epoch: 31 , batch: 513 , training loss: 4.798863\n",
      "[INFO] Epoch: 31 , batch: 514 , training loss: 4.419891\n",
      "[INFO] Epoch: 31 , batch: 515 , training loss: 4.674368\n",
      "[INFO] Epoch: 31 , batch: 516 , training loss: 4.451092\n",
      "[INFO] Epoch: 31 , batch: 517 , training loss: 4.447215\n",
      "[INFO] Epoch: 31 , batch: 518 , training loss: 4.399580\n",
      "[INFO] Epoch: 31 , batch: 519 , training loss: 4.247167\n",
      "[INFO] Epoch: 31 , batch: 520 , training loss: 4.474576\n",
      "[INFO] Epoch: 31 , batch: 521 , training loss: 4.452713\n",
      "[INFO] Epoch: 31 , batch: 522 , training loss: 4.547291\n",
      "[INFO] Epoch: 31 , batch: 523 , training loss: 4.449594\n",
      "[INFO] Epoch: 31 , batch: 524 , training loss: 4.738609\n",
      "[INFO] Epoch: 31 , batch: 525 , training loss: 4.614087\n",
      "[INFO] Epoch: 31 , batch: 526 , training loss: 4.402189\n",
      "[INFO] Epoch: 31 , batch: 527 , training loss: 4.457119\n",
      "[INFO] Epoch: 31 , batch: 528 , training loss: 4.453800\n",
      "[INFO] Epoch: 31 , batch: 529 , training loss: 4.419273\n",
      "[INFO] Epoch: 31 , batch: 530 , training loss: 4.280232\n",
      "[INFO] Epoch: 31 , batch: 531 , training loss: 4.436056\n",
      "[INFO] Epoch: 31 , batch: 532 , training loss: 4.330544\n",
      "[INFO] Epoch: 31 , batch: 533 , training loss: 4.487057\n",
      "[INFO] Epoch: 31 , batch: 534 , training loss: 4.486348\n",
      "[INFO] Epoch: 31 , batch: 535 , training loss: 4.474412\n",
      "[INFO] Epoch: 31 , batch: 536 , training loss: 4.338859\n",
      "[INFO] Epoch: 31 , batch: 537 , training loss: 4.306290\n",
      "[INFO] Epoch: 31 , batch: 538 , training loss: 4.379746\n",
      "[INFO] Epoch: 31 , batch: 539 , training loss: 4.512727\n",
      "[INFO] Epoch: 31 , batch: 540 , training loss: 5.057854\n",
      "[INFO] Epoch: 31 , batch: 541 , training loss: 4.887752\n",
      "[INFO] Epoch: 31 , batch: 542 , training loss: 4.725195\n",
      "[INFO] Epoch: 32 , batch: 0 , training loss: 3.791841\n",
      "[INFO] Epoch: 32 , batch: 1 , training loss: 3.639603\n",
      "[INFO] Epoch: 32 , batch: 2 , training loss: 3.749439\n",
      "[INFO] Epoch: 32 , batch: 3 , training loss: 3.640752\n",
      "[INFO] Epoch: 32 , batch: 4 , training loss: 3.956949\n",
      "[INFO] Epoch: 32 , batch: 5 , training loss: 3.664492\n",
      "[INFO] Epoch: 32 , batch: 6 , training loss: 4.053085\n",
      "[INFO] Epoch: 32 , batch: 7 , training loss: 3.867754\n",
      "[INFO] Epoch: 32 , batch: 8 , training loss: 3.580883\n",
      "[INFO] Epoch: 32 , batch: 9 , training loss: 3.837747\n",
      "[INFO] Epoch: 32 , batch: 10 , training loss: 3.760196\n",
      "[INFO] Epoch: 32 , batch: 11 , training loss: 3.721830\n",
      "[INFO] Epoch: 32 , batch: 12 , training loss: 3.602451\n",
      "[INFO] Epoch: 32 , batch: 13 , training loss: 3.630723\n",
      "[INFO] Epoch: 32 , batch: 14 , training loss: 3.518516\n",
      "[INFO] Epoch: 32 , batch: 15 , training loss: 3.753647\n",
      "[INFO] Epoch: 32 , batch: 16 , training loss: 3.629937\n",
      "[INFO] Epoch: 32 , batch: 17 , training loss: 3.752083\n",
      "[INFO] Epoch: 32 , batch: 18 , training loss: 3.666073\n",
      "[INFO] Epoch: 32 , batch: 19 , training loss: 3.468066\n",
      "[INFO] Epoch: 32 , batch: 20 , training loss: 3.416687\n",
      "[INFO] Epoch: 32 , batch: 21 , training loss: 3.551570\n",
      "[INFO] Epoch: 32 , batch: 22 , training loss: 3.458519\n",
      "[INFO] Epoch: 32 , batch: 23 , training loss: 3.654638\n",
      "[INFO] Epoch: 32 , batch: 24 , training loss: 3.492069\n",
      "[INFO] Epoch: 32 , batch: 25 , training loss: 3.619238\n",
      "[INFO] Epoch: 32 , batch: 26 , training loss: 3.483687\n",
      "[INFO] Epoch: 32 , batch: 27 , training loss: 3.445143\n",
      "[INFO] Epoch: 32 , batch: 28 , training loss: 3.634167\n",
      "[INFO] Epoch: 32 , batch: 29 , training loss: 3.475486\n",
      "[INFO] Epoch: 32 , batch: 30 , training loss: 3.491536\n",
      "[INFO] Epoch: 32 , batch: 31 , training loss: 3.620287\n",
      "[INFO] Epoch: 32 , batch: 32 , training loss: 3.569542\n",
      "[INFO] Epoch: 32 , batch: 33 , training loss: 3.616411\n",
      "[INFO] Epoch: 32 , batch: 34 , training loss: 3.589122\n",
      "[INFO] Epoch: 32 , batch: 35 , training loss: 3.520245\n",
      "[INFO] Epoch: 32 , batch: 36 , training loss: 3.633848\n",
      "[INFO] Epoch: 32 , batch: 37 , training loss: 3.480982\n",
      "[INFO] Epoch: 32 , batch: 38 , training loss: 3.583746\n",
      "[INFO] Epoch: 32 , batch: 39 , training loss: 3.362297\n",
      "[INFO] Epoch: 32 , batch: 40 , training loss: 3.608269\n",
      "[INFO] Epoch: 32 , batch: 41 , training loss: 3.531444\n",
      "[INFO] Epoch: 32 , batch: 42 , training loss: 4.038145\n",
      "[INFO] Epoch: 32 , batch: 43 , training loss: 3.763852\n",
      "[INFO] Epoch: 32 , batch: 44 , training loss: 4.084212\n",
      "[INFO] Epoch: 32 , batch: 45 , training loss: 4.025086\n",
      "[INFO] Epoch: 32 , batch: 46 , training loss: 4.041915\n",
      "[INFO] Epoch: 32 , batch: 47 , training loss: 3.623002\n",
      "[INFO] Epoch: 32 , batch: 48 , training loss: 3.598620\n",
      "[INFO] Epoch: 32 , batch: 49 , training loss: 3.822110\n",
      "[INFO] Epoch: 32 , batch: 50 , training loss: 3.586648\n",
      "[INFO] Epoch: 32 , batch: 51 , training loss: 3.850823\n",
      "[INFO] Epoch: 32 , batch: 52 , training loss: 3.669017\n",
      "[INFO] Epoch: 32 , batch: 53 , training loss: 3.752056\n",
      "[INFO] Epoch: 32 , batch: 54 , training loss: 3.791192\n",
      "[INFO] Epoch: 32 , batch: 55 , training loss: 3.862828\n",
      "[INFO] Epoch: 32 , batch: 56 , training loss: 3.663170\n",
      "[INFO] Epoch: 32 , batch: 57 , training loss: 3.599173\n",
      "[INFO] Epoch: 32 , batch: 58 , training loss: 3.680225\n",
      "[INFO] Epoch: 32 , batch: 59 , training loss: 3.774415\n",
      "[INFO] Epoch: 32 , batch: 60 , training loss: 3.714754\n",
      "[INFO] Epoch: 32 , batch: 61 , training loss: 3.785355\n",
      "[INFO] Epoch: 32 , batch: 62 , training loss: 3.664671\n",
      "[INFO] Epoch: 32 , batch: 63 , training loss: 3.832041\n",
      "[INFO] Epoch: 32 , batch: 64 , training loss: 4.045527\n",
      "[INFO] Epoch: 32 , batch: 65 , training loss: 3.746784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 32 , batch: 66 , training loss: 3.619447\n",
      "[INFO] Epoch: 32 , batch: 67 , training loss: 3.639624\n",
      "[INFO] Epoch: 32 , batch: 68 , training loss: 3.802388\n",
      "[INFO] Epoch: 32 , batch: 69 , training loss: 3.711071\n",
      "[INFO] Epoch: 32 , batch: 70 , training loss: 3.936340\n",
      "[INFO] Epoch: 32 , batch: 71 , training loss: 3.780233\n",
      "[INFO] Epoch: 32 , batch: 72 , training loss: 3.865255\n",
      "[INFO] Epoch: 32 , batch: 73 , training loss: 3.797639\n",
      "[INFO] Epoch: 32 , batch: 74 , training loss: 3.945843\n",
      "[INFO] Epoch: 32 , batch: 75 , training loss: 3.760534\n",
      "[INFO] Epoch: 32 , batch: 76 , training loss: 3.869043\n",
      "[INFO] Epoch: 32 , batch: 77 , training loss: 3.813991\n",
      "[INFO] Epoch: 32 , batch: 78 , training loss: 3.891828\n",
      "[INFO] Epoch: 32 , batch: 79 , training loss: 3.753758\n",
      "[INFO] Epoch: 32 , batch: 80 , training loss: 3.962753\n",
      "[INFO] Epoch: 32 , batch: 81 , training loss: 3.916887\n",
      "[INFO] Epoch: 32 , batch: 82 , training loss: 3.882239\n",
      "[INFO] Epoch: 32 , batch: 83 , training loss: 3.951222\n",
      "[INFO] Epoch: 32 , batch: 84 , training loss: 3.900659\n",
      "[INFO] Epoch: 32 , batch: 85 , training loss: 4.025082\n",
      "[INFO] Epoch: 32 , batch: 86 , training loss: 3.921534\n",
      "[INFO] Epoch: 32 , batch: 87 , training loss: 3.908358\n",
      "[INFO] Epoch: 32 , batch: 88 , training loss: 4.047541\n",
      "[INFO] Epoch: 32 , batch: 89 , training loss: 3.830322\n",
      "[INFO] Epoch: 32 , batch: 90 , training loss: 3.962252\n",
      "[INFO] Epoch: 32 , batch: 91 , training loss: 3.853728\n",
      "[INFO] Epoch: 32 , batch: 92 , training loss: 3.890943\n",
      "[INFO] Epoch: 32 , batch: 93 , training loss: 3.963077\n",
      "[INFO] Epoch: 32 , batch: 94 , training loss: 4.132712\n",
      "[INFO] Epoch: 32 , batch: 95 , training loss: 3.890428\n",
      "[INFO] Epoch: 32 , batch: 96 , training loss: 3.846781\n",
      "[INFO] Epoch: 32 , batch: 97 , training loss: 3.813769\n",
      "[INFO] Epoch: 32 , batch: 98 , training loss: 3.760098\n",
      "[INFO] Epoch: 32 , batch: 99 , training loss: 3.880907\n",
      "[INFO] Epoch: 32 , batch: 100 , training loss: 3.777013\n",
      "[INFO] Epoch: 32 , batch: 101 , training loss: 3.804505\n",
      "[INFO] Epoch: 32 , batch: 102 , training loss: 3.966616\n",
      "[INFO] Epoch: 32 , batch: 103 , training loss: 3.747298\n",
      "[INFO] Epoch: 32 , batch: 104 , training loss: 3.685050\n",
      "[INFO] Epoch: 32 , batch: 105 , training loss: 3.946177\n",
      "[INFO] Epoch: 32 , batch: 106 , training loss: 3.968616\n",
      "[INFO] Epoch: 32 , batch: 107 , training loss: 3.791154\n",
      "[INFO] Epoch: 32 , batch: 108 , training loss: 3.752045\n",
      "[INFO] Epoch: 32 , batch: 109 , training loss: 3.690427\n",
      "[INFO] Epoch: 32 , batch: 110 , training loss: 3.861315\n",
      "[INFO] Epoch: 32 , batch: 111 , training loss: 3.942130\n",
      "[INFO] Epoch: 32 , batch: 112 , training loss: 3.853849\n",
      "[INFO] Epoch: 32 , batch: 113 , training loss: 3.831042\n",
      "[INFO] Epoch: 32 , batch: 114 , training loss: 3.824085\n",
      "[INFO] Epoch: 32 , batch: 115 , training loss: 3.845434\n",
      "[INFO] Epoch: 32 , batch: 116 , training loss: 3.750431\n",
      "[INFO] Epoch: 32 , batch: 117 , training loss: 3.967256\n",
      "[INFO] Epoch: 32 , batch: 118 , training loss: 3.927322\n",
      "[INFO] Epoch: 32 , batch: 119 , training loss: 4.106740\n",
      "[INFO] Epoch: 32 , batch: 120 , training loss: 4.039783\n",
      "[INFO] Epoch: 32 , batch: 121 , training loss: 3.933483\n",
      "[INFO] Epoch: 32 , batch: 122 , training loss: 3.835019\n",
      "[INFO] Epoch: 32 , batch: 123 , training loss: 3.833126\n",
      "[INFO] Epoch: 32 , batch: 124 , training loss: 3.952915\n",
      "[INFO] Epoch: 32 , batch: 125 , training loss: 3.752809\n",
      "[INFO] Epoch: 32 , batch: 126 , training loss: 3.780975\n",
      "[INFO] Epoch: 32 , batch: 127 , training loss: 3.782595\n",
      "[INFO] Epoch: 32 , batch: 128 , training loss: 3.921104\n",
      "[INFO] Epoch: 32 , batch: 129 , training loss: 3.883158\n",
      "[INFO] Epoch: 32 , batch: 130 , training loss: 3.861285\n",
      "[INFO] Epoch: 32 , batch: 131 , training loss: 3.862017\n",
      "[INFO] Epoch: 32 , batch: 132 , training loss: 3.898323\n",
      "[INFO] Epoch: 32 , batch: 133 , training loss: 3.848885\n",
      "[INFO] Epoch: 32 , batch: 134 , training loss: 3.605736\n",
      "[INFO] Epoch: 32 , batch: 135 , training loss: 3.684659\n",
      "[INFO] Epoch: 32 , batch: 136 , training loss: 3.953509\n",
      "[INFO] Epoch: 32 , batch: 137 , training loss: 3.885438\n",
      "[INFO] Epoch: 32 , batch: 138 , training loss: 3.941149\n",
      "[INFO] Epoch: 32 , batch: 139 , training loss: 4.491466\n",
      "[INFO] Epoch: 32 , batch: 140 , training loss: 4.302845\n",
      "[INFO] Epoch: 32 , batch: 141 , training loss: 4.077668\n",
      "[INFO] Epoch: 32 , batch: 142 , training loss: 3.771130\n",
      "[INFO] Epoch: 32 , batch: 143 , training loss: 3.904612\n",
      "[INFO] Epoch: 32 , batch: 144 , training loss: 3.743226\n",
      "[INFO] Epoch: 32 , batch: 145 , training loss: 3.816066\n",
      "[INFO] Epoch: 32 , batch: 146 , training loss: 4.022234\n",
      "[INFO] Epoch: 32 , batch: 147 , training loss: 3.684667\n",
      "[INFO] Epoch: 32 , batch: 148 , training loss: 3.652426\n",
      "[INFO] Epoch: 32 , batch: 149 , training loss: 3.727947\n",
      "[INFO] Epoch: 32 , batch: 150 , training loss: 4.010159\n",
      "[INFO] Epoch: 32 , batch: 151 , training loss: 3.856850\n",
      "[INFO] Epoch: 32 , batch: 152 , training loss: 3.846148\n",
      "[INFO] Epoch: 32 , batch: 153 , training loss: 3.882442\n",
      "[INFO] Epoch: 32 , batch: 154 , training loss: 3.978285\n",
      "[INFO] Epoch: 32 , batch: 155 , training loss: 4.165677\n",
      "[INFO] Epoch: 32 , batch: 156 , training loss: 3.927242\n",
      "[INFO] Epoch: 32 , batch: 157 , training loss: 3.884329\n",
      "[INFO] Epoch: 32 , batch: 158 , training loss: 4.013624\n",
      "[INFO] Epoch: 32 , batch: 159 , training loss: 3.977966\n",
      "[INFO] Epoch: 32 , batch: 160 , training loss: 4.167306\n",
      "[INFO] Epoch: 32 , batch: 161 , training loss: 4.247294\n",
      "[INFO] Epoch: 32 , batch: 162 , training loss: 4.213327\n",
      "[INFO] Epoch: 32 , batch: 163 , training loss: 4.414050\n",
      "[INFO] Epoch: 32 , batch: 164 , training loss: 4.336862\n",
      "[INFO] Epoch: 32 , batch: 165 , training loss: 4.273823\n",
      "[INFO] Epoch: 32 , batch: 166 , training loss: 4.145115\n",
      "[INFO] Epoch: 32 , batch: 167 , training loss: 4.269647\n",
      "[INFO] Epoch: 32 , batch: 168 , training loss: 3.902162\n",
      "[INFO] Epoch: 32 , batch: 169 , training loss: 3.876996\n",
      "[INFO] Epoch: 32 , batch: 170 , training loss: 4.032838\n",
      "[INFO] Epoch: 32 , batch: 171 , training loss: 3.487907\n",
      "[INFO] Epoch: 32 , batch: 172 , training loss: 3.727461\n",
      "[INFO] Epoch: 32 , batch: 173 , training loss: 4.065194\n",
      "[INFO] Epoch: 32 , batch: 174 , training loss: 4.469825\n",
      "[INFO] Epoch: 32 , batch: 175 , training loss: 4.747728\n",
      "[INFO] Epoch: 32 , batch: 176 , training loss: 4.500579\n",
      "[INFO] Epoch: 32 , batch: 177 , training loss: 4.099411\n",
      "[INFO] Epoch: 32 , batch: 178 , training loss: 4.099189\n",
      "[INFO] Epoch: 32 , batch: 179 , training loss: 4.137926\n",
      "[INFO] Epoch: 32 , batch: 180 , training loss: 4.093653\n",
      "[INFO] Epoch: 32 , batch: 181 , training loss: 4.390490\n",
      "[INFO] Epoch: 32 , batch: 182 , training loss: 4.302240\n",
      "[INFO] Epoch: 32 , batch: 183 , training loss: 4.275468\n",
      "[INFO] Epoch: 32 , batch: 184 , training loss: 4.174031\n",
      "[INFO] Epoch: 32 , batch: 185 , training loss: 4.133943\n",
      "[INFO] Epoch: 32 , batch: 186 , training loss: 4.276961\n",
      "[INFO] Epoch: 32 , batch: 187 , training loss: 4.370316\n",
      "[INFO] Epoch: 32 , batch: 188 , training loss: 4.369079\n",
      "[INFO] Epoch: 32 , batch: 189 , training loss: 4.293281\n",
      "[INFO] Epoch: 32 , batch: 190 , training loss: 4.326063\n",
      "[INFO] Epoch: 32 , batch: 191 , training loss: 4.436146\n",
      "[INFO] Epoch: 32 , batch: 192 , training loss: 4.266968\n",
      "[INFO] Epoch: 32 , batch: 193 , training loss: 4.357239\n",
      "[INFO] Epoch: 32 , batch: 194 , training loss: 4.312353\n",
      "[INFO] Epoch: 32 , batch: 195 , training loss: 4.234977\n",
      "[INFO] Epoch: 32 , batch: 196 , training loss: 4.088747\n",
      "[INFO] Epoch: 32 , batch: 197 , training loss: 4.179019\n",
      "[INFO] Epoch: 32 , batch: 198 , training loss: 4.093910\n",
      "[INFO] Epoch: 32 , batch: 199 , training loss: 4.226511\n",
      "[INFO] Epoch: 32 , batch: 200 , training loss: 4.125059\n",
      "[INFO] Epoch: 32 , batch: 201 , training loss: 4.030308\n",
      "[INFO] Epoch: 32 , batch: 202 , training loss: 4.052830\n",
      "[INFO] Epoch: 32 , batch: 203 , training loss: 4.162724\n",
      "[INFO] Epoch: 32 , batch: 204 , training loss: 4.265644\n",
      "[INFO] Epoch: 32 , batch: 205 , training loss: 3.857354\n",
      "[INFO] Epoch: 32 , batch: 206 , training loss: 3.783724\n",
      "[INFO] Epoch: 32 , batch: 207 , training loss: 3.777392\n",
      "[INFO] Epoch: 32 , batch: 208 , training loss: 4.091403\n",
      "[INFO] Epoch: 32 , batch: 209 , training loss: 4.056272\n",
      "[INFO] Epoch: 32 , batch: 210 , training loss: 4.075113\n",
      "[INFO] Epoch: 32 , batch: 211 , training loss: 4.068764\n",
      "[INFO] Epoch: 32 , batch: 212 , training loss: 4.169745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 32 , batch: 213 , training loss: 4.126152\n",
      "[INFO] Epoch: 32 , batch: 214 , training loss: 4.188742\n",
      "[INFO] Epoch: 32 , batch: 215 , training loss: 4.385780\n",
      "[INFO] Epoch: 32 , batch: 216 , training loss: 4.144672\n",
      "[INFO] Epoch: 32 , batch: 217 , training loss: 4.045425\n",
      "[INFO] Epoch: 32 , batch: 218 , training loss: 4.039977\n",
      "[INFO] Epoch: 32 , batch: 219 , training loss: 4.165647\n",
      "[INFO] Epoch: 32 , batch: 220 , training loss: 3.987085\n",
      "[INFO] Epoch: 32 , batch: 221 , training loss: 4.006928\n",
      "[INFO] Epoch: 32 , batch: 222 , training loss: 4.122513\n",
      "[INFO] Epoch: 32 , batch: 223 , training loss: 4.227894\n",
      "[INFO] Epoch: 32 , batch: 224 , training loss: 4.278648\n",
      "[INFO] Epoch: 32 , batch: 225 , training loss: 4.168919\n",
      "[INFO] Epoch: 32 , batch: 226 , training loss: 4.299278\n",
      "[INFO] Epoch: 32 , batch: 227 , training loss: 4.274763\n",
      "[INFO] Epoch: 32 , batch: 228 , training loss: 4.269151\n",
      "[INFO] Epoch: 32 , batch: 229 , training loss: 4.159721\n",
      "[INFO] Epoch: 32 , batch: 230 , training loss: 4.008792\n",
      "[INFO] Epoch: 32 , batch: 231 , training loss: 3.870021\n",
      "[INFO] Epoch: 32 , batch: 232 , training loss: 4.021163\n",
      "[INFO] Epoch: 32 , batch: 233 , training loss: 4.051954\n",
      "[INFO] Epoch: 32 , batch: 234 , training loss: 3.725164\n",
      "[INFO] Epoch: 32 , batch: 235 , training loss: 3.846919\n",
      "[INFO] Epoch: 32 , batch: 236 , training loss: 3.956204\n",
      "[INFO] Epoch: 32 , batch: 237 , training loss: 4.179590\n",
      "[INFO] Epoch: 32 , batch: 238 , training loss: 3.953944\n",
      "[INFO] Epoch: 32 , batch: 239 , training loss: 3.979995\n",
      "[INFO] Epoch: 32 , batch: 240 , training loss: 4.041420\n",
      "[INFO] Epoch: 32 , batch: 241 , training loss: 3.827420\n",
      "[INFO] Epoch: 32 , batch: 242 , training loss: 3.855865\n",
      "[INFO] Epoch: 32 , batch: 243 , training loss: 4.128152\n",
      "[INFO] Epoch: 32 , batch: 244 , training loss: 4.098995\n",
      "[INFO] Epoch: 32 , batch: 245 , training loss: 4.052008\n",
      "[INFO] Epoch: 32 , batch: 246 , training loss: 3.753258\n",
      "[INFO] Epoch: 32 , batch: 247 , training loss: 3.911270\n",
      "[INFO] Epoch: 32 , batch: 248 , training loss: 3.997967\n",
      "[INFO] Epoch: 32 , batch: 249 , training loss: 3.982935\n",
      "[INFO] Epoch: 32 , batch: 250 , training loss: 3.795996\n",
      "[INFO] Epoch: 32 , batch: 251 , training loss: 4.220281\n",
      "[INFO] Epoch: 32 , batch: 252 , training loss: 3.936161\n",
      "[INFO] Epoch: 32 , batch: 253 , training loss: 3.848478\n",
      "[INFO] Epoch: 32 , batch: 254 , training loss: 4.115071\n",
      "[INFO] Epoch: 32 , batch: 255 , training loss: 4.088512\n",
      "[INFO] Epoch: 32 , batch: 256 , training loss: 4.075904\n",
      "[INFO] Epoch: 32 , batch: 257 , training loss: 4.257061\n",
      "[INFO] Epoch: 32 , batch: 258 , training loss: 4.260559\n",
      "[INFO] Epoch: 32 , batch: 259 , training loss: 4.301978\n",
      "[INFO] Epoch: 32 , batch: 260 , training loss: 4.073659\n",
      "[INFO] Epoch: 32 , batch: 261 , training loss: 4.233520\n",
      "[INFO] Epoch: 32 , batch: 262 , training loss: 4.364087\n",
      "[INFO] Epoch: 32 , batch: 263 , training loss: 4.566409\n",
      "[INFO] Epoch: 32 , batch: 264 , training loss: 3.934497\n",
      "[INFO] Epoch: 32 , batch: 265 , training loss: 4.022404\n",
      "[INFO] Epoch: 32 , batch: 266 , training loss: 4.449035\n",
      "[INFO] Epoch: 32 , batch: 267 , training loss: 4.159178\n",
      "[INFO] Epoch: 32 , batch: 268 , training loss: 4.099885\n",
      "[INFO] Epoch: 32 , batch: 269 , training loss: 4.101867\n",
      "[INFO] Epoch: 32 , batch: 270 , training loss: 4.088548\n",
      "[INFO] Epoch: 32 , batch: 271 , training loss: 4.142506\n",
      "[INFO] Epoch: 32 , batch: 272 , training loss: 4.122215\n",
      "[INFO] Epoch: 32 , batch: 273 , training loss: 4.135835\n",
      "[INFO] Epoch: 32 , batch: 274 , training loss: 4.223609\n",
      "[INFO] Epoch: 32 , batch: 275 , training loss: 4.107205\n",
      "[INFO] Epoch: 32 , batch: 276 , training loss: 4.161083\n",
      "[INFO] Epoch: 32 , batch: 277 , training loss: 4.330256\n",
      "[INFO] Epoch: 32 , batch: 278 , training loss: 3.980226\n",
      "[INFO] Epoch: 32 , batch: 279 , training loss: 3.998417\n",
      "[INFO] Epoch: 32 , batch: 280 , training loss: 3.972411\n",
      "[INFO] Epoch: 32 , batch: 281 , training loss: 4.119753\n",
      "[INFO] Epoch: 32 , batch: 282 , training loss: 4.002825\n",
      "[INFO] Epoch: 32 , batch: 283 , training loss: 4.013422\n",
      "[INFO] Epoch: 32 , batch: 284 , training loss: 4.064843\n",
      "[INFO] Epoch: 32 , batch: 285 , training loss: 3.990853\n",
      "[INFO] Epoch: 32 , batch: 286 , training loss: 4.009529\n",
      "[INFO] Epoch: 32 , batch: 287 , training loss: 3.960979\n",
      "[INFO] Epoch: 32 , batch: 288 , training loss: 3.901447\n",
      "[INFO] Epoch: 32 , batch: 289 , training loss: 3.974108\n",
      "[INFO] Epoch: 32 , batch: 290 , training loss: 3.765017\n",
      "[INFO] Epoch: 32 , batch: 291 , training loss: 3.749533\n",
      "[INFO] Epoch: 32 , batch: 292 , training loss: 3.858605\n",
      "[INFO] Epoch: 32 , batch: 293 , training loss: 3.779894\n",
      "[INFO] Epoch: 32 , batch: 294 , training loss: 4.447234\n",
      "[INFO] Epoch: 32 , batch: 295 , training loss: 4.239924\n",
      "[INFO] Epoch: 32 , batch: 296 , training loss: 4.161955\n",
      "[INFO] Epoch: 32 , batch: 297 , training loss: 4.107143\n",
      "[INFO] Epoch: 32 , batch: 298 , training loss: 3.950012\n",
      "[INFO] Epoch: 32 , batch: 299 , training loss: 3.998094\n",
      "[INFO] Epoch: 32 , batch: 300 , training loss: 3.969871\n",
      "[INFO] Epoch: 32 , batch: 301 , training loss: 3.894595\n",
      "[INFO] Epoch: 32 , batch: 302 , training loss: 4.067583\n",
      "[INFO] Epoch: 32 , batch: 303 , training loss: 4.078735\n",
      "[INFO] Epoch: 32 , batch: 304 , training loss: 4.228808\n",
      "[INFO] Epoch: 32 , batch: 305 , training loss: 4.045638\n",
      "[INFO] Epoch: 32 , batch: 306 , training loss: 4.148743\n",
      "[INFO] Epoch: 32 , batch: 307 , training loss: 4.174391\n",
      "[INFO] Epoch: 32 , batch: 308 , training loss: 3.986905\n",
      "[INFO] Epoch: 32 , batch: 309 , training loss: 3.990637\n",
      "[INFO] Epoch: 32 , batch: 310 , training loss: 3.917864\n",
      "[INFO] Epoch: 32 , batch: 311 , training loss: 3.913398\n",
      "[INFO] Epoch: 32 , batch: 312 , training loss: 3.817890\n",
      "[INFO] Epoch: 32 , batch: 313 , training loss: 3.906186\n",
      "[INFO] Epoch: 32 , batch: 314 , training loss: 3.982866\n",
      "[INFO] Epoch: 32 , batch: 315 , training loss: 4.073708\n",
      "[INFO] Epoch: 32 , batch: 316 , training loss: 4.320117\n",
      "[INFO] Epoch: 32 , batch: 317 , training loss: 4.669840\n",
      "[INFO] Epoch: 32 , batch: 318 , training loss: 4.781127\n",
      "[INFO] Epoch: 32 , batch: 319 , training loss: 4.500927\n",
      "[INFO] Epoch: 32 , batch: 320 , training loss: 4.001636\n",
      "[INFO] Epoch: 32 , batch: 321 , training loss: 3.839906\n",
      "[INFO] Epoch: 32 , batch: 322 , training loss: 3.947922\n",
      "[INFO] Epoch: 32 , batch: 323 , training loss: 3.979804\n",
      "[INFO] Epoch: 32 , batch: 324 , training loss: 3.970366\n",
      "[INFO] Epoch: 32 , batch: 325 , training loss: 4.104493\n",
      "[INFO] Epoch: 32 , batch: 326 , training loss: 4.128435\n",
      "[INFO] Epoch: 32 , batch: 327 , training loss: 4.060291\n",
      "[INFO] Epoch: 32 , batch: 328 , training loss: 4.065604\n",
      "[INFO] Epoch: 32 , batch: 329 , training loss: 3.969538\n",
      "[INFO] Epoch: 32 , batch: 330 , training loss: 3.961149\n",
      "[INFO] Epoch: 32 , batch: 331 , training loss: 4.130716\n",
      "[INFO] Epoch: 32 , batch: 332 , training loss: 3.968295\n",
      "[INFO] Epoch: 32 , batch: 333 , training loss: 3.933521\n",
      "[INFO] Epoch: 32 , batch: 334 , training loss: 3.961835\n",
      "[INFO] Epoch: 32 , batch: 335 , training loss: 4.073707\n",
      "[INFO] Epoch: 32 , batch: 336 , training loss: 4.086151\n",
      "[INFO] Epoch: 32 , batch: 337 , training loss: 4.125227\n",
      "[INFO] Epoch: 32 , batch: 338 , training loss: 4.351741\n",
      "[INFO] Epoch: 32 , batch: 339 , training loss: 4.182748\n",
      "[INFO] Epoch: 32 , batch: 340 , training loss: 4.341563\n",
      "[INFO] Epoch: 32 , batch: 341 , training loss: 4.093410\n",
      "[INFO] Epoch: 32 , batch: 342 , training loss: 3.895288\n",
      "[INFO] Epoch: 32 , batch: 343 , training loss: 3.971308\n",
      "[INFO] Epoch: 32 , batch: 344 , training loss: 3.842136\n",
      "[INFO] Epoch: 32 , batch: 345 , training loss: 3.968896\n",
      "[INFO] Epoch: 32 , batch: 346 , training loss: 4.008345\n",
      "[INFO] Epoch: 32 , batch: 347 , training loss: 3.919380\n",
      "[INFO] Epoch: 32 , batch: 348 , training loss: 4.017764\n",
      "[INFO] Epoch: 32 , batch: 349 , training loss: 4.118091\n",
      "[INFO] Epoch: 32 , batch: 350 , training loss: 3.968567\n",
      "[INFO] Epoch: 32 , batch: 351 , training loss: 4.047071\n",
      "[INFO] Epoch: 32 , batch: 352 , training loss: 4.063818\n",
      "[INFO] Epoch: 32 , batch: 353 , training loss: 4.056744\n",
      "[INFO] Epoch: 32 , batch: 354 , training loss: 4.143610\n",
      "[INFO] Epoch: 32 , batch: 355 , training loss: 4.122658\n",
      "[INFO] Epoch: 32 , batch: 356 , training loss: 3.992033\n",
      "[INFO] Epoch: 32 , batch: 357 , training loss: 4.066936\n",
      "[INFO] Epoch: 32 , batch: 358 , training loss: 3.982950\n",
      "[INFO] Epoch: 32 , batch: 359 , training loss: 3.988169\n",
      "[INFO] Epoch: 32 , batch: 360 , training loss: 4.086006\n",
      "[INFO] Epoch: 32 , batch: 361 , training loss: 4.051009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 32 , batch: 362 , training loss: 4.152466\n",
      "[INFO] Epoch: 32 , batch: 363 , training loss: 4.032396\n",
      "[INFO] Epoch: 32 , batch: 364 , training loss: 4.106094\n",
      "[INFO] Epoch: 32 , batch: 365 , training loss: 3.991474\n",
      "[INFO] Epoch: 32 , batch: 366 , training loss: 4.113510\n",
      "[INFO] Epoch: 32 , batch: 367 , training loss: 4.166070\n",
      "[INFO] Epoch: 32 , batch: 368 , training loss: 4.550293\n",
      "[INFO] Epoch: 32 , batch: 369 , training loss: 4.245582\n",
      "[INFO] Epoch: 32 , batch: 370 , training loss: 3.995403\n",
      "[INFO] Epoch: 32 , batch: 371 , training loss: 4.475663\n",
      "[INFO] Epoch: 32 , batch: 372 , training loss: 4.692660\n",
      "[INFO] Epoch: 32 , batch: 373 , training loss: 4.734065\n",
      "[INFO] Epoch: 32 , batch: 374 , training loss: 4.853612\n",
      "[INFO] Epoch: 32 , batch: 375 , training loss: 4.810639\n",
      "[INFO] Epoch: 32 , batch: 376 , training loss: 4.726190\n",
      "[INFO] Epoch: 32 , batch: 377 , training loss: 4.452930\n",
      "[INFO] Epoch: 32 , batch: 378 , training loss: 4.569161\n",
      "[INFO] Epoch: 32 , batch: 379 , training loss: 4.525328\n",
      "[INFO] Epoch: 32 , batch: 380 , training loss: 4.695134\n",
      "[INFO] Epoch: 32 , batch: 381 , training loss: 4.395792\n",
      "[INFO] Epoch: 32 , batch: 382 , training loss: 4.672681\n",
      "[INFO] Epoch: 32 , batch: 383 , training loss: 4.712880\n",
      "[INFO] Epoch: 32 , batch: 384 , training loss: 4.707438\n",
      "[INFO] Epoch: 32 , batch: 385 , training loss: 4.348873\n",
      "[INFO] Epoch: 32 , batch: 386 , training loss: 4.616710\n",
      "[INFO] Epoch: 32 , batch: 387 , training loss: 4.573485\n",
      "[INFO] Epoch: 32 , batch: 388 , training loss: 4.400004\n",
      "[INFO] Epoch: 32 , batch: 389 , training loss: 4.199258\n",
      "[INFO] Epoch: 32 , batch: 390 , training loss: 4.212779\n",
      "[INFO] Epoch: 32 , batch: 391 , training loss: 4.243494\n",
      "[INFO] Epoch: 32 , batch: 392 , training loss: 4.602587\n",
      "[INFO] Epoch: 32 , batch: 393 , training loss: 4.493309\n",
      "[INFO] Epoch: 32 , batch: 394 , training loss: 4.592290\n",
      "[INFO] Epoch: 32 , batch: 395 , training loss: 4.407257\n",
      "[INFO] Epoch: 32 , batch: 396 , training loss: 4.230197\n",
      "[INFO] Epoch: 32 , batch: 397 , training loss: 4.374496\n",
      "[INFO] Epoch: 32 , batch: 398 , training loss: 4.235400\n",
      "[INFO] Epoch: 32 , batch: 399 , training loss: 4.312329\n",
      "[INFO] Epoch: 32 , batch: 400 , training loss: 4.272589\n",
      "[INFO] Epoch: 32 , batch: 401 , training loss: 4.710680\n",
      "[INFO] Epoch: 32 , batch: 402 , training loss: 4.430107\n",
      "[INFO] Epoch: 32 , batch: 403 , training loss: 4.256368\n",
      "[INFO] Epoch: 32 , batch: 404 , training loss: 4.441294\n",
      "[INFO] Epoch: 32 , batch: 405 , training loss: 4.493232\n",
      "[INFO] Epoch: 32 , batch: 406 , training loss: 4.384185\n",
      "[INFO] Epoch: 32 , batch: 407 , training loss: 4.430681\n",
      "[INFO] Epoch: 32 , batch: 408 , training loss: 4.382409\n",
      "[INFO] Epoch: 32 , batch: 409 , training loss: 4.422345\n",
      "[INFO] Epoch: 32 , batch: 410 , training loss: 4.480305\n",
      "[INFO] Epoch: 32 , batch: 411 , training loss: 4.639408\n",
      "[INFO] Epoch: 32 , batch: 412 , training loss: 4.484642\n",
      "[INFO] Epoch: 32 , batch: 413 , training loss: 4.334572\n",
      "[INFO] Epoch: 32 , batch: 414 , training loss: 4.382714\n",
      "[INFO] Epoch: 32 , batch: 415 , training loss: 4.401614\n",
      "[INFO] Epoch: 32 , batch: 416 , training loss: 4.487158\n",
      "[INFO] Epoch: 32 , batch: 417 , training loss: 4.387758\n",
      "[INFO] Epoch: 32 , batch: 418 , training loss: 4.464970\n",
      "[INFO] Epoch: 32 , batch: 419 , training loss: 4.426441\n",
      "[INFO] Epoch: 32 , batch: 420 , training loss: 4.399846\n",
      "[INFO] Epoch: 32 , batch: 421 , training loss: 4.372921\n",
      "[INFO] Epoch: 32 , batch: 422 , training loss: 4.207905\n",
      "[INFO] Epoch: 32 , batch: 423 , training loss: 4.436511\n",
      "[INFO] Epoch: 32 , batch: 424 , training loss: 4.627625\n",
      "[INFO] Epoch: 32 , batch: 425 , training loss: 4.465830\n",
      "[INFO] Epoch: 32 , batch: 426 , training loss: 4.209373\n",
      "[INFO] Epoch: 32 , batch: 427 , training loss: 4.471448\n",
      "[INFO] Epoch: 32 , batch: 428 , training loss: 4.325219\n",
      "[INFO] Epoch: 32 , batch: 429 , training loss: 4.232329\n",
      "[INFO] Epoch: 32 , batch: 430 , training loss: 4.454315\n",
      "[INFO] Epoch: 32 , batch: 431 , training loss: 4.061850\n",
      "[INFO] Epoch: 32 , batch: 432 , training loss: 4.123893\n",
      "[INFO] Epoch: 32 , batch: 433 , training loss: 4.156030\n",
      "[INFO] Epoch: 32 , batch: 434 , training loss: 4.029290\n",
      "[INFO] Epoch: 32 , batch: 435 , training loss: 4.381543\n",
      "[INFO] Epoch: 32 , batch: 436 , training loss: 4.441983\n",
      "[INFO] Epoch: 32 , batch: 437 , training loss: 4.218884\n",
      "[INFO] Epoch: 32 , batch: 438 , training loss: 4.070925\n",
      "[INFO] Epoch: 32 , batch: 439 , training loss: 4.316197\n",
      "[INFO] Epoch: 32 , batch: 440 , training loss: 4.426608\n",
      "[INFO] Epoch: 32 , batch: 441 , training loss: 4.516477\n",
      "[INFO] Epoch: 32 , batch: 442 , training loss: 4.284178\n",
      "[INFO] Epoch: 32 , batch: 443 , training loss: 4.477857\n",
      "[INFO] Epoch: 32 , batch: 444 , training loss: 4.086696\n",
      "[INFO] Epoch: 32 , batch: 445 , training loss: 3.991436\n",
      "[INFO] Epoch: 32 , batch: 446 , training loss: 3.911933\n",
      "[INFO] Epoch: 32 , batch: 447 , training loss: 4.116287\n",
      "[INFO] Epoch: 32 , batch: 448 , training loss: 4.234406\n",
      "[INFO] Epoch: 32 , batch: 449 , training loss: 4.595481\n",
      "[INFO] Epoch: 32 , batch: 450 , training loss: 4.680076\n",
      "[INFO] Epoch: 32 , batch: 451 , training loss: 4.566996\n",
      "[INFO] Epoch: 32 , batch: 452 , training loss: 4.387595\n",
      "[INFO] Epoch: 32 , batch: 453 , training loss: 4.165538\n",
      "[INFO] Epoch: 32 , batch: 454 , training loss: 4.309562\n",
      "[INFO] Epoch: 32 , batch: 455 , training loss: 4.371231\n",
      "[INFO] Epoch: 32 , batch: 456 , training loss: 4.348588\n",
      "[INFO] Epoch: 32 , batch: 457 , training loss: 4.429056\n",
      "[INFO] Epoch: 32 , batch: 458 , training loss: 4.178354\n",
      "[INFO] Epoch: 32 , batch: 459 , training loss: 4.141042\n",
      "[INFO] Epoch: 32 , batch: 460 , training loss: 4.262112\n",
      "[INFO] Epoch: 32 , batch: 461 , training loss: 4.236583\n",
      "[INFO] Epoch: 32 , batch: 462 , training loss: 4.281126\n",
      "[INFO] Epoch: 32 , batch: 463 , training loss: 4.194405\n",
      "[INFO] Epoch: 32 , batch: 464 , training loss: 4.356445\n",
      "[INFO] Epoch: 32 , batch: 465 , training loss: 4.326205\n",
      "[INFO] Epoch: 32 , batch: 466 , training loss: 4.436446\n",
      "[INFO] Epoch: 32 , batch: 467 , training loss: 4.377840\n",
      "[INFO] Epoch: 32 , batch: 468 , training loss: 4.342517\n",
      "[INFO] Epoch: 32 , batch: 469 , training loss: 4.365903\n",
      "[INFO] Epoch: 32 , batch: 470 , training loss: 4.188647\n",
      "[INFO] Epoch: 32 , batch: 471 , training loss: 4.304567\n",
      "[INFO] Epoch: 32 , batch: 472 , training loss: 4.363068\n",
      "[INFO] Epoch: 32 , batch: 473 , training loss: 4.267560\n",
      "[INFO] Epoch: 32 , batch: 474 , training loss: 4.050439\n",
      "[INFO] Epoch: 32 , batch: 475 , training loss: 3.937595\n",
      "[INFO] Epoch: 32 , batch: 476 , training loss: 4.343622\n",
      "[INFO] Epoch: 32 , batch: 477 , training loss: 4.446078\n",
      "[INFO] Epoch: 32 , batch: 478 , training loss: 4.438618\n",
      "[INFO] Epoch: 32 , batch: 479 , training loss: 4.408229\n",
      "[INFO] Epoch: 32 , batch: 480 , training loss: 4.548329\n",
      "[INFO] Epoch: 32 , batch: 481 , training loss: 4.426425\n",
      "[INFO] Epoch: 32 , batch: 482 , training loss: 4.551426\n",
      "[INFO] Epoch: 32 , batch: 483 , training loss: 4.391076\n",
      "[INFO] Epoch: 32 , batch: 484 , training loss: 4.187283\n",
      "[INFO] Epoch: 32 , batch: 485 , training loss: 4.251189\n",
      "[INFO] Epoch: 32 , batch: 486 , training loss: 4.175895\n",
      "[INFO] Epoch: 32 , batch: 487 , training loss: 4.160514\n",
      "[INFO] Epoch: 32 , batch: 488 , training loss: 4.326070\n",
      "[INFO] Epoch: 32 , batch: 489 , training loss: 4.237036\n",
      "[INFO] Epoch: 32 , batch: 490 , training loss: 4.296388\n",
      "[INFO] Epoch: 32 , batch: 491 , training loss: 4.219900\n",
      "[INFO] Epoch: 32 , batch: 492 , training loss: 4.206389\n",
      "[INFO] Epoch: 32 , batch: 493 , training loss: 4.357111\n",
      "[INFO] Epoch: 32 , batch: 494 , training loss: 4.254634\n",
      "[INFO] Epoch: 32 , batch: 495 , training loss: 4.443177\n",
      "[INFO] Epoch: 32 , batch: 496 , training loss: 4.304286\n",
      "[INFO] Epoch: 32 , batch: 497 , training loss: 4.356810\n",
      "[INFO] Epoch: 32 , batch: 498 , training loss: 4.316682\n",
      "[INFO] Epoch: 32 , batch: 499 , training loss: 4.407632\n",
      "[INFO] Epoch: 32 , batch: 500 , training loss: 4.507849\n",
      "[INFO] Epoch: 32 , batch: 501 , training loss: 4.866339\n",
      "[INFO] Epoch: 32 , batch: 502 , training loss: 4.886079\n",
      "[INFO] Epoch: 32 , batch: 503 , training loss: 4.554662\n",
      "[INFO] Epoch: 32 , batch: 504 , training loss: 4.709587\n",
      "[INFO] Epoch: 32 , batch: 505 , training loss: 4.669549\n",
      "[INFO] Epoch: 32 , batch: 506 , training loss: 4.628333\n",
      "[INFO] Epoch: 32 , batch: 507 , training loss: 4.713086\n",
      "[INFO] Epoch: 32 , batch: 508 , training loss: 4.601781\n",
      "[INFO] Epoch: 32 , batch: 509 , training loss: 4.415440\n",
      "[INFO] Epoch: 32 , batch: 510 , training loss: 4.496112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 32 , batch: 511 , training loss: 4.427435\n",
      "[INFO] Epoch: 32 , batch: 512 , training loss: 4.498258\n",
      "[INFO] Epoch: 32 , batch: 513 , training loss: 4.786511\n",
      "[INFO] Epoch: 32 , batch: 514 , training loss: 4.395192\n",
      "[INFO] Epoch: 32 , batch: 515 , training loss: 4.663582\n",
      "[INFO] Epoch: 32 , batch: 516 , training loss: 4.452941\n",
      "[INFO] Epoch: 32 , batch: 517 , training loss: 4.420442\n",
      "[INFO] Epoch: 32 , batch: 518 , training loss: 4.383073\n",
      "[INFO] Epoch: 32 , batch: 519 , training loss: 4.223521\n",
      "[INFO] Epoch: 32 , batch: 520 , training loss: 4.466412\n",
      "[INFO] Epoch: 32 , batch: 521 , training loss: 4.429401\n",
      "[INFO] Epoch: 32 , batch: 522 , training loss: 4.525654\n",
      "[INFO] Epoch: 32 , batch: 523 , training loss: 4.430895\n",
      "[INFO] Epoch: 32 , batch: 524 , training loss: 4.719809\n",
      "[INFO] Epoch: 32 , batch: 525 , training loss: 4.586673\n",
      "[INFO] Epoch: 32 , batch: 526 , training loss: 4.384715\n",
      "[INFO] Epoch: 32 , batch: 527 , training loss: 4.448321\n",
      "[INFO] Epoch: 32 , batch: 528 , training loss: 4.454410\n",
      "[INFO] Epoch: 32 , batch: 529 , training loss: 4.425139\n",
      "[INFO] Epoch: 32 , batch: 530 , training loss: 4.271036\n",
      "[INFO] Epoch: 32 , batch: 531 , training loss: 4.421292\n",
      "[INFO] Epoch: 32 , batch: 532 , training loss: 4.320096\n",
      "[INFO] Epoch: 32 , batch: 533 , training loss: 4.451633\n",
      "[INFO] Epoch: 32 , batch: 534 , training loss: 4.454809\n",
      "[INFO] Epoch: 32 , batch: 535 , training loss: 4.463606\n",
      "[INFO] Epoch: 32 , batch: 536 , training loss: 4.309376\n",
      "[INFO] Epoch: 32 , batch: 537 , training loss: 4.289883\n",
      "[INFO] Epoch: 32 , batch: 538 , training loss: 4.376723\n",
      "[INFO] Epoch: 32 , batch: 539 , training loss: 4.483330\n",
      "[INFO] Epoch: 32 , batch: 540 , training loss: 5.008226\n",
      "[INFO] Epoch: 32 , batch: 541 , training loss: 4.825347\n",
      "[INFO] Epoch: 32 , batch: 542 , training loss: 4.699232\n",
      "[INFO] Epoch: 33 , batch: 0 , training loss: 3.722772\n",
      "[INFO] Epoch: 33 , batch: 1 , training loss: 3.536094\n",
      "[INFO] Epoch: 33 , batch: 2 , training loss: 3.697353\n",
      "[INFO] Epoch: 33 , batch: 3 , training loss: 3.564962\n",
      "[INFO] Epoch: 33 , batch: 4 , training loss: 3.893811\n",
      "[INFO] Epoch: 33 , batch: 5 , training loss: 3.586332\n",
      "[INFO] Epoch: 33 , batch: 6 , training loss: 3.892845\n",
      "[INFO] Epoch: 33 , batch: 7 , training loss: 3.811036\n",
      "[INFO] Epoch: 33 , batch: 8 , training loss: 3.509667\n",
      "[INFO] Epoch: 33 , batch: 9 , training loss: 3.778353\n",
      "[INFO] Epoch: 33 , batch: 10 , training loss: 3.723825\n",
      "[INFO] Epoch: 33 , batch: 11 , training loss: 3.661340\n",
      "[INFO] Epoch: 33 , batch: 12 , training loss: 3.581129\n",
      "[INFO] Epoch: 33 , batch: 13 , training loss: 3.595113\n",
      "[INFO] Epoch: 33 , batch: 14 , training loss: 3.512880\n",
      "[INFO] Epoch: 33 , batch: 15 , training loss: 3.725752\n",
      "[INFO] Epoch: 33 , batch: 16 , training loss: 3.595190\n",
      "[INFO] Epoch: 33 , batch: 17 , training loss: 3.718159\n",
      "[INFO] Epoch: 33 , batch: 18 , training loss: 3.633306\n",
      "[INFO] Epoch: 33 , batch: 19 , training loss: 3.441016\n",
      "[INFO] Epoch: 33 , batch: 20 , training loss: 3.358519\n",
      "[INFO] Epoch: 33 , batch: 21 , training loss: 3.519152\n",
      "[INFO] Epoch: 33 , batch: 22 , training loss: 3.428114\n",
      "[INFO] Epoch: 33 , batch: 23 , training loss: 3.620948\n",
      "[INFO] Epoch: 33 , batch: 24 , training loss: 3.452411\n",
      "[INFO] Epoch: 33 , batch: 25 , training loss: 3.581017\n",
      "[INFO] Epoch: 33 , batch: 26 , training loss: 3.488875\n",
      "[INFO] Epoch: 33 , batch: 27 , training loss: 3.412792\n",
      "[INFO] Epoch: 33 , batch: 28 , training loss: 3.613652\n",
      "[INFO] Epoch: 33 , batch: 29 , training loss: 3.449072\n",
      "[INFO] Epoch: 33 , batch: 30 , training loss: 3.466921\n",
      "[INFO] Epoch: 33 , batch: 31 , training loss: 3.587055\n",
      "[INFO] Epoch: 33 , batch: 32 , training loss: 3.539908\n",
      "[INFO] Epoch: 33 , batch: 33 , training loss: 3.586995\n",
      "[INFO] Epoch: 33 , batch: 34 , training loss: 3.549939\n",
      "[INFO] Epoch: 33 , batch: 35 , training loss: 3.483103\n",
      "[INFO] Epoch: 33 , batch: 36 , training loss: 3.613902\n",
      "[INFO] Epoch: 33 , batch: 37 , training loss: 3.445903\n",
      "[INFO] Epoch: 33 , batch: 38 , training loss: 3.561144\n",
      "[INFO] Epoch: 33 , batch: 39 , training loss: 3.328692\n",
      "[INFO] Epoch: 33 , batch: 40 , training loss: 3.577706\n",
      "[INFO] Epoch: 33 , batch: 41 , training loss: 3.482493\n",
      "[INFO] Epoch: 33 , batch: 42 , training loss: 3.970161\n",
      "[INFO] Epoch: 33 , batch: 43 , training loss: 3.688811\n",
      "[INFO] Epoch: 33 , batch: 44 , training loss: 4.037572\n",
      "[INFO] Epoch: 33 , batch: 45 , training loss: 3.973672\n",
      "[INFO] Epoch: 33 , batch: 46 , training loss: 3.933046\n",
      "[INFO] Epoch: 33 , batch: 47 , training loss: 3.582749\n",
      "[INFO] Epoch: 33 , batch: 48 , training loss: 3.549149\n",
      "[INFO] Epoch: 33 , batch: 49 , training loss: 3.775625\n",
      "[INFO] Epoch: 33 , batch: 50 , training loss: 3.556843\n",
      "[INFO] Epoch: 33 , batch: 51 , training loss: 3.800917\n",
      "[INFO] Epoch: 33 , batch: 52 , training loss: 3.631910\n",
      "[INFO] Epoch: 33 , batch: 53 , training loss: 3.756063\n",
      "[INFO] Epoch: 33 , batch: 54 , training loss: 3.776065\n",
      "[INFO] Epoch: 33 , batch: 55 , training loss: 3.866315\n",
      "[INFO] Epoch: 33 , batch: 56 , training loss: 3.672645\n",
      "[INFO] Epoch: 33 , batch: 57 , training loss: 3.584548\n",
      "[INFO] Epoch: 33 , batch: 58 , training loss: 3.667293\n",
      "[INFO] Epoch: 33 , batch: 59 , training loss: 3.717768\n",
      "[INFO] Epoch: 33 , batch: 60 , training loss: 3.696007\n",
      "[INFO] Epoch: 33 , batch: 61 , training loss: 3.745704\n",
      "[INFO] Epoch: 33 , batch: 62 , training loss: 3.624571\n",
      "[INFO] Epoch: 33 , batch: 63 , training loss: 3.832170\n",
      "[INFO] Epoch: 33 , batch: 64 , training loss: 4.033922\n",
      "[INFO] Epoch: 33 , batch: 65 , training loss: 3.708333\n",
      "[INFO] Epoch: 33 , batch: 66 , training loss: 3.579215\n",
      "[INFO] Epoch: 33 , batch: 67 , training loss: 3.617903\n",
      "[INFO] Epoch: 33 , batch: 68 , training loss: 3.772711\n",
      "[INFO] Epoch: 33 , batch: 69 , training loss: 3.704855\n",
      "[INFO] Epoch: 33 , batch: 70 , training loss: 3.916430\n",
      "[INFO] Epoch: 33 , batch: 71 , training loss: 3.768275\n",
      "[INFO] Epoch: 33 , batch: 72 , training loss: 3.843110\n",
      "[INFO] Epoch: 33 , batch: 73 , training loss: 3.793657\n",
      "[INFO] Epoch: 33 , batch: 74 , training loss: 3.885923\n",
      "[INFO] Epoch: 33 , batch: 75 , training loss: 3.746694\n",
      "[INFO] Epoch: 33 , batch: 76 , training loss: 3.852140\n",
      "[INFO] Epoch: 33 , batch: 77 , training loss: 3.803981\n",
      "[INFO] Epoch: 33 , batch: 78 , training loss: 3.875428\n",
      "[INFO] Epoch: 33 , batch: 79 , training loss: 3.754832\n",
      "[INFO] Epoch: 33 , batch: 80 , training loss: 3.967385\n",
      "[INFO] Epoch: 33 , batch: 81 , training loss: 3.877686\n",
      "[INFO] Epoch: 33 , batch: 82 , training loss: 3.863302\n",
      "[INFO] Epoch: 33 , batch: 83 , training loss: 3.936491\n",
      "[INFO] Epoch: 33 , batch: 84 , training loss: 3.887465\n",
      "[INFO] Epoch: 33 , batch: 85 , training loss: 3.998636\n",
      "[INFO] Epoch: 33 , batch: 86 , training loss: 3.920267\n",
      "[INFO] Epoch: 33 , batch: 87 , training loss: 3.911043\n",
      "[INFO] Epoch: 33 , batch: 88 , training loss: 4.037588\n",
      "[INFO] Epoch: 33 , batch: 89 , training loss: 3.820850\n",
      "[INFO] Epoch: 33 , batch: 90 , training loss: 3.933476\n",
      "[INFO] Epoch: 33 , batch: 91 , training loss: 3.833999\n",
      "[INFO] Epoch: 33 , batch: 92 , training loss: 3.871864\n",
      "[INFO] Epoch: 33 , batch: 93 , training loss: 3.946242\n",
      "[INFO] Epoch: 33 , batch: 94 , training loss: 4.102430\n",
      "[INFO] Epoch: 33 , batch: 95 , training loss: 3.851460\n",
      "[INFO] Epoch: 33 , batch: 96 , training loss: 3.862482\n",
      "[INFO] Epoch: 33 , batch: 97 , training loss: 3.806138\n",
      "[INFO] Epoch: 33 , batch: 98 , training loss: 3.723470\n",
      "[INFO] Epoch: 33 , batch: 99 , training loss: 3.867210\n",
      "[INFO] Epoch: 33 , batch: 100 , training loss: 3.752870\n",
      "[INFO] Epoch: 33 , batch: 101 , training loss: 3.789987\n",
      "[INFO] Epoch: 33 , batch: 102 , training loss: 3.930191\n",
      "[INFO] Epoch: 33 , batch: 103 , training loss: 3.708392\n",
      "[INFO] Epoch: 33 , batch: 104 , training loss: 3.672437\n",
      "[INFO] Epoch: 33 , batch: 105 , training loss: 3.922074\n",
      "[INFO] Epoch: 33 , batch: 106 , training loss: 3.928900\n",
      "[INFO] Epoch: 33 , batch: 107 , training loss: 3.763286\n",
      "[INFO] Epoch: 33 , batch: 108 , training loss: 3.736358\n",
      "[INFO] Epoch: 33 , batch: 109 , training loss: 3.677837\n",
      "[INFO] Epoch: 33 , batch: 110 , training loss: 3.836851\n",
      "[INFO] Epoch: 33 , batch: 111 , training loss: 3.927891\n",
      "[INFO] Epoch: 33 , batch: 112 , training loss: 3.823311\n",
      "[INFO] Epoch: 33 , batch: 113 , training loss: 3.846053\n",
      "[INFO] Epoch: 33 , batch: 114 , training loss: 3.817132\n",
      "[INFO] Epoch: 33 , batch: 115 , training loss: 3.840798\n",
      "[INFO] Epoch: 33 , batch: 116 , training loss: 3.742603\n",
      "[INFO] Epoch: 33 , batch: 117 , training loss: 3.954676\n",
      "[INFO] Epoch: 33 , batch: 118 , training loss: 3.910425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 33 , batch: 119 , training loss: 4.038016\n",
      "[INFO] Epoch: 33 , batch: 120 , training loss: 4.032959\n",
      "[INFO] Epoch: 33 , batch: 121 , training loss: 3.926364\n",
      "[INFO] Epoch: 33 , batch: 122 , training loss: 3.829093\n",
      "[INFO] Epoch: 33 , batch: 123 , training loss: 3.801893\n",
      "[INFO] Epoch: 33 , batch: 124 , training loss: 3.925070\n",
      "[INFO] Epoch: 33 , batch: 125 , training loss: 3.731564\n",
      "[INFO] Epoch: 33 , batch: 126 , training loss: 3.760999\n",
      "[INFO] Epoch: 33 , batch: 127 , training loss: 3.754057\n",
      "[INFO] Epoch: 33 , batch: 128 , training loss: 3.892486\n",
      "[INFO] Epoch: 33 , batch: 129 , training loss: 3.852489\n",
      "[INFO] Epoch: 33 , batch: 130 , training loss: 3.845422\n",
      "[INFO] Epoch: 33 , batch: 131 , training loss: 3.844744\n",
      "[INFO] Epoch: 33 , batch: 132 , training loss: 3.867831\n",
      "[INFO] Epoch: 33 , batch: 133 , training loss: 3.828863\n",
      "[INFO] Epoch: 33 , batch: 134 , training loss: 3.596459\n",
      "[INFO] Epoch: 33 , batch: 135 , training loss: 3.666399\n",
      "[INFO] Epoch: 33 , batch: 136 , training loss: 3.955953\n",
      "[INFO] Epoch: 33 , batch: 137 , training loss: 3.890911\n",
      "[INFO] Epoch: 33 , batch: 138 , training loss: 3.932052\n",
      "[INFO] Epoch: 33 , batch: 139 , training loss: 4.445488\n",
      "[INFO] Epoch: 33 , batch: 140 , training loss: 4.250369\n",
      "[INFO] Epoch: 33 , batch: 141 , training loss: 4.069539\n",
      "[INFO] Epoch: 33 , batch: 142 , training loss: 3.764191\n",
      "[INFO] Epoch: 33 , batch: 143 , training loss: 3.897101\n",
      "[INFO] Epoch: 33 , batch: 144 , training loss: 3.726896\n",
      "[INFO] Epoch: 33 , batch: 145 , training loss: 3.813587\n",
      "[INFO] Epoch: 33 , batch: 146 , training loss: 4.021648\n",
      "[INFO] Epoch: 33 , batch: 147 , training loss: 3.665444\n",
      "[INFO] Epoch: 33 , batch: 148 , training loss: 3.647654\n",
      "[INFO] Epoch: 33 , batch: 149 , training loss: 3.710710\n",
      "[INFO] Epoch: 33 , batch: 150 , training loss: 4.000938\n",
      "[INFO] Epoch: 33 , batch: 151 , training loss: 3.831138\n",
      "[INFO] Epoch: 33 , batch: 152 , training loss: 3.851788\n",
      "[INFO] Epoch: 33 , batch: 153 , training loss: 3.854602\n",
      "[INFO] Epoch: 33 , batch: 154 , training loss: 3.954782\n",
      "[INFO] Epoch: 33 , batch: 155 , training loss: 4.147740\n",
      "[INFO] Epoch: 33 , batch: 156 , training loss: 3.898310\n",
      "[INFO] Epoch: 33 , batch: 157 , training loss: 3.866758\n",
      "[INFO] Epoch: 33 , batch: 158 , training loss: 3.971871\n",
      "[INFO] Epoch: 33 , batch: 159 , training loss: 3.953517\n",
      "[INFO] Epoch: 33 , batch: 160 , training loss: 4.187914\n",
      "[INFO] Epoch: 33 , batch: 161 , training loss: 4.222659\n",
      "[INFO] Epoch: 33 , batch: 162 , training loss: 4.194499\n",
      "[INFO] Epoch: 33 , batch: 163 , training loss: 4.375881\n",
      "[INFO] Epoch: 33 , batch: 164 , training loss: 4.309303\n",
      "[INFO] Epoch: 33 , batch: 165 , training loss: 4.240259\n",
      "[INFO] Epoch: 33 , batch: 166 , training loss: 4.081942\n",
      "[INFO] Epoch: 33 , batch: 167 , training loss: 4.206409\n",
      "[INFO] Epoch: 33 , batch: 168 , training loss: 3.825595\n",
      "[INFO] Epoch: 33 , batch: 169 , training loss: 3.871417\n",
      "[INFO] Epoch: 33 , batch: 170 , training loss: 4.038276\n",
      "[INFO] Epoch: 33 , batch: 171 , training loss: 3.423903\n",
      "[INFO] Epoch: 33 , batch: 172 , training loss: 3.691818\n",
      "[INFO] Epoch: 33 , batch: 173 , training loss: 4.020250\n",
      "[INFO] Epoch: 33 , batch: 174 , training loss: 4.451432\n",
      "[INFO] Epoch: 33 , batch: 175 , training loss: 4.732909\n",
      "[INFO] Epoch: 33 , batch: 176 , training loss: 4.422695\n",
      "[INFO] Epoch: 33 , batch: 177 , training loss: 4.023649\n",
      "[INFO] Epoch: 33 , batch: 178 , training loss: 4.078880\n",
      "[INFO] Epoch: 33 , batch: 179 , training loss: 4.121711\n",
      "[INFO] Epoch: 33 , batch: 180 , training loss: 4.070826\n",
      "[INFO] Epoch: 33 , batch: 181 , training loss: 4.357006\n",
      "[INFO] Epoch: 33 , batch: 182 , training loss: 4.296313\n",
      "[INFO] Epoch: 33 , batch: 183 , training loss: 4.264622\n",
      "[INFO] Epoch: 33 , batch: 184 , training loss: 4.133561\n",
      "[INFO] Epoch: 33 , batch: 185 , training loss: 4.115434\n",
      "[INFO] Epoch: 33 , batch: 186 , training loss: 4.247726\n",
      "[INFO] Epoch: 33 , batch: 187 , training loss: 4.349547\n",
      "[INFO] Epoch: 33 , batch: 188 , training loss: 4.352248\n",
      "[INFO] Epoch: 33 , batch: 189 , training loss: 4.261594\n",
      "[INFO] Epoch: 33 , batch: 190 , training loss: 4.287288\n",
      "[INFO] Epoch: 33 , batch: 191 , training loss: 4.388200\n",
      "[INFO] Epoch: 33 , batch: 192 , training loss: 4.231103\n",
      "[INFO] Epoch: 33 , batch: 193 , training loss: 4.328837\n",
      "[INFO] Epoch: 33 , batch: 194 , training loss: 4.272761\n",
      "[INFO] Epoch: 33 , batch: 195 , training loss: 4.224220\n",
      "[INFO] Epoch: 33 , batch: 196 , training loss: 4.075180\n",
      "[INFO] Epoch: 33 , batch: 197 , training loss: 4.153440\n",
      "[INFO] Epoch: 33 , batch: 198 , training loss: 4.070284\n",
      "[INFO] Epoch: 33 , batch: 199 , training loss: 4.210842\n",
      "[INFO] Epoch: 33 , batch: 200 , training loss: 4.127028\n",
      "[INFO] Epoch: 33 , batch: 201 , training loss: 4.015445\n",
      "[INFO] Epoch: 33 , batch: 202 , training loss: 4.034184\n",
      "[INFO] Epoch: 33 , batch: 203 , training loss: 4.143730\n",
      "[INFO] Epoch: 33 , batch: 204 , training loss: 4.234572\n",
      "[INFO] Epoch: 33 , batch: 205 , training loss: 3.840738\n",
      "[INFO] Epoch: 33 , batch: 206 , training loss: 3.767415\n",
      "[INFO] Epoch: 33 , batch: 207 , training loss: 3.769552\n",
      "[INFO] Epoch: 33 , batch: 208 , training loss: 4.066063\n",
      "[INFO] Epoch: 33 , batch: 209 , training loss: 4.024918\n",
      "[INFO] Epoch: 33 , batch: 210 , training loss: 4.064022\n",
      "[INFO] Epoch: 33 , batch: 211 , training loss: 4.041039\n",
      "[INFO] Epoch: 33 , batch: 212 , training loss: 4.141714\n",
      "[INFO] Epoch: 33 , batch: 213 , training loss: 4.123027\n",
      "[INFO] Epoch: 33 , batch: 214 , training loss: 4.222025\n",
      "[INFO] Epoch: 33 , batch: 215 , training loss: 4.382896\n",
      "[INFO] Epoch: 33 , batch: 216 , training loss: 4.107157\n",
      "[INFO] Epoch: 33 , batch: 217 , training loss: 4.027521\n",
      "[INFO] Epoch: 33 , batch: 218 , training loss: 4.034847\n",
      "[INFO] Epoch: 33 , batch: 219 , training loss: 4.129090\n",
      "[INFO] Epoch: 33 , batch: 220 , training loss: 3.970387\n",
      "[INFO] Epoch: 33 , batch: 221 , training loss: 3.990019\n",
      "[INFO] Epoch: 33 , batch: 222 , training loss: 4.116520\n",
      "[INFO] Epoch: 33 , batch: 223 , training loss: 4.215295\n",
      "[INFO] Epoch: 33 , batch: 224 , training loss: 4.276959\n",
      "[INFO] Epoch: 33 , batch: 225 , training loss: 4.158625\n",
      "[INFO] Epoch: 33 , batch: 226 , training loss: 4.280640\n",
      "[INFO] Epoch: 33 , batch: 227 , training loss: 4.258154\n",
      "[INFO] Epoch: 33 , batch: 228 , training loss: 4.260538\n",
      "[INFO] Epoch: 33 , batch: 229 , training loss: 4.151817\n",
      "[INFO] Epoch: 33 , batch: 230 , training loss: 4.004040\n",
      "[INFO] Epoch: 33 , batch: 231 , training loss: 3.860635\n",
      "[INFO] Epoch: 33 , batch: 232 , training loss: 3.996042\n",
      "[INFO] Epoch: 33 , batch: 233 , training loss: 4.053279\n",
      "[INFO] Epoch: 33 , batch: 234 , training loss: 3.731342\n",
      "[INFO] Epoch: 33 , batch: 235 , training loss: 3.832072\n",
      "[INFO] Epoch: 33 , batch: 236 , training loss: 3.948017\n",
      "[INFO] Epoch: 33 , batch: 237 , training loss: 4.161930\n",
      "[INFO] Epoch: 33 , batch: 238 , training loss: 3.934384\n",
      "[INFO] Epoch: 33 , batch: 239 , training loss: 3.960248\n",
      "[INFO] Epoch: 33 , batch: 240 , training loss: 4.009776\n",
      "[INFO] Epoch: 33 , batch: 241 , training loss: 3.817170\n",
      "[INFO] Epoch: 33 , batch: 242 , training loss: 3.841199\n",
      "[INFO] Epoch: 33 , batch: 243 , training loss: 4.121201\n",
      "[INFO] Epoch: 33 , batch: 244 , training loss: 4.098136\n",
      "[INFO] Epoch: 33 , batch: 245 , training loss: 4.030180\n",
      "[INFO] Epoch: 33 , batch: 246 , training loss: 3.748335\n",
      "[INFO] Epoch: 33 , batch: 247 , training loss: 3.916939\n",
      "[INFO] Epoch: 33 , batch: 248 , training loss: 3.995231\n",
      "[INFO] Epoch: 33 , batch: 249 , training loss: 3.964236\n",
      "[INFO] Epoch: 33 , batch: 250 , training loss: 3.787474\n",
      "[INFO] Epoch: 33 , batch: 251 , training loss: 4.209188\n",
      "[INFO] Epoch: 33 , batch: 252 , training loss: 3.923848\n",
      "[INFO] Epoch: 33 , batch: 253 , training loss: 3.841647\n",
      "[INFO] Epoch: 33 , batch: 254 , training loss: 4.097826\n",
      "[INFO] Epoch: 33 , batch: 255 , training loss: 4.088807\n",
      "[INFO] Epoch: 33 , batch: 256 , training loss: 4.071576\n",
      "[INFO] Epoch: 33 , batch: 257 , training loss: 4.243266\n",
      "[INFO] Epoch: 33 , batch: 258 , training loss: 4.221510\n",
      "[INFO] Epoch: 33 , batch: 259 , training loss: 4.293292\n",
      "[INFO] Epoch: 33 , batch: 260 , training loss: 4.053223\n",
      "[INFO] Epoch: 33 , batch: 261 , training loss: 4.212576\n",
      "[INFO] Epoch: 33 , batch: 262 , training loss: 4.358435\n",
      "[INFO] Epoch: 33 , batch: 263 , training loss: 4.549448\n",
      "[INFO] Epoch: 33 , batch: 264 , training loss: 3.913057\n",
      "[INFO] Epoch: 33 , batch: 265 , training loss: 4.018606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 33 , batch: 266 , training loss: 4.419462\n",
      "[INFO] Epoch: 33 , batch: 267 , training loss: 4.162787\n",
      "[INFO] Epoch: 33 , batch: 268 , training loss: 4.084313\n",
      "[INFO] Epoch: 33 , batch: 269 , training loss: 4.059581\n",
      "[INFO] Epoch: 33 , batch: 270 , training loss: 4.095456\n",
      "[INFO] Epoch: 33 , batch: 271 , training loss: 4.116087\n",
      "[INFO] Epoch: 33 , batch: 272 , training loss: 4.115236\n",
      "[INFO] Epoch: 33 , batch: 273 , training loss: 4.125507\n",
      "[INFO] Epoch: 33 , batch: 274 , training loss: 4.213133\n",
      "[INFO] Epoch: 33 , batch: 275 , training loss: 4.079698\n",
      "[INFO] Epoch: 33 , batch: 276 , training loss: 4.159118\n",
      "[INFO] Epoch: 33 , batch: 277 , training loss: 4.310812\n",
      "[INFO] Epoch: 33 , batch: 278 , training loss: 3.986187\n",
      "[INFO] Epoch: 33 , batch: 279 , training loss: 3.978367\n",
      "[INFO] Epoch: 33 , batch: 280 , training loss: 3.973094\n",
      "[INFO] Epoch: 33 , batch: 281 , training loss: 4.097239\n",
      "[INFO] Epoch: 33 , batch: 282 , training loss: 3.995094\n",
      "[INFO] Epoch: 33 , batch: 283 , training loss: 3.986982\n",
      "[INFO] Epoch: 33 , batch: 284 , training loss: 4.049658\n",
      "[INFO] Epoch: 33 , batch: 285 , training loss: 3.977046\n",
      "[INFO] Epoch: 33 , batch: 286 , training loss: 3.983817\n",
      "[INFO] Epoch: 33 , batch: 287 , training loss: 3.934188\n",
      "[INFO] Epoch: 33 , batch: 288 , training loss: 3.892677\n",
      "[INFO] Epoch: 33 , batch: 289 , training loss: 3.958894\n",
      "[INFO] Epoch: 33 , batch: 290 , training loss: 3.743150\n",
      "[INFO] Epoch: 33 , batch: 291 , training loss: 3.740926\n",
      "[INFO] Epoch: 33 , batch: 292 , training loss: 3.839797\n",
      "[INFO] Epoch: 33 , batch: 293 , training loss: 3.768674\n",
      "[INFO] Epoch: 33 , batch: 294 , training loss: 4.419425\n",
      "[INFO] Epoch: 33 , batch: 295 , training loss: 4.224242\n",
      "[INFO] Epoch: 33 , batch: 296 , training loss: 4.148670\n",
      "[INFO] Epoch: 33 , batch: 297 , training loss: 4.091816\n",
      "[INFO] Epoch: 33 , batch: 298 , training loss: 3.938589\n",
      "[INFO] Epoch: 33 , batch: 299 , training loss: 3.988714\n",
      "[INFO] Epoch: 33 , batch: 300 , training loss: 3.963142\n",
      "[INFO] Epoch: 33 , batch: 301 , training loss: 3.888858\n",
      "[INFO] Epoch: 33 , batch: 302 , training loss: 4.048970\n",
      "[INFO] Epoch: 33 , batch: 303 , training loss: 4.060353\n",
      "[INFO] Epoch: 33 , batch: 304 , training loss: 4.217720\n",
      "[INFO] Epoch: 33 , batch: 305 , training loss: 4.012598\n",
      "[INFO] Epoch: 33 , batch: 306 , training loss: 4.141729\n",
      "[INFO] Epoch: 33 , batch: 307 , training loss: 4.173667\n",
      "[INFO] Epoch: 33 , batch: 308 , training loss: 3.971249\n",
      "[INFO] Epoch: 33 , batch: 309 , training loss: 3.982715\n",
      "[INFO] Epoch: 33 , batch: 310 , training loss: 3.913159\n",
      "[INFO] Epoch: 33 , batch: 311 , training loss: 3.902515\n",
      "[INFO] Epoch: 33 , batch: 312 , training loss: 3.818381\n",
      "[INFO] Epoch: 33 , batch: 313 , training loss: 3.893262\n",
      "[INFO] Epoch: 33 , batch: 314 , training loss: 3.957719\n",
      "[INFO] Epoch: 33 , batch: 315 , training loss: 4.058323\n",
      "[INFO] Epoch: 33 , batch: 316 , training loss: 4.294186\n",
      "[INFO] Epoch: 33 , batch: 317 , training loss: 4.637809\n",
      "[INFO] Epoch: 33 , batch: 318 , training loss: 4.747241\n",
      "[INFO] Epoch: 33 , batch: 319 , training loss: 4.477305\n",
      "[INFO] Epoch: 33 , batch: 320 , training loss: 4.003607\n",
      "[INFO] Epoch: 33 , batch: 321 , training loss: 3.837463\n",
      "[INFO] Epoch: 33 , batch: 322 , training loss: 3.930848\n",
      "[INFO] Epoch: 33 , batch: 323 , training loss: 3.981851\n",
      "[INFO] Epoch: 33 , batch: 324 , training loss: 3.954838\n",
      "[INFO] Epoch: 33 , batch: 325 , training loss: 4.062912\n",
      "[INFO] Epoch: 33 , batch: 326 , training loss: 4.123940\n",
      "[INFO] Epoch: 33 , batch: 327 , training loss: 4.063045\n",
      "[INFO] Epoch: 33 , batch: 328 , training loss: 4.041298\n",
      "[INFO] Epoch: 33 , batch: 329 , training loss: 3.970733\n",
      "[INFO] Epoch: 33 , batch: 330 , training loss: 3.954814\n",
      "[INFO] Epoch: 33 , batch: 331 , training loss: 4.116622\n",
      "[INFO] Epoch: 33 , batch: 332 , training loss: 3.967362\n",
      "[INFO] Epoch: 33 , batch: 333 , training loss: 3.921147\n",
      "[INFO] Epoch: 33 , batch: 334 , training loss: 3.957501\n",
      "[INFO] Epoch: 33 , batch: 335 , training loss: 4.066938\n",
      "[INFO] Epoch: 33 , batch: 336 , training loss: 4.064773\n",
      "[INFO] Epoch: 33 , batch: 337 , training loss: 4.126183\n",
      "[INFO] Epoch: 33 , batch: 338 , training loss: 4.319629\n",
      "[INFO] Epoch: 33 , batch: 339 , training loss: 4.163138\n",
      "[INFO] Epoch: 33 , batch: 340 , training loss: 4.318388\n",
      "[INFO] Epoch: 33 , batch: 341 , training loss: 4.078440\n",
      "[INFO] Epoch: 33 , batch: 342 , training loss: 3.887031\n",
      "[INFO] Epoch: 33 , batch: 343 , training loss: 3.953592\n",
      "[INFO] Epoch: 33 , batch: 344 , training loss: 3.815902\n",
      "[INFO] Epoch: 33 , batch: 345 , training loss: 3.954282\n",
      "[INFO] Epoch: 33 , batch: 346 , training loss: 4.000384\n",
      "[INFO] Epoch: 33 , batch: 347 , training loss: 3.902088\n",
      "[INFO] Epoch: 33 , batch: 348 , training loss: 4.010608\n",
      "[INFO] Epoch: 33 , batch: 349 , training loss: 4.111087\n",
      "[INFO] Epoch: 33 , batch: 350 , training loss: 3.952132\n",
      "[INFO] Epoch: 33 , batch: 351 , training loss: 4.024556\n",
      "[INFO] Epoch: 33 , batch: 352 , training loss: 4.052282\n",
      "[INFO] Epoch: 33 , batch: 353 , training loss: 4.033482\n",
      "[INFO] Epoch: 33 , batch: 354 , training loss: 4.131017\n",
      "[INFO] Epoch: 33 , batch: 355 , training loss: 4.099793\n",
      "[INFO] Epoch: 33 , batch: 356 , training loss: 3.986329\n",
      "[INFO] Epoch: 33 , batch: 357 , training loss: 4.064669\n",
      "[INFO] Epoch: 33 , batch: 358 , training loss: 3.952947\n",
      "[INFO] Epoch: 33 , batch: 359 , training loss: 3.967047\n",
      "[INFO] Epoch: 33 , batch: 360 , training loss: 4.078005\n",
      "[INFO] Epoch: 33 , batch: 361 , training loss: 4.034889\n",
      "[INFO] Epoch: 33 , batch: 362 , training loss: 4.139176\n",
      "[INFO] Epoch: 33 , batch: 363 , training loss: 4.022097\n",
      "[INFO] Epoch: 33 , batch: 364 , training loss: 4.098762\n",
      "[INFO] Epoch: 33 , batch: 365 , training loss: 3.999174\n",
      "[INFO] Epoch: 33 , batch: 366 , training loss: 4.104941\n",
      "[INFO] Epoch: 33 , batch: 367 , training loss: 4.149868\n",
      "[INFO] Epoch: 33 , batch: 368 , training loss: 4.538238\n",
      "[INFO] Epoch: 33 , batch: 369 , training loss: 4.228665\n",
      "[INFO] Epoch: 33 , batch: 370 , training loss: 3.994213\n",
      "[INFO] Epoch: 33 , batch: 371 , training loss: 4.440289\n",
      "[INFO] Epoch: 33 , batch: 372 , training loss: 4.671884\n",
      "[INFO] Epoch: 33 , batch: 373 , training loss: 4.716223\n",
      "[INFO] Epoch: 33 , batch: 374 , training loss: 4.837519\n",
      "[INFO] Epoch: 33 , batch: 375 , training loss: 4.802082\n",
      "[INFO] Epoch: 33 , batch: 376 , training loss: 4.690347\n",
      "[INFO] Epoch: 33 , batch: 377 , training loss: 4.445314\n",
      "[INFO] Epoch: 33 , batch: 378 , training loss: 4.537867\n",
      "[INFO] Epoch: 33 , batch: 379 , training loss: 4.509404\n",
      "[INFO] Epoch: 33 , batch: 380 , training loss: 4.678440\n",
      "[INFO] Epoch: 33 , batch: 381 , training loss: 4.393342\n",
      "[INFO] Epoch: 33 , batch: 382 , training loss: 4.653516\n",
      "[INFO] Epoch: 33 , batch: 383 , training loss: 4.681126\n",
      "[INFO] Epoch: 33 , batch: 384 , training loss: 4.666210\n",
      "[INFO] Epoch: 33 , batch: 385 , training loss: 4.318896\n",
      "[INFO] Epoch: 33 , batch: 386 , training loss: 4.590077\n",
      "[INFO] Epoch: 33 , batch: 387 , training loss: 4.521575\n",
      "[INFO] Epoch: 33 , batch: 388 , training loss: 4.364326\n",
      "[INFO] Epoch: 33 , batch: 389 , training loss: 4.177972\n",
      "[INFO] Epoch: 33 , batch: 390 , training loss: 4.206642\n",
      "[INFO] Epoch: 33 , batch: 391 , training loss: 4.230828\n",
      "[INFO] Epoch: 33 , batch: 392 , training loss: 4.591140\n",
      "[INFO] Epoch: 33 , batch: 393 , training loss: 4.478989\n",
      "[INFO] Epoch: 33 , batch: 394 , training loss: 4.584282\n",
      "[INFO] Epoch: 33 , batch: 395 , training loss: 4.387446\n",
      "[INFO] Epoch: 33 , batch: 396 , training loss: 4.216214\n",
      "[INFO] Epoch: 33 , batch: 397 , training loss: 4.354943\n",
      "[INFO] Epoch: 33 , batch: 398 , training loss: 4.226401\n",
      "[INFO] Epoch: 33 , batch: 399 , training loss: 4.302853\n",
      "[INFO] Epoch: 33 , batch: 400 , training loss: 4.256565\n",
      "[INFO] Epoch: 33 , batch: 401 , training loss: 4.710892\n",
      "[INFO] Epoch: 33 , batch: 402 , training loss: 4.409494\n",
      "[INFO] Epoch: 33 , batch: 403 , training loss: 4.240190\n",
      "[INFO] Epoch: 33 , batch: 404 , training loss: 4.421239\n",
      "[INFO] Epoch: 33 , batch: 405 , training loss: 4.472356\n",
      "[INFO] Epoch: 33 , batch: 406 , training loss: 4.375625\n",
      "[INFO] Epoch: 33 , batch: 407 , training loss: 4.415434\n",
      "[INFO] Epoch: 33 , batch: 408 , training loss: 4.369645\n",
      "[INFO] Epoch: 33 , batch: 409 , training loss: 4.400901\n",
      "[INFO] Epoch: 33 , batch: 410 , training loss: 4.475965\n",
      "[INFO] Epoch: 33 , batch: 411 , training loss: 4.625520\n",
      "[INFO] Epoch: 33 , batch: 412 , training loss: 4.453503\n",
      "[INFO] Epoch: 33 , batch: 413 , training loss: 4.325786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 33 , batch: 414 , training loss: 4.367879\n",
      "[INFO] Epoch: 33 , batch: 415 , training loss: 4.410565\n",
      "[INFO] Epoch: 33 , batch: 416 , training loss: 4.474958\n",
      "[INFO] Epoch: 33 , batch: 417 , training loss: 4.372542\n",
      "[INFO] Epoch: 33 , batch: 418 , training loss: 4.443975\n",
      "[INFO] Epoch: 33 , batch: 419 , training loss: 4.424774\n",
      "[INFO] Epoch: 33 , batch: 420 , training loss: 4.385020\n",
      "[INFO] Epoch: 33 , batch: 421 , training loss: 4.352038\n",
      "[INFO] Epoch: 33 , batch: 422 , training loss: 4.209873\n",
      "[INFO] Epoch: 33 , batch: 423 , training loss: 4.417816\n",
      "[INFO] Epoch: 33 , batch: 424 , training loss: 4.614674\n",
      "[INFO] Epoch: 33 , batch: 425 , training loss: 4.440253\n",
      "[INFO] Epoch: 33 , batch: 426 , training loss: 4.208042\n",
      "[INFO] Epoch: 33 , batch: 427 , training loss: 4.451704\n",
      "[INFO] Epoch: 33 , batch: 428 , training loss: 4.305670\n",
      "[INFO] Epoch: 33 , batch: 429 , training loss: 4.219430\n",
      "[INFO] Epoch: 33 , batch: 430 , training loss: 4.443536\n",
      "[INFO] Epoch: 33 , batch: 431 , training loss: 4.055984\n",
      "[INFO] Epoch: 33 , batch: 432 , training loss: 4.113023\n",
      "[INFO] Epoch: 33 , batch: 433 , training loss: 4.151134\n",
      "[INFO] Epoch: 33 , batch: 434 , training loss: 4.036430\n",
      "[INFO] Epoch: 33 , batch: 435 , training loss: 4.389795\n",
      "[INFO] Epoch: 33 , batch: 436 , training loss: 4.418753\n",
      "[INFO] Epoch: 33 , batch: 437 , training loss: 4.208661\n",
      "[INFO] Epoch: 33 , batch: 438 , training loss: 4.071556\n",
      "[INFO] Epoch: 33 , batch: 439 , training loss: 4.320406\n",
      "[INFO] Epoch: 33 , batch: 440 , training loss: 4.418430\n",
      "[INFO] Epoch: 33 , batch: 441 , training loss: 4.508239\n",
      "[INFO] Epoch: 33 , batch: 442 , training loss: 4.286960\n",
      "[INFO] Epoch: 33 , batch: 443 , training loss: 4.456187\n",
      "[INFO] Epoch: 33 , batch: 444 , training loss: 4.070694\n",
      "[INFO] Epoch: 33 , batch: 445 , training loss: 3.972069\n",
      "[INFO] Epoch: 33 , batch: 446 , training loss: 3.908694\n",
      "[INFO] Epoch: 33 , batch: 447 , training loss: 4.102013\n",
      "[INFO] Epoch: 33 , batch: 448 , training loss: 4.221511\n",
      "[INFO] Epoch: 33 , batch: 449 , training loss: 4.592935\n",
      "[INFO] Epoch: 33 , batch: 450 , training loss: 4.658339\n",
      "[INFO] Epoch: 33 , batch: 451 , training loss: 4.547616\n",
      "[INFO] Epoch: 33 , batch: 452 , training loss: 4.372422\n",
      "[INFO] Epoch: 33 , batch: 453 , training loss: 4.152224\n",
      "[INFO] Epoch: 33 , batch: 454 , training loss: 4.297636\n",
      "[INFO] Epoch: 33 , batch: 455 , training loss: 4.359059\n",
      "[INFO] Epoch: 33 , batch: 456 , training loss: 4.336790\n",
      "[INFO] Epoch: 33 , batch: 457 , training loss: 4.418187\n",
      "[INFO] Epoch: 33 , batch: 458 , training loss: 4.167795\n",
      "[INFO] Epoch: 33 , batch: 459 , training loss: 4.139871\n",
      "[INFO] Epoch: 33 , batch: 460 , training loss: 4.236822\n",
      "[INFO] Epoch: 33 , batch: 461 , training loss: 4.219406\n",
      "[INFO] Epoch: 33 , batch: 462 , training loss: 4.276084\n",
      "[INFO] Epoch: 33 , batch: 463 , training loss: 4.192050\n",
      "[INFO] Epoch: 33 , batch: 464 , training loss: 4.351453\n",
      "[INFO] Epoch: 33 , batch: 465 , training loss: 4.316998\n",
      "[INFO] Epoch: 33 , batch: 466 , training loss: 4.411801\n",
      "[INFO] Epoch: 33 , batch: 467 , training loss: 4.360695\n",
      "[INFO] Epoch: 33 , batch: 468 , training loss: 4.335190\n",
      "[INFO] Epoch: 33 , batch: 469 , training loss: 4.344418\n",
      "[INFO] Epoch: 33 , batch: 470 , training loss: 4.191738\n",
      "[INFO] Epoch: 33 , batch: 471 , training loss: 4.285795\n",
      "[INFO] Epoch: 33 , batch: 472 , training loss: 4.342324\n",
      "[INFO] Epoch: 33 , batch: 473 , training loss: 4.249098\n",
      "[INFO] Epoch: 33 , batch: 474 , training loss: 4.035900\n",
      "[INFO] Epoch: 33 , batch: 475 , training loss: 3.922329\n",
      "[INFO] Epoch: 33 , batch: 476 , training loss: 4.338406\n",
      "[INFO] Epoch: 33 , batch: 477 , training loss: 4.430584\n",
      "[INFO] Epoch: 33 , batch: 478 , training loss: 4.426317\n",
      "[INFO] Epoch: 33 , batch: 479 , training loss: 4.418239\n",
      "[INFO] Epoch: 33 , batch: 480 , training loss: 4.533825\n",
      "[INFO] Epoch: 33 , batch: 481 , training loss: 4.426295\n",
      "[INFO] Epoch: 33 , batch: 482 , training loss: 4.533873\n",
      "[INFO] Epoch: 33 , batch: 483 , training loss: 4.363328\n",
      "[INFO] Epoch: 33 , batch: 484 , training loss: 4.166841\n",
      "[INFO] Epoch: 33 , batch: 485 , training loss: 4.232650\n",
      "[INFO] Epoch: 33 , batch: 486 , training loss: 4.168157\n",
      "[INFO] Epoch: 33 , batch: 487 , training loss: 4.152959\n",
      "[INFO] Epoch: 33 , batch: 488 , training loss: 4.320909\n",
      "[INFO] Epoch: 33 , batch: 489 , training loss: 4.225983\n",
      "[INFO] Epoch: 33 , batch: 490 , training loss: 4.293406\n",
      "[INFO] Epoch: 33 , batch: 491 , training loss: 4.208303\n",
      "[INFO] Epoch: 33 , batch: 492 , training loss: 4.191889\n",
      "[INFO] Epoch: 33 , batch: 493 , training loss: 4.354765\n",
      "[INFO] Epoch: 33 , batch: 494 , training loss: 4.251215\n",
      "[INFO] Epoch: 33 , batch: 495 , training loss: 4.429053\n",
      "[INFO] Epoch: 33 , batch: 496 , training loss: 4.295713\n",
      "[INFO] Epoch: 33 , batch: 497 , training loss: 4.338398\n",
      "[INFO] Epoch: 33 , batch: 498 , training loss: 4.305087\n",
      "[INFO] Epoch: 33 , batch: 499 , training loss: 4.391572\n",
      "[INFO] Epoch: 33 , batch: 500 , training loss: 4.498648\n",
      "[INFO] Epoch: 33 , batch: 501 , training loss: 4.845005\n",
      "[INFO] Epoch: 33 , batch: 502 , training loss: 4.860409\n",
      "[INFO] Epoch: 33 , batch: 503 , training loss: 4.537804\n",
      "[INFO] Epoch: 33 , batch: 504 , training loss: 4.691247\n",
      "[INFO] Epoch: 33 , batch: 505 , training loss: 4.643488\n",
      "[INFO] Epoch: 33 , batch: 506 , training loss: 4.618282\n",
      "[INFO] Epoch: 33 , batch: 507 , training loss: 4.675860\n",
      "[INFO] Epoch: 33 , batch: 508 , training loss: 4.589859\n",
      "[INFO] Epoch: 33 , batch: 509 , training loss: 4.397824\n",
      "[INFO] Epoch: 33 , batch: 510 , training loss: 4.487296\n",
      "[INFO] Epoch: 33 , batch: 511 , training loss: 4.409862\n",
      "[INFO] Epoch: 33 , batch: 512 , training loss: 4.489659\n",
      "[INFO] Epoch: 33 , batch: 513 , training loss: 4.754942\n",
      "[INFO] Epoch: 33 , batch: 514 , training loss: 4.381765\n",
      "[INFO] Epoch: 33 , batch: 515 , training loss: 4.635340\n",
      "[INFO] Epoch: 33 , batch: 516 , training loss: 4.432668\n",
      "[INFO] Epoch: 33 , batch: 517 , training loss: 4.416769\n",
      "[INFO] Epoch: 33 , batch: 518 , training loss: 4.385067\n",
      "[INFO] Epoch: 33 , batch: 519 , training loss: 4.225778\n",
      "[INFO] Epoch: 33 , batch: 520 , training loss: 4.460870\n",
      "[INFO] Epoch: 33 , batch: 521 , training loss: 4.425117\n",
      "[INFO] Epoch: 33 , batch: 522 , training loss: 4.520647\n",
      "[INFO] Epoch: 33 , batch: 523 , training loss: 4.422585\n",
      "[INFO] Epoch: 33 , batch: 524 , training loss: 4.709126\n",
      "[INFO] Epoch: 33 , batch: 525 , training loss: 4.591000\n",
      "[INFO] Epoch: 33 , batch: 526 , training loss: 4.375951\n",
      "[INFO] Epoch: 33 , batch: 527 , training loss: 4.418458\n",
      "[INFO] Epoch: 33 , batch: 528 , training loss: 4.437963\n",
      "[INFO] Epoch: 33 , batch: 529 , training loss: 4.412922\n",
      "[INFO] Epoch: 33 , batch: 530 , training loss: 4.260960\n",
      "[INFO] Epoch: 33 , batch: 531 , training loss: 4.408798\n",
      "[INFO] Epoch: 33 , batch: 532 , training loss: 4.303222\n",
      "[INFO] Epoch: 33 , batch: 533 , training loss: 4.454171\n",
      "[INFO] Epoch: 33 , batch: 534 , training loss: 4.462090\n",
      "[INFO] Epoch: 33 , batch: 535 , training loss: 4.453665\n",
      "[INFO] Epoch: 33 , batch: 536 , training loss: 4.299408\n",
      "[INFO] Epoch: 33 , batch: 537 , training loss: 4.280733\n",
      "[INFO] Epoch: 33 , batch: 538 , training loss: 4.351304\n",
      "[INFO] Epoch: 33 , batch: 539 , training loss: 4.461653\n",
      "[INFO] Epoch: 33 , batch: 540 , training loss: 4.972503\n",
      "[INFO] Epoch: 33 , batch: 541 , training loss: 4.796727\n",
      "[INFO] Epoch: 33 , batch: 542 , training loss: 4.677795\n",
      "[INFO] Epoch: 34 , batch: 0 , training loss: 3.644149\n",
      "[INFO] Epoch: 34 , batch: 1 , training loss: 3.515669\n",
      "[INFO] Epoch: 34 , batch: 2 , training loss: 3.707042\n",
      "[INFO] Epoch: 34 , batch: 3 , training loss: 3.539146\n",
      "[INFO] Epoch: 34 , batch: 4 , training loss: 3.867199\n",
      "[INFO] Epoch: 34 , batch: 5 , training loss: 3.596521\n",
      "[INFO] Epoch: 34 , batch: 6 , training loss: 3.892764\n",
      "[INFO] Epoch: 34 , batch: 7 , training loss: 3.822149\n",
      "[INFO] Epoch: 34 , batch: 8 , training loss: 3.526200\n",
      "[INFO] Epoch: 34 , batch: 9 , training loss: 3.755234\n",
      "[INFO] Epoch: 34 , batch: 10 , training loss: 3.731365\n",
      "[INFO] Epoch: 34 , batch: 11 , training loss: 3.663417\n",
      "[INFO] Epoch: 34 , batch: 12 , training loss: 3.543979\n",
      "[INFO] Epoch: 34 , batch: 13 , training loss: 3.590126\n",
      "[INFO] Epoch: 34 , batch: 14 , training loss: 3.478809\n",
      "[INFO] Epoch: 34 , batch: 15 , training loss: 3.703213\n",
      "[INFO] Epoch: 34 , batch: 16 , training loss: 3.568862\n",
      "[INFO] Epoch: 34 , batch: 17 , training loss: 3.696018\n",
      "[INFO] Epoch: 34 , batch: 18 , training loss: 3.617095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 34 , batch: 19 , training loss: 3.409581\n",
      "[INFO] Epoch: 34 , batch: 20 , training loss: 3.369801\n",
      "[INFO] Epoch: 34 , batch: 21 , training loss: 3.519717\n",
      "[INFO] Epoch: 34 , batch: 22 , training loss: 3.389922\n",
      "[INFO] Epoch: 34 , batch: 23 , training loss: 3.612661\n",
      "[INFO] Epoch: 34 , batch: 24 , training loss: 3.444280\n",
      "[INFO] Epoch: 34 , batch: 25 , training loss: 3.556497\n",
      "[INFO] Epoch: 34 , batch: 26 , training loss: 3.449393\n",
      "[INFO] Epoch: 34 , batch: 27 , training loss: 3.422933\n",
      "[INFO] Epoch: 34 , batch: 28 , training loss: 3.589621\n",
      "[INFO] Epoch: 34 , batch: 29 , training loss: 3.414711\n",
      "[INFO] Epoch: 34 , batch: 30 , training loss: 3.451874\n",
      "[INFO] Epoch: 34 , batch: 31 , training loss: 3.579164\n",
      "[INFO] Epoch: 34 , batch: 32 , training loss: 3.517343\n",
      "[INFO] Epoch: 34 , batch: 33 , training loss: 3.563540\n",
      "[INFO] Epoch: 34 , batch: 34 , training loss: 3.573353\n",
      "[INFO] Epoch: 34 , batch: 35 , training loss: 3.461931\n",
      "[INFO] Epoch: 34 , batch: 36 , training loss: 3.618672\n",
      "[INFO] Epoch: 34 , batch: 37 , training loss: 3.425630\n",
      "[INFO] Epoch: 34 , batch: 38 , training loss: 3.522453\n",
      "[INFO] Epoch: 34 , batch: 39 , training loss: 3.326402\n",
      "[INFO] Epoch: 34 , batch: 40 , training loss: 3.548776\n",
      "[INFO] Epoch: 34 , batch: 41 , training loss: 3.485337\n",
      "[INFO] Epoch: 34 , batch: 42 , training loss: 3.929099\n",
      "[INFO] Epoch: 34 , batch: 43 , training loss: 3.728084\n",
      "[INFO] Epoch: 34 , batch: 44 , training loss: 4.008278\n",
      "[INFO] Epoch: 34 , batch: 45 , training loss: 3.943056\n",
      "[INFO] Epoch: 34 , batch: 46 , training loss: 3.875598\n",
      "[INFO] Epoch: 34 , batch: 47 , training loss: 3.547411\n",
      "[INFO] Epoch: 34 , batch: 48 , training loss: 3.516096\n",
      "[INFO] Epoch: 34 , batch: 49 , training loss: 3.803151\n",
      "[INFO] Epoch: 34 , batch: 50 , training loss: 3.556326\n",
      "[INFO] Epoch: 34 , batch: 51 , training loss: 3.813181\n",
      "[INFO] Epoch: 34 , batch: 52 , training loss: 3.658065\n",
      "[INFO] Epoch: 34 , batch: 53 , training loss: 3.730826\n",
      "[INFO] Epoch: 34 , batch: 54 , training loss: 3.771106\n",
      "[INFO] Epoch: 34 , batch: 55 , training loss: 3.865665\n",
      "[INFO] Epoch: 34 , batch: 56 , training loss: 3.663368\n",
      "[INFO] Epoch: 34 , batch: 57 , training loss: 3.610022\n",
      "[INFO] Epoch: 34 , batch: 58 , training loss: 3.669131\n",
      "[INFO] Epoch: 34 , batch: 59 , training loss: 3.750941\n",
      "[INFO] Epoch: 34 , batch: 60 , training loss: 3.690180\n",
      "[INFO] Epoch: 34 , batch: 61 , training loss: 3.787404\n",
      "[INFO] Epoch: 34 , batch: 62 , training loss: 3.636137\n",
      "[INFO] Epoch: 34 , batch: 63 , training loss: 3.853841\n",
      "[INFO] Epoch: 34 , batch: 64 , training loss: 4.044072\n",
      "[INFO] Epoch: 34 , batch: 65 , training loss: 3.748829\n",
      "[INFO] Epoch: 34 , batch: 66 , training loss: 3.630571\n",
      "[INFO] Epoch: 34 , batch: 67 , training loss: 3.642598\n",
      "[INFO] Epoch: 34 , batch: 68 , training loss: 3.789655\n",
      "[INFO] Epoch: 34 , batch: 69 , training loss: 3.741698\n",
      "[INFO] Epoch: 34 , batch: 70 , training loss: 3.957430\n",
      "[INFO] Epoch: 34 , batch: 71 , training loss: 3.792269\n",
      "[INFO] Epoch: 34 , batch: 72 , training loss: 3.867292\n",
      "[INFO] Epoch: 34 , batch: 73 , training loss: 3.807257\n",
      "[INFO] Epoch: 34 , batch: 74 , training loss: 3.930038\n",
      "[INFO] Epoch: 34 , batch: 75 , training loss: 3.766642\n",
      "[INFO] Epoch: 34 , batch: 76 , training loss: 3.878113\n",
      "[INFO] Epoch: 34 , batch: 77 , training loss: 3.839939\n",
      "[INFO] Epoch: 34 , batch: 78 , training loss: 3.892713\n",
      "[INFO] Epoch: 34 , batch: 79 , training loss: 3.796940\n",
      "[INFO] Epoch: 34 , batch: 80 , training loss: 3.992856\n",
      "[INFO] Epoch: 34 , batch: 81 , training loss: 3.915195\n",
      "[INFO] Epoch: 34 , batch: 82 , training loss: 3.889872\n",
      "[INFO] Epoch: 34 , batch: 83 , training loss: 3.960078\n",
      "[INFO] Epoch: 34 , batch: 84 , training loss: 3.922949\n",
      "[INFO] Epoch: 34 , batch: 85 , training loss: 4.029200\n",
      "[INFO] Epoch: 34 , batch: 86 , training loss: 3.932917\n",
      "[INFO] Epoch: 34 , batch: 87 , training loss: 3.914355\n",
      "[INFO] Epoch: 34 , batch: 88 , training loss: 4.038545\n",
      "[INFO] Epoch: 34 , batch: 89 , training loss: 3.815150\n",
      "[INFO] Epoch: 34 , batch: 90 , training loss: 3.943200\n",
      "[INFO] Epoch: 34 , batch: 91 , training loss: 3.837957\n",
      "[INFO] Epoch: 34 , batch: 92 , training loss: 3.895257\n",
      "[INFO] Epoch: 34 , batch: 93 , training loss: 3.981122\n",
      "[INFO] Epoch: 34 , batch: 94 , training loss: 4.131438\n",
      "[INFO] Epoch: 34 , batch: 95 , training loss: 3.883926\n",
      "[INFO] Epoch: 34 , batch: 96 , training loss: 3.868885\n",
      "[INFO] Epoch: 34 , batch: 97 , training loss: 3.844552\n",
      "[INFO] Epoch: 34 , batch: 98 , training loss: 3.768312\n",
      "[INFO] Epoch: 34 , batch: 99 , training loss: 3.914656\n",
      "[INFO] Epoch: 34 , batch: 100 , training loss: 3.747933\n",
      "[INFO] Epoch: 34 , batch: 101 , training loss: 3.794664\n",
      "[INFO] Epoch: 34 , batch: 102 , training loss: 3.948893\n",
      "[INFO] Epoch: 34 , batch: 103 , training loss: 3.748270\n",
      "[INFO] Epoch: 34 , batch: 104 , training loss: 3.674620\n",
      "[INFO] Epoch: 34 , batch: 105 , training loss: 3.939078\n",
      "[INFO] Epoch: 34 , batch: 106 , training loss: 3.965363\n",
      "[INFO] Epoch: 34 , batch: 107 , training loss: 3.780077\n",
      "[INFO] Epoch: 34 , batch: 108 , training loss: 3.758558\n",
      "[INFO] Epoch: 34 , batch: 109 , training loss: 3.674549\n",
      "[INFO] Epoch: 34 , batch: 110 , training loss: 3.849666\n",
      "[INFO] Epoch: 34 , batch: 111 , training loss: 3.953282\n",
      "[INFO] Epoch: 34 , batch: 112 , training loss: 3.867153\n",
      "[INFO] Epoch: 34 , batch: 113 , training loss: 3.847299\n",
      "[INFO] Epoch: 34 , batch: 114 , training loss: 3.820003\n",
      "[INFO] Epoch: 34 , batch: 115 , training loss: 3.850100\n",
      "[INFO] Epoch: 34 , batch: 116 , training loss: 3.752055\n",
      "[INFO] Epoch: 34 , batch: 117 , training loss: 3.971150\n",
      "[INFO] Epoch: 34 , batch: 118 , training loss: 3.937570\n",
      "[INFO] Epoch: 34 , batch: 119 , training loss: 4.075048\n",
      "[INFO] Epoch: 34 , batch: 120 , training loss: 4.038230\n",
      "[INFO] Epoch: 34 , batch: 121 , training loss: 3.937041\n",
      "[INFO] Epoch: 34 , batch: 122 , training loss: 3.841268\n",
      "[INFO] Epoch: 34 , batch: 123 , training loss: 3.838678\n",
      "[INFO] Epoch: 34 , batch: 124 , training loss: 3.929506\n",
      "[INFO] Epoch: 34 , batch: 125 , training loss: 3.743652\n",
      "[INFO] Epoch: 34 , batch: 126 , training loss: 3.775939\n",
      "[INFO] Epoch: 34 , batch: 127 , training loss: 3.739016\n",
      "[INFO] Epoch: 34 , batch: 128 , training loss: 3.923549\n",
      "[INFO] Epoch: 34 , batch: 129 , training loss: 3.871984\n",
      "[INFO] Epoch: 34 , batch: 130 , training loss: 3.841223\n",
      "[INFO] Epoch: 34 , batch: 131 , training loss: 3.863729\n",
      "[INFO] Epoch: 34 , batch: 132 , training loss: 3.897096\n",
      "[INFO] Epoch: 34 , batch: 133 , training loss: 3.835943\n",
      "[INFO] Epoch: 34 , batch: 134 , training loss: 3.619645\n",
      "[INFO] Epoch: 34 , batch: 135 , training loss: 3.664543\n",
      "[INFO] Epoch: 34 , batch: 136 , training loss: 3.965876\n",
      "[INFO] Epoch: 34 , batch: 137 , training loss: 3.885036\n",
      "[INFO] Epoch: 34 , batch: 138 , training loss: 3.930313\n",
      "[INFO] Epoch: 34 , batch: 139 , training loss: 4.479940\n",
      "[INFO] Epoch: 34 , batch: 140 , training loss: 4.278976\n",
      "[INFO] Epoch: 34 , batch: 141 , training loss: 4.080403\n",
      "[INFO] Epoch: 34 , batch: 142 , training loss: 3.784843\n",
      "[INFO] Epoch: 34 , batch: 143 , training loss: 3.917369\n",
      "[INFO] Epoch: 34 , batch: 144 , training loss: 3.736849\n",
      "[INFO] Epoch: 34 , batch: 145 , training loss: 3.833840\n",
      "[INFO] Epoch: 34 , batch: 146 , training loss: 4.021852\n",
      "[INFO] Epoch: 34 , batch: 147 , training loss: 3.673333\n",
      "[INFO] Epoch: 34 , batch: 148 , training loss: 3.656806\n",
      "[INFO] Epoch: 34 , batch: 149 , training loss: 3.755163\n",
      "[INFO] Epoch: 34 , batch: 150 , training loss: 4.027071\n",
      "[INFO] Epoch: 34 , batch: 151 , training loss: 3.841419\n",
      "[INFO] Epoch: 34 , batch: 152 , training loss: 3.857423\n",
      "[INFO] Epoch: 34 , batch: 153 , training loss: 3.866663\n",
      "[INFO] Epoch: 34 , batch: 154 , training loss: 3.969101\n",
      "[INFO] Epoch: 34 , batch: 155 , training loss: 4.141312\n",
      "[INFO] Epoch: 34 , batch: 156 , training loss: 3.907553\n",
      "[INFO] Epoch: 34 , batch: 157 , training loss: 3.894674\n",
      "[INFO] Epoch: 34 , batch: 158 , training loss: 3.994669\n",
      "[INFO] Epoch: 34 , batch: 159 , training loss: 3.950423\n",
      "[INFO] Epoch: 34 , batch: 160 , training loss: 4.153073\n",
      "[INFO] Epoch: 34 , batch: 161 , training loss: 4.252689\n",
      "[INFO] Epoch: 34 , batch: 162 , training loss: 4.201671\n",
      "[INFO] Epoch: 34 , batch: 163 , training loss: 4.396839\n",
      "[INFO] Epoch: 34 , batch: 164 , training loss: 4.311132\n",
      "[INFO] Epoch: 34 , batch: 165 , training loss: 4.270319\n",
      "[INFO] Epoch: 34 , batch: 166 , training loss: 4.116096\n",
      "[INFO] Epoch: 34 , batch: 167 , training loss: 4.255625\n",
      "[INFO] Epoch: 34 , batch: 168 , training loss: 3.811438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 34 , batch: 169 , training loss: 3.858562\n",
      "[INFO] Epoch: 34 , batch: 170 , training loss: 4.030783\n",
      "[INFO] Epoch: 34 , batch: 171 , training loss: 3.471776\n",
      "[INFO] Epoch: 34 , batch: 172 , training loss: 3.693494\n",
      "[INFO] Epoch: 34 , batch: 173 , training loss: 4.046732\n",
      "[INFO] Epoch: 34 , batch: 174 , training loss: 4.484552\n",
      "[INFO] Epoch: 34 , batch: 175 , training loss: 4.779904\n",
      "[INFO] Epoch: 34 , batch: 176 , training loss: 4.457862\n",
      "[INFO] Epoch: 34 , batch: 177 , training loss: 4.077371\n",
      "[INFO] Epoch: 34 , batch: 178 , training loss: 4.110666\n",
      "[INFO] Epoch: 34 , batch: 179 , training loss: 4.133894\n",
      "[INFO] Epoch: 34 , batch: 180 , training loss: 4.072179\n",
      "[INFO] Epoch: 34 , batch: 181 , training loss: 4.362174\n",
      "[INFO] Epoch: 34 , batch: 182 , training loss: 4.308970\n",
      "[INFO] Epoch: 34 , batch: 183 , training loss: 4.284769\n",
      "[INFO] Epoch: 34 , batch: 184 , training loss: 4.158641\n",
      "[INFO] Epoch: 34 , batch: 185 , training loss: 4.109417\n",
      "[INFO] Epoch: 34 , batch: 186 , training loss: 4.258532\n",
      "[INFO] Epoch: 34 , batch: 187 , training loss: 4.359413\n",
      "[INFO] Epoch: 34 , batch: 188 , training loss: 4.369997\n",
      "[INFO] Epoch: 34 , batch: 189 , training loss: 4.264540\n",
      "[INFO] Epoch: 34 , batch: 190 , training loss: 4.320974\n",
      "[INFO] Epoch: 34 , batch: 191 , training loss: 4.397355\n",
      "[INFO] Epoch: 34 , batch: 192 , training loss: 4.250062\n",
      "[INFO] Epoch: 34 , batch: 193 , training loss: 4.350796\n",
      "[INFO] Epoch: 34 , batch: 194 , training loss: 4.285720\n",
      "[INFO] Epoch: 34 , batch: 195 , training loss: 4.213277\n",
      "[INFO] Epoch: 34 , batch: 196 , training loss: 4.075382\n",
      "[INFO] Epoch: 34 , batch: 197 , training loss: 4.164015\n",
      "[INFO] Epoch: 34 , batch: 198 , training loss: 4.078382\n",
      "[INFO] Epoch: 34 , batch: 199 , training loss: 4.211723\n",
      "[INFO] Epoch: 34 , batch: 200 , training loss: 4.120946\n",
      "[INFO] Epoch: 34 , batch: 201 , training loss: 4.036844\n",
      "[INFO] Epoch: 34 , batch: 202 , training loss: 4.040690\n",
      "[INFO] Epoch: 34 , batch: 203 , training loss: 4.140570\n",
      "[INFO] Epoch: 34 , batch: 204 , training loss: 4.241755\n",
      "[INFO] Epoch: 34 , batch: 205 , training loss: 3.846981\n",
      "[INFO] Epoch: 34 , batch: 206 , training loss: 3.770976\n",
      "[INFO] Epoch: 34 , batch: 207 , training loss: 3.764952\n",
      "[INFO] Epoch: 34 , batch: 208 , training loss: 4.091123\n",
      "[INFO] Epoch: 34 , batch: 209 , training loss: 4.037796\n",
      "[INFO] Epoch: 34 , batch: 210 , training loss: 4.072563\n",
      "[INFO] Epoch: 34 , batch: 211 , training loss: 4.057112\n",
      "[INFO] Epoch: 34 , batch: 212 , training loss: 4.163193\n",
      "[INFO] Epoch: 34 , batch: 213 , training loss: 4.114263\n",
      "[INFO] Epoch: 34 , batch: 214 , training loss: 4.186396\n",
      "[INFO] Epoch: 34 , batch: 215 , training loss: 4.378817\n",
      "[INFO] Epoch: 34 , batch: 216 , training loss: 4.105751\n",
      "[INFO] Epoch: 34 , batch: 217 , training loss: 4.043454\n",
      "[INFO] Epoch: 34 , batch: 218 , training loss: 4.029109\n",
      "[INFO] Epoch: 34 , batch: 219 , training loss: 4.152949\n",
      "[INFO] Epoch: 34 , batch: 220 , training loss: 3.964497\n",
      "[INFO] Epoch: 34 , batch: 221 , training loss: 3.991114\n",
      "[INFO] Epoch: 34 , batch: 222 , training loss: 4.127994\n",
      "[INFO] Epoch: 34 , batch: 223 , training loss: 4.221148\n",
      "[INFO] Epoch: 34 , batch: 224 , training loss: 4.271726\n",
      "[INFO] Epoch: 34 , batch: 225 , training loss: 4.168613\n",
      "[INFO] Epoch: 34 , batch: 226 , training loss: 4.298582\n",
      "[INFO] Epoch: 34 , batch: 227 , training loss: 4.264324\n",
      "[INFO] Epoch: 34 , batch: 228 , training loss: 4.280662\n",
      "[INFO] Epoch: 34 , batch: 229 , training loss: 4.146667\n",
      "[INFO] Epoch: 34 , batch: 230 , training loss: 3.995167\n",
      "[INFO] Epoch: 34 , batch: 231 , training loss: 3.855618\n",
      "[INFO] Epoch: 34 , batch: 232 , training loss: 4.022183\n",
      "[INFO] Epoch: 34 , batch: 233 , training loss: 4.048766\n",
      "[INFO] Epoch: 34 , batch: 234 , training loss: 3.718494\n",
      "[INFO] Epoch: 34 , batch: 235 , training loss: 3.847695\n",
      "[INFO] Epoch: 34 , batch: 236 , training loss: 3.953863\n",
      "[INFO] Epoch: 34 , batch: 237 , training loss: 4.175526\n",
      "[INFO] Epoch: 34 , batch: 238 , training loss: 3.926642\n",
      "[INFO] Epoch: 34 , batch: 239 , training loss: 3.987474\n",
      "[INFO] Epoch: 34 , batch: 240 , training loss: 4.011139\n",
      "[INFO] Epoch: 34 , batch: 241 , training loss: 3.824909\n",
      "[INFO] Epoch: 34 , batch: 242 , training loss: 3.831720\n",
      "[INFO] Epoch: 34 , batch: 243 , training loss: 4.128293\n",
      "[INFO] Epoch: 34 , batch: 244 , training loss: 4.088292\n",
      "[INFO] Epoch: 34 , batch: 245 , training loss: 4.050109\n",
      "[INFO] Epoch: 34 , batch: 246 , training loss: 3.756117\n",
      "[INFO] Epoch: 34 , batch: 247 , training loss: 3.918467\n",
      "[INFO] Epoch: 34 , batch: 248 , training loss: 3.985297\n",
      "[INFO] Epoch: 34 , batch: 249 , training loss: 3.959584\n",
      "[INFO] Epoch: 34 , batch: 250 , training loss: 3.789430\n",
      "[INFO] Epoch: 34 , batch: 251 , training loss: 4.200297\n",
      "[INFO] Epoch: 34 , batch: 252 , training loss: 3.930348\n",
      "[INFO] Epoch: 34 , batch: 253 , training loss: 3.836164\n",
      "[INFO] Epoch: 34 , batch: 254 , training loss: 4.102785\n",
      "[INFO] Epoch: 34 , batch: 255 , training loss: 4.080233\n",
      "[INFO] Epoch: 34 , batch: 256 , training loss: 4.068602\n",
      "[INFO] Epoch: 34 , batch: 257 , training loss: 4.248147\n",
      "[INFO] Epoch: 34 , batch: 258 , training loss: 4.235013\n",
      "[INFO] Epoch: 34 , batch: 259 , training loss: 4.291528\n",
      "[INFO] Epoch: 34 , batch: 260 , training loss: 4.087719\n",
      "[INFO] Epoch: 34 , batch: 261 , training loss: 4.221348\n",
      "[INFO] Epoch: 34 , batch: 262 , training loss: 4.364583\n",
      "[INFO] Epoch: 34 , batch: 263 , training loss: 4.534818\n",
      "[INFO] Epoch: 34 , batch: 264 , training loss: 3.915315\n",
      "[INFO] Epoch: 34 , batch: 265 , training loss: 4.020811\n",
      "[INFO] Epoch: 34 , batch: 266 , training loss: 4.422373\n",
      "[INFO] Epoch: 34 , batch: 267 , training loss: 4.159820\n",
      "[INFO] Epoch: 34 , batch: 268 , training loss: 4.072285\n",
      "[INFO] Epoch: 34 , batch: 269 , training loss: 4.072381\n",
      "[INFO] Epoch: 34 , batch: 270 , training loss: 4.105555\n",
      "[INFO] Epoch: 34 , batch: 271 , training loss: 4.130754\n",
      "[INFO] Epoch: 34 , batch: 272 , training loss: 4.114692\n",
      "[INFO] Epoch: 34 , batch: 273 , training loss: 4.121456\n",
      "[INFO] Epoch: 34 , batch: 274 , training loss: 4.219534\n",
      "[INFO] Epoch: 34 , batch: 275 , training loss: 4.087818\n",
      "[INFO] Epoch: 34 , batch: 276 , training loss: 4.163704\n",
      "[INFO] Epoch: 34 , batch: 277 , training loss: 4.308215\n",
      "[INFO] Epoch: 34 , batch: 278 , training loss: 3.983320\n",
      "[INFO] Epoch: 34 , batch: 279 , training loss: 3.995875\n",
      "[INFO] Epoch: 34 , batch: 280 , training loss: 3.972704\n",
      "[INFO] Epoch: 34 , batch: 281 , training loss: 4.092572\n",
      "[INFO] Epoch: 34 , batch: 282 , training loss: 3.999499\n",
      "[INFO] Epoch: 34 , batch: 283 , training loss: 4.008557\n",
      "[INFO] Epoch: 34 , batch: 284 , training loss: 4.054192\n",
      "[INFO] Epoch: 34 , batch: 285 , training loss: 3.983972\n",
      "[INFO] Epoch: 34 , batch: 286 , training loss: 3.991067\n",
      "[INFO] Epoch: 34 , batch: 287 , training loss: 3.936685\n",
      "[INFO] Epoch: 34 , batch: 288 , training loss: 3.890824\n",
      "[INFO] Epoch: 34 , batch: 289 , training loss: 3.962865\n",
      "[INFO] Epoch: 34 , batch: 290 , training loss: 3.754522\n",
      "[INFO] Epoch: 34 , batch: 291 , training loss: 3.744638\n",
      "[INFO] Epoch: 34 , batch: 292 , training loss: 3.837000\n",
      "[INFO] Epoch: 34 , batch: 293 , training loss: 3.778554\n",
      "[INFO] Epoch: 34 , batch: 294 , training loss: 4.433390\n",
      "[INFO] Epoch: 34 , batch: 295 , training loss: 4.233375\n",
      "[INFO] Epoch: 34 , batch: 296 , training loss: 4.160780\n",
      "[INFO] Epoch: 34 , batch: 297 , training loss: 4.090715\n",
      "[INFO] Epoch: 34 , batch: 298 , training loss: 3.947509\n",
      "[INFO] Epoch: 34 , batch: 299 , training loss: 3.984867\n",
      "[INFO] Epoch: 34 , batch: 300 , training loss: 3.969891\n",
      "[INFO] Epoch: 34 , batch: 301 , training loss: 3.873813\n",
      "[INFO] Epoch: 34 , batch: 302 , training loss: 4.062434\n",
      "[INFO] Epoch: 34 , batch: 303 , training loss: 4.078511\n",
      "[INFO] Epoch: 34 , batch: 304 , training loss: 4.214096\n",
      "[INFO] Epoch: 34 , batch: 305 , training loss: 4.033073\n",
      "[INFO] Epoch: 34 , batch: 306 , training loss: 4.156264\n",
      "[INFO] Epoch: 34 , batch: 307 , training loss: 4.165352\n",
      "[INFO] Epoch: 34 , batch: 308 , training loss: 3.962696\n",
      "[INFO] Epoch: 34 , batch: 309 , training loss: 3.982221\n",
      "[INFO] Epoch: 34 , batch: 310 , training loss: 3.908987\n",
      "[INFO] Epoch: 34 , batch: 311 , training loss: 3.910920\n",
      "[INFO] Epoch: 34 , batch: 312 , training loss: 3.813224\n",
      "[INFO] Epoch: 34 , batch: 313 , training loss: 3.908134\n",
      "[INFO] Epoch: 34 , batch: 314 , training loss: 3.963227\n",
      "[INFO] Epoch: 34 , batch: 315 , training loss: 4.067354\n",
      "[INFO] Epoch: 34 , batch: 316 , training loss: 4.293647\n",
      "[INFO] Epoch: 34 , batch: 317 , training loss: 4.668778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 34 , batch: 318 , training loss: 4.765192\n",
      "[INFO] Epoch: 34 , batch: 319 , training loss: 4.464533\n",
      "[INFO] Epoch: 34 , batch: 320 , training loss: 4.008420\n",
      "[INFO] Epoch: 34 , batch: 321 , training loss: 3.824811\n",
      "[INFO] Epoch: 34 , batch: 322 , training loss: 3.927377\n",
      "[INFO] Epoch: 34 , batch: 323 , training loss: 3.977174\n",
      "[INFO] Epoch: 34 , batch: 324 , training loss: 3.956572\n",
      "[INFO] Epoch: 34 , batch: 325 , training loss: 4.065741\n",
      "[INFO] Epoch: 34 , batch: 326 , training loss: 4.114354\n",
      "[INFO] Epoch: 34 , batch: 327 , training loss: 4.057064\n",
      "[INFO] Epoch: 34 , batch: 328 , training loss: 4.042120\n",
      "[INFO] Epoch: 34 , batch: 329 , training loss: 3.963194\n",
      "[INFO] Epoch: 34 , batch: 330 , training loss: 3.953103\n",
      "[INFO] Epoch: 34 , batch: 331 , training loss: 4.099089\n",
      "[INFO] Epoch: 34 , batch: 332 , training loss: 3.974853\n",
      "[INFO] Epoch: 34 , batch: 333 , training loss: 3.924758\n",
      "[INFO] Epoch: 34 , batch: 334 , training loss: 3.948000\n",
      "[INFO] Epoch: 34 , batch: 335 , training loss: 4.059168\n",
      "[INFO] Epoch: 34 , batch: 336 , training loss: 4.068596\n",
      "[INFO] Epoch: 34 , batch: 337 , training loss: 4.125885\n",
      "[INFO] Epoch: 34 , batch: 338 , training loss: 4.333065\n",
      "[INFO] Epoch: 34 , batch: 339 , training loss: 4.165239\n",
      "[INFO] Epoch: 34 , batch: 340 , training loss: 4.337949\n",
      "[INFO] Epoch: 34 , batch: 341 , training loss: 4.080922\n",
      "[INFO] Epoch: 34 , batch: 342 , training loss: 3.896467\n",
      "[INFO] Epoch: 34 , batch: 343 , training loss: 3.947485\n",
      "[INFO] Epoch: 34 , batch: 344 , training loss: 3.832737\n",
      "[INFO] Epoch: 34 , batch: 345 , training loss: 3.949559\n",
      "[INFO] Epoch: 34 , batch: 346 , training loss: 3.992452\n",
      "[INFO] Epoch: 34 , batch: 347 , training loss: 3.918766\n",
      "[INFO] Epoch: 34 , batch: 348 , training loss: 4.011365\n",
      "[INFO] Epoch: 34 , batch: 349 , training loss: 4.106692\n",
      "[INFO] Epoch: 34 , batch: 350 , training loss: 3.937455\n",
      "[INFO] Epoch: 34 , batch: 351 , training loss: 4.056608\n",
      "[INFO] Epoch: 34 , batch: 352 , training loss: 4.042710\n",
      "[INFO] Epoch: 34 , batch: 353 , training loss: 4.018400\n",
      "[INFO] Epoch: 34 , batch: 354 , training loss: 4.135628\n",
      "[INFO] Epoch: 34 , batch: 355 , training loss: 4.096723\n",
      "[INFO] Epoch: 34 , batch: 356 , training loss: 3.987468\n",
      "[INFO] Epoch: 34 , batch: 357 , training loss: 4.061092\n",
      "[INFO] Epoch: 34 , batch: 358 , training loss: 3.961173\n",
      "[INFO] Epoch: 34 , batch: 359 , training loss: 3.964001\n",
      "[INFO] Epoch: 34 , batch: 360 , training loss: 4.086375\n",
      "[INFO] Epoch: 34 , batch: 361 , training loss: 4.049901\n",
      "[INFO] Epoch: 34 , batch: 362 , training loss: 4.142498\n",
      "[INFO] Epoch: 34 , batch: 363 , training loss: 4.035945\n",
      "[INFO] Epoch: 34 , batch: 364 , training loss: 4.089927\n",
      "[INFO] Epoch: 34 , batch: 365 , training loss: 4.012141\n",
      "[INFO] Epoch: 34 , batch: 366 , training loss: 4.099547\n",
      "[INFO] Epoch: 34 , batch: 367 , training loss: 4.142878\n",
      "[INFO] Epoch: 34 , batch: 368 , training loss: 4.522796\n",
      "[INFO] Epoch: 34 , batch: 369 , training loss: 4.218648\n",
      "[INFO] Epoch: 34 , batch: 370 , training loss: 4.011557\n",
      "[INFO] Epoch: 34 , batch: 371 , training loss: 4.438191\n",
      "[INFO] Epoch: 34 , batch: 372 , training loss: 4.622266\n",
      "[INFO] Epoch: 34 , batch: 373 , training loss: 4.727468\n",
      "[INFO] Epoch: 34 , batch: 374 , training loss: 4.848725\n",
      "[INFO] Epoch: 34 , batch: 375 , training loss: 4.814117\n",
      "[INFO] Epoch: 34 , batch: 376 , training loss: 4.688918\n",
      "[INFO] Epoch: 34 , batch: 377 , training loss: 4.455256\n",
      "[INFO] Epoch: 34 , batch: 378 , training loss: 4.543046\n",
      "[INFO] Epoch: 34 , batch: 379 , training loss: 4.519180\n",
      "[INFO] Epoch: 34 , batch: 380 , training loss: 4.689183\n",
      "[INFO] Epoch: 34 , batch: 381 , training loss: 4.391018\n",
      "[INFO] Epoch: 34 , batch: 382 , training loss: 4.662080\n",
      "[INFO] Epoch: 34 , batch: 383 , training loss: 4.693265\n",
      "[INFO] Epoch: 34 , batch: 384 , training loss: 4.667040\n",
      "[INFO] Epoch: 34 , batch: 385 , training loss: 4.349377\n",
      "[INFO] Epoch: 34 , batch: 386 , training loss: 4.599470\n",
      "[INFO] Epoch: 34 , batch: 387 , training loss: 4.558844\n",
      "[INFO] Epoch: 34 , batch: 388 , training loss: 4.371135\n",
      "[INFO] Epoch: 34 , batch: 389 , training loss: 4.175223\n",
      "[INFO] Epoch: 34 , batch: 390 , training loss: 4.206502\n",
      "[INFO] Epoch: 34 , batch: 391 , training loss: 4.239617\n",
      "[INFO] Epoch: 34 , batch: 392 , training loss: 4.600144\n",
      "[INFO] Epoch: 34 , batch: 393 , training loss: 4.493920\n",
      "[INFO] Epoch: 34 , batch: 394 , training loss: 4.563057\n",
      "[INFO] Epoch: 34 , batch: 395 , training loss: 4.401769\n",
      "[INFO] Epoch: 34 , batch: 396 , training loss: 4.227947\n",
      "[INFO] Epoch: 34 , batch: 397 , training loss: 4.357491\n",
      "[INFO] Epoch: 34 , batch: 398 , training loss: 4.240234\n",
      "[INFO] Epoch: 34 , batch: 399 , training loss: 4.300643\n",
      "[INFO] Epoch: 34 , batch: 400 , training loss: 4.268842\n",
      "[INFO] Epoch: 34 , batch: 401 , training loss: 4.705363\n",
      "[INFO] Epoch: 34 , batch: 402 , training loss: 4.413972\n",
      "[INFO] Epoch: 34 , batch: 403 , training loss: 4.258667\n",
      "[INFO] Epoch: 34 , batch: 404 , training loss: 4.429271\n",
      "[INFO] Epoch: 34 , batch: 405 , training loss: 4.478655\n",
      "[INFO] Epoch: 34 , batch: 406 , training loss: 4.371293\n",
      "[INFO] Epoch: 34 , batch: 407 , training loss: 4.430299\n",
      "[INFO] Epoch: 34 , batch: 408 , training loss: 4.393378\n",
      "[INFO] Epoch: 34 , batch: 409 , training loss: 4.400675\n",
      "[INFO] Epoch: 34 , batch: 410 , training loss: 4.483283\n",
      "[INFO] Epoch: 34 , batch: 411 , training loss: 4.622002\n",
      "[INFO] Epoch: 34 , batch: 412 , training loss: 4.462811\n",
      "[INFO] Epoch: 34 , batch: 413 , training loss: 4.335982\n",
      "[INFO] Epoch: 34 , batch: 414 , training loss: 4.378367\n",
      "[INFO] Epoch: 34 , batch: 415 , training loss: 4.408860\n",
      "[INFO] Epoch: 34 , batch: 416 , training loss: 4.481724\n",
      "[INFO] Epoch: 34 , batch: 417 , training loss: 4.392120\n",
      "[INFO] Epoch: 34 , batch: 418 , training loss: 4.451363\n",
      "[INFO] Epoch: 34 , batch: 419 , training loss: 4.422040\n",
      "[INFO] Epoch: 34 , batch: 420 , training loss: 4.373370\n",
      "[INFO] Epoch: 34 , batch: 421 , training loss: 4.352732\n",
      "[INFO] Epoch: 34 , batch: 422 , training loss: 4.205044\n",
      "[INFO] Epoch: 34 , batch: 423 , training loss: 4.432568\n",
      "[INFO] Epoch: 34 , batch: 424 , training loss: 4.623130\n",
      "[INFO] Epoch: 34 , batch: 425 , training loss: 4.457860\n",
      "[INFO] Epoch: 34 , batch: 426 , training loss: 4.210706\n",
      "[INFO] Epoch: 34 , batch: 427 , training loss: 4.452927\n",
      "[INFO] Epoch: 34 , batch: 428 , training loss: 4.303688\n",
      "[INFO] Epoch: 34 , batch: 429 , training loss: 4.218146\n",
      "[INFO] Epoch: 34 , batch: 430 , training loss: 4.440681\n",
      "[INFO] Epoch: 34 , batch: 431 , training loss: 4.045274\n",
      "[INFO] Epoch: 34 , batch: 432 , training loss: 4.112954\n",
      "[INFO] Epoch: 34 , batch: 433 , training loss: 4.160534\n",
      "[INFO] Epoch: 34 , batch: 434 , training loss: 4.035928\n",
      "[INFO] Epoch: 34 , batch: 435 , training loss: 4.393959\n",
      "[INFO] Epoch: 34 , batch: 436 , training loss: 4.415850\n",
      "[INFO] Epoch: 34 , batch: 437 , training loss: 4.206685\n",
      "[INFO] Epoch: 34 , batch: 438 , training loss: 4.058771\n",
      "[INFO] Epoch: 34 , batch: 439 , training loss: 4.323110\n",
      "[INFO] Epoch: 34 , batch: 440 , training loss: 4.433935\n",
      "[INFO] Epoch: 34 , batch: 441 , training loss: 4.516222\n",
      "[INFO] Epoch: 34 , batch: 442 , training loss: 4.277032\n",
      "[INFO] Epoch: 34 , batch: 443 , training loss: 4.478940\n",
      "[INFO] Epoch: 34 , batch: 444 , training loss: 4.072108\n",
      "[INFO] Epoch: 34 , batch: 445 , training loss: 3.973245\n",
      "[INFO] Epoch: 34 , batch: 446 , training loss: 3.913787\n",
      "[INFO] Epoch: 34 , batch: 447 , training loss: 4.108222\n",
      "[INFO] Epoch: 34 , batch: 448 , training loss: 4.224513\n",
      "[INFO] Epoch: 34 , batch: 449 , training loss: 4.590315\n",
      "[INFO] Epoch: 34 , batch: 450 , training loss: 4.671470\n",
      "[INFO] Epoch: 34 , batch: 451 , training loss: 4.541399\n",
      "[INFO] Epoch: 34 , batch: 452 , training loss: 4.377833\n",
      "[INFO] Epoch: 34 , batch: 453 , training loss: 4.149440\n",
      "[INFO] Epoch: 34 , batch: 454 , training loss: 4.301375\n",
      "[INFO] Epoch: 34 , batch: 455 , training loss: 4.364289\n",
      "[INFO] Epoch: 34 , batch: 456 , training loss: 4.342156\n",
      "[INFO] Epoch: 34 , batch: 457 , training loss: 4.424474\n",
      "[INFO] Epoch: 34 , batch: 458 , training loss: 4.173044\n",
      "[INFO] Epoch: 34 , batch: 459 , training loss: 4.138216\n",
      "[INFO] Epoch: 34 , batch: 460 , training loss: 4.254267\n",
      "[INFO] Epoch: 34 , batch: 461 , training loss: 4.221141\n",
      "[INFO] Epoch: 34 , batch: 462 , training loss: 4.277324\n",
      "[INFO] Epoch: 34 , batch: 463 , training loss: 4.199403\n",
      "[INFO] Epoch: 34 , batch: 464 , training loss: 4.377072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 34 , batch: 465 , training loss: 4.314412\n",
      "[INFO] Epoch: 34 , batch: 466 , training loss: 4.425302\n",
      "[INFO] Epoch: 34 , batch: 467 , training loss: 4.356271\n",
      "[INFO] Epoch: 34 , batch: 468 , training loss: 4.338089\n",
      "[INFO] Epoch: 34 , batch: 469 , training loss: 4.345575\n",
      "[INFO] Epoch: 34 , batch: 470 , training loss: 4.196441\n",
      "[INFO] Epoch: 34 , batch: 471 , training loss: 4.290296\n",
      "[INFO] Epoch: 34 , batch: 472 , training loss: 4.350308\n",
      "[INFO] Epoch: 34 , batch: 473 , training loss: 4.271887\n",
      "[INFO] Epoch: 34 , batch: 474 , training loss: 4.026095\n",
      "[INFO] Epoch: 34 , batch: 475 , training loss: 3.929250\n",
      "[INFO] Epoch: 34 , batch: 476 , training loss: 4.334669\n",
      "[INFO] Epoch: 34 , batch: 477 , training loss: 4.428618\n",
      "[INFO] Epoch: 34 , batch: 478 , training loss: 4.426598\n",
      "[INFO] Epoch: 34 , batch: 479 , training loss: 4.408574\n",
      "[INFO] Epoch: 34 , batch: 480 , training loss: 4.546876\n",
      "[INFO] Epoch: 34 , batch: 481 , training loss: 4.418316\n",
      "[INFO] Epoch: 34 , batch: 482 , training loss: 4.543960\n",
      "[INFO] Epoch: 34 , batch: 483 , training loss: 4.379299\n",
      "[INFO] Epoch: 34 , batch: 484 , training loss: 4.173508\n",
      "[INFO] Epoch: 34 , batch: 485 , training loss: 4.254083\n",
      "[INFO] Epoch: 34 , batch: 486 , training loss: 4.166582\n",
      "[INFO] Epoch: 34 , batch: 487 , training loss: 4.156558\n",
      "[INFO] Epoch: 34 , batch: 488 , training loss: 4.320504\n",
      "[INFO] Epoch: 34 , batch: 489 , training loss: 4.227454\n",
      "[INFO] Epoch: 34 , batch: 490 , training loss: 4.289684\n",
      "[INFO] Epoch: 34 , batch: 491 , training loss: 4.212133\n",
      "[INFO] Epoch: 34 , batch: 492 , training loss: 4.198921\n",
      "[INFO] Epoch: 34 , batch: 493 , training loss: 4.338031\n",
      "[INFO] Epoch: 34 , batch: 494 , training loss: 4.261536\n",
      "[INFO] Epoch: 34 , batch: 495 , training loss: 4.435621\n",
      "[INFO] Epoch: 34 , batch: 496 , training loss: 4.289842\n",
      "[INFO] Epoch: 34 , batch: 497 , training loss: 4.351642\n",
      "[INFO] Epoch: 34 , batch: 498 , training loss: 4.308918\n",
      "[INFO] Epoch: 34 , batch: 499 , training loss: 4.396198\n",
      "[INFO] Epoch: 34 , batch: 500 , training loss: 4.484528\n",
      "[INFO] Epoch: 34 , batch: 501 , training loss: 4.844007\n",
      "[INFO] Epoch: 34 , batch: 502 , training loss: 4.862826\n",
      "[INFO] Epoch: 34 , batch: 503 , training loss: 4.538080\n",
      "[INFO] Epoch: 34 , batch: 504 , training loss: 4.679410\n",
      "[INFO] Epoch: 34 , batch: 505 , training loss: 4.648867\n",
      "[INFO] Epoch: 34 , batch: 506 , training loss: 4.615334\n",
      "[INFO] Epoch: 34 , batch: 507 , training loss: 4.666319\n",
      "[INFO] Epoch: 34 , batch: 508 , training loss: 4.569873\n",
      "[INFO] Epoch: 34 , batch: 509 , training loss: 4.397661\n",
      "[INFO] Epoch: 34 , batch: 510 , training loss: 4.492307\n",
      "[INFO] Epoch: 34 , batch: 511 , training loss: 4.397573\n",
      "[INFO] Epoch: 34 , batch: 512 , training loss: 4.473327\n",
      "[INFO] Epoch: 34 , batch: 513 , training loss: 4.757792\n",
      "[INFO] Epoch: 34 , batch: 514 , training loss: 4.368505\n",
      "[INFO] Epoch: 34 , batch: 515 , training loss: 4.650455\n",
      "[INFO] Epoch: 34 , batch: 516 , training loss: 4.435934\n",
      "[INFO] Epoch: 34 , batch: 517 , training loss: 4.416072\n",
      "[INFO] Epoch: 34 , batch: 518 , training loss: 4.372195\n",
      "[INFO] Epoch: 34 , batch: 519 , training loss: 4.212235\n",
      "[INFO] Epoch: 34 , batch: 520 , training loss: 4.436367\n",
      "[INFO] Epoch: 34 , batch: 521 , training loss: 4.415247\n",
      "[INFO] Epoch: 34 , batch: 522 , training loss: 4.523231\n",
      "[INFO] Epoch: 34 , batch: 523 , training loss: 4.418471\n",
      "[INFO] Epoch: 34 , batch: 524 , training loss: 4.695258\n",
      "[INFO] Epoch: 34 , batch: 525 , training loss: 4.581576\n",
      "[INFO] Epoch: 34 , batch: 526 , training loss: 4.376584\n",
      "[INFO] Epoch: 34 , batch: 527 , training loss: 4.423541\n",
      "[INFO] Epoch: 34 , batch: 528 , training loss: 4.437752\n",
      "[INFO] Epoch: 34 , batch: 529 , training loss: 4.406917\n",
      "[INFO] Epoch: 34 , batch: 530 , training loss: 4.261806\n",
      "[INFO] Epoch: 34 , batch: 531 , training loss: 4.401296\n",
      "[INFO] Epoch: 34 , batch: 532 , training loss: 4.298878\n",
      "[INFO] Epoch: 34 , batch: 533 , training loss: 4.452411\n",
      "[INFO] Epoch: 34 , batch: 534 , training loss: 4.441520\n",
      "[INFO] Epoch: 34 , batch: 535 , training loss: 4.442359\n",
      "[INFO] Epoch: 34 , batch: 536 , training loss: 4.295571\n",
      "[INFO] Epoch: 34 , batch: 537 , training loss: 4.264830\n",
      "[INFO] Epoch: 34 , batch: 538 , training loss: 4.369339\n",
      "[INFO] Epoch: 34 , batch: 539 , training loss: 4.461236\n",
      "[INFO] Epoch: 34 , batch: 540 , training loss: 4.968843\n",
      "[INFO] Epoch: 34 , batch: 541 , training loss: 4.801869\n",
      "[INFO] Epoch: 34 , batch: 542 , training loss: 4.691943\n",
      "[INFO] Epoch: 35 , batch: 0 , training loss: 3.649214\n",
      "[INFO] Epoch: 35 , batch: 1 , training loss: 3.513430\n",
      "[INFO] Epoch: 35 , batch: 2 , training loss: 3.693841\n",
      "[INFO] Epoch: 35 , batch: 3 , training loss: 3.547632\n",
      "[INFO] Epoch: 35 , batch: 4 , training loss: 3.880406\n",
      "[INFO] Epoch: 35 , batch: 5 , training loss: 3.578388\n",
      "[INFO] Epoch: 35 , batch: 6 , training loss: 3.896687\n",
      "[INFO] Epoch: 35 , batch: 7 , training loss: 3.827332\n",
      "[INFO] Epoch: 35 , batch: 8 , training loss: 3.538634\n",
      "[INFO] Epoch: 35 , batch: 9 , training loss: 3.788975\n",
      "[INFO] Epoch: 35 , batch: 10 , training loss: 3.723271\n",
      "[INFO] Epoch: 35 , batch: 11 , training loss: 3.654214\n",
      "[INFO] Epoch: 35 , batch: 12 , training loss: 3.537482\n",
      "[INFO] Epoch: 35 , batch: 13 , training loss: 3.582119\n",
      "[INFO] Epoch: 35 , batch: 14 , training loss: 3.478770\n",
      "[INFO] Epoch: 35 , batch: 15 , training loss: 3.706360\n",
      "[INFO] Epoch: 35 , batch: 16 , training loss: 3.549855\n",
      "[INFO] Epoch: 35 , batch: 17 , training loss: 3.707415\n",
      "[INFO] Epoch: 35 , batch: 18 , training loss: 3.657654\n",
      "[INFO] Epoch: 35 , batch: 19 , training loss: 3.411541\n",
      "[INFO] Epoch: 35 , batch: 20 , training loss: 3.385464\n",
      "[INFO] Epoch: 35 , batch: 21 , training loss: 3.544815\n",
      "[INFO] Epoch: 35 , batch: 22 , training loss: 3.402961\n",
      "[INFO] Epoch: 35 , batch: 23 , training loss: 3.577531\n",
      "[INFO] Epoch: 35 , batch: 24 , training loss: 3.464824\n",
      "[INFO] Epoch: 35 , batch: 25 , training loss: 3.579306\n",
      "[INFO] Epoch: 35 , batch: 26 , training loss: 3.473844\n",
      "[INFO] Epoch: 35 , batch: 27 , training loss: 3.433594\n",
      "[INFO] Epoch: 35 , batch: 28 , training loss: 3.605351\n",
      "[INFO] Epoch: 35 , batch: 29 , training loss: 3.416345\n",
      "[INFO] Epoch: 35 , batch: 30 , training loss: 3.427128\n",
      "[INFO] Epoch: 35 , batch: 31 , training loss: 3.546655\n",
      "[INFO] Epoch: 35 , batch: 32 , training loss: 3.531592\n",
      "[INFO] Epoch: 35 , batch: 33 , training loss: 3.579211\n",
      "[INFO] Epoch: 35 , batch: 34 , training loss: 3.550801\n",
      "[INFO] Epoch: 35 , batch: 35 , training loss: 3.492109\n",
      "[INFO] Epoch: 35 , batch: 36 , training loss: 3.629718\n",
      "[INFO] Epoch: 35 , batch: 37 , training loss: 3.426784\n",
      "[INFO] Epoch: 35 , batch: 38 , training loss: 3.533803\n",
      "[INFO] Epoch: 35 , batch: 39 , training loss: 3.343048\n",
      "[INFO] Epoch: 35 , batch: 40 , training loss: 3.539975\n",
      "[INFO] Epoch: 35 , batch: 41 , training loss: 3.469905\n",
      "[INFO] Epoch: 35 , batch: 42 , training loss: 3.934703\n",
      "[INFO] Epoch: 35 , batch: 43 , training loss: 3.715472\n",
      "[INFO] Epoch: 35 , batch: 44 , training loss: 4.041684\n",
      "[INFO] Epoch: 35 , batch: 45 , training loss: 3.967458\n",
      "[INFO] Epoch: 35 , batch: 46 , training loss: 3.875268\n",
      "[INFO] Epoch: 35 , batch: 47 , training loss: 3.568884\n",
      "[INFO] Epoch: 35 , batch: 48 , training loss: 3.505930\n",
      "[INFO] Epoch: 35 , batch: 49 , training loss: 3.803817\n",
      "[INFO] Epoch: 35 , batch: 50 , training loss: 3.570725\n",
      "[INFO] Epoch: 35 , batch: 51 , training loss: 3.789416\n",
      "[INFO] Epoch: 35 , batch: 52 , training loss: 3.634935\n",
      "[INFO] Epoch: 35 , batch: 53 , training loss: 3.722825\n",
      "[INFO] Epoch: 35 , batch: 54 , training loss: 3.747714\n",
      "[INFO] Epoch: 35 , batch: 55 , training loss: 3.832874\n",
      "[INFO] Epoch: 35 , batch: 56 , training loss: 3.645138\n",
      "[INFO] Epoch: 35 , batch: 57 , training loss: 3.563845\n",
      "[INFO] Epoch: 35 , batch: 58 , training loss: 3.661306\n",
      "[INFO] Epoch: 35 , batch: 59 , training loss: 3.746786\n",
      "[INFO] Epoch: 35 , batch: 60 , training loss: 3.667259\n",
      "[INFO] Epoch: 35 , batch: 61 , training loss: 3.725799\n",
      "[INFO] Epoch: 35 , batch: 62 , training loss: 3.625947\n",
      "[INFO] Epoch: 35 , batch: 63 , training loss: 3.809437\n",
      "[INFO] Epoch: 35 , batch: 64 , training loss: 4.041828\n",
      "[INFO] Epoch: 35 , batch: 65 , training loss: 3.719045\n",
      "[INFO] Epoch: 35 , batch: 66 , training loss: 3.570825\n",
      "[INFO] Epoch: 35 , batch: 67 , training loss: 3.607605\n",
      "[INFO] Epoch: 35 , batch: 68 , training loss: 3.750670\n",
      "[INFO] Epoch: 35 , batch: 69 , training loss: 3.692173\n",
      "[INFO] Epoch: 35 , batch: 70 , training loss: 3.919116\n",
      "[INFO] Epoch: 35 , batch: 71 , training loss: 3.768007\n",
      "[INFO] Epoch: 35 , batch: 72 , training loss: 3.861394\n",
      "[INFO] Epoch: 35 , batch: 73 , training loss: 3.784704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 35 , batch: 74 , training loss: 3.892614\n",
      "[INFO] Epoch: 35 , batch: 75 , training loss: 3.759027\n",
      "[INFO] Epoch: 35 , batch: 76 , training loss: 3.865823\n",
      "[INFO] Epoch: 35 , batch: 77 , training loss: 3.796015\n",
      "[INFO] Epoch: 35 , batch: 78 , training loss: 3.894827\n",
      "[INFO] Epoch: 35 , batch: 79 , training loss: 3.775017\n",
      "[INFO] Epoch: 35 , batch: 80 , training loss: 3.970923\n",
      "[INFO] Epoch: 35 , batch: 81 , training loss: 3.870256\n",
      "[INFO] Epoch: 35 , batch: 82 , training loss: 3.865292\n",
      "[INFO] Epoch: 35 , batch: 83 , training loss: 3.933715\n",
      "[INFO] Epoch: 35 , batch: 84 , training loss: 3.916046\n",
      "[INFO] Epoch: 35 , batch: 85 , training loss: 4.021913\n",
      "[INFO] Epoch: 35 , batch: 86 , training loss: 3.907251\n",
      "[INFO] Epoch: 35 , batch: 87 , training loss: 3.890787\n",
      "[INFO] Epoch: 35 , batch: 88 , training loss: 4.006386\n",
      "[INFO] Epoch: 35 , batch: 89 , training loss: 3.816726\n",
      "[INFO] Epoch: 35 , batch: 90 , training loss: 3.935230\n",
      "[INFO] Epoch: 35 , batch: 91 , training loss: 3.832564\n",
      "[INFO] Epoch: 35 , batch: 92 , training loss: 3.877166\n",
      "[INFO] Epoch: 35 , batch: 93 , training loss: 3.960774\n",
      "[INFO] Epoch: 35 , batch: 94 , training loss: 4.097520\n",
      "[INFO] Epoch: 35 , batch: 95 , training loss: 3.883482\n",
      "[INFO] Epoch: 35 , batch: 96 , training loss: 3.848167\n",
      "[INFO] Epoch: 35 , batch: 97 , training loss: 3.807999\n",
      "[INFO] Epoch: 35 , batch: 98 , training loss: 3.733761\n",
      "[INFO] Epoch: 35 , batch: 99 , training loss: 3.860850\n",
      "[INFO] Epoch: 35 , batch: 100 , training loss: 3.750471\n",
      "[INFO] Epoch: 35 , batch: 101 , training loss: 3.762510\n",
      "[INFO] Epoch: 35 , batch: 102 , training loss: 3.947610\n",
      "[INFO] Epoch: 35 , batch: 103 , training loss: 3.728922\n",
      "[INFO] Epoch: 35 , batch: 104 , training loss: 3.687565\n",
      "[INFO] Epoch: 35 , batch: 105 , training loss: 3.945032\n",
      "[INFO] Epoch: 35 , batch: 106 , training loss: 3.967939\n",
      "[INFO] Epoch: 35 , batch: 107 , training loss: 3.784438\n",
      "[INFO] Epoch: 35 , batch: 108 , training loss: 3.744064\n",
      "[INFO] Epoch: 35 , batch: 109 , training loss: 3.676480\n",
      "[INFO] Epoch: 35 , batch: 110 , training loss: 3.840265\n",
      "[INFO] Epoch: 35 , batch: 111 , training loss: 3.936692\n",
      "[INFO] Epoch: 35 , batch: 112 , training loss: 3.823887\n",
      "[INFO] Epoch: 35 , batch: 113 , training loss: 3.851570\n",
      "[INFO] Epoch: 35 , batch: 114 , training loss: 3.812382\n",
      "[INFO] Epoch: 35 , batch: 115 , training loss: 3.830948\n",
      "[INFO] Epoch: 35 , batch: 116 , training loss: 3.724940\n",
      "[INFO] Epoch: 35 , batch: 117 , training loss: 3.951386\n",
      "[INFO] Epoch: 35 , batch: 118 , training loss: 3.911367\n",
      "[INFO] Epoch: 35 , batch: 119 , training loss: 4.062720\n",
      "[INFO] Epoch: 35 , batch: 120 , training loss: 4.032776\n",
      "[INFO] Epoch: 35 , batch: 121 , training loss: 3.907952\n",
      "[INFO] Epoch: 35 , batch: 122 , training loss: 3.815892\n",
      "[INFO] Epoch: 35 , batch: 123 , training loss: 3.818764\n",
      "[INFO] Epoch: 35 , batch: 124 , training loss: 3.938665\n",
      "[INFO] Epoch: 35 , batch: 125 , training loss: 3.745357\n",
      "[INFO] Epoch: 35 , batch: 126 , training loss: 3.774849\n",
      "[INFO] Epoch: 35 , batch: 127 , training loss: 3.753966\n",
      "[INFO] Epoch: 35 , batch: 128 , training loss: 3.886350\n",
      "[INFO] Epoch: 35 , batch: 129 , training loss: 3.854201\n",
      "[INFO] Epoch: 35 , batch: 130 , training loss: 3.826274\n",
      "[INFO] Epoch: 35 , batch: 131 , training loss: 3.852328\n",
      "[INFO] Epoch: 35 , batch: 132 , training loss: 3.896462\n",
      "[INFO] Epoch: 35 , batch: 133 , training loss: 3.850240\n",
      "[INFO] Epoch: 35 , batch: 134 , training loss: 3.609388\n",
      "[INFO] Epoch: 35 , batch: 135 , training loss: 3.677082\n",
      "[INFO] Epoch: 35 , batch: 136 , training loss: 3.951891\n",
      "[INFO] Epoch: 35 , batch: 137 , training loss: 3.871327\n",
      "[INFO] Epoch: 35 , batch: 138 , training loss: 3.927081\n",
      "[INFO] Epoch: 35 , batch: 139 , training loss: 4.451236\n",
      "[INFO] Epoch: 35 , batch: 140 , training loss: 4.259324\n",
      "[INFO] Epoch: 35 , batch: 141 , training loss: 4.044819\n",
      "[INFO] Epoch: 35 , batch: 142 , training loss: 3.739369\n",
      "[INFO] Epoch: 35 , batch: 143 , training loss: 3.903977\n",
      "[INFO] Epoch: 35 , batch: 144 , training loss: 3.751894\n",
      "[INFO] Epoch: 35 , batch: 145 , training loss: 3.828665\n",
      "[INFO] Epoch: 35 , batch: 146 , training loss: 4.016790\n",
      "[INFO] Epoch: 35 , batch: 147 , training loss: 3.658351\n",
      "[INFO] Epoch: 35 , batch: 148 , training loss: 3.629816\n",
      "[INFO] Epoch: 35 , batch: 149 , training loss: 3.735952\n",
      "[INFO] Epoch: 35 , batch: 150 , training loss: 4.014863\n",
      "[INFO] Epoch: 35 , batch: 151 , training loss: 3.839584\n",
      "[INFO] Epoch: 35 , batch: 152 , training loss: 3.851089\n",
      "[INFO] Epoch: 35 , batch: 153 , training loss: 3.856864\n",
      "[INFO] Epoch: 35 , batch: 154 , training loss: 3.949852\n",
      "[INFO] Epoch: 35 , batch: 155 , training loss: 4.161357\n",
      "[INFO] Epoch: 35 , batch: 156 , training loss: 3.915421\n",
      "[INFO] Epoch: 35 , batch: 157 , training loss: 3.883458\n",
      "[INFO] Epoch: 35 , batch: 158 , training loss: 3.984460\n",
      "[INFO] Epoch: 35 , batch: 159 , training loss: 3.928037\n",
      "[INFO] Epoch: 35 , batch: 160 , training loss: 4.157954\n",
      "[INFO] Epoch: 35 , batch: 161 , training loss: 4.264645\n",
      "[INFO] Epoch: 35 , batch: 162 , training loss: 4.201774\n",
      "[INFO] Epoch: 35 , batch: 163 , training loss: 4.404835\n",
      "[INFO] Epoch: 35 , batch: 164 , training loss: 4.355957\n",
      "[INFO] Epoch: 35 , batch: 165 , training loss: 4.266523\n",
      "[INFO] Epoch: 35 , batch: 166 , training loss: 4.108753\n",
      "[INFO] Epoch: 35 , batch: 167 , training loss: 4.191694\n",
      "[INFO] Epoch: 35 , batch: 168 , training loss: 3.815334\n",
      "[INFO] Epoch: 35 , batch: 169 , training loss: 3.835522\n",
      "[INFO] Epoch: 35 , batch: 170 , training loss: 3.992808\n",
      "[INFO] Epoch: 35 , batch: 171 , training loss: 3.437736\n",
      "[INFO] Epoch: 35 , batch: 172 , training loss: 3.653907\n",
      "[INFO] Epoch: 35 , batch: 173 , training loss: 3.991109\n",
      "[INFO] Epoch: 35 , batch: 174 , training loss: 4.470030\n",
      "[INFO] Epoch: 35 , batch: 175 , training loss: 4.749005\n",
      "[INFO] Epoch: 35 , batch: 176 , training loss: 4.426095\n",
      "[INFO] Epoch: 35 , batch: 177 , training loss: 4.026523\n",
      "[INFO] Epoch: 35 , batch: 178 , training loss: 4.078898\n",
      "[INFO] Epoch: 35 , batch: 179 , training loss: 4.148414\n",
      "[INFO] Epoch: 35 , batch: 180 , training loss: 4.105323\n",
      "[INFO] Epoch: 35 , batch: 181 , training loss: 4.345044\n",
      "[INFO] Epoch: 35 , batch: 182 , training loss: 4.296904\n",
      "[INFO] Epoch: 35 , batch: 183 , training loss: 4.302544\n",
      "[INFO] Epoch: 35 , batch: 184 , training loss: 4.147826\n",
      "[INFO] Epoch: 35 , batch: 185 , training loss: 4.134544\n",
      "[INFO] Epoch: 35 , batch: 186 , training loss: 4.277198\n",
      "[INFO] Epoch: 35 , batch: 187 , training loss: 4.344385\n",
      "[INFO] Epoch: 35 , batch: 188 , training loss: 4.341215\n",
      "[INFO] Epoch: 35 , batch: 189 , training loss: 4.263667\n",
      "[INFO] Epoch: 35 , batch: 190 , training loss: 4.303854\n",
      "[INFO] Epoch: 35 , batch: 191 , training loss: 4.395257\n",
      "[INFO] Epoch: 35 , batch: 192 , training loss: 4.245368\n",
      "[INFO] Epoch: 35 , batch: 193 , training loss: 4.336640\n",
      "[INFO] Epoch: 35 , batch: 194 , training loss: 4.278312\n",
      "[INFO] Epoch: 35 , batch: 195 , training loss: 4.201910\n",
      "[INFO] Epoch: 35 , batch: 196 , training loss: 4.058506\n",
      "[INFO] Epoch: 35 , batch: 197 , training loss: 4.138013\n",
      "[INFO] Epoch: 35 , batch: 198 , training loss: 4.072831\n",
      "[INFO] Epoch: 35 , batch: 199 , training loss: 4.218181\n",
      "[INFO] Epoch: 35 , batch: 200 , training loss: 4.121077\n",
      "[INFO] Epoch: 35 , batch: 201 , training loss: 4.016047\n",
      "[INFO] Epoch: 35 , batch: 202 , training loss: 4.024773\n",
      "[INFO] Epoch: 35 , batch: 203 , training loss: 4.155175\n",
      "[INFO] Epoch: 35 , batch: 204 , training loss: 4.221527\n",
      "[INFO] Epoch: 35 , batch: 205 , training loss: 3.850060\n",
      "[INFO] Epoch: 35 , batch: 206 , training loss: 3.755694\n",
      "[INFO] Epoch: 35 , batch: 207 , training loss: 3.754807\n",
      "[INFO] Epoch: 35 , batch: 208 , training loss: 4.072821\n",
      "[INFO] Epoch: 35 , batch: 209 , training loss: 4.037454\n",
      "[INFO] Epoch: 35 , batch: 210 , training loss: 4.082821\n",
      "[INFO] Epoch: 35 , batch: 211 , training loss: 4.048956\n",
      "[INFO] Epoch: 35 , batch: 212 , training loss: 4.127137\n",
      "[INFO] Epoch: 35 , batch: 213 , training loss: 4.096394\n",
      "[INFO] Epoch: 35 , batch: 214 , training loss: 4.179283\n",
      "[INFO] Epoch: 35 , batch: 215 , training loss: 4.368853\n",
      "[INFO] Epoch: 35 , batch: 216 , training loss: 4.098311\n",
      "[INFO] Epoch: 35 , batch: 217 , training loss: 4.039113\n",
      "[INFO] Epoch: 35 , batch: 218 , training loss: 4.035033\n",
      "[INFO] Epoch: 35 , batch: 219 , training loss: 4.133127\n",
      "[INFO] Epoch: 35 , batch: 220 , training loss: 3.972326\n",
      "[INFO] Epoch: 35 , batch: 221 , training loss: 3.977003\n",
      "[INFO] Epoch: 35 , batch: 222 , training loss: 4.113752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 35 , batch: 223 , training loss: 4.218127\n",
      "[INFO] Epoch: 35 , batch: 224 , training loss: 4.281819\n",
      "[INFO] Epoch: 35 , batch: 225 , training loss: 4.172050\n",
      "[INFO] Epoch: 35 , batch: 226 , training loss: 4.277870\n",
      "[INFO] Epoch: 35 , batch: 227 , training loss: 4.264202\n",
      "[INFO] Epoch: 35 , batch: 228 , training loss: 4.278067\n",
      "[INFO] Epoch: 35 , batch: 229 , training loss: 4.139383\n",
      "[INFO] Epoch: 35 , batch: 230 , training loss: 3.984666\n",
      "[INFO] Epoch: 35 , batch: 231 , training loss: 3.855997\n",
      "[INFO] Epoch: 35 , batch: 232 , training loss: 4.010991\n",
      "[INFO] Epoch: 35 , batch: 233 , training loss: 4.030157\n",
      "[INFO] Epoch: 35 , batch: 234 , training loss: 3.723961\n",
      "[INFO] Epoch: 35 , batch: 235 , training loss: 3.825337\n",
      "[INFO] Epoch: 35 , batch: 236 , training loss: 3.932311\n",
      "[INFO] Epoch: 35 , batch: 237 , training loss: 4.153251\n",
      "[INFO] Epoch: 35 , batch: 238 , training loss: 3.931041\n",
      "[INFO] Epoch: 35 , batch: 239 , training loss: 3.962413\n",
      "[INFO] Epoch: 35 , batch: 240 , training loss: 4.010997\n",
      "[INFO] Epoch: 35 , batch: 241 , training loss: 3.821265\n",
      "[INFO] Epoch: 35 , batch: 242 , training loss: 3.834110\n",
      "[INFO] Epoch: 35 , batch: 243 , training loss: 4.121152\n",
      "[INFO] Epoch: 35 , batch: 244 , training loss: 4.061054\n",
      "[INFO] Epoch: 35 , batch: 245 , training loss: 4.033221\n",
      "[INFO] Epoch: 35 , batch: 246 , training loss: 3.758564\n",
      "[INFO] Epoch: 35 , batch: 247 , training loss: 3.915469\n",
      "[INFO] Epoch: 35 , batch: 248 , training loss: 3.985099\n",
      "[INFO] Epoch: 35 , batch: 249 , training loss: 3.961292\n",
      "[INFO] Epoch: 35 , batch: 250 , training loss: 3.773677\n",
      "[INFO] Epoch: 35 , batch: 251 , training loss: 4.209736\n",
      "[INFO] Epoch: 35 , batch: 252 , training loss: 3.915236\n",
      "[INFO] Epoch: 35 , batch: 253 , training loss: 3.843262\n",
      "[INFO] Epoch: 35 , batch: 254 , training loss: 4.097334\n",
      "[INFO] Epoch: 35 , batch: 255 , training loss: 4.084637\n",
      "[INFO] Epoch: 35 , batch: 256 , training loss: 4.053465\n",
      "[INFO] Epoch: 35 , batch: 257 , training loss: 4.219509\n",
      "[INFO] Epoch: 35 , batch: 258 , training loss: 4.221686\n",
      "[INFO] Epoch: 35 , batch: 259 , training loss: 4.279353\n",
      "[INFO] Epoch: 35 , batch: 260 , training loss: 4.058427\n",
      "[INFO] Epoch: 35 , batch: 261 , training loss: 4.226703\n",
      "[INFO] Epoch: 35 , batch: 262 , training loss: 4.349480\n",
      "[INFO] Epoch: 35 , batch: 263 , training loss: 4.534650\n",
      "[INFO] Epoch: 35 , batch: 264 , training loss: 3.899497\n",
      "[INFO] Epoch: 35 , batch: 265 , training loss: 4.011500\n",
      "[INFO] Epoch: 35 , batch: 266 , training loss: 4.409651\n",
      "[INFO] Epoch: 35 , batch: 267 , training loss: 4.144756\n",
      "[INFO] Epoch: 35 , batch: 268 , training loss: 4.091838\n",
      "[INFO] Epoch: 35 , batch: 269 , training loss: 4.067466\n",
      "[INFO] Epoch: 35 , batch: 270 , training loss: 4.074212\n",
      "[INFO] Epoch: 35 , batch: 271 , training loss: 4.112692\n",
      "[INFO] Epoch: 35 , batch: 272 , training loss: 4.103256\n",
      "[INFO] Epoch: 35 , batch: 273 , training loss: 4.116982\n",
      "[INFO] Epoch: 35 , batch: 274 , training loss: 4.206678\n",
      "[INFO] Epoch: 35 , batch: 275 , training loss: 4.075788\n",
      "[INFO] Epoch: 35 , batch: 276 , training loss: 4.140497\n",
      "[INFO] Epoch: 35 , batch: 277 , training loss: 4.309295\n",
      "[INFO] Epoch: 35 , batch: 278 , training loss: 3.975584\n",
      "[INFO] Epoch: 35 , batch: 279 , training loss: 3.989361\n",
      "[INFO] Epoch: 35 , batch: 280 , training loss: 3.990560\n",
      "[INFO] Epoch: 35 , batch: 281 , training loss: 4.107279\n",
      "[INFO] Epoch: 35 , batch: 282 , training loss: 4.002287\n",
      "[INFO] Epoch: 35 , batch: 283 , training loss: 3.988368\n",
      "[INFO] Epoch: 35 , batch: 284 , training loss: 4.048195\n",
      "[INFO] Epoch: 35 , batch: 285 , training loss: 3.970782\n",
      "[INFO] Epoch: 35 , batch: 286 , training loss: 3.982585\n",
      "[INFO] Epoch: 35 , batch: 287 , training loss: 3.945749\n",
      "[INFO] Epoch: 35 , batch: 288 , training loss: 3.875325\n",
      "[INFO] Epoch: 35 , batch: 289 , training loss: 3.963688\n",
      "[INFO] Epoch: 35 , batch: 290 , training loss: 3.731244\n",
      "[INFO] Epoch: 35 , batch: 291 , training loss: 3.728092\n",
      "[INFO] Epoch: 35 , batch: 292 , training loss: 3.818383\n",
      "[INFO] Epoch: 35 , batch: 293 , training loss: 3.759963\n",
      "[INFO] Epoch: 35 , batch: 294 , training loss: 4.422494\n",
      "[INFO] Epoch: 35 , batch: 295 , training loss: 4.210546\n",
      "[INFO] Epoch: 35 , batch: 296 , training loss: 4.140090\n",
      "[INFO] Epoch: 35 , batch: 297 , training loss: 4.089984\n",
      "[INFO] Epoch: 35 , batch: 298 , training loss: 3.941726\n",
      "[INFO] Epoch: 35 , batch: 299 , training loss: 3.969090\n",
      "[INFO] Epoch: 35 , batch: 300 , training loss: 3.973388\n",
      "[INFO] Epoch: 35 , batch: 301 , training loss: 3.881985\n",
      "[INFO] Epoch: 35 , batch: 302 , training loss: 4.062779\n",
      "[INFO] Epoch: 35 , batch: 303 , training loss: 4.060970\n",
      "[INFO] Epoch: 35 , batch: 304 , training loss: 4.188766\n",
      "[INFO] Epoch: 35 , batch: 305 , training loss: 4.011349\n",
      "[INFO] Epoch: 35 , batch: 306 , training loss: 4.148258\n",
      "[INFO] Epoch: 35 , batch: 307 , training loss: 4.181363\n",
      "[INFO] Epoch: 35 , batch: 308 , training loss: 3.953061\n",
      "[INFO] Epoch: 35 , batch: 309 , training loss: 3.957893\n",
      "[INFO] Epoch: 35 , batch: 310 , training loss: 3.905531\n",
      "[INFO] Epoch: 35 , batch: 311 , training loss: 3.901034\n",
      "[INFO] Epoch: 35 , batch: 312 , training loss: 3.814264\n",
      "[INFO] Epoch: 35 , batch: 313 , training loss: 3.893328\n",
      "[INFO] Epoch: 35 , batch: 314 , training loss: 3.965048\n",
      "[INFO] Epoch: 35 , batch: 315 , training loss: 4.064561\n",
      "[INFO] Epoch: 35 , batch: 316 , training loss: 4.287724\n",
      "[INFO] Epoch: 35 , batch: 317 , training loss: 4.629408\n",
      "[INFO] Epoch: 35 , batch: 318 , training loss: 4.758106\n",
      "[INFO] Epoch: 35 , batch: 319 , training loss: 4.456964\n",
      "[INFO] Epoch: 35 , batch: 320 , training loss: 4.011778\n",
      "[INFO] Epoch: 35 , batch: 321 , training loss: 3.829952\n",
      "[INFO] Epoch: 35 , batch: 322 , training loss: 3.921913\n",
      "[INFO] Epoch: 35 , batch: 323 , training loss: 3.972371\n",
      "[INFO] Epoch: 35 , batch: 324 , training loss: 3.951659\n",
      "[INFO] Epoch: 35 , batch: 325 , training loss: 4.042560\n",
      "[INFO] Epoch: 35 , batch: 326 , training loss: 4.125588\n",
      "[INFO] Epoch: 35 , batch: 327 , training loss: 4.044709\n",
      "[INFO] Epoch: 35 , batch: 328 , training loss: 4.044280\n",
      "[INFO] Epoch: 35 , batch: 329 , training loss: 3.956213\n",
      "[INFO] Epoch: 35 , batch: 330 , training loss: 3.957776\n",
      "[INFO] Epoch: 35 , batch: 331 , training loss: 4.110662\n",
      "[INFO] Epoch: 35 , batch: 332 , training loss: 3.965494\n",
      "[INFO] Epoch: 35 , batch: 333 , training loss: 3.932540\n",
      "[INFO] Epoch: 35 , batch: 334 , training loss: 3.946436\n",
      "[INFO] Epoch: 35 , batch: 335 , training loss: 4.064852\n",
      "[INFO] Epoch: 35 , batch: 336 , training loss: 4.068101\n",
      "[INFO] Epoch: 35 , batch: 337 , training loss: 4.123592\n",
      "[INFO] Epoch: 35 , batch: 338 , training loss: 4.332406\n",
      "[INFO] Epoch: 35 , batch: 339 , training loss: 4.163061\n",
      "[INFO] Epoch: 35 , batch: 340 , training loss: 4.322800\n",
      "[INFO] Epoch: 35 , batch: 341 , training loss: 4.083735\n",
      "[INFO] Epoch: 35 , batch: 342 , training loss: 3.878303\n",
      "[INFO] Epoch: 35 , batch: 343 , training loss: 3.944052\n",
      "[INFO] Epoch: 35 , batch: 344 , training loss: 3.831294\n",
      "[INFO] Epoch: 35 , batch: 345 , training loss: 3.960386\n",
      "[INFO] Epoch: 35 , batch: 346 , training loss: 3.984911\n",
      "[INFO] Epoch: 35 , batch: 347 , training loss: 3.907102\n",
      "[INFO] Epoch: 35 , batch: 348 , training loss: 4.005770\n",
      "[INFO] Epoch: 35 , batch: 349 , training loss: 4.102563\n",
      "[INFO] Epoch: 35 , batch: 350 , training loss: 3.941462\n",
      "[INFO] Epoch: 35 , batch: 351 , training loss: 4.036007\n",
      "[INFO] Epoch: 35 , batch: 352 , training loss: 4.052583\n",
      "[INFO] Epoch: 35 , batch: 353 , training loss: 4.016539\n",
      "[INFO] Epoch: 35 , batch: 354 , training loss: 4.124191\n",
      "[INFO] Epoch: 35 , batch: 355 , training loss: 4.115210\n",
      "[INFO] Epoch: 35 , batch: 356 , training loss: 3.973644\n",
      "[INFO] Epoch: 35 , batch: 357 , training loss: 4.058941\n",
      "[INFO] Epoch: 35 , batch: 358 , training loss: 3.960634\n",
      "[INFO] Epoch: 35 , batch: 359 , training loss: 3.962746\n",
      "[INFO] Epoch: 35 , batch: 360 , training loss: 4.073318\n",
      "[INFO] Epoch: 35 , batch: 361 , training loss: 4.048619\n",
      "[INFO] Epoch: 35 , batch: 362 , training loss: 4.150862\n",
      "[INFO] Epoch: 35 , batch: 363 , training loss: 4.014445\n",
      "[INFO] Epoch: 35 , batch: 364 , training loss: 4.091362\n",
      "[INFO] Epoch: 35 , batch: 365 , training loss: 3.989601\n",
      "[INFO] Epoch: 35 , batch: 366 , training loss: 4.093939\n",
      "[INFO] Epoch: 35 , batch: 367 , training loss: 4.119048\n",
      "[INFO] Epoch: 35 , batch: 368 , training loss: 4.500407\n",
      "[INFO] Epoch: 35 , batch: 369 , training loss: 4.202062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 35 , batch: 370 , training loss: 3.993152\n",
      "[INFO] Epoch: 35 , batch: 371 , training loss: 4.414570\n",
      "[INFO] Epoch: 35 , batch: 372 , training loss: 4.633874\n",
      "[INFO] Epoch: 35 , batch: 373 , training loss: 4.699029\n",
      "[INFO] Epoch: 35 , batch: 374 , training loss: 4.799288\n",
      "[INFO] Epoch: 35 , batch: 375 , training loss: 4.774526\n",
      "[INFO] Epoch: 35 , batch: 376 , training loss: 4.691085\n",
      "[INFO] Epoch: 35 , batch: 377 , training loss: 4.443355\n",
      "[INFO] Epoch: 35 , batch: 378 , training loss: 4.552569\n",
      "[INFO] Epoch: 35 , batch: 379 , training loss: 4.519925\n",
      "[INFO] Epoch: 35 , batch: 380 , training loss: 4.680715\n",
      "[INFO] Epoch: 35 , batch: 381 , training loss: 4.382539\n",
      "[INFO] Epoch: 35 , batch: 382 , training loss: 4.658705\n",
      "[INFO] Epoch: 35 , batch: 383 , training loss: 4.685750\n",
      "[INFO] Epoch: 35 , batch: 384 , training loss: 4.660384\n",
      "[INFO] Epoch: 35 , batch: 385 , training loss: 4.326752\n",
      "[INFO] Epoch: 35 , batch: 386 , training loss: 4.601643\n",
      "[INFO] Epoch: 35 , batch: 387 , training loss: 4.540289\n",
      "[INFO] Epoch: 35 , batch: 388 , training loss: 4.383104\n",
      "[INFO] Epoch: 35 , batch: 389 , training loss: 4.177958\n",
      "[INFO] Epoch: 35 , batch: 390 , training loss: 4.200303\n",
      "[INFO] Epoch: 35 , batch: 391 , training loss: 4.217104\n",
      "[INFO] Epoch: 35 , batch: 392 , training loss: 4.592526\n",
      "[INFO] Epoch: 35 , batch: 393 , training loss: 4.479731\n",
      "[INFO] Epoch: 35 , batch: 394 , training loss: 4.555380\n",
      "[INFO] Epoch: 35 , batch: 395 , training loss: 4.399387\n",
      "[INFO] Epoch: 35 , batch: 396 , training loss: 4.210040\n",
      "[INFO] Epoch: 35 , batch: 397 , training loss: 4.334935\n",
      "[INFO] Epoch: 35 , batch: 398 , training loss: 4.211965\n",
      "[INFO] Epoch: 35 , batch: 399 , training loss: 4.281385\n",
      "[INFO] Epoch: 35 , batch: 400 , training loss: 4.248987\n",
      "[INFO] Epoch: 35 , batch: 401 , training loss: 4.696517\n",
      "[INFO] Epoch: 35 , batch: 402 , training loss: 4.405163\n",
      "[INFO] Epoch: 35 , batch: 403 , training loss: 4.229905\n",
      "[INFO] Epoch: 35 , batch: 404 , training loss: 4.406856\n",
      "[INFO] Epoch: 35 , batch: 405 , training loss: 4.478317\n",
      "[INFO] Epoch: 35 , batch: 406 , training loss: 4.364731\n",
      "[INFO] Epoch: 35 , batch: 407 , training loss: 4.402335\n",
      "[INFO] Epoch: 35 , batch: 408 , training loss: 4.374483\n",
      "[INFO] Epoch: 35 , batch: 409 , training loss: 4.407250\n",
      "[INFO] Epoch: 35 , batch: 410 , training loss: 4.465388\n",
      "[INFO] Epoch: 35 , batch: 411 , training loss: 4.629354\n",
      "[INFO] Epoch: 35 , batch: 412 , training loss: 4.443476\n",
      "[INFO] Epoch: 35 , batch: 413 , training loss: 4.331027\n",
      "[INFO] Epoch: 35 , batch: 414 , training loss: 4.363499\n",
      "[INFO] Epoch: 35 , batch: 415 , training loss: 4.414094\n",
      "[INFO] Epoch: 35 , batch: 416 , training loss: 4.467559\n",
      "[INFO] Epoch: 35 , batch: 417 , training loss: 4.382770\n",
      "[INFO] Epoch: 35 , batch: 418 , training loss: 4.437353\n",
      "[INFO] Epoch: 35 , batch: 419 , training loss: 4.414186\n",
      "[INFO] Epoch: 35 , batch: 420 , training loss: 4.352833\n",
      "[INFO] Epoch: 35 , batch: 421 , training loss: 4.354210\n",
      "[INFO] Epoch: 35 , batch: 422 , training loss: 4.212088\n",
      "[INFO] Epoch: 35 , batch: 423 , training loss: 4.420831\n",
      "[INFO] Epoch: 35 , batch: 424 , training loss: 4.614721\n",
      "[INFO] Epoch: 35 , batch: 425 , training loss: 4.448359\n",
      "[INFO] Epoch: 35 , batch: 426 , training loss: 4.207569\n",
      "[INFO] Epoch: 35 , batch: 427 , training loss: 4.442309\n",
      "[INFO] Epoch: 35 , batch: 428 , training loss: 4.300460\n",
      "[INFO] Epoch: 35 , batch: 429 , training loss: 4.215600\n",
      "[INFO] Epoch: 35 , batch: 430 , training loss: 4.441243\n",
      "[INFO] Epoch: 35 , batch: 431 , training loss: 4.043715\n",
      "[INFO] Epoch: 35 , batch: 432 , training loss: 4.099680\n",
      "[INFO] Epoch: 35 , batch: 433 , training loss: 4.157032\n",
      "[INFO] Epoch: 35 , batch: 434 , training loss: 4.030696\n",
      "[INFO] Epoch: 35 , batch: 435 , training loss: 4.375786\n",
      "[INFO] Epoch: 35 , batch: 436 , training loss: 4.407755\n",
      "[INFO] Epoch: 35 , batch: 437 , training loss: 4.201530\n",
      "[INFO] Epoch: 35 , batch: 438 , training loss: 4.070710\n",
      "[INFO] Epoch: 35 , batch: 439 , training loss: 4.318705\n",
      "[INFO] Epoch: 35 , batch: 440 , training loss: 4.424222\n",
      "[INFO] Epoch: 35 , batch: 441 , training loss: 4.503932\n",
      "[INFO] Epoch: 35 , batch: 442 , training loss: 4.277823\n",
      "[INFO] Epoch: 35 , batch: 443 , training loss: 4.467819\n",
      "[INFO] Epoch: 35 , batch: 444 , training loss: 4.060621\n",
      "[INFO] Epoch: 35 , batch: 445 , training loss: 3.980979\n",
      "[INFO] Epoch: 35 , batch: 446 , training loss: 3.909423\n",
      "[INFO] Epoch: 35 , batch: 447 , training loss: 4.102267\n",
      "[INFO] Epoch: 35 , batch: 448 , training loss: 4.214978\n",
      "[INFO] Epoch: 35 , batch: 449 , training loss: 4.588300\n",
      "[INFO] Epoch: 35 , batch: 450 , training loss: 4.660112\n",
      "[INFO] Epoch: 35 , batch: 451 , training loss: 4.557348\n",
      "[INFO] Epoch: 35 , batch: 452 , training loss: 4.375317\n",
      "[INFO] Epoch: 35 , batch: 453 , training loss: 4.149937\n",
      "[INFO] Epoch: 35 , batch: 454 , training loss: 4.277210\n",
      "[INFO] Epoch: 35 , batch: 455 , training loss: 4.348420\n",
      "[INFO] Epoch: 35 , batch: 456 , training loss: 4.335024\n",
      "[INFO] Epoch: 35 , batch: 457 , training loss: 4.405809\n",
      "[INFO] Epoch: 35 , batch: 458 , training loss: 4.164601\n",
      "[INFO] Epoch: 35 , batch: 459 , training loss: 4.136157\n",
      "[INFO] Epoch: 35 , batch: 460 , training loss: 4.250266\n",
      "[INFO] Epoch: 35 , batch: 461 , training loss: 4.217919\n",
      "[INFO] Epoch: 35 , batch: 462 , training loss: 4.268033\n",
      "[INFO] Epoch: 35 , batch: 463 , training loss: 4.175300\n",
      "[INFO] Epoch: 35 , batch: 464 , training loss: 4.367112\n",
      "[INFO] Epoch: 35 , batch: 465 , training loss: 4.298510\n",
      "[INFO] Epoch: 35 , batch: 466 , training loss: 4.417313\n",
      "[INFO] Epoch: 35 , batch: 467 , training loss: 4.362221\n",
      "[INFO] Epoch: 35 , batch: 468 , training loss: 4.335531\n",
      "[INFO] Epoch: 35 , batch: 469 , training loss: 4.344953\n",
      "[INFO] Epoch: 35 , batch: 470 , training loss: 4.192410\n",
      "[INFO] Epoch: 35 , batch: 471 , training loss: 4.283091\n",
      "[INFO] Epoch: 35 , batch: 472 , training loss: 4.343816\n",
      "[INFO] Epoch: 35 , batch: 473 , training loss: 4.268767\n",
      "[INFO] Epoch: 35 , batch: 474 , training loss: 4.043218\n",
      "[INFO] Epoch: 35 , batch: 475 , training loss: 3.919262\n",
      "[INFO] Epoch: 35 , batch: 476 , training loss: 4.332047\n",
      "[INFO] Epoch: 35 , batch: 477 , training loss: 4.434643\n",
      "[INFO] Epoch: 35 , batch: 478 , training loss: 4.428616\n",
      "[INFO] Epoch: 35 , batch: 479 , training loss: 4.412664\n",
      "[INFO] Epoch: 35 , batch: 480 , training loss: 4.549448\n",
      "[INFO] Epoch: 35 , batch: 481 , training loss: 4.427440\n",
      "[INFO] Epoch: 35 , batch: 482 , training loss: 4.530586\n",
      "[INFO] Epoch: 35 , batch: 483 , training loss: 4.372269\n",
      "[INFO] Epoch: 35 , batch: 484 , training loss: 4.162186\n",
      "[INFO] Epoch: 35 , batch: 485 , training loss: 4.265732\n",
      "[INFO] Epoch: 35 , batch: 486 , training loss: 4.175509\n",
      "[INFO] Epoch: 35 , batch: 487 , training loss: 4.153288\n",
      "[INFO] Epoch: 35 , batch: 488 , training loss: 4.334527\n",
      "[INFO] Epoch: 35 , batch: 489 , training loss: 4.240081\n",
      "[INFO] Epoch: 35 , batch: 490 , training loss: 4.300750\n",
      "[INFO] Epoch: 35 , batch: 491 , training loss: 4.216756\n",
      "[INFO] Epoch: 35 , batch: 492 , training loss: 4.194480\n",
      "[INFO] Epoch: 35 , batch: 493 , training loss: 4.354689\n",
      "[INFO] Epoch: 35 , batch: 494 , training loss: 4.254345\n",
      "[INFO] Epoch: 35 , batch: 495 , training loss: 4.434685\n",
      "[INFO] Epoch: 35 , batch: 496 , training loss: 4.312611\n",
      "[INFO] Epoch: 35 , batch: 497 , training loss: 4.342363\n",
      "[INFO] Epoch: 35 , batch: 498 , training loss: 4.302268\n",
      "[INFO] Epoch: 35 , batch: 499 , training loss: 4.388827\n",
      "[INFO] Epoch: 35 , batch: 500 , training loss: 4.508499\n",
      "[INFO] Epoch: 35 , batch: 501 , training loss: 4.838151\n",
      "[INFO] Epoch: 35 , batch: 502 , training loss: 4.873411\n",
      "[INFO] Epoch: 35 , batch: 503 , training loss: 4.523499\n",
      "[INFO] Epoch: 35 , batch: 504 , training loss: 4.673437\n",
      "[INFO] Epoch: 35 , batch: 505 , training loss: 4.651952\n",
      "[INFO] Epoch: 35 , batch: 506 , training loss: 4.624015\n",
      "[INFO] Epoch: 35 , batch: 507 , training loss: 4.679463\n",
      "[INFO] Epoch: 35 , batch: 508 , training loss: 4.581470\n",
      "[INFO] Epoch: 35 , batch: 509 , training loss: 4.398562\n",
      "[INFO] Epoch: 35 , batch: 510 , training loss: 4.480913\n",
      "[INFO] Epoch: 35 , batch: 511 , training loss: 4.406480\n",
      "[INFO] Epoch: 35 , batch: 512 , training loss: 4.498290\n",
      "[INFO] Epoch: 35 , batch: 513 , training loss: 4.767676\n",
      "[INFO] Epoch: 35 , batch: 514 , training loss: 4.387615\n",
      "[INFO] Epoch: 35 , batch: 515 , training loss: 4.641237\n",
      "[INFO] Epoch: 35 , batch: 516 , training loss: 4.432566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 35 , batch: 517 , training loss: 4.410056\n",
      "[INFO] Epoch: 35 , batch: 518 , training loss: 4.376837\n",
      "[INFO] Epoch: 35 , batch: 519 , training loss: 4.223073\n",
      "[INFO] Epoch: 35 , batch: 520 , training loss: 4.455454\n",
      "[INFO] Epoch: 35 , batch: 521 , training loss: 4.425757\n",
      "[INFO] Epoch: 35 , batch: 522 , training loss: 4.521273\n",
      "[INFO] Epoch: 35 , batch: 523 , training loss: 4.418279\n",
      "[INFO] Epoch: 35 , batch: 524 , training loss: 4.708583\n",
      "[INFO] Epoch: 35 , batch: 525 , training loss: 4.580516\n",
      "[INFO] Epoch: 35 , batch: 526 , training loss: 4.368480\n",
      "[INFO] Epoch: 35 , batch: 527 , training loss: 4.423064\n",
      "[INFO] Epoch: 35 , batch: 528 , training loss: 4.445760\n",
      "[INFO] Epoch: 35 , batch: 529 , training loss: 4.417846\n",
      "[INFO] Epoch: 35 , batch: 530 , training loss: 4.275466\n",
      "[INFO] Epoch: 35 , batch: 531 , training loss: 4.410434\n",
      "[INFO] Epoch: 35 , batch: 532 , training loss: 4.319003\n",
      "[INFO] Epoch: 35 , batch: 533 , training loss: 4.454413\n",
      "[INFO] Epoch: 35 , batch: 534 , training loss: 4.444065\n",
      "[INFO] Epoch: 35 , batch: 535 , training loss: 4.452601\n",
      "[INFO] Epoch: 35 , batch: 536 , training loss: 4.295699\n",
      "[INFO] Epoch: 35 , batch: 537 , training loss: 4.268008\n",
      "[INFO] Epoch: 35 , batch: 538 , training loss: 4.372192\n",
      "[INFO] Epoch: 35 , batch: 539 , training loss: 4.463591\n",
      "[INFO] Epoch: 35 , batch: 540 , training loss: 4.972504\n",
      "[INFO] Epoch: 35 , batch: 541 , training loss: 4.804681\n",
      "[INFO] Epoch: 35 , batch: 542 , training loss: 4.681813\n",
      "[INFO] Epoch: 36 , batch: 0 , training loss: 3.639293\n",
      "[INFO] Epoch: 36 , batch: 1 , training loss: 3.555733\n",
      "[INFO] Epoch: 36 , batch: 2 , training loss: 3.709646\n",
      "[INFO] Epoch: 36 , batch: 3 , training loss: 3.552554\n",
      "[INFO] Epoch: 36 , batch: 4 , training loss: 3.879481\n",
      "[INFO] Epoch: 36 , batch: 5 , training loss: 3.592888\n",
      "[INFO] Epoch: 36 , batch: 6 , training loss: 3.891818\n",
      "[INFO] Epoch: 36 , batch: 7 , training loss: 3.824841\n",
      "[INFO] Epoch: 36 , batch: 8 , training loss: 3.536999\n",
      "[INFO] Epoch: 36 , batch: 9 , training loss: 3.771131\n",
      "[INFO] Epoch: 36 , batch: 10 , training loss: 3.713069\n",
      "[INFO] Epoch: 36 , batch: 11 , training loss: 3.661129\n",
      "[INFO] Epoch: 36 , batch: 12 , training loss: 3.544054\n",
      "[INFO] Epoch: 36 , batch: 13 , training loss: 3.579753\n",
      "[INFO] Epoch: 36 , batch: 14 , training loss: 3.481032\n",
      "[INFO] Epoch: 36 , batch: 15 , training loss: 3.705987\n",
      "[INFO] Epoch: 36 , batch: 16 , training loss: 3.584392\n",
      "[INFO] Epoch: 36 , batch: 17 , training loss: 3.719269\n",
      "[INFO] Epoch: 36 , batch: 18 , training loss: 3.645371\n",
      "[INFO] Epoch: 36 , batch: 19 , training loss: 3.415063\n",
      "[INFO] Epoch: 36 , batch: 20 , training loss: 3.369457\n",
      "[INFO] Epoch: 36 , batch: 21 , training loss: 3.526513\n",
      "[INFO] Epoch: 36 , batch: 22 , training loss: 3.402303\n",
      "[INFO] Epoch: 36 , batch: 23 , training loss: 3.609273\n",
      "[INFO] Epoch: 36 , batch: 24 , training loss: 3.489021\n",
      "[INFO] Epoch: 36 , batch: 25 , training loss: 3.577351\n",
      "[INFO] Epoch: 36 , batch: 26 , training loss: 3.480400\n",
      "[INFO] Epoch: 36 , batch: 27 , training loss: 3.401726\n",
      "[INFO] Epoch: 36 , batch: 28 , training loss: 3.647367\n",
      "[INFO] Epoch: 36 , batch: 29 , training loss: 3.407901\n",
      "[INFO] Epoch: 36 , batch: 30 , training loss: 3.452841\n",
      "[INFO] Epoch: 36 , batch: 31 , training loss: 3.569636\n",
      "[INFO] Epoch: 36 , batch: 32 , training loss: 3.528047\n",
      "[INFO] Epoch: 36 , batch: 33 , training loss: 3.573187\n",
      "[INFO] Epoch: 36 , batch: 34 , training loss: 3.530932\n",
      "[INFO] Epoch: 36 , batch: 35 , training loss: 3.494869\n",
      "[INFO] Epoch: 36 , batch: 36 , training loss: 3.595256\n",
      "[INFO] Epoch: 36 , batch: 37 , training loss: 3.429300\n",
      "[INFO] Epoch: 36 , batch: 38 , training loss: 3.535203\n",
      "[INFO] Epoch: 36 , batch: 39 , training loss: 3.333696\n",
      "[INFO] Epoch: 36 , batch: 40 , training loss: 3.571739\n",
      "[INFO] Epoch: 36 , batch: 41 , training loss: 3.453620\n",
      "[INFO] Epoch: 36 , batch: 42 , training loss: 3.914569\n",
      "[INFO] Epoch: 36 , batch: 43 , training loss: 3.716872\n",
      "[INFO] Epoch: 36 , batch: 44 , training loss: 4.042433\n",
      "[INFO] Epoch: 36 , batch: 45 , training loss: 3.948997\n",
      "[INFO] Epoch: 36 , batch: 46 , training loss: 3.856744\n",
      "[INFO] Epoch: 36 , batch: 47 , training loss: 3.581350\n",
      "[INFO] Epoch: 36 , batch: 48 , training loss: 3.547383\n",
      "[INFO] Epoch: 36 , batch: 49 , training loss: 3.796027\n",
      "[INFO] Epoch: 36 , batch: 50 , training loss: 3.566551\n",
      "[INFO] Epoch: 36 , batch: 51 , training loss: 3.815559\n",
      "[INFO] Epoch: 36 , batch: 52 , training loss: 3.668148\n",
      "[INFO] Epoch: 36 , batch: 53 , training loss: 3.713202\n",
      "[INFO] Epoch: 36 , batch: 54 , training loss: 3.745136\n",
      "[INFO] Epoch: 36 , batch: 55 , training loss: 3.851186\n",
      "[INFO] Epoch: 36 , batch: 56 , training loss: 3.644918\n",
      "[INFO] Epoch: 36 , batch: 57 , training loss: 3.576923\n",
      "[INFO] Epoch: 36 , batch: 58 , training loss: 3.660185\n",
      "[INFO] Epoch: 36 , batch: 59 , training loss: 3.728048\n",
      "[INFO] Epoch: 36 , batch: 60 , training loss: 3.655462\n",
      "[INFO] Epoch: 36 , batch: 61 , training loss: 3.744481\n",
      "[INFO] Epoch: 36 , batch: 62 , training loss: 3.625695\n",
      "[INFO] Epoch: 36 , batch: 63 , training loss: 3.790059\n",
      "[INFO] Epoch: 36 , batch: 64 , training loss: 4.024880\n",
      "[INFO] Epoch: 36 , batch: 65 , training loss: 3.727599\n",
      "[INFO] Epoch: 36 , batch: 66 , training loss: 3.588949\n",
      "[INFO] Epoch: 36 , batch: 67 , training loss: 3.607812\n",
      "[INFO] Epoch: 36 , batch: 68 , training loss: 3.771042\n",
      "[INFO] Epoch: 36 , batch: 69 , training loss: 3.683817\n",
      "[INFO] Epoch: 36 , batch: 70 , training loss: 3.909384\n",
      "[INFO] Epoch: 36 , batch: 71 , training loss: 3.782557\n",
      "[INFO] Epoch: 36 , batch: 72 , training loss: 3.843205\n",
      "[INFO] Epoch: 36 , batch: 73 , training loss: 3.815123\n",
      "[INFO] Epoch: 36 , batch: 74 , training loss: 3.871433\n",
      "[INFO] Epoch: 36 , batch: 75 , training loss: 3.741697\n",
      "[INFO] Epoch: 36 , batch: 76 , training loss: 3.838102\n",
      "[INFO] Epoch: 36 , batch: 77 , training loss: 3.766828\n",
      "[INFO] Epoch: 36 , batch: 78 , training loss: 3.881020\n",
      "[INFO] Epoch: 36 , batch: 79 , training loss: 3.770250\n",
      "[INFO] Epoch: 36 , batch: 80 , training loss: 3.950991\n",
      "[INFO] Epoch: 36 , batch: 81 , training loss: 3.880061\n",
      "[INFO] Epoch: 36 , batch: 82 , training loss: 3.852310\n",
      "[INFO] Epoch: 36 , batch: 83 , training loss: 3.920558\n",
      "[INFO] Epoch: 36 , batch: 84 , training loss: 3.914088\n",
      "[INFO] Epoch: 36 , batch: 85 , training loss: 3.997809\n",
      "[INFO] Epoch: 36 , batch: 86 , training loss: 3.887177\n",
      "[INFO] Epoch: 36 , batch: 87 , training loss: 3.884177\n",
      "[INFO] Epoch: 36 , batch: 88 , training loss: 4.021265\n",
      "[INFO] Epoch: 36 , batch: 89 , training loss: 3.798059\n",
      "[INFO] Epoch: 36 , batch: 90 , training loss: 3.932906\n",
      "[INFO] Epoch: 36 , batch: 91 , training loss: 3.809806\n",
      "[INFO] Epoch: 36 , batch: 92 , training loss: 3.841514\n",
      "[INFO] Epoch: 36 , batch: 93 , training loss: 3.939470\n",
      "[INFO] Epoch: 36 , batch: 94 , training loss: 4.093052\n",
      "[INFO] Epoch: 36 , batch: 95 , training loss: 3.852694\n",
      "[INFO] Epoch: 36 , batch: 96 , training loss: 3.835851\n",
      "[INFO] Epoch: 36 , batch: 97 , training loss: 3.769200\n",
      "[INFO] Epoch: 36 , batch: 98 , training loss: 3.732727\n",
      "[INFO] Epoch: 36 , batch: 99 , training loss: 3.848682\n",
      "[INFO] Epoch: 36 , batch: 100 , training loss: 3.739241\n",
      "[INFO] Epoch: 36 , batch: 101 , training loss: 3.768955\n",
      "[INFO] Epoch: 36 , batch: 102 , training loss: 3.939371\n",
      "[INFO] Epoch: 36 , batch: 103 , training loss: 3.713079\n",
      "[INFO] Epoch: 36 , batch: 104 , training loss: 3.670120\n",
      "[INFO] Epoch: 36 , batch: 105 , training loss: 3.903807\n",
      "[INFO] Epoch: 36 , batch: 106 , training loss: 3.964102\n",
      "[INFO] Epoch: 36 , batch: 107 , training loss: 3.789993\n",
      "[INFO] Epoch: 36 , batch: 108 , training loss: 3.731434\n",
      "[INFO] Epoch: 36 , batch: 109 , training loss: 3.649112\n",
      "[INFO] Epoch: 36 , batch: 110 , training loss: 3.824104\n",
      "[INFO] Epoch: 36 , batch: 111 , training loss: 3.922884\n",
      "[INFO] Epoch: 36 , batch: 112 , training loss: 3.855140\n",
      "[INFO] Epoch: 36 , batch: 113 , training loss: 3.818083\n",
      "[INFO] Epoch: 36 , batch: 114 , training loss: 3.779271\n",
      "[INFO] Epoch: 36 , batch: 115 , training loss: 3.816504\n",
      "[INFO] Epoch: 36 , batch: 116 , training loss: 3.727114\n",
      "[INFO] Epoch: 36 , batch: 117 , training loss: 3.936639\n",
      "[INFO] Epoch: 36 , batch: 118 , training loss: 3.928327\n",
      "[INFO] Epoch: 36 , batch: 119 , training loss: 4.057554\n",
      "[INFO] Epoch: 36 , batch: 120 , training loss: 4.036387\n",
      "[INFO] Epoch: 36 , batch: 121 , training loss: 3.919054\n",
      "[INFO] Epoch: 36 , batch: 122 , training loss: 3.820252\n",
      "[INFO] Epoch: 36 , batch: 123 , training loss: 3.804595\n",
      "[INFO] Epoch: 36 , batch: 124 , training loss: 3.951408\n",
      "[INFO] Epoch: 36 , batch: 125 , training loss: 3.731041\n",
      "[INFO] Epoch: 36 , batch: 126 , training loss: 3.783702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 36 , batch: 127 , training loss: 3.761168\n",
      "[INFO] Epoch: 36 , batch: 128 , training loss: 3.887804\n",
      "[INFO] Epoch: 36 , batch: 129 , training loss: 3.852901\n",
      "[INFO] Epoch: 36 , batch: 130 , training loss: 3.828585\n",
      "[INFO] Epoch: 36 , batch: 131 , training loss: 3.842996\n",
      "[INFO] Epoch: 36 , batch: 132 , training loss: 3.875335\n",
      "[INFO] Epoch: 36 , batch: 133 , training loss: 3.836711\n",
      "[INFO] Epoch: 36 , batch: 134 , training loss: 3.589538\n",
      "[INFO] Epoch: 36 , batch: 135 , training loss: 3.653909\n",
      "[INFO] Epoch: 36 , batch: 136 , training loss: 3.923165\n",
      "[INFO] Epoch: 36 , batch: 137 , training loss: 3.873804\n",
      "[INFO] Epoch: 36 , batch: 138 , training loss: 3.929308\n",
      "[INFO] Epoch: 36 , batch: 139 , training loss: 4.454882\n",
      "[INFO] Epoch: 36 , batch: 140 , training loss: 4.249244\n",
      "[INFO] Epoch: 36 , batch: 141 , training loss: 4.038443\n",
      "[INFO] Epoch: 36 , batch: 142 , training loss: 3.743370\n",
      "[INFO] Epoch: 36 , batch: 143 , training loss: 3.912631\n",
      "[INFO] Epoch: 36 , batch: 144 , training loss: 3.754534\n",
      "[INFO] Epoch: 36 , batch: 145 , training loss: 3.832962\n",
      "[INFO] Epoch: 36 , batch: 146 , training loss: 4.023049\n",
      "[INFO] Epoch: 36 , batch: 147 , training loss: 3.664326\n",
      "[INFO] Epoch: 36 , batch: 148 , training loss: 3.652529\n",
      "[INFO] Epoch: 36 , batch: 149 , training loss: 3.719735\n",
      "[INFO] Epoch: 36 , batch: 150 , training loss: 4.031503\n",
      "[INFO] Epoch: 36 , batch: 151 , training loss: 3.829682\n",
      "[INFO] Epoch: 36 , batch: 152 , training loss: 3.857251\n",
      "[INFO] Epoch: 36 , batch: 153 , training loss: 3.838203\n",
      "[INFO] Epoch: 36 , batch: 154 , training loss: 3.930737\n",
      "[INFO] Epoch: 36 , batch: 155 , training loss: 4.123243\n",
      "[INFO] Epoch: 36 , batch: 156 , training loss: 3.875621\n",
      "[INFO] Epoch: 36 , batch: 157 , training loss: 3.882648\n",
      "[INFO] Epoch: 36 , batch: 158 , training loss: 3.952513\n",
      "[INFO] Epoch: 36 , batch: 159 , training loss: 3.926138\n",
      "[INFO] Epoch: 36 , batch: 160 , training loss: 4.140031\n",
      "[INFO] Epoch: 36 , batch: 161 , training loss: 4.222424\n",
      "[INFO] Epoch: 36 , batch: 162 , training loss: 4.180280\n",
      "[INFO] Epoch: 36 , batch: 163 , training loss: 4.379491\n",
      "[INFO] Epoch: 36 , batch: 164 , training loss: 4.313156\n",
      "[INFO] Epoch: 36 , batch: 165 , training loss: 4.235468\n",
      "[INFO] Epoch: 36 , batch: 166 , training loss: 4.094451\n",
      "[INFO] Epoch: 36 , batch: 167 , training loss: 4.179702\n",
      "[INFO] Epoch: 36 , batch: 168 , training loss: 3.807544\n",
      "[INFO] Epoch: 36 , batch: 169 , training loss: 3.792920\n",
      "[INFO] Epoch: 36 , batch: 170 , training loss: 4.004695\n",
      "[INFO] Epoch: 36 , batch: 171 , training loss: 3.415061\n",
      "[INFO] Epoch: 36 , batch: 172 , training loss: 3.670261\n",
      "[INFO] Epoch: 36 , batch: 173 , training loss: 4.002449\n",
      "[INFO] Epoch: 36 , batch: 174 , training loss: 4.478471\n",
      "[INFO] Epoch: 36 , batch: 175 , training loss: 4.763431\n",
      "[INFO] Epoch: 36 , batch: 176 , training loss: 4.429528\n",
      "[INFO] Epoch: 36 , batch: 177 , training loss: 4.018956\n",
      "[INFO] Epoch: 36 , batch: 178 , training loss: 4.038681\n",
      "[INFO] Epoch: 36 , batch: 179 , training loss: 4.103049\n",
      "[INFO] Epoch: 36 , batch: 180 , training loss: 4.057814\n",
      "[INFO] Epoch: 36 , batch: 181 , training loss: 4.337540\n",
      "[INFO] Epoch: 36 , batch: 182 , training loss: 4.282085\n",
      "[INFO] Epoch: 36 , batch: 183 , training loss: 4.248532\n",
      "[INFO] Epoch: 36 , batch: 184 , training loss: 4.148004\n",
      "[INFO] Epoch: 36 , batch: 185 , training loss: 4.113132\n",
      "[INFO] Epoch: 36 , batch: 186 , training loss: 4.264675\n",
      "[INFO] Epoch: 36 , batch: 187 , training loss: 4.347305\n",
      "[INFO] Epoch: 36 , batch: 188 , training loss: 4.335570\n",
      "[INFO] Epoch: 36 , batch: 189 , training loss: 4.272986\n",
      "[INFO] Epoch: 36 , batch: 190 , training loss: 4.293239\n",
      "[INFO] Epoch: 36 , batch: 191 , training loss: 4.388846\n",
      "[INFO] Epoch: 36 , batch: 192 , training loss: 4.233970\n",
      "[INFO] Epoch: 36 , batch: 193 , training loss: 4.327063\n",
      "[INFO] Epoch: 36 , batch: 194 , training loss: 4.262290\n",
      "[INFO] Epoch: 36 , batch: 195 , training loss: 4.199662\n",
      "[INFO] Epoch: 36 , batch: 196 , training loss: 4.056792\n",
      "[INFO] Epoch: 36 , batch: 197 , training loss: 4.150469\n",
      "[INFO] Epoch: 36 , batch: 198 , training loss: 4.067594\n",
      "[INFO] Epoch: 36 , batch: 199 , training loss: 4.197289\n",
      "[INFO] Epoch: 36 , batch: 200 , training loss: 4.108645\n",
      "[INFO] Epoch: 36 , batch: 201 , training loss: 4.007497\n",
      "[INFO] Epoch: 36 , batch: 202 , training loss: 4.016062\n",
      "[INFO] Epoch: 36 , batch: 203 , training loss: 4.133811\n",
      "[INFO] Epoch: 36 , batch: 204 , training loss: 4.226552\n",
      "[INFO] Epoch: 36 , batch: 205 , training loss: 3.829915\n",
      "[INFO] Epoch: 36 , batch: 206 , training loss: 3.752731\n",
      "[INFO] Epoch: 36 , batch: 207 , training loss: 3.764329\n",
      "[INFO] Epoch: 36 , batch: 208 , training loss: 4.075047\n",
      "[INFO] Epoch: 36 , batch: 209 , training loss: 4.040034\n",
      "[INFO] Epoch: 36 , batch: 210 , training loss: 4.068156\n",
      "[INFO] Epoch: 36 , batch: 211 , training loss: 4.049790\n",
      "[INFO] Epoch: 36 , batch: 212 , training loss: 4.141284\n",
      "[INFO] Epoch: 36 , batch: 213 , training loss: 4.096019\n",
      "[INFO] Epoch: 36 , batch: 214 , training loss: 4.185971\n",
      "[INFO] Epoch: 36 , batch: 215 , training loss: 4.358803\n",
      "[INFO] Epoch: 36 , batch: 216 , training loss: 4.092165\n",
      "[INFO] Epoch: 36 , batch: 217 , training loss: 4.020435\n",
      "[INFO] Epoch: 36 , batch: 218 , training loss: 4.029433\n",
      "[INFO] Epoch: 36 , batch: 219 , training loss: 4.151589\n",
      "[INFO] Epoch: 36 , batch: 220 , training loss: 3.970045\n",
      "[INFO] Epoch: 36 , batch: 221 , training loss: 3.985491\n",
      "[INFO] Epoch: 36 , batch: 222 , training loss: 4.111694\n",
      "[INFO] Epoch: 36 , batch: 223 , training loss: 4.191603\n",
      "[INFO] Epoch: 36 , batch: 224 , training loss: 4.260004\n",
      "[INFO] Epoch: 36 , batch: 225 , training loss: 4.151248\n",
      "[INFO] Epoch: 36 , batch: 226 , training loss: 4.278688\n",
      "[INFO] Epoch: 36 , batch: 227 , training loss: 4.266745\n",
      "[INFO] Epoch: 36 , batch: 228 , training loss: 4.274853\n",
      "[INFO] Epoch: 36 , batch: 229 , training loss: 4.141251\n",
      "[INFO] Epoch: 36 , batch: 230 , training loss: 3.987277\n",
      "[INFO] Epoch: 36 , batch: 231 , training loss: 3.856868\n",
      "[INFO] Epoch: 36 , batch: 232 , training loss: 4.006500\n",
      "[INFO] Epoch: 36 , batch: 233 , training loss: 4.042743\n",
      "[INFO] Epoch: 36 , batch: 234 , training loss: 3.705724\n",
      "[INFO] Epoch: 36 , batch: 235 , training loss: 3.825330\n",
      "[INFO] Epoch: 36 , batch: 236 , training loss: 3.949512\n",
      "[INFO] Epoch: 36 , batch: 237 , training loss: 4.161944\n",
      "[INFO] Epoch: 36 , batch: 238 , training loss: 3.946962\n",
      "[INFO] Epoch: 36 , batch: 239 , training loss: 3.978520\n",
      "[INFO] Epoch: 36 , batch: 240 , training loss: 3.998587\n",
      "[INFO] Epoch: 36 , batch: 241 , training loss: 3.826609\n",
      "[INFO] Epoch: 36 , batch: 242 , training loss: 3.831482\n",
      "[INFO] Epoch: 36 , batch: 243 , training loss: 4.136167\n",
      "[INFO] Epoch: 36 , batch: 244 , training loss: 4.055528\n",
      "[INFO] Epoch: 36 , batch: 245 , training loss: 4.035333\n",
      "[INFO] Epoch: 36 , batch: 246 , training loss: 3.754146\n",
      "[INFO] Epoch: 36 , batch: 247 , training loss: 3.900677\n",
      "[INFO] Epoch: 36 , batch: 248 , training loss: 3.997087\n",
      "[INFO] Epoch: 36 , batch: 249 , training loss: 3.968218\n",
      "[INFO] Epoch: 36 , batch: 250 , training loss: 3.779379\n",
      "[INFO] Epoch: 36 , batch: 251 , training loss: 4.200899\n",
      "[INFO] Epoch: 36 , batch: 252 , training loss: 3.926523\n",
      "[INFO] Epoch: 36 , batch: 253 , training loss: 3.827919\n",
      "[INFO] Epoch: 36 , batch: 254 , training loss: 4.106493\n",
      "[INFO] Epoch: 36 , batch: 255 , training loss: 4.100966\n",
      "[INFO] Epoch: 36 , batch: 256 , training loss: 4.052741\n",
      "[INFO] Epoch: 36 , batch: 257 , training loss: 4.243484\n",
      "[INFO] Epoch: 36 , batch: 258 , training loss: 4.231611\n",
      "[INFO] Epoch: 36 , batch: 259 , training loss: 4.272223\n",
      "[INFO] Epoch: 36 , batch: 260 , training loss: 4.057616\n",
      "[INFO] Epoch: 36 , batch: 261 , training loss: 4.228281\n",
      "[INFO] Epoch: 36 , batch: 262 , training loss: 4.375154\n",
      "[INFO] Epoch: 36 , batch: 263 , training loss: 4.535704\n",
      "[INFO] Epoch: 36 , batch: 264 , training loss: 3.903736\n",
      "[INFO] Epoch: 36 , batch: 265 , training loss: 4.007529\n",
      "[INFO] Epoch: 36 , batch: 266 , training loss: 4.403217\n",
      "[INFO] Epoch: 36 , batch: 267 , training loss: 4.148156\n",
      "[INFO] Epoch: 36 , batch: 268 , training loss: 4.072371\n",
      "[INFO] Epoch: 36 , batch: 269 , training loss: 4.067726\n",
      "[INFO] Epoch: 36 , batch: 270 , training loss: 4.082645\n",
      "[INFO] Epoch: 36 , batch: 271 , training loss: 4.115458\n",
      "[INFO] Epoch: 36 , batch: 272 , training loss: 4.116116\n",
      "[INFO] Epoch: 36 , batch: 273 , training loss: 4.124433\n",
      "[INFO] Epoch: 36 , batch: 274 , training loss: 4.215942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 36 , batch: 275 , training loss: 4.086976\n",
      "[INFO] Epoch: 36 , batch: 276 , training loss: 4.144368\n",
      "[INFO] Epoch: 36 , batch: 277 , training loss: 4.298071\n",
      "[INFO] Epoch: 36 , batch: 278 , training loss: 3.980088\n",
      "[INFO] Epoch: 36 , batch: 279 , training loss: 3.986315\n",
      "[INFO] Epoch: 36 , batch: 280 , training loss: 3.972628\n",
      "[INFO] Epoch: 36 , batch: 281 , training loss: 4.098809\n",
      "[INFO] Epoch: 36 , batch: 282 , training loss: 3.996089\n",
      "[INFO] Epoch: 36 , batch: 283 , training loss: 3.994790\n",
      "[INFO] Epoch: 36 , batch: 284 , training loss: 4.043300\n",
      "[INFO] Epoch: 36 , batch: 285 , training loss: 3.984307\n",
      "[INFO] Epoch: 36 , batch: 286 , training loss: 3.968808\n",
      "[INFO] Epoch: 36 , batch: 287 , training loss: 3.935164\n",
      "[INFO] Epoch: 36 , batch: 288 , training loss: 3.886410\n",
      "[INFO] Epoch: 36 , batch: 289 , training loss: 3.955086\n",
      "[INFO] Epoch: 36 , batch: 290 , training loss: 3.746151\n",
      "[INFO] Epoch: 36 , batch: 291 , training loss: 3.738295\n",
      "[INFO] Epoch: 36 , batch: 292 , training loss: 3.826791\n",
      "[INFO] Epoch: 36 , batch: 293 , training loss: 3.744977\n",
      "[INFO] Epoch: 36 , batch: 294 , training loss: 4.418366\n",
      "[INFO] Epoch: 36 , batch: 295 , training loss: 4.218265\n",
      "[INFO] Epoch: 36 , batch: 296 , training loss: 4.148980\n",
      "[INFO] Epoch: 36 , batch: 297 , training loss: 4.086730\n",
      "[INFO] Epoch: 36 , batch: 298 , training loss: 3.929089\n",
      "[INFO] Epoch: 36 , batch: 299 , training loss: 3.976522\n",
      "[INFO] Epoch: 36 , batch: 300 , training loss: 3.960669\n",
      "[INFO] Epoch: 36 , batch: 301 , training loss: 3.882203\n",
      "[INFO] Epoch: 36 , batch: 302 , training loss: 4.074889\n",
      "[INFO] Epoch: 36 , batch: 303 , training loss: 4.076425\n",
      "[INFO] Epoch: 36 , batch: 304 , training loss: 4.199890\n",
      "[INFO] Epoch: 36 , batch: 305 , training loss: 4.025291\n",
      "[INFO] Epoch: 36 , batch: 306 , training loss: 4.145595\n",
      "[INFO] Epoch: 36 , batch: 307 , training loss: 4.163316\n",
      "[INFO] Epoch: 36 , batch: 308 , training loss: 3.965715\n",
      "[INFO] Epoch: 36 , batch: 309 , training loss: 3.973118\n",
      "[INFO] Epoch: 36 , batch: 310 , training loss: 3.897934\n",
      "[INFO] Epoch: 36 , batch: 311 , training loss: 3.902003\n",
      "[INFO] Epoch: 36 , batch: 312 , training loss: 3.817261\n",
      "[INFO] Epoch: 36 , batch: 313 , training loss: 3.900093\n",
      "[INFO] Epoch: 36 , batch: 314 , training loss: 3.964837\n",
      "[INFO] Epoch: 36 , batch: 315 , training loss: 4.046579\n",
      "[INFO] Epoch: 36 , batch: 316 , training loss: 4.283919\n",
      "[INFO] Epoch: 36 , batch: 317 , training loss: 4.622547\n",
      "[INFO] Epoch: 36 , batch: 318 , training loss: 4.760737\n",
      "[INFO] Epoch: 36 , batch: 319 , training loss: 4.443951\n",
      "[INFO] Epoch: 36 , batch: 320 , training loss: 4.011382\n",
      "[INFO] Epoch: 36 , batch: 321 , training loss: 3.827667\n",
      "[INFO] Epoch: 36 , batch: 322 , training loss: 3.934342\n",
      "[INFO] Epoch: 36 , batch: 323 , training loss: 3.974182\n",
      "[INFO] Epoch: 36 , batch: 324 , training loss: 3.947206\n",
      "[INFO] Epoch: 36 , batch: 325 , training loss: 4.043427\n",
      "[INFO] Epoch: 36 , batch: 326 , training loss: 4.119951\n",
      "[INFO] Epoch: 36 , batch: 327 , training loss: 4.050200\n",
      "[INFO] Epoch: 36 , batch: 328 , training loss: 4.040716\n",
      "[INFO] Epoch: 36 , batch: 329 , training loss: 3.968157\n",
      "[INFO] Epoch: 36 , batch: 330 , training loss: 3.947752\n",
      "[INFO] Epoch: 36 , batch: 331 , training loss: 4.094104\n",
      "[INFO] Epoch: 36 , batch: 332 , training loss: 3.960081\n",
      "[INFO] Epoch: 36 , batch: 333 , training loss: 3.921581\n",
      "[INFO] Epoch: 36 , batch: 334 , training loss: 3.938627\n",
      "[INFO] Epoch: 36 , batch: 335 , training loss: 4.065812\n",
      "[INFO] Epoch: 36 , batch: 336 , training loss: 4.075411\n",
      "[INFO] Epoch: 36 , batch: 337 , training loss: 4.120062\n",
      "[INFO] Epoch: 36 , batch: 338 , training loss: 4.325378\n",
      "[INFO] Epoch: 36 , batch: 339 , training loss: 4.154390\n",
      "[INFO] Epoch: 36 , batch: 340 , training loss: 4.327046\n",
      "[INFO] Epoch: 36 , batch: 341 , training loss: 4.074289\n",
      "[INFO] Epoch: 36 , batch: 342 , training loss: 3.879040\n",
      "[INFO] Epoch: 36 , batch: 343 , training loss: 3.938399\n",
      "[INFO] Epoch: 36 , batch: 344 , training loss: 3.828296\n",
      "[INFO] Epoch: 36 , batch: 345 , training loss: 3.951078\n",
      "[INFO] Epoch: 36 , batch: 346 , training loss: 3.997479\n",
      "[INFO] Epoch: 36 , batch: 347 , training loss: 3.893837\n",
      "[INFO] Epoch: 36 , batch: 348 , training loss: 4.004568\n",
      "[INFO] Epoch: 36 , batch: 349 , training loss: 4.094012\n",
      "[INFO] Epoch: 36 , batch: 350 , training loss: 3.950169\n",
      "[INFO] Epoch: 36 , batch: 351 , training loss: 4.039997\n",
      "[INFO] Epoch: 36 , batch: 352 , training loss: 4.044815\n",
      "[INFO] Epoch: 36 , batch: 353 , training loss: 4.020308\n",
      "[INFO] Epoch: 36 , batch: 354 , training loss: 4.127645\n",
      "[INFO] Epoch: 36 , batch: 355 , training loss: 4.113530\n",
      "[INFO] Epoch: 36 , batch: 356 , training loss: 3.988226\n",
      "[INFO] Epoch: 36 , batch: 357 , training loss: 4.066271\n",
      "[INFO] Epoch: 36 , batch: 358 , training loss: 3.955725\n",
      "[INFO] Epoch: 36 , batch: 359 , training loss: 3.955095\n",
      "[INFO] Epoch: 36 , batch: 360 , training loss: 4.075692\n",
      "[INFO] Epoch: 36 , batch: 361 , training loss: 4.053191\n",
      "[INFO] Epoch: 36 , batch: 362 , training loss: 4.150343\n",
      "[INFO] Epoch: 36 , batch: 363 , training loss: 4.017681\n",
      "[INFO] Epoch: 36 , batch: 364 , training loss: 4.084553\n",
      "[INFO] Epoch: 36 , batch: 365 , training loss: 3.989545\n",
      "[INFO] Epoch: 36 , batch: 366 , training loss: 4.094106\n",
      "[INFO] Epoch: 36 , batch: 367 , training loss: 4.153549\n",
      "[INFO] Epoch: 36 , batch: 368 , training loss: 4.507631\n",
      "[INFO] Epoch: 36 , batch: 369 , training loss: 4.215714\n",
      "[INFO] Epoch: 36 , batch: 370 , training loss: 3.991784\n",
      "[INFO] Epoch: 36 , batch: 371 , training loss: 4.412250\n",
      "[INFO] Epoch: 36 , batch: 372 , training loss: 4.635311\n",
      "[INFO] Epoch: 36 , batch: 373 , training loss: 4.696423\n",
      "[INFO] Epoch: 36 , batch: 374 , training loss: 4.837585\n",
      "[INFO] Epoch: 36 , batch: 375 , training loss: 4.787922\n",
      "[INFO] Epoch: 36 , batch: 376 , training loss: 4.674075\n",
      "[INFO] Epoch: 36 , batch: 377 , training loss: 4.445625\n",
      "[INFO] Epoch: 36 , batch: 378 , training loss: 4.556720\n",
      "[INFO] Epoch: 36 , batch: 379 , training loss: 4.514132\n",
      "[INFO] Epoch: 36 , batch: 380 , training loss: 4.691301\n",
      "[INFO] Epoch: 36 , batch: 381 , training loss: 4.391607\n",
      "[INFO] Epoch: 36 , batch: 382 , training loss: 4.667221\n",
      "[INFO] Epoch: 36 , batch: 383 , training loss: 4.682913\n",
      "[INFO] Epoch: 36 , batch: 384 , training loss: 4.685360\n",
      "[INFO] Epoch: 36 , batch: 385 , training loss: 4.307777\n",
      "[INFO] Epoch: 36 , batch: 386 , training loss: 4.591372\n",
      "[INFO] Epoch: 36 , batch: 387 , training loss: 4.558352\n",
      "[INFO] Epoch: 36 , batch: 388 , training loss: 4.370677\n",
      "[INFO] Epoch: 36 , batch: 389 , training loss: 4.176792\n",
      "[INFO] Epoch: 36 , batch: 390 , training loss: 4.202055\n",
      "[INFO] Epoch: 36 , batch: 391 , training loss: 4.213961\n",
      "[INFO] Epoch: 36 , batch: 392 , training loss: 4.615614\n",
      "[INFO] Epoch: 36 , batch: 393 , training loss: 4.475037\n",
      "[INFO] Epoch: 36 , batch: 394 , training loss: 4.567753\n",
      "[INFO] Epoch: 36 , batch: 395 , training loss: 4.400428\n",
      "[INFO] Epoch: 36 , batch: 396 , training loss: 4.207491\n",
      "[INFO] Epoch: 36 , batch: 397 , training loss: 4.361444\n",
      "[INFO] Epoch: 36 , batch: 398 , training loss: 4.215287\n",
      "[INFO] Epoch: 36 , batch: 399 , training loss: 4.299738\n",
      "[INFO] Epoch: 36 , batch: 400 , training loss: 4.262501\n",
      "[INFO] Epoch: 36 , batch: 401 , training loss: 4.698631\n",
      "[INFO] Epoch: 36 , batch: 402 , training loss: 4.423973\n",
      "[INFO] Epoch: 36 , batch: 403 , training loss: 4.266795\n",
      "[INFO] Epoch: 36 , batch: 404 , training loss: 4.419991\n",
      "[INFO] Epoch: 36 , batch: 405 , training loss: 4.468909\n",
      "[INFO] Epoch: 36 , batch: 406 , training loss: 4.365637\n",
      "[INFO] Epoch: 36 , batch: 407 , training loss: 4.411700\n",
      "[INFO] Epoch: 36 , batch: 408 , training loss: 4.390997\n",
      "[INFO] Epoch: 36 , batch: 409 , training loss: 4.407801\n",
      "[INFO] Epoch: 36 , batch: 410 , training loss: 4.468296\n",
      "[INFO] Epoch: 36 , batch: 411 , training loss: 4.600550\n",
      "[INFO] Epoch: 36 , batch: 412 , training loss: 4.437616\n",
      "[INFO] Epoch: 36 , batch: 413 , training loss: 4.325709\n",
      "[INFO] Epoch: 36 , batch: 414 , training loss: 4.368499\n",
      "[INFO] Epoch: 36 , batch: 415 , training loss: 4.416306\n",
      "[INFO] Epoch: 36 , batch: 416 , training loss: 4.468750\n",
      "[INFO] Epoch: 36 , batch: 417 , training loss: 4.383824\n",
      "[INFO] Epoch: 36 , batch: 418 , training loss: 4.456648\n",
      "[INFO] Epoch: 36 , batch: 419 , training loss: 4.411967\n",
      "[INFO] Epoch: 36 , batch: 420 , training loss: 4.371771\n",
      "[INFO] Epoch: 36 , batch: 421 , training loss: 4.353613\n",
      "[INFO] Epoch: 36 , batch: 422 , training loss: 4.201514\n",
      "[INFO] Epoch: 36 , batch: 423 , training loss: 4.431676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 36 , batch: 424 , training loss: 4.612988\n",
      "[INFO] Epoch: 36 , batch: 425 , training loss: 4.458148\n",
      "[INFO] Epoch: 36 , batch: 426 , training loss: 4.204048\n",
      "[INFO] Epoch: 36 , batch: 427 , training loss: 4.454833\n",
      "[INFO] Epoch: 36 , batch: 428 , training loss: 4.314732\n",
      "[INFO] Epoch: 36 , batch: 429 , training loss: 4.213966\n",
      "[INFO] Epoch: 36 , batch: 430 , training loss: 4.439541\n",
      "[INFO] Epoch: 36 , batch: 431 , training loss: 4.050364\n",
      "[INFO] Epoch: 36 , batch: 432 , training loss: 4.111505\n",
      "[INFO] Epoch: 36 , batch: 433 , training loss: 4.149686\n",
      "[INFO] Epoch: 36 , batch: 434 , training loss: 4.038296\n",
      "[INFO] Epoch: 36 , batch: 435 , training loss: 4.374269\n",
      "[INFO] Epoch: 36 , batch: 436 , training loss: 4.412995\n",
      "[INFO] Epoch: 36 , batch: 437 , training loss: 4.227060\n",
      "[INFO] Epoch: 36 , batch: 438 , training loss: 4.073155\n",
      "[INFO] Epoch: 36 , batch: 439 , training loss: 4.332076\n",
      "[INFO] Epoch: 36 , batch: 440 , training loss: 4.431825\n",
      "[INFO] Epoch: 36 , batch: 441 , training loss: 4.518878\n",
      "[INFO] Epoch: 36 , batch: 442 , training loss: 4.278625\n",
      "[INFO] Epoch: 36 , batch: 443 , training loss: 4.464463\n",
      "[INFO] Epoch: 36 , batch: 444 , training loss: 4.073718\n",
      "[INFO] Epoch: 36 , batch: 445 , training loss: 3.974682\n",
      "[INFO] Epoch: 36 , batch: 446 , training loss: 3.912109\n",
      "[INFO] Epoch: 36 , batch: 447 , training loss: 4.110330\n",
      "[INFO] Epoch: 36 , batch: 448 , training loss: 4.225966\n",
      "[INFO] Epoch: 36 , batch: 449 , training loss: 4.596615\n",
      "[INFO] Epoch: 36 , batch: 450 , training loss: 4.668764\n",
      "[INFO] Epoch: 36 , batch: 451 , training loss: 4.548352\n",
      "[INFO] Epoch: 36 , batch: 452 , training loss: 4.395189\n",
      "[INFO] Epoch: 36 , batch: 453 , training loss: 4.161477\n",
      "[INFO] Epoch: 36 , batch: 454 , training loss: 4.290137\n",
      "[INFO] Epoch: 36 , batch: 455 , training loss: 4.361645\n",
      "[INFO] Epoch: 36 , batch: 456 , training loss: 4.345858\n",
      "[INFO] Epoch: 36 , batch: 457 , training loss: 4.403389\n",
      "[INFO] Epoch: 36 , batch: 458 , training loss: 4.155357\n",
      "[INFO] Epoch: 36 , batch: 459 , training loss: 4.142274\n",
      "[INFO] Epoch: 36 , batch: 460 , training loss: 4.252154\n",
      "[INFO] Epoch: 36 , batch: 461 , training loss: 4.225638\n",
      "[INFO] Epoch: 36 , batch: 462 , training loss: 4.283017\n",
      "[INFO] Epoch: 36 , batch: 463 , training loss: 4.192026\n",
      "[INFO] Epoch: 36 , batch: 464 , training loss: 4.363909\n",
      "[INFO] Epoch: 36 , batch: 465 , training loss: 4.302447\n",
      "[INFO] Epoch: 36 , batch: 466 , training loss: 4.423338\n",
      "[INFO] Epoch: 36 , batch: 467 , training loss: 4.379224\n",
      "[INFO] Epoch: 36 , batch: 468 , training loss: 4.332216\n",
      "[INFO] Epoch: 36 , batch: 469 , training loss: 4.339293\n",
      "[INFO] Epoch: 36 , batch: 470 , training loss: 4.191502\n",
      "[INFO] Epoch: 36 , batch: 471 , training loss: 4.295662\n",
      "[INFO] Epoch: 36 , batch: 472 , training loss: 4.340486\n",
      "[INFO] Epoch: 36 , batch: 473 , training loss: 4.266796\n",
      "[INFO] Epoch: 36 , batch: 474 , training loss: 4.043167\n",
      "[INFO] Epoch: 36 , batch: 475 , training loss: 3.916225\n",
      "[INFO] Epoch: 36 , batch: 476 , training loss: 4.339277\n",
      "[INFO] Epoch: 36 , batch: 477 , training loss: 4.432520\n",
      "[INFO] Epoch: 36 , batch: 478 , training loss: 4.423359\n",
      "[INFO] Epoch: 36 , batch: 479 , training loss: 4.408315\n",
      "[INFO] Epoch: 36 , batch: 480 , training loss: 4.530990\n",
      "[INFO] Epoch: 36 , batch: 481 , training loss: 4.409816\n",
      "[INFO] Epoch: 36 , batch: 482 , training loss: 4.530645\n",
      "[INFO] Epoch: 36 , batch: 483 , training loss: 4.377810\n",
      "[INFO] Epoch: 36 , batch: 484 , training loss: 4.167348\n",
      "[INFO] Epoch: 36 , batch: 485 , training loss: 4.262085\n",
      "[INFO] Epoch: 36 , batch: 486 , training loss: 4.158852\n",
      "[INFO] Epoch: 36 , batch: 487 , training loss: 4.141127\n",
      "[INFO] Epoch: 36 , batch: 488 , training loss: 4.319246\n",
      "[INFO] Epoch: 36 , batch: 489 , training loss: 4.216996\n",
      "[INFO] Epoch: 36 , batch: 490 , training loss: 4.289146\n",
      "[INFO] Epoch: 36 , batch: 491 , training loss: 4.197805\n",
      "[INFO] Epoch: 36 , batch: 492 , training loss: 4.186271\n",
      "[INFO] Epoch: 36 , batch: 493 , training loss: 4.343227\n",
      "[INFO] Epoch: 36 , batch: 494 , training loss: 4.264820\n",
      "[INFO] Epoch: 36 , batch: 495 , training loss: 4.422906\n",
      "[INFO] Epoch: 36 , batch: 496 , training loss: 4.291431\n",
      "[INFO] Epoch: 36 , batch: 497 , training loss: 4.347922\n",
      "[INFO] Epoch: 36 , batch: 498 , training loss: 4.299050\n",
      "[INFO] Epoch: 36 , batch: 499 , training loss: 4.380880\n",
      "[INFO] Epoch: 36 , batch: 500 , training loss: 4.492610\n",
      "[INFO] Epoch: 36 , batch: 501 , training loss: 4.859274\n",
      "[INFO] Epoch: 36 , batch: 502 , training loss: 4.863380\n",
      "[INFO] Epoch: 36 , batch: 503 , training loss: 4.513428\n",
      "[INFO] Epoch: 36 , batch: 504 , training loss: 4.670449\n",
      "[INFO] Epoch: 36 , batch: 505 , training loss: 4.633462\n",
      "[INFO] Epoch: 36 , batch: 506 , training loss: 4.621337\n",
      "[INFO] Epoch: 36 , batch: 507 , training loss: 4.658442\n",
      "[INFO] Epoch: 36 , batch: 508 , training loss: 4.580426\n",
      "[INFO] Epoch: 36 , batch: 509 , training loss: 4.397036\n",
      "[INFO] Epoch: 36 , batch: 510 , training loss: 4.483595\n",
      "[INFO] Epoch: 36 , batch: 511 , training loss: 4.399132\n",
      "[INFO] Epoch: 36 , batch: 512 , training loss: 4.498205\n",
      "[INFO] Epoch: 36 , batch: 513 , training loss: 4.747285\n",
      "[INFO] Epoch: 36 , batch: 514 , training loss: 4.383141\n",
      "[INFO] Epoch: 36 , batch: 515 , training loss: 4.650876\n",
      "[INFO] Epoch: 36 , batch: 516 , training loss: 4.441374\n",
      "[INFO] Epoch: 36 , batch: 517 , training loss: 4.406289\n",
      "[INFO] Epoch: 36 , batch: 518 , training loss: 4.382053\n",
      "[INFO] Epoch: 36 , batch: 519 , training loss: 4.214516\n",
      "[INFO] Epoch: 36 , batch: 520 , training loss: 4.452060\n",
      "[INFO] Epoch: 36 , batch: 521 , training loss: 4.429606\n",
      "[INFO] Epoch: 36 , batch: 522 , training loss: 4.519717\n",
      "[INFO] Epoch: 36 , batch: 523 , training loss: 4.422277\n",
      "[INFO] Epoch: 36 , batch: 524 , training loss: 4.706969\n",
      "[INFO] Epoch: 36 , batch: 525 , training loss: 4.580129\n",
      "[INFO] Epoch: 36 , batch: 526 , training loss: 4.382960\n",
      "[INFO] Epoch: 36 , batch: 527 , training loss: 4.424481\n",
      "[INFO] Epoch: 36 , batch: 528 , training loss: 4.440617\n",
      "[INFO] Epoch: 36 , batch: 529 , training loss: 4.432623\n",
      "[INFO] Epoch: 36 , batch: 530 , training loss: 4.263530\n",
      "[INFO] Epoch: 36 , batch: 531 , training loss: 4.408628\n",
      "[INFO] Epoch: 36 , batch: 532 , training loss: 4.318895\n",
      "[INFO] Epoch: 36 , batch: 533 , training loss: 4.447480\n",
      "[INFO] Epoch: 36 , batch: 534 , training loss: 4.459309\n",
      "[INFO] Epoch: 36 , batch: 535 , training loss: 4.467370\n",
      "[INFO] Epoch: 36 , batch: 536 , training loss: 4.308570\n",
      "[INFO] Epoch: 36 , batch: 537 , training loss: 4.274132\n",
      "[INFO] Epoch: 36 , batch: 538 , training loss: 4.359190\n",
      "[INFO] Epoch: 36 , batch: 539 , training loss: 4.450664\n",
      "[INFO] Epoch: 36 , batch: 540 , training loss: 5.011002\n",
      "[INFO] Epoch: 36 , batch: 541 , training loss: 4.794596\n",
      "[INFO] Epoch: 36 , batch: 542 , training loss: 4.687973\n",
      "[INFO] Epoch: 37 , batch: 0 , training loss: 3.636476\n",
      "[INFO] Epoch: 37 , batch: 1 , training loss: 3.506697\n",
      "[INFO] Epoch: 37 , batch: 2 , training loss: 3.709453\n",
      "[INFO] Epoch: 37 , batch: 3 , training loss: 3.571500\n",
      "[INFO] Epoch: 37 , batch: 4 , training loss: 3.881354\n",
      "[INFO] Epoch: 37 , batch: 5 , training loss: 3.539936\n",
      "[INFO] Epoch: 37 , batch: 6 , training loss: 3.876257\n",
      "[INFO] Epoch: 37 , batch: 7 , training loss: 3.820416\n",
      "[INFO] Epoch: 37 , batch: 8 , training loss: 3.521176\n",
      "[INFO] Epoch: 37 , batch: 9 , training loss: 3.775126\n",
      "[INFO] Epoch: 37 , batch: 10 , training loss: 3.737245\n",
      "[INFO] Epoch: 37 , batch: 11 , training loss: 3.656502\n",
      "[INFO] Epoch: 37 , batch: 12 , training loss: 3.518673\n",
      "[INFO] Epoch: 37 , batch: 13 , training loss: 3.557675\n",
      "[INFO] Epoch: 37 , batch: 14 , training loss: 3.504166\n",
      "[INFO] Epoch: 37 , batch: 15 , training loss: 3.713130\n",
      "[INFO] Epoch: 37 , batch: 16 , training loss: 3.561063\n",
      "[INFO] Epoch: 37 , batch: 17 , training loss: 3.687187\n",
      "[INFO] Epoch: 37 , batch: 18 , training loss: 3.653995\n",
      "[INFO] Epoch: 37 , batch: 19 , training loss: 3.416303\n",
      "[INFO] Epoch: 37 , batch: 20 , training loss: 3.360410\n",
      "[INFO] Epoch: 37 , batch: 21 , training loss: 3.520561\n",
      "[INFO] Epoch: 37 , batch: 22 , training loss: 3.420485\n",
      "[INFO] Epoch: 37 , batch: 23 , training loss: 3.607766\n",
      "[INFO] Epoch: 37 , batch: 24 , training loss: 3.437247\n",
      "[INFO] Epoch: 37 , batch: 25 , training loss: 3.586059\n",
      "[INFO] Epoch: 37 , batch: 26 , training loss: 3.453033\n",
      "[INFO] Epoch: 37 , batch: 27 , training loss: 3.399519\n",
      "[INFO] Epoch: 37 , batch: 28 , training loss: 3.602293\n",
      "[INFO] Epoch: 37 , batch: 29 , training loss: 3.384776\n",
      "[INFO] Epoch: 37 , batch: 30 , training loss: 3.422864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 37 , batch: 31 , training loss: 3.545161\n",
      "[INFO] Epoch: 37 , batch: 32 , training loss: 3.521429\n",
      "[INFO] Epoch: 37 , batch: 33 , training loss: 3.551753\n",
      "[INFO] Epoch: 37 , batch: 34 , training loss: 3.527877\n",
      "[INFO] Epoch: 37 , batch: 35 , training loss: 3.492611\n",
      "[INFO] Epoch: 37 , batch: 36 , training loss: 3.585309\n",
      "[INFO] Epoch: 37 , batch: 37 , training loss: 3.436116\n",
      "[INFO] Epoch: 37 , batch: 38 , training loss: 3.511672\n",
      "[INFO] Epoch: 37 , batch: 39 , training loss: 3.345697\n",
      "[INFO] Epoch: 37 , batch: 40 , training loss: 3.576025\n",
      "[INFO] Epoch: 37 , batch: 41 , training loss: 3.453083\n",
      "[INFO] Epoch: 37 , batch: 42 , training loss: 3.928131\n",
      "[INFO] Epoch: 37 , batch: 43 , training loss: 3.685344\n",
      "[INFO] Epoch: 37 , batch: 44 , training loss: 4.049249\n",
      "[INFO] Epoch: 37 , batch: 45 , training loss: 4.006593\n",
      "[INFO] Epoch: 37 , batch: 46 , training loss: 3.909363\n",
      "[INFO] Epoch: 37 , batch: 47 , training loss: 3.552333\n",
      "[INFO] Epoch: 37 , batch: 48 , training loss: 3.546522\n",
      "[INFO] Epoch: 37 , batch: 49 , training loss: 3.842191\n",
      "[INFO] Epoch: 37 , batch: 50 , training loss: 3.560444\n",
      "[INFO] Epoch: 37 , batch: 51 , training loss: 3.802985\n",
      "[INFO] Epoch: 37 , batch: 52 , training loss: 3.633598\n",
      "[INFO] Epoch: 37 , batch: 53 , training loss: 3.714265\n",
      "[INFO] Epoch: 37 , batch: 54 , training loss: 3.739543\n",
      "[INFO] Epoch: 37 , batch: 55 , training loss: 3.803968\n",
      "[INFO] Epoch: 37 , batch: 56 , training loss: 3.649586\n",
      "[INFO] Epoch: 37 , batch: 57 , training loss: 3.580304\n",
      "[INFO] Epoch: 37 , batch: 58 , training loss: 3.673514\n",
      "[INFO] Epoch: 37 , batch: 59 , training loss: 3.733693\n",
      "[INFO] Epoch: 37 , batch: 60 , training loss: 3.641582\n",
      "[INFO] Epoch: 37 , batch: 61 , training loss: 3.737509\n",
      "[INFO] Epoch: 37 , batch: 62 , training loss: 3.638895\n",
      "[INFO] Epoch: 37 , batch: 63 , training loss: 3.800411\n",
      "[INFO] Epoch: 37 , batch: 64 , training loss: 4.027166\n",
      "[INFO] Epoch: 37 , batch: 65 , training loss: 3.709372\n",
      "[INFO] Epoch: 37 , batch: 66 , training loss: 3.601340\n",
      "[INFO] Epoch: 37 , batch: 67 , training loss: 3.601889\n",
      "[INFO] Epoch: 37 , batch: 68 , training loss: 3.738486\n",
      "[INFO] Epoch: 37 , batch: 69 , training loss: 3.678968\n",
      "[INFO] Epoch: 37 , batch: 70 , training loss: 3.908559\n",
      "[INFO] Epoch: 37 , batch: 71 , training loss: 3.789231\n",
      "[INFO] Epoch: 37 , batch: 72 , training loss: 3.834552\n",
      "[INFO] Epoch: 37 , batch: 73 , training loss: 3.772302\n",
      "[INFO] Epoch: 37 , batch: 74 , training loss: 3.880266\n",
      "[INFO] Epoch: 37 , batch: 75 , training loss: 3.754175\n",
      "[INFO] Epoch: 37 , batch: 76 , training loss: 3.853256\n",
      "[INFO] Epoch: 37 , batch: 77 , training loss: 3.755517\n",
      "[INFO] Epoch: 37 , batch: 78 , training loss: 3.848341\n",
      "[INFO] Epoch: 37 , batch: 79 , training loss: 3.741730\n",
      "[INFO] Epoch: 37 , batch: 80 , training loss: 3.931104\n",
      "[INFO] Epoch: 37 , batch: 81 , training loss: 3.858429\n",
      "[INFO] Epoch: 37 , batch: 82 , training loss: 3.853301\n",
      "[INFO] Epoch: 37 , batch: 83 , training loss: 3.926038\n",
      "[INFO] Epoch: 37 , batch: 84 , training loss: 3.898914\n",
      "[INFO] Epoch: 37 , batch: 85 , training loss: 4.003534\n",
      "[INFO] Epoch: 37 , batch: 86 , training loss: 3.880932\n",
      "[INFO] Epoch: 37 , batch: 87 , training loss: 3.881967\n",
      "[INFO] Epoch: 37 , batch: 88 , training loss: 4.003774\n",
      "[INFO] Epoch: 37 , batch: 89 , training loss: 3.795375\n",
      "[INFO] Epoch: 37 , batch: 90 , training loss: 3.915205\n",
      "[INFO] Epoch: 37 , batch: 91 , training loss: 3.807660\n",
      "[INFO] Epoch: 37 , batch: 92 , training loss: 3.855496\n",
      "[INFO] Epoch: 37 , batch: 93 , training loss: 3.933464\n",
      "[INFO] Epoch: 37 , batch: 94 , training loss: 4.097645\n",
      "[INFO] Epoch: 37 , batch: 95 , training loss: 3.857038\n",
      "[INFO] Epoch: 37 , batch: 96 , training loss: 3.848857\n",
      "[INFO] Epoch: 37 , batch: 97 , training loss: 3.782955\n",
      "[INFO] Epoch: 37 , batch: 98 , training loss: 3.719558\n",
      "[INFO] Epoch: 37 , batch: 99 , training loss: 3.858222\n",
      "[INFO] Epoch: 37 , batch: 100 , training loss: 3.737658\n",
      "[INFO] Epoch: 37 , batch: 101 , training loss: 3.763947\n",
      "[INFO] Epoch: 37 , batch: 102 , training loss: 3.952524\n",
      "[INFO] Epoch: 37 , batch: 103 , training loss: 3.714554\n",
      "[INFO] Epoch: 37 , batch: 104 , training loss: 3.657248\n",
      "[INFO] Epoch: 37 , batch: 105 , training loss: 3.899848\n",
      "[INFO] Epoch: 37 , batch: 106 , training loss: 3.953918\n",
      "[INFO] Epoch: 37 , batch: 107 , training loss: 3.775268\n",
      "[INFO] Epoch: 37 , batch: 108 , training loss: 3.731073\n",
      "[INFO] Epoch: 37 , batch: 109 , training loss: 3.664946\n",
      "[INFO] Epoch: 37 , batch: 110 , training loss: 3.821624\n",
      "[INFO] Epoch: 37 , batch: 111 , training loss: 3.903173\n",
      "[INFO] Epoch: 37 , batch: 112 , training loss: 3.846339\n",
      "[INFO] Epoch: 37 , batch: 113 , training loss: 3.839089\n",
      "[INFO] Epoch: 37 , batch: 114 , training loss: 3.809200\n",
      "[INFO] Epoch: 37 , batch: 115 , training loss: 3.829531\n",
      "[INFO] Epoch: 37 , batch: 116 , training loss: 3.715923\n",
      "[INFO] Epoch: 37 , batch: 117 , training loss: 3.920699\n",
      "[INFO] Epoch: 37 , batch: 118 , training loss: 3.905745\n",
      "[INFO] Epoch: 37 , batch: 119 , training loss: 4.062166\n",
      "[INFO] Epoch: 37 , batch: 120 , training loss: 4.023116\n",
      "[INFO] Epoch: 37 , batch: 121 , training loss: 3.892596\n",
      "[INFO] Epoch: 37 , batch: 122 , training loss: 3.808046\n",
      "[INFO] Epoch: 37 , batch: 123 , training loss: 3.790442\n",
      "[INFO] Epoch: 37 , batch: 124 , training loss: 3.903255\n",
      "[INFO] Epoch: 37 , batch: 125 , training loss: 3.707323\n",
      "[INFO] Epoch: 37 , batch: 126 , training loss: 3.763640\n",
      "[INFO] Epoch: 37 , batch: 127 , training loss: 3.747105\n",
      "[INFO] Epoch: 37 , batch: 128 , training loss: 3.874415\n",
      "[INFO] Epoch: 37 , batch: 129 , training loss: 3.830771\n",
      "[INFO] Epoch: 37 , batch: 130 , training loss: 3.828665\n",
      "[INFO] Epoch: 37 , batch: 131 , training loss: 3.813998\n",
      "[INFO] Epoch: 37 , batch: 132 , training loss: 3.856829\n",
      "[INFO] Epoch: 37 , batch: 133 , training loss: 3.815041\n",
      "[INFO] Epoch: 37 , batch: 134 , training loss: 3.563845\n",
      "[INFO] Epoch: 37 , batch: 135 , training loss: 3.650702\n",
      "[INFO] Epoch: 37 , batch: 136 , training loss: 3.920999\n",
      "[INFO] Epoch: 37 , batch: 137 , training loss: 3.877968\n",
      "[INFO] Epoch: 37 , batch: 138 , training loss: 3.902834\n",
      "[INFO] Epoch: 37 , batch: 139 , training loss: 4.433853\n",
      "[INFO] Epoch: 37 , batch: 140 , training loss: 4.229165\n",
      "[INFO] Epoch: 37 , batch: 141 , training loss: 4.026096\n",
      "[INFO] Epoch: 37 , batch: 142 , training loss: 3.744850\n",
      "[INFO] Epoch: 37 , batch: 143 , training loss: 3.898331\n",
      "[INFO] Epoch: 37 , batch: 144 , training loss: 3.755044\n",
      "[INFO] Epoch: 37 , batch: 145 , training loss: 3.816207\n",
      "[INFO] Epoch: 37 , batch: 146 , training loss: 4.006558\n",
      "[INFO] Epoch: 37 , batch: 147 , training loss: 3.660242\n",
      "[INFO] Epoch: 37 , batch: 148 , training loss: 3.659200\n",
      "[INFO] Epoch: 37 , batch: 149 , training loss: 3.730315\n",
      "[INFO] Epoch: 37 , batch: 150 , training loss: 3.990459\n",
      "[INFO] Epoch: 37 , batch: 151 , training loss: 3.836495\n",
      "[INFO] Epoch: 37 , batch: 152 , training loss: 3.863563\n",
      "[INFO] Epoch: 37 , batch: 153 , training loss: 3.864278\n",
      "[INFO] Epoch: 37 , batch: 154 , training loss: 3.958356\n",
      "[INFO] Epoch: 37 , batch: 155 , training loss: 4.138678\n",
      "[INFO] Epoch: 37 , batch: 156 , training loss: 3.898871\n",
      "[INFO] Epoch: 37 , batch: 157 , training loss: 3.871555\n",
      "[INFO] Epoch: 37 , batch: 158 , training loss: 3.968459\n",
      "[INFO] Epoch: 37 , batch: 159 , training loss: 3.908393\n",
      "[INFO] Epoch: 37 , batch: 160 , training loss: 4.170871\n",
      "[INFO] Epoch: 37 , batch: 161 , training loss: 4.207430\n",
      "[INFO] Epoch: 37 , batch: 162 , training loss: 4.196447\n",
      "[INFO] Epoch: 37 , batch: 163 , training loss: 4.375751\n",
      "[INFO] Epoch: 37 , batch: 164 , training loss: 4.354959\n",
      "[INFO] Epoch: 37 , batch: 165 , training loss: 4.303357\n",
      "[INFO] Epoch: 37 , batch: 166 , training loss: 4.180593\n",
      "[INFO] Epoch: 37 , batch: 167 , training loss: 4.302798\n",
      "[INFO] Epoch: 37 , batch: 168 , training loss: 3.945034\n",
      "[INFO] Epoch: 37 , batch: 169 , training loss: 3.949307\n",
      "[INFO] Epoch: 37 , batch: 170 , training loss: 4.087004\n",
      "[INFO] Epoch: 37 , batch: 171 , training loss: 3.564084\n",
      "[INFO] Epoch: 37 , batch: 172 , training loss: 3.747594\n",
      "[INFO] Epoch: 37 , batch: 173 , training loss: 4.087744\n",
      "[INFO] Epoch: 37 , batch: 174 , training loss: 4.564233\n",
      "[INFO] Epoch: 37 , batch: 175 , training loss: 4.868016\n",
      "[INFO] Epoch: 37 , batch: 176 , training loss: 4.562424\n",
      "[INFO] Epoch: 37 , batch: 177 , training loss: 4.129264\n",
      "[INFO] Epoch: 37 , batch: 178 , training loss: 4.176387\n",
      "[INFO] Epoch: 37 , batch: 179 , training loss: 4.214221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 37 , batch: 180 , training loss: 4.164264\n",
      "[INFO] Epoch: 37 , batch: 181 , training loss: 4.436489\n",
      "[INFO] Epoch: 37 , batch: 182 , training loss: 4.366288\n",
      "[INFO] Epoch: 37 , batch: 183 , training loss: 4.352732\n",
      "[INFO] Epoch: 37 , batch: 184 , training loss: 4.241742\n",
      "[INFO] Epoch: 37 , batch: 185 , training loss: 4.211580\n",
      "[INFO] Epoch: 37 , batch: 186 , training loss: 4.371560\n",
      "[INFO] Epoch: 37 , batch: 187 , training loss: 4.430622\n",
      "[INFO] Epoch: 37 , batch: 188 , training loss: 4.437229\n",
      "[INFO] Epoch: 37 , batch: 189 , training loss: 4.365757\n",
      "[INFO] Epoch: 37 , batch: 190 , training loss: 4.381150\n",
      "[INFO] Epoch: 37 , batch: 191 , training loss: 4.471437\n",
      "[INFO] Epoch: 37 , batch: 192 , training loss: 4.304593\n",
      "[INFO] Epoch: 37 , batch: 193 , training loss: 4.409979\n",
      "[INFO] Epoch: 37 , batch: 194 , training loss: 4.359212\n",
      "[INFO] Epoch: 37 , batch: 195 , training loss: 4.290950\n",
      "[INFO] Epoch: 37 , batch: 196 , training loss: 4.127093\n",
      "[INFO] Epoch: 37 , batch: 197 , training loss: 4.239878\n",
      "[INFO] Epoch: 37 , batch: 198 , training loss: 4.130674\n",
      "[INFO] Epoch: 37 , batch: 199 , training loss: 4.288845\n",
      "[INFO] Epoch: 37 , batch: 200 , training loss: 4.196798\n",
      "[INFO] Epoch: 37 , batch: 201 , training loss: 4.073723\n",
      "[INFO] Epoch: 37 , batch: 202 , training loss: 4.082259\n",
      "[INFO] Epoch: 37 , batch: 203 , training loss: 4.198147\n",
      "[INFO] Epoch: 37 , batch: 204 , training loss: 4.294549\n",
      "[INFO] Epoch: 37 , batch: 205 , training loss: 3.896085\n",
      "[INFO] Epoch: 37 , batch: 206 , training loss: 3.805004\n",
      "[INFO] Epoch: 37 , batch: 207 , training loss: 3.816340\n",
      "[INFO] Epoch: 37 , batch: 208 , training loss: 4.147760\n",
      "[INFO] Epoch: 37 , batch: 209 , training loss: 4.111785\n",
      "[INFO] Epoch: 37 , batch: 210 , training loss: 4.120433\n",
      "[INFO] Epoch: 37 , batch: 211 , training loss: 4.133899\n",
      "[INFO] Epoch: 37 , batch: 212 , training loss: 4.235277\n",
      "[INFO] Epoch: 37 , batch: 213 , training loss: 4.184579\n",
      "[INFO] Epoch: 37 , batch: 214 , training loss: 4.243711\n",
      "[INFO] Epoch: 37 , batch: 215 , training loss: 4.411821\n",
      "[INFO] Epoch: 37 , batch: 216 , training loss: 4.154150\n",
      "[INFO] Epoch: 37 , batch: 217 , training loss: 4.102793\n",
      "[INFO] Epoch: 37 , batch: 218 , training loss: 4.077270\n",
      "[INFO] Epoch: 37 , batch: 219 , training loss: 4.212377\n",
      "[INFO] Epoch: 37 , batch: 220 , training loss: 4.020497\n",
      "[INFO] Epoch: 37 , batch: 221 , training loss: 4.022424\n",
      "[INFO] Epoch: 37 , batch: 222 , training loss: 4.167466\n",
      "[INFO] Epoch: 37 , batch: 223 , training loss: 4.275255\n",
      "[INFO] Epoch: 37 , batch: 224 , training loss: 4.301360\n",
      "[INFO] Epoch: 37 , batch: 225 , training loss: 4.183545\n",
      "[INFO] Epoch: 37 , batch: 226 , training loss: 4.336496\n",
      "[INFO] Epoch: 37 , batch: 227 , training loss: 4.297011\n",
      "[INFO] Epoch: 37 , batch: 228 , training loss: 4.326880\n",
      "[INFO] Epoch: 37 , batch: 229 , training loss: 4.166604\n",
      "[INFO] Epoch: 37 , batch: 230 , training loss: 4.033706\n",
      "[INFO] Epoch: 37 , batch: 231 , training loss: 3.874929\n",
      "[INFO] Epoch: 37 , batch: 232 , training loss: 4.058476\n",
      "[INFO] Epoch: 37 , batch: 233 , training loss: 4.076830\n",
      "[INFO] Epoch: 37 , batch: 234 , training loss: 3.750728\n",
      "[INFO] Epoch: 37 , batch: 235 , training loss: 3.878565\n",
      "[INFO] Epoch: 37 , batch: 236 , training loss: 3.992414\n",
      "[INFO] Epoch: 37 , batch: 237 , training loss: 4.191212\n",
      "[INFO] Epoch: 37 , batch: 238 , training loss: 3.971198\n",
      "[INFO] Epoch: 37 , batch: 239 , training loss: 4.014096\n",
      "[INFO] Epoch: 37 , batch: 240 , training loss: 4.045448\n",
      "[INFO] Epoch: 37 , batch: 241 , training loss: 3.874048\n",
      "[INFO] Epoch: 37 , batch: 242 , training loss: 3.869460\n",
      "[INFO] Epoch: 37 , batch: 243 , training loss: 4.161194\n",
      "[INFO] Epoch: 37 , batch: 244 , training loss: 4.102191\n",
      "[INFO] Epoch: 37 , batch: 245 , training loss: 4.072183\n",
      "[INFO] Epoch: 37 , batch: 246 , training loss: 3.786231\n",
      "[INFO] Epoch: 37 , batch: 247 , training loss: 3.936607\n",
      "[INFO] Epoch: 37 , batch: 248 , training loss: 4.002293\n",
      "[INFO] Epoch: 37 , batch: 249 , training loss: 3.997807\n",
      "[INFO] Epoch: 37 , batch: 250 , training loss: 3.807837\n",
      "[INFO] Epoch: 37 , batch: 251 , training loss: 4.229759\n",
      "[INFO] Epoch: 37 , batch: 252 , training loss: 3.947191\n",
      "[INFO] Epoch: 37 , batch: 253 , training loss: 3.856514\n",
      "[INFO] Epoch: 37 , batch: 254 , training loss: 4.149704\n",
      "[INFO] Epoch: 37 , batch: 255 , training loss: 4.126766\n",
      "[INFO] Epoch: 37 , batch: 256 , training loss: 4.097410\n",
      "[INFO] Epoch: 37 , batch: 257 , training loss: 4.289094\n",
      "[INFO] Epoch: 37 , batch: 258 , training loss: 4.272747\n",
      "[INFO] Epoch: 37 , batch: 259 , training loss: 4.349937\n",
      "[INFO] Epoch: 37 , batch: 260 , training loss: 4.091026\n",
      "[INFO] Epoch: 37 , batch: 261 , training loss: 4.283142\n",
      "[INFO] Epoch: 37 , batch: 262 , training loss: 4.397332\n",
      "[INFO] Epoch: 37 , batch: 263 , training loss: 4.576414\n",
      "[INFO] Epoch: 37 , batch: 264 , training loss: 3.929333\n",
      "[INFO] Epoch: 37 , batch: 265 , training loss: 4.064256\n",
      "[INFO] Epoch: 37 , batch: 266 , training loss: 4.468531\n",
      "[INFO] Epoch: 37 , batch: 267 , training loss: 4.201254\n",
      "[INFO] Epoch: 37 , batch: 268 , training loss: 4.104826\n",
      "[INFO] Epoch: 37 , batch: 269 , training loss: 4.105603\n",
      "[INFO] Epoch: 37 , batch: 270 , training loss: 4.127803\n",
      "[INFO] Epoch: 37 , batch: 271 , training loss: 4.164557\n",
      "[INFO] Epoch: 37 , batch: 272 , training loss: 4.150934\n",
      "[INFO] Epoch: 37 , batch: 273 , training loss: 4.146505\n",
      "[INFO] Epoch: 37 , batch: 274 , training loss: 4.235594\n",
      "[INFO] Epoch: 37 , batch: 275 , training loss: 4.109125\n",
      "[INFO] Epoch: 37 , batch: 276 , training loss: 4.176294\n",
      "[INFO] Epoch: 37 , batch: 277 , training loss: 4.328173\n",
      "[INFO] Epoch: 37 , batch: 278 , training loss: 4.012699\n",
      "[INFO] Epoch: 37 , batch: 279 , training loss: 4.016977\n",
      "[INFO] Epoch: 37 , batch: 280 , training loss: 3.992101\n",
      "[INFO] Epoch: 37 , batch: 281 , training loss: 4.131567\n",
      "[INFO] Epoch: 37 , batch: 282 , training loss: 4.021352\n",
      "[INFO] Epoch: 37 , batch: 283 , training loss: 4.021997\n",
      "[INFO] Epoch: 37 , batch: 284 , training loss: 4.075951\n",
      "[INFO] Epoch: 37 , batch: 285 , training loss: 4.016348\n",
      "[INFO] Epoch: 37 , batch: 286 , training loss: 4.013385\n",
      "[INFO] Epoch: 37 , batch: 287 , training loss: 3.969311\n",
      "[INFO] Epoch: 37 , batch: 288 , training loss: 3.919305\n",
      "[INFO] Epoch: 37 , batch: 289 , training loss: 3.998651\n",
      "[INFO] Epoch: 37 , batch: 290 , training loss: 3.785193\n",
      "[INFO] Epoch: 37 , batch: 291 , training loss: 3.754161\n",
      "[INFO] Epoch: 37 , batch: 292 , training loss: 3.862916\n",
      "[INFO] Epoch: 37 , batch: 293 , training loss: 3.796080\n",
      "[INFO] Epoch: 37 , batch: 294 , training loss: 4.443767\n",
      "[INFO] Epoch: 37 , batch: 295 , training loss: 4.251759\n",
      "[INFO] Epoch: 37 , batch: 296 , training loss: 4.189446\n",
      "[INFO] Epoch: 37 , batch: 297 , training loss: 4.116469\n",
      "[INFO] Epoch: 37 , batch: 298 , training loss: 3.967375\n",
      "[INFO] Epoch: 37 , batch: 299 , training loss: 3.992827\n",
      "[INFO] Epoch: 37 , batch: 300 , training loss: 3.995602\n",
      "[INFO] Epoch: 37 , batch: 301 , training loss: 3.912687\n",
      "[INFO] Epoch: 37 , batch: 302 , training loss: 4.084486\n",
      "[INFO] Epoch: 37 , batch: 303 , training loss: 4.107224\n",
      "[INFO] Epoch: 37 , batch: 304 , training loss: 4.233626\n",
      "[INFO] Epoch: 37 , batch: 305 , training loss: 4.068835\n",
      "[INFO] Epoch: 37 , batch: 306 , training loss: 4.175307\n",
      "[INFO] Epoch: 37 , batch: 307 , training loss: 4.177712\n",
      "[INFO] Epoch: 37 , batch: 308 , training loss: 4.009002\n",
      "[INFO] Epoch: 37 , batch: 309 , training loss: 3.998546\n",
      "[INFO] Epoch: 37 , batch: 310 , training loss: 3.918225\n",
      "[INFO] Epoch: 37 , batch: 311 , training loss: 3.932055\n",
      "[INFO] Epoch: 37 , batch: 312 , training loss: 3.846927\n",
      "[INFO] Epoch: 37 , batch: 313 , training loss: 3.937420\n",
      "[INFO] Epoch: 37 , batch: 314 , training loss: 3.995148\n",
      "[INFO] Epoch: 37 , batch: 315 , training loss: 4.079505\n",
      "[INFO] Epoch: 37 , batch: 316 , training loss: 4.320493\n",
      "[INFO] Epoch: 37 , batch: 317 , training loss: 4.683989\n",
      "[INFO] Epoch: 37 , batch: 318 , training loss: 4.824722\n",
      "[INFO] Epoch: 37 , batch: 319 , training loss: 4.467322\n",
      "[INFO] Epoch: 37 , batch: 320 , training loss: 4.033082\n",
      "[INFO] Epoch: 37 , batch: 321 , training loss: 3.849559\n",
      "[INFO] Epoch: 37 , batch: 322 , training loss: 3.972543\n",
      "[INFO] Epoch: 37 , batch: 323 , training loss: 3.998019\n",
      "[INFO] Epoch: 37 , batch: 324 , training loss: 3.957331\n",
      "[INFO] Epoch: 37 , batch: 325 , training loss: 4.095076\n",
      "[INFO] Epoch: 37 , batch: 326 , training loss: 4.154291\n",
      "[INFO] Epoch: 37 , batch: 327 , training loss: 4.081584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 37 , batch: 328 , training loss: 4.089221\n",
      "[INFO] Epoch: 37 , batch: 329 , training loss: 3.990282\n",
      "[INFO] Epoch: 37 , batch: 330 , training loss: 3.985059\n",
      "[INFO] Epoch: 37 , batch: 331 , training loss: 4.119886\n",
      "[INFO] Epoch: 37 , batch: 332 , training loss: 3.977207\n",
      "[INFO] Epoch: 37 , batch: 333 , training loss: 3.936370\n",
      "[INFO] Epoch: 37 , batch: 334 , training loss: 3.966008\n",
      "[INFO] Epoch: 37 , batch: 335 , training loss: 4.086621\n",
      "[INFO] Epoch: 37 , batch: 336 , training loss: 4.098118\n",
      "[INFO] Epoch: 37 , batch: 337 , training loss: 4.142827\n",
      "[INFO] Epoch: 37 , batch: 338 , training loss: 4.356237\n",
      "[INFO] Epoch: 37 , batch: 339 , training loss: 4.189417\n",
      "[INFO] Epoch: 37 , batch: 340 , training loss: 4.364027\n",
      "[INFO] Epoch: 37 , batch: 341 , training loss: 4.115335\n",
      "[INFO] Epoch: 37 , batch: 342 , training loss: 3.905984\n",
      "[INFO] Epoch: 37 , batch: 343 , training loss: 3.951183\n",
      "[INFO] Epoch: 37 , batch: 344 , training loss: 3.851841\n",
      "[INFO] Epoch: 37 , batch: 345 , training loss: 3.971887\n",
      "[INFO] Epoch: 37 , batch: 346 , training loss: 4.026622\n",
      "[INFO] Epoch: 37 , batch: 347 , training loss: 3.928957\n",
      "[INFO] Epoch: 37 , batch: 348 , training loss: 4.039117\n",
      "[INFO] Epoch: 37 , batch: 349 , training loss: 4.137679\n",
      "[INFO] Epoch: 37 , batch: 350 , training loss: 3.965092\n",
      "[INFO] Epoch: 37 , batch: 351 , training loss: 4.057773\n",
      "[INFO] Epoch: 37 , batch: 352 , training loss: 4.046484\n",
      "[INFO] Epoch: 37 , batch: 353 , training loss: 4.044029\n",
      "[INFO] Epoch: 37 , batch: 354 , training loss: 4.138641\n",
      "[INFO] Epoch: 37 , batch: 355 , training loss: 4.138610\n",
      "[INFO] Epoch: 37 , batch: 356 , training loss: 4.009202\n",
      "[INFO] Epoch: 37 , batch: 357 , training loss: 4.079610\n",
      "[INFO] Epoch: 37 , batch: 358 , training loss: 3.984096\n",
      "[INFO] Epoch: 37 , batch: 359 , training loss: 3.980576\n",
      "[INFO] Epoch: 37 , batch: 360 , training loss: 4.103893\n",
      "[INFO] Epoch: 37 , batch: 361 , training loss: 4.062846\n",
      "[INFO] Epoch: 37 , batch: 362 , training loss: 4.171081\n",
      "[INFO] Epoch: 37 , batch: 363 , training loss: 4.041614\n",
      "[INFO] Epoch: 37 , batch: 364 , training loss: 4.098366\n",
      "[INFO] Epoch: 37 , batch: 365 , training loss: 4.018944\n",
      "[INFO] Epoch: 37 , batch: 366 , training loss: 4.112283\n",
      "[INFO] Epoch: 37 , batch: 367 , training loss: 4.155508\n",
      "[INFO] Epoch: 37 , batch: 368 , training loss: 4.560822\n",
      "[INFO] Epoch: 37 , batch: 369 , training loss: 4.242765\n",
      "[INFO] Epoch: 37 , batch: 370 , training loss: 4.009417\n",
      "[INFO] Epoch: 37 , batch: 371 , training loss: 4.462481\n",
      "[INFO] Epoch: 37 , batch: 372 , training loss: 4.694873\n",
      "[INFO] Epoch: 37 , batch: 373 , training loss: 4.773477\n",
      "[INFO] Epoch: 37 , batch: 374 , training loss: 4.930571\n",
      "[INFO] Epoch: 37 , batch: 375 , training loss: 4.849642\n",
      "[INFO] Epoch: 37 , batch: 376 , training loss: 4.712627\n",
      "[INFO] Epoch: 37 , batch: 377 , training loss: 4.470787\n",
      "[INFO] Epoch: 37 , batch: 378 , training loss: 4.570523\n",
      "[INFO] Epoch: 37 , batch: 379 , training loss: 4.525114\n",
      "[INFO] Epoch: 37 , batch: 380 , training loss: 4.703181\n",
      "[INFO] Epoch: 37 , batch: 381 , training loss: 4.419442\n",
      "[INFO] Epoch: 37 , batch: 382 , training loss: 4.682581\n",
      "[INFO] Epoch: 37 , batch: 383 , training loss: 4.725910\n",
      "[INFO] Epoch: 37 , batch: 384 , training loss: 4.720256\n",
      "[INFO] Epoch: 37 , batch: 385 , training loss: 4.372178\n",
      "[INFO] Epoch: 37 , batch: 386 , training loss: 4.636170\n",
      "[INFO] Epoch: 37 , batch: 387 , training loss: 4.577798\n",
      "[INFO] Epoch: 37 , batch: 388 , training loss: 4.409944\n",
      "[INFO] Epoch: 37 , batch: 389 , training loss: 4.215949\n",
      "[INFO] Epoch: 37 , batch: 390 , training loss: 4.214118\n",
      "[INFO] Epoch: 37 , batch: 391 , training loss: 4.271440\n",
      "[INFO] Epoch: 37 , batch: 392 , training loss: 4.642795\n",
      "[INFO] Epoch: 37 , batch: 393 , training loss: 4.523625\n",
      "[INFO] Epoch: 37 , batch: 394 , training loss: 4.605497\n",
      "[INFO] Epoch: 37 , batch: 395 , training loss: 4.435917\n",
      "[INFO] Epoch: 37 , batch: 396 , training loss: 4.243378\n",
      "[INFO] Epoch: 37 , batch: 397 , training loss: 4.399224\n",
      "[INFO] Epoch: 37 , batch: 398 , training loss: 4.260623\n",
      "[INFO] Epoch: 37 , batch: 399 , training loss: 4.331582\n",
      "[INFO] Epoch: 37 , batch: 400 , training loss: 4.305900\n",
      "[INFO] Epoch: 37 , batch: 401 , training loss: 4.725153\n",
      "[INFO] Epoch: 37 , batch: 402 , training loss: 4.472769\n",
      "[INFO] Epoch: 37 , batch: 403 , training loss: 4.304026\n",
      "[INFO] Epoch: 37 , batch: 404 , training loss: 4.470917\n",
      "[INFO] Epoch: 37 , batch: 405 , training loss: 4.536405\n",
      "[INFO] Epoch: 37 , batch: 406 , training loss: 4.421700\n",
      "[INFO] Epoch: 37 , batch: 407 , training loss: 4.462916\n",
      "[INFO] Epoch: 37 , batch: 408 , training loss: 4.438154\n",
      "[INFO] Epoch: 37 , batch: 409 , training loss: 4.454715\n",
      "[INFO] Epoch: 37 , batch: 410 , training loss: 4.507952\n",
      "[INFO] Epoch: 37 , batch: 411 , training loss: 4.671988\n",
      "[INFO] Epoch: 37 , batch: 412 , training loss: 4.496157\n",
      "[INFO] Epoch: 37 , batch: 413 , training loss: 4.360274\n",
      "[INFO] Epoch: 37 , batch: 414 , training loss: 4.415453\n",
      "[INFO] Epoch: 37 , batch: 415 , training loss: 4.460577\n",
      "[INFO] Epoch: 37 , batch: 416 , training loss: 4.516775\n",
      "[INFO] Epoch: 37 , batch: 417 , training loss: 4.439276\n",
      "[INFO] Epoch: 37 , batch: 418 , training loss: 4.500160\n",
      "[INFO] Epoch: 37 , batch: 419 , training loss: 4.466245\n",
      "[INFO] Epoch: 37 , batch: 420 , training loss: 4.439222\n",
      "[INFO] Epoch: 37 , batch: 421 , training loss: 4.408616\n",
      "[INFO] Epoch: 37 , batch: 422 , training loss: 4.258465\n",
      "[INFO] Epoch: 37 , batch: 423 , training loss: 4.489734\n",
      "[INFO] Epoch: 37 , batch: 424 , training loss: 4.665754\n",
      "[INFO] Epoch: 37 , batch: 425 , training loss: 4.515619\n",
      "[INFO] Epoch: 37 , batch: 426 , training loss: 4.255373\n",
      "[INFO] Epoch: 37 , batch: 427 , training loss: 4.514923\n",
      "[INFO] Epoch: 37 , batch: 428 , training loss: 4.372270\n",
      "[INFO] Epoch: 37 , batch: 429 , training loss: 4.271174\n",
      "[INFO] Epoch: 37 , batch: 430 , training loss: 4.490122\n",
      "[INFO] Epoch: 37 , batch: 431 , training loss: 4.109205\n",
      "[INFO] Epoch: 37 , batch: 432 , training loss: 4.174087\n",
      "[INFO] Epoch: 37 , batch: 433 , training loss: 4.207897\n",
      "[INFO] Epoch: 37 , batch: 434 , training loss: 4.078433\n",
      "[INFO] Epoch: 37 , batch: 435 , training loss: 4.440991\n",
      "[INFO] Epoch: 37 , batch: 436 , training loss: 4.476772\n",
      "[INFO] Epoch: 37 , batch: 437 , training loss: 4.277975\n",
      "[INFO] Epoch: 37 , batch: 438 , training loss: 4.131102\n",
      "[INFO] Epoch: 37 , batch: 439 , training loss: 4.360458\n",
      "[INFO] Epoch: 37 , batch: 440 , training loss: 4.473526\n",
      "[INFO] Epoch: 37 , batch: 441 , training loss: 4.566887\n",
      "[INFO] Epoch: 37 , batch: 442 , training loss: 4.327190\n",
      "[INFO] Epoch: 37 , batch: 443 , training loss: 4.523730\n",
      "[INFO] Epoch: 37 , batch: 444 , training loss: 4.118135\n",
      "[INFO] Epoch: 37 , batch: 445 , training loss: 4.005647\n",
      "[INFO] Epoch: 37 , batch: 446 , training loss: 3.941471\n",
      "[INFO] Epoch: 37 , batch: 447 , training loss: 4.137995\n",
      "[INFO] Epoch: 37 , batch: 448 , training loss: 4.267184\n",
      "[INFO] Epoch: 37 , batch: 449 , training loss: 4.659316\n",
      "[INFO] Epoch: 37 , batch: 450 , training loss: 4.726376\n",
      "[INFO] Epoch: 37 , batch: 451 , training loss: 4.604386\n",
      "[INFO] Epoch: 37 , batch: 452 , training loss: 4.425387\n",
      "[INFO] Epoch: 37 , batch: 453 , training loss: 4.187904\n",
      "[INFO] Epoch: 37 , batch: 454 , training loss: 4.337055\n",
      "[INFO] Epoch: 37 , batch: 455 , training loss: 4.399548\n",
      "[INFO] Epoch: 37 , batch: 456 , training loss: 4.395586\n",
      "[INFO] Epoch: 37 , batch: 457 , training loss: 4.445359\n",
      "[INFO] Epoch: 37 , batch: 458 , training loss: 4.206286\n",
      "[INFO] Epoch: 37 , batch: 459 , training loss: 4.169555\n",
      "[INFO] Epoch: 37 , batch: 460 , training loss: 4.294830\n",
      "[INFO] Epoch: 37 , batch: 461 , training loss: 4.275363\n",
      "[INFO] Epoch: 37 , batch: 462 , training loss: 4.343125\n",
      "[INFO] Epoch: 37 , batch: 463 , training loss: 4.237065\n",
      "[INFO] Epoch: 37 , batch: 464 , training loss: 4.405384\n",
      "[INFO] Epoch: 37 , batch: 465 , training loss: 4.361096\n",
      "[INFO] Epoch: 37 , batch: 466 , training loss: 4.474083\n",
      "[INFO] Epoch: 37 , batch: 467 , training loss: 4.418365\n",
      "[INFO] Epoch: 37 , batch: 468 , training loss: 4.372381\n",
      "[INFO] Epoch: 37 , batch: 469 , training loss: 4.396467\n",
      "[INFO] Epoch: 37 , batch: 470 , training loss: 4.222330\n",
      "[INFO] Epoch: 37 , batch: 471 , training loss: 4.322180\n",
      "[INFO] Epoch: 37 , batch: 472 , training loss: 4.378580\n",
      "[INFO] Epoch: 37 , batch: 473 , training loss: 4.299332\n",
      "[INFO] Epoch: 37 , batch: 474 , training loss: 4.094978\n",
      "[INFO] Epoch: 37 , batch: 475 , training loss: 3.958383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 37 , batch: 476 , training loss: 4.379848\n",
      "[INFO] Epoch: 37 , batch: 477 , training loss: 4.479384\n",
      "[INFO] Epoch: 37 , batch: 478 , training loss: 4.483866\n",
      "[INFO] Epoch: 37 , batch: 479 , training loss: 4.455313\n",
      "[INFO] Epoch: 37 , batch: 480 , training loss: 4.581612\n",
      "[INFO] Epoch: 37 , batch: 481 , training loss: 4.455904\n",
      "[INFO] Epoch: 37 , batch: 482 , training loss: 4.596781\n",
      "[INFO] Epoch: 37 , batch: 483 , training loss: 4.402544\n",
      "[INFO] Epoch: 37 , batch: 484 , training loss: 4.193437\n",
      "[INFO] Epoch: 37 , batch: 485 , training loss: 4.307532\n",
      "[INFO] Epoch: 37 , batch: 486 , training loss: 4.202361\n",
      "[INFO] Epoch: 37 , batch: 487 , training loss: 4.182830\n",
      "[INFO] Epoch: 37 , batch: 488 , training loss: 4.364654\n",
      "[INFO] Epoch: 37 , batch: 489 , training loss: 4.252235\n",
      "[INFO] Epoch: 37 , batch: 490 , training loss: 4.321928\n",
      "[INFO] Epoch: 37 , batch: 491 , training loss: 4.242165\n",
      "[INFO] Epoch: 37 , batch: 492 , training loss: 4.219019\n",
      "[INFO] Epoch: 37 , batch: 493 , training loss: 4.375771\n",
      "[INFO] Epoch: 37 , batch: 494 , training loss: 4.306324\n",
      "[INFO] Epoch: 37 , batch: 495 , training loss: 4.465492\n",
      "[INFO] Epoch: 37 , batch: 496 , training loss: 4.327608\n",
      "[INFO] Epoch: 37 , batch: 497 , training loss: 4.375473\n",
      "[INFO] Epoch: 37 , batch: 498 , training loss: 4.361302\n",
      "[INFO] Epoch: 37 , batch: 499 , training loss: 4.419542\n",
      "[INFO] Epoch: 37 , batch: 500 , training loss: 4.580783\n",
      "[INFO] Epoch: 37 , batch: 501 , training loss: 4.950421\n",
      "[INFO] Epoch: 37 , batch: 502 , training loss: 4.967955\n",
      "[INFO] Epoch: 37 , batch: 503 , training loss: 4.614264\n",
      "[INFO] Epoch: 37 , batch: 504 , training loss: 4.784248\n",
      "[INFO] Epoch: 37 , batch: 505 , training loss: 4.731792\n",
      "[INFO] Epoch: 37 , batch: 506 , training loss: 4.712601\n",
      "[INFO] Epoch: 37 , batch: 507 , training loss: 4.777311\n",
      "[INFO] Epoch: 37 , batch: 508 , training loss: 4.726832\n",
      "[INFO] Epoch: 37 , batch: 509 , training loss: 4.529973\n",
      "[INFO] Epoch: 37 , batch: 510 , training loss: 4.598846\n",
      "[INFO] Epoch: 37 , batch: 511 , training loss: 4.487414\n",
      "[INFO] Epoch: 37 , batch: 512 , training loss: 4.593535\n",
      "[INFO] Epoch: 37 , batch: 513 , training loss: 4.867465\n",
      "[INFO] Epoch: 37 , batch: 514 , training loss: 4.499472\n",
      "[INFO] Epoch: 37 , batch: 515 , training loss: 4.773012\n",
      "[INFO] Epoch: 37 , batch: 516 , training loss: 4.559353\n",
      "[INFO] Epoch: 37 , batch: 517 , training loss: 4.546818\n",
      "[INFO] Epoch: 37 , batch: 518 , training loss: 4.502995\n",
      "[INFO] Epoch: 37 , batch: 519 , training loss: 4.314812\n",
      "[INFO] Epoch: 37 , batch: 520 , training loss: 4.577028\n",
      "[INFO] Epoch: 37 , batch: 521 , training loss: 4.563077\n",
      "[INFO] Epoch: 37 , batch: 522 , training loss: 4.626508\n",
      "[INFO] Epoch: 37 , batch: 523 , training loss: 4.554419\n",
      "[INFO] Epoch: 37 , batch: 524 , training loss: 4.831108\n",
      "[INFO] Epoch: 37 , batch: 525 , training loss: 4.740993\n",
      "[INFO] Epoch: 37 , batch: 526 , training loss: 4.515592\n",
      "[INFO] Epoch: 37 , batch: 527 , training loss: 4.551207\n",
      "[INFO] Epoch: 37 , batch: 528 , training loss: 4.557127\n",
      "[INFO] Epoch: 37 , batch: 529 , training loss: 4.529781\n",
      "[INFO] Epoch: 37 , batch: 530 , training loss: 4.384211\n",
      "[INFO] Epoch: 37 , batch: 531 , training loss: 4.524343\n",
      "[INFO] Epoch: 37 , batch: 532 , training loss: 4.433473\n",
      "[INFO] Epoch: 37 , batch: 533 , training loss: 4.557294\n",
      "[INFO] Epoch: 37 , batch: 534 , training loss: 4.548425\n",
      "[INFO] Epoch: 37 , batch: 535 , training loss: 4.535496\n",
      "[INFO] Epoch: 37 , batch: 536 , training loss: 4.417235\n",
      "[INFO] Epoch: 37 , batch: 537 , training loss: 4.388958\n",
      "[INFO] Epoch: 37 , batch: 538 , training loss: 4.462523\n",
      "[INFO] Epoch: 37 , batch: 539 , training loss: 4.608070\n",
      "[INFO] Epoch: 37 , batch: 540 , training loss: 5.207476\n",
      "[INFO] Epoch: 37 , batch: 541 , training loss: 5.067160\n",
      "[INFO] Epoch: 37 , batch: 542 , training loss: 4.857039\n",
      "[INFO] Epoch: 38 , batch: 0 , training loss: 4.106315\n",
      "[INFO] Epoch: 38 , batch: 1 , training loss: 3.875754\n",
      "[INFO] Epoch: 38 , batch: 2 , training loss: 3.923284\n",
      "[INFO] Epoch: 38 , batch: 3 , training loss: 3.909357\n",
      "[INFO] Epoch: 38 , batch: 4 , training loss: 4.199059\n",
      "[INFO] Epoch: 38 , batch: 5 , training loss: 3.895499\n",
      "[INFO] Epoch: 38 , batch: 6 , training loss: 4.309703\n",
      "[INFO] Epoch: 38 , batch: 7 , training loss: 4.123610\n",
      "[INFO] Epoch: 38 , batch: 8 , training loss: 3.762439\n",
      "[INFO] Epoch: 38 , batch: 9 , training loss: 4.019742\n",
      "[INFO] Epoch: 38 , batch: 10 , training loss: 3.887234\n",
      "[INFO] Epoch: 38 , batch: 11 , training loss: 3.834898\n",
      "[INFO] Epoch: 38 , batch: 12 , training loss: 3.765559\n",
      "[INFO] Epoch: 38 , batch: 13 , training loss: 3.717110\n",
      "[INFO] Epoch: 38 , batch: 14 , training loss: 3.647712\n",
      "[INFO] Epoch: 38 , batch: 15 , training loss: 3.899975\n",
      "[INFO] Epoch: 38 , batch: 16 , training loss: 3.746628\n",
      "[INFO] Epoch: 38 , batch: 17 , training loss: 3.896152\n",
      "[INFO] Epoch: 38 , batch: 18 , training loss: 3.826593\n",
      "[INFO] Epoch: 38 , batch: 19 , training loss: 3.600679\n",
      "[INFO] Epoch: 38 , batch: 20 , training loss: 3.552319\n",
      "[INFO] Epoch: 38 , batch: 21 , training loss: 3.660471\n",
      "[INFO] Epoch: 38 , batch: 22 , training loss: 3.664671\n",
      "[INFO] Epoch: 38 , batch: 23 , training loss: 3.814253\n",
      "[INFO] Epoch: 38 , batch: 24 , training loss: 3.670068\n",
      "[INFO] Epoch: 38 , batch: 25 , training loss: 3.778456\n",
      "[INFO] Epoch: 38 , batch: 26 , training loss: 3.626084\n",
      "[INFO] Epoch: 38 , batch: 27 , training loss: 3.605756\n",
      "[INFO] Epoch: 38 , batch: 28 , training loss: 3.826039\n",
      "[INFO] Epoch: 38 , batch: 29 , training loss: 3.603469\n",
      "[INFO] Epoch: 38 , batch: 30 , training loss: 3.648352\n",
      "[INFO] Epoch: 38 , batch: 31 , training loss: 3.770798\n",
      "[INFO] Epoch: 38 , batch: 32 , training loss: 3.706643\n",
      "[INFO] Epoch: 38 , batch: 33 , training loss: 3.737209\n",
      "[INFO] Epoch: 38 , batch: 34 , training loss: 3.747806\n",
      "[INFO] Epoch: 38 , batch: 35 , training loss: 3.694519\n",
      "[INFO] Epoch: 38 , batch: 36 , training loss: 3.782153\n",
      "[INFO] Epoch: 38 , batch: 37 , training loss: 3.618869\n",
      "[INFO] Epoch: 38 , batch: 38 , training loss: 3.737764\n",
      "[INFO] Epoch: 38 , batch: 39 , training loss: 3.539948\n",
      "[INFO] Epoch: 38 , batch: 40 , training loss: 3.705445\n",
      "[INFO] Epoch: 38 , batch: 41 , training loss: 3.753032\n",
      "[INFO] Epoch: 38 , batch: 42 , training loss: 4.295263\n",
      "[INFO] Epoch: 38 , batch: 43 , training loss: 4.060887\n",
      "[INFO] Epoch: 38 , batch: 44 , training loss: 4.342155\n",
      "[INFO] Epoch: 38 , batch: 45 , training loss: 4.311288\n",
      "[INFO] Epoch: 38 , batch: 46 , training loss: 4.476457\n",
      "[INFO] Epoch: 38 , batch: 47 , training loss: 3.807795\n",
      "[INFO] Epoch: 38 , batch: 48 , training loss: 3.888668\n",
      "[INFO] Epoch: 38 , batch: 49 , training loss: 4.064688\n",
      "[INFO] Epoch: 38 , batch: 50 , training loss: 3.799012\n",
      "[INFO] Epoch: 38 , batch: 51 , training loss: 3.955938\n",
      "[INFO] Epoch: 38 , batch: 52 , training loss: 3.778318\n",
      "[INFO] Epoch: 38 , batch: 53 , training loss: 3.929390\n",
      "[INFO] Epoch: 38 , batch: 54 , training loss: 3.921113\n",
      "[INFO] Epoch: 38 , batch: 55 , training loss: 3.973331\n",
      "[INFO] Epoch: 38 , batch: 56 , training loss: 3.832278\n",
      "[INFO] Epoch: 38 , batch: 57 , training loss: 3.739738\n",
      "[INFO] Epoch: 38 , batch: 58 , training loss: 3.784878\n",
      "[INFO] Epoch: 38 , batch: 59 , training loss: 3.894069\n",
      "[INFO] Epoch: 38 , batch: 60 , training loss: 3.767696\n",
      "[INFO] Epoch: 38 , batch: 61 , training loss: 3.892405\n",
      "[INFO] Epoch: 38 , batch: 62 , training loss: 3.749058\n",
      "[INFO] Epoch: 38 , batch: 63 , training loss: 3.919538\n",
      "[INFO] Epoch: 38 , batch: 64 , training loss: 4.128994\n",
      "[INFO] Epoch: 38 , batch: 65 , training loss: 3.847891\n",
      "[INFO] Epoch: 38 , batch: 66 , training loss: 3.700804\n",
      "[INFO] Epoch: 38 , batch: 67 , training loss: 3.725781\n",
      "[INFO] Epoch: 38 , batch: 68 , training loss: 3.921147\n",
      "[INFO] Epoch: 38 , batch: 69 , training loss: 3.854837\n",
      "[INFO] Epoch: 38 , batch: 70 , training loss: 4.055619\n",
      "[INFO] Epoch: 38 , batch: 71 , training loss: 3.904260\n",
      "[INFO] Epoch: 38 , batch: 72 , training loss: 3.957615\n",
      "[INFO] Epoch: 38 , batch: 73 , training loss: 3.894512\n",
      "[INFO] Epoch: 38 , batch: 74 , training loss: 4.027632\n",
      "[INFO] Epoch: 38 , batch: 75 , training loss: 3.824788\n",
      "[INFO] Epoch: 38 , batch: 76 , training loss: 4.001765\n",
      "[INFO] Epoch: 38 , batch: 77 , training loss: 3.895445\n",
      "[INFO] Epoch: 38 , batch: 78 , training loss: 3.991542\n",
      "[INFO] Epoch: 38 , batch: 79 , training loss: 3.847433\n",
      "[INFO] Epoch: 38 , batch: 80 , training loss: 4.035270\n",
      "[INFO] Epoch: 38 , batch: 81 , training loss: 3.973696\n",
      "[INFO] Epoch: 38 , batch: 82 , training loss: 3.924177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 38 , batch: 83 , training loss: 4.003229\n",
      "[INFO] Epoch: 38 , batch: 84 , training loss: 4.020035\n",
      "[INFO] Epoch: 38 , batch: 85 , training loss: 4.119086\n",
      "[INFO] Epoch: 38 , batch: 86 , training loss: 4.007653\n",
      "[INFO] Epoch: 38 , batch: 87 , training loss: 3.960510\n",
      "[INFO] Epoch: 38 , batch: 88 , training loss: 4.132483\n",
      "[INFO] Epoch: 38 , batch: 89 , training loss: 3.912516\n",
      "[INFO] Epoch: 38 , batch: 90 , training loss: 4.000498\n",
      "[INFO] Epoch: 38 , batch: 91 , training loss: 3.910438\n",
      "[INFO] Epoch: 38 , batch: 92 , training loss: 3.960234\n",
      "[INFO] Epoch: 38 , batch: 93 , training loss: 4.063503\n",
      "[INFO] Epoch: 38 , batch: 94 , training loss: 4.193195\n",
      "[INFO] Epoch: 38 , batch: 95 , training loss: 3.984835\n",
      "[INFO] Epoch: 38 , batch: 96 , training loss: 3.920993\n",
      "[INFO] Epoch: 38 , batch: 97 , training loss: 3.880279\n",
      "[INFO] Epoch: 38 , batch: 98 , training loss: 3.841633\n",
      "[INFO] Epoch: 38 , batch: 99 , training loss: 3.928305\n",
      "[INFO] Epoch: 38 , batch: 100 , training loss: 3.827433\n",
      "[INFO] Epoch: 38 , batch: 101 , training loss: 3.863013\n",
      "[INFO] Epoch: 38 , batch: 102 , training loss: 4.021958\n",
      "[INFO] Epoch: 38 , batch: 103 , training loss: 3.796834\n",
      "[INFO] Epoch: 38 , batch: 104 , training loss: 3.738610\n",
      "[INFO] Epoch: 38 , batch: 105 , training loss: 3.994182\n",
      "[INFO] Epoch: 38 , batch: 106 , training loss: 4.041803\n",
      "[INFO] Epoch: 38 , batch: 107 , training loss: 3.883511\n",
      "[INFO] Epoch: 38 , batch: 108 , training loss: 3.786647\n",
      "[INFO] Epoch: 38 , batch: 109 , training loss: 3.733814\n",
      "[INFO] Epoch: 38 , batch: 110 , training loss: 3.931257\n",
      "[INFO] Epoch: 38 , batch: 111 , training loss: 3.976795\n",
      "[INFO] Epoch: 38 , batch: 112 , training loss: 3.929599\n",
      "[INFO] Epoch: 38 , batch: 113 , training loss: 3.931494\n",
      "[INFO] Epoch: 38 , batch: 114 , training loss: 3.896707\n",
      "[INFO] Epoch: 38 , batch: 115 , training loss: 3.924358\n",
      "[INFO] Epoch: 38 , batch: 116 , training loss: 3.834902\n",
      "[INFO] Epoch: 38 , batch: 117 , training loss: 4.050674\n",
      "[INFO] Epoch: 38 , batch: 118 , training loss: 4.041393\n",
      "[INFO] Epoch: 38 , batch: 119 , training loss: 4.195237\n",
      "[INFO] Epoch: 38 , batch: 120 , training loss: 4.102553\n",
      "[INFO] Epoch: 38 , batch: 121 , training loss: 3.987913\n",
      "[INFO] Epoch: 38 , batch: 122 , training loss: 3.892302\n",
      "[INFO] Epoch: 38 , batch: 123 , training loss: 3.903229\n",
      "[INFO] Epoch: 38 , batch: 124 , training loss: 4.018577\n",
      "[INFO] Epoch: 38 , batch: 125 , training loss: 3.778638\n",
      "[INFO] Epoch: 38 , batch: 126 , training loss: 3.816880\n",
      "[INFO] Epoch: 38 , batch: 127 , training loss: 3.851309\n",
      "[INFO] Epoch: 38 , batch: 128 , training loss: 3.986949\n",
      "[INFO] Epoch: 38 , batch: 129 , training loss: 3.907816\n",
      "[INFO] Epoch: 38 , batch: 130 , training loss: 3.946249\n",
      "[INFO] Epoch: 38 , batch: 131 , training loss: 3.941548\n",
      "[INFO] Epoch: 38 , batch: 132 , training loss: 3.949209\n",
      "[INFO] Epoch: 38 , batch: 133 , training loss: 3.908028\n",
      "[INFO] Epoch: 38 , batch: 134 , training loss: 3.674317\n",
      "[INFO] Epoch: 38 , batch: 135 , training loss: 3.750401\n",
      "[INFO] Epoch: 38 , batch: 136 , training loss: 3.991486\n",
      "[INFO] Epoch: 38 , batch: 137 , training loss: 3.984305\n",
      "[INFO] Epoch: 38 , batch: 138 , training loss: 4.015197\n",
      "[INFO] Epoch: 38 , batch: 139 , training loss: 4.591411\n",
      "[INFO] Epoch: 38 , batch: 140 , training loss: 4.418493\n",
      "[INFO] Epoch: 38 , batch: 141 , training loss: 4.189323\n",
      "[INFO] Epoch: 38 , batch: 142 , training loss: 3.845327\n",
      "[INFO] Epoch: 38 , batch: 143 , training loss: 3.967293\n",
      "[INFO] Epoch: 38 , batch: 144 , training loss: 3.840131\n",
      "[INFO] Epoch: 38 , batch: 145 , training loss: 3.912953\n",
      "[INFO] Epoch: 38 , batch: 146 , training loss: 4.114182\n",
      "[INFO] Epoch: 38 , batch: 147 , training loss: 3.736184\n",
      "[INFO] Epoch: 38 , batch: 148 , training loss: 3.729444\n",
      "[INFO] Epoch: 38 , batch: 149 , training loss: 3.834592\n",
      "[INFO] Epoch: 38 , batch: 150 , training loss: 4.101395\n",
      "[INFO] Epoch: 38 , batch: 151 , training loss: 3.892570\n",
      "[INFO] Epoch: 38 , batch: 152 , training loss: 3.906743\n",
      "[INFO] Epoch: 38 , batch: 153 , training loss: 3.967093\n",
      "[INFO] Epoch: 38 , batch: 154 , training loss: 4.024937\n",
      "[INFO] Epoch: 38 , batch: 155 , training loss: 4.244706\n",
      "[INFO] Epoch: 38 , batch: 156 , training loss: 3.999645\n",
      "[INFO] Epoch: 38 , batch: 157 , training loss: 3.942961\n",
      "[INFO] Epoch: 38 , batch: 158 , training loss: 4.109865\n",
      "[INFO] Epoch: 38 , batch: 159 , training loss: 4.061726\n",
      "[INFO] Epoch: 38 , batch: 160 , training loss: 4.324795\n",
      "[INFO] Epoch: 38 , batch: 161 , training loss: 4.315983\n",
      "[INFO] Epoch: 38 , batch: 162 , training loss: 4.275798\n",
      "[INFO] Epoch: 38 , batch: 163 , training loss: 4.464740\n",
      "[INFO] Epoch: 38 , batch: 164 , training loss: 4.392932\n",
      "[INFO] Epoch: 38 , batch: 165 , training loss: 4.348941\n",
      "[INFO] Epoch: 38 , batch: 166 , training loss: 4.234722\n",
      "[INFO] Epoch: 38 , batch: 167 , training loss: 4.407307\n",
      "[INFO] Epoch: 38 , batch: 168 , training loss: 4.092147\n",
      "[INFO] Epoch: 38 , batch: 169 , training loss: 4.052750\n",
      "[INFO] Epoch: 38 , batch: 170 , training loss: 4.222271\n",
      "[INFO] Epoch: 38 , batch: 171 , training loss: 3.666665\n",
      "[INFO] Epoch: 38 , batch: 172 , training loss: 3.864257\n",
      "[INFO] Epoch: 38 , batch: 173 , training loss: 4.156453\n",
      "[INFO] Epoch: 38 , batch: 174 , training loss: 4.604247\n",
      "[INFO] Epoch: 38 , batch: 175 , training loss: 4.859636\n",
      "[INFO] Epoch: 38 , batch: 176 , training loss: 4.509634\n",
      "[INFO] Epoch: 38 , batch: 177 , training loss: 4.158078\n",
      "[INFO] Epoch: 38 , batch: 178 , training loss: 4.152977\n",
      "[INFO] Epoch: 38 , batch: 179 , training loss: 4.183096\n",
      "[INFO] Epoch: 38 , batch: 180 , training loss: 4.134836\n",
      "[INFO] Epoch: 38 , batch: 181 , training loss: 4.428981\n",
      "[INFO] Epoch: 38 , batch: 182 , training loss: 4.351261\n",
      "[INFO] Epoch: 38 , batch: 183 , training loss: 4.338049\n",
      "[INFO] Epoch: 38 , batch: 184 , training loss: 4.212432\n",
      "[INFO] Epoch: 38 , batch: 185 , training loss: 4.189422\n",
      "[INFO] Epoch: 38 , batch: 186 , training loss: 4.336060\n",
      "[INFO] Epoch: 38 , batch: 187 , training loss: 4.426420\n",
      "[INFO] Epoch: 38 , batch: 188 , training loss: 4.415346\n",
      "[INFO] Epoch: 38 , batch: 189 , training loss: 4.326963\n",
      "[INFO] Epoch: 38 , batch: 190 , training loss: 4.359690\n",
      "[INFO] Epoch: 38 , batch: 191 , training loss: 4.468630\n",
      "[INFO] Epoch: 38 , batch: 192 , training loss: 4.298911\n",
      "[INFO] Epoch: 38 , batch: 193 , training loss: 4.392234\n",
      "[INFO] Epoch: 38 , batch: 194 , training loss: 4.324993\n",
      "[INFO] Epoch: 38 , batch: 195 , training loss: 4.286094\n",
      "[INFO] Epoch: 38 , batch: 196 , training loss: 4.110271\n",
      "[INFO] Epoch: 38 , batch: 197 , training loss: 4.236876\n",
      "[INFO] Epoch: 38 , batch: 198 , training loss: 4.144871\n",
      "[INFO] Epoch: 38 , batch: 199 , training loss: 4.257295\n",
      "[INFO] Epoch: 38 , batch: 200 , training loss: 4.148584\n",
      "[INFO] Epoch: 38 , batch: 201 , training loss: 4.060785\n",
      "[INFO] Epoch: 38 , batch: 202 , training loss: 4.056884\n",
      "[INFO] Epoch: 38 , batch: 203 , training loss: 4.202292\n",
      "[INFO] Epoch: 38 , batch: 204 , training loss: 4.265847\n",
      "[INFO] Epoch: 38 , batch: 205 , training loss: 3.872254\n",
      "[INFO] Epoch: 38 , batch: 206 , training loss: 3.791789\n",
      "[INFO] Epoch: 38 , batch: 207 , training loss: 3.813638\n",
      "[INFO] Epoch: 38 , batch: 208 , training loss: 4.132943\n",
      "[INFO] Epoch: 38 , batch: 209 , training loss: 4.098978\n",
      "[INFO] Epoch: 38 , batch: 210 , training loss: 4.108389\n",
      "[INFO] Epoch: 38 , batch: 211 , training loss: 4.094423\n",
      "[INFO] Epoch: 38 , batch: 212 , training loss: 4.202943\n",
      "[INFO] Epoch: 38 , batch: 213 , training loss: 4.147528\n",
      "[INFO] Epoch: 38 , batch: 214 , training loss: 4.258348\n",
      "[INFO] Epoch: 38 , batch: 215 , training loss: 4.424331\n",
      "[INFO] Epoch: 38 , batch: 216 , training loss: 4.143506\n",
      "[INFO] Epoch: 38 , batch: 217 , training loss: 4.088519\n",
      "[INFO] Epoch: 38 , batch: 218 , training loss: 4.076738\n",
      "[INFO] Epoch: 38 , batch: 219 , training loss: 4.188329\n",
      "[INFO] Epoch: 38 , batch: 220 , training loss: 3.999099\n",
      "[INFO] Epoch: 38 , batch: 221 , training loss: 4.017136\n",
      "[INFO] Epoch: 38 , batch: 222 , training loss: 4.157189\n",
      "[INFO] Epoch: 38 , batch: 223 , training loss: 4.273768\n",
      "[INFO] Epoch: 38 , batch: 224 , training loss: 4.308491\n",
      "[INFO] Epoch: 38 , batch: 225 , training loss: 4.188745\n",
      "[INFO] Epoch: 38 , batch: 226 , training loss: 4.317655\n",
      "[INFO] Epoch: 38 , batch: 227 , training loss: 4.279082\n",
      "[INFO] Epoch: 38 , batch: 228 , training loss: 4.330228\n",
      "[INFO] Epoch: 38 , batch: 229 , training loss: 4.180455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 38 , batch: 230 , training loss: 4.029061\n",
      "[INFO] Epoch: 38 , batch: 231 , training loss: 3.875345\n",
      "[INFO] Epoch: 38 , batch: 232 , training loss: 4.029478\n",
      "[INFO] Epoch: 38 , batch: 233 , training loss: 4.065241\n",
      "[INFO] Epoch: 38 , batch: 234 , training loss: 3.763948\n",
      "[INFO] Epoch: 38 , batch: 235 , training loss: 3.879607\n",
      "[INFO] Epoch: 38 , batch: 236 , training loss: 3.972356\n",
      "[INFO] Epoch: 38 , batch: 237 , training loss: 4.180316\n",
      "[INFO] Epoch: 38 , batch: 238 , training loss: 3.965048\n",
      "[INFO] Epoch: 38 , batch: 239 , training loss: 4.015954\n",
      "[INFO] Epoch: 38 , batch: 240 , training loss: 4.050003\n",
      "[INFO] Epoch: 38 , batch: 241 , training loss: 3.855691\n",
      "[INFO] Epoch: 38 , batch: 242 , training loss: 3.865218\n",
      "[INFO] Epoch: 38 , batch: 243 , training loss: 4.166114\n",
      "[INFO] Epoch: 38 , batch: 244 , training loss: 4.107244\n",
      "[INFO] Epoch: 38 , batch: 245 , training loss: 4.075829\n",
      "[INFO] Epoch: 38 , batch: 246 , training loss: 3.779120\n",
      "[INFO] Epoch: 38 , batch: 247 , training loss: 3.939020\n",
      "[INFO] Epoch: 38 , batch: 248 , training loss: 4.009293\n",
      "[INFO] Epoch: 38 , batch: 249 , training loss: 3.996866\n",
      "[INFO] Epoch: 38 , batch: 250 , training loss: 3.818155\n",
      "[INFO] Epoch: 38 , batch: 251 , training loss: 4.240977\n",
      "[INFO] Epoch: 38 , batch: 252 , training loss: 3.958779\n",
      "[INFO] Epoch: 38 , batch: 253 , training loss: 3.868402\n",
      "[INFO] Epoch: 38 , batch: 254 , training loss: 4.130639\n",
      "[INFO] Epoch: 38 , batch: 255 , training loss: 4.116109\n",
      "[INFO] Epoch: 38 , batch: 256 , training loss: 4.098112\n",
      "[INFO] Epoch: 38 , batch: 257 , training loss: 4.277515\n",
      "[INFO] Epoch: 38 , batch: 258 , training loss: 4.280449\n",
      "[INFO] Epoch: 38 , batch: 259 , training loss: 4.348260\n",
      "[INFO] Epoch: 38 , batch: 260 , training loss: 4.100000\n",
      "[INFO] Epoch: 38 , batch: 261 , training loss: 4.272692\n",
      "[INFO] Epoch: 38 , batch: 262 , training loss: 4.400655\n",
      "[INFO] Epoch: 38 , batch: 263 , training loss: 4.577211\n",
      "[INFO] Epoch: 38 , batch: 264 , training loss: 3.942433\n",
      "[INFO] Epoch: 38 , batch: 265 , training loss: 4.040295\n",
      "[INFO] Epoch: 38 , batch: 266 , training loss: 4.459647\n",
      "[INFO] Epoch: 38 , batch: 267 , training loss: 4.192012\n",
      "[INFO] Epoch: 38 , batch: 268 , training loss: 4.101370\n",
      "[INFO] Epoch: 38 , batch: 269 , training loss: 4.107855\n",
      "[INFO] Epoch: 38 , batch: 270 , training loss: 4.104344\n",
      "[INFO] Epoch: 38 , batch: 271 , training loss: 4.156169\n",
      "[INFO] Epoch: 38 , batch: 272 , training loss: 4.142141\n",
      "[INFO] Epoch: 38 , batch: 273 , training loss: 4.146048\n",
      "[INFO] Epoch: 38 , batch: 274 , training loss: 4.241148\n",
      "[INFO] Epoch: 38 , batch: 275 , training loss: 4.124242\n",
      "[INFO] Epoch: 38 , batch: 276 , training loss: 4.177946\n",
      "[INFO] Epoch: 38 , batch: 277 , training loss: 4.335925\n",
      "[INFO] Epoch: 38 , batch: 278 , training loss: 4.014636\n",
      "[INFO] Epoch: 38 , batch: 279 , training loss: 4.017833\n",
      "[INFO] Epoch: 38 , batch: 280 , training loss: 3.998498\n",
      "[INFO] Epoch: 38 , batch: 281 , training loss: 4.126413\n",
      "[INFO] Epoch: 38 , batch: 282 , training loss: 4.032012\n",
      "[INFO] Epoch: 38 , batch: 283 , training loss: 4.026037\n",
      "[INFO] Epoch: 38 , batch: 284 , training loss: 4.064973\n",
      "[INFO] Epoch: 38 , batch: 285 , training loss: 4.012243\n",
      "[INFO] Epoch: 38 , batch: 286 , training loss: 4.041555\n",
      "[INFO] Epoch: 38 , batch: 287 , training loss: 3.958594\n",
      "[INFO] Epoch: 38 , batch: 288 , training loss: 3.913923\n",
      "[INFO] Epoch: 38 , batch: 289 , training loss: 3.995395\n",
      "[INFO] Epoch: 38 , batch: 290 , training loss: 3.774824\n",
      "[INFO] Epoch: 38 , batch: 291 , training loss: 3.788690\n",
      "[INFO] Epoch: 38 , batch: 292 , training loss: 3.855000\n",
      "[INFO] Epoch: 38 , batch: 293 , training loss: 3.804848\n",
      "[INFO] Epoch: 38 , batch: 294 , training loss: 4.432969\n",
      "[INFO] Epoch: 38 , batch: 295 , training loss: 4.264539\n",
      "[INFO] Epoch: 38 , batch: 296 , training loss: 4.177884\n",
      "[INFO] Epoch: 38 , batch: 297 , training loss: 4.117302\n",
      "[INFO] Epoch: 38 , batch: 298 , training loss: 3.963067\n",
      "[INFO] Epoch: 38 , batch: 299 , training loss: 4.003671\n",
      "[INFO] Epoch: 38 , batch: 300 , training loss: 3.982268\n",
      "[INFO] Epoch: 38 , batch: 301 , training loss: 3.918242\n",
      "[INFO] Epoch: 38 , batch: 302 , training loss: 4.083504\n",
      "[INFO] Epoch: 38 , batch: 303 , training loss: 4.091729\n",
      "[INFO] Epoch: 38 , batch: 304 , training loss: 4.229678\n",
      "[INFO] Epoch: 38 , batch: 305 , training loss: 4.075914\n",
      "[INFO] Epoch: 38 , batch: 306 , training loss: 4.173678\n",
      "[INFO] Epoch: 38 , batch: 307 , training loss: 4.191948\n",
      "[INFO] Epoch: 38 , batch: 308 , training loss: 3.996960\n",
      "[INFO] Epoch: 38 , batch: 309 , training loss: 3.999050\n",
      "[INFO] Epoch: 38 , batch: 310 , training loss: 3.918244\n",
      "[INFO] Epoch: 38 , batch: 311 , training loss: 3.923649\n",
      "[INFO] Epoch: 38 , batch: 312 , training loss: 3.822603\n",
      "[INFO] Epoch: 38 , batch: 313 , training loss: 3.931870\n",
      "[INFO] Epoch: 38 , batch: 314 , training loss: 3.993876\n",
      "[INFO] Epoch: 38 , batch: 315 , training loss: 4.076230\n",
      "[INFO] Epoch: 38 , batch: 316 , training loss: 4.318074\n",
      "[INFO] Epoch: 38 , batch: 317 , training loss: 4.678740\n",
      "[INFO] Epoch: 38 , batch: 318 , training loss: 4.795167\n",
      "[INFO] Epoch: 38 , batch: 319 , training loss: 4.487617\n",
      "[INFO] Epoch: 38 , batch: 320 , training loss: 4.032391\n",
      "[INFO] Epoch: 38 , batch: 321 , training loss: 3.842511\n",
      "[INFO] Epoch: 38 , batch: 322 , training loss: 3.969894\n",
      "[INFO] Epoch: 38 , batch: 323 , training loss: 3.993227\n",
      "[INFO] Epoch: 38 , batch: 324 , training loss: 3.967155\n",
      "[INFO] Epoch: 38 , batch: 325 , training loss: 4.093808\n",
      "[INFO] Epoch: 38 , batch: 326 , training loss: 4.154847\n",
      "[INFO] Epoch: 38 , batch: 327 , training loss: 4.071022\n",
      "[INFO] Epoch: 38 , batch: 328 , training loss: 4.066224\n",
      "[INFO] Epoch: 38 , batch: 329 , training loss: 3.984922\n",
      "[INFO] Epoch: 38 , batch: 330 , training loss: 3.970127\n",
      "[INFO] Epoch: 38 , batch: 331 , training loss: 4.126676\n",
      "[INFO] Epoch: 38 , batch: 332 , training loss: 3.968541\n",
      "[INFO] Epoch: 38 , batch: 333 , training loss: 3.938681\n",
      "[INFO] Epoch: 38 , batch: 334 , training loss: 3.972091\n",
      "[INFO] Epoch: 38 , batch: 335 , training loss: 4.108486\n",
      "[INFO] Epoch: 38 , batch: 336 , training loss: 4.098087\n",
      "[INFO] Epoch: 38 , batch: 337 , training loss: 4.135356\n",
      "[INFO] Epoch: 38 , batch: 338 , training loss: 4.357372\n",
      "[INFO] Epoch: 38 , batch: 339 , training loss: 4.179847\n",
      "[INFO] Epoch: 38 , batch: 340 , training loss: 4.366476\n",
      "[INFO] Epoch: 38 , batch: 341 , training loss: 4.127496\n",
      "[INFO] Epoch: 38 , batch: 342 , training loss: 3.900219\n",
      "[INFO] Epoch: 38 , batch: 343 , training loss: 3.985415\n",
      "[INFO] Epoch: 38 , batch: 344 , training loss: 3.854777\n",
      "[INFO] Epoch: 38 , batch: 345 , training loss: 3.969202\n",
      "[INFO] Epoch: 38 , batch: 346 , training loss: 4.032962\n",
      "[INFO] Epoch: 38 , batch: 347 , training loss: 3.925529\n",
      "[INFO] Epoch: 38 , batch: 348 , training loss: 4.054780\n",
      "[INFO] Epoch: 38 , batch: 349 , training loss: 4.131131\n",
      "[INFO] Epoch: 38 , batch: 350 , training loss: 3.973471\n",
      "[INFO] Epoch: 38 , batch: 351 , training loss: 4.068958\n",
      "[INFO] Epoch: 38 , batch: 352 , training loss: 4.072026\n",
      "[INFO] Epoch: 38 , batch: 353 , training loss: 4.051767\n",
      "[INFO] Epoch: 38 , batch: 354 , training loss: 4.139789\n",
      "[INFO] Epoch: 38 , batch: 355 , training loss: 4.145081\n",
      "[INFO] Epoch: 38 , batch: 356 , training loss: 4.015115\n",
      "[INFO] Epoch: 38 , batch: 357 , training loss: 4.088050\n",
      "[INFO] Epoch: 38 , batch: 358 , training loss: 3.994106\n",
      "[INFO] Epoch: 38 , batch: 359 , training loss: 4.001387\n",
      "[INFO] Epoch: 38 , batch: 360 , training loss: 4.097714\n",
      "[INFO] Epoch: 38 , batch: 361 , training loss: 4.073375\n",
      "[INFO] Epoch: 38 , batch: 362 , training loss: 4.169547\n",
      "[INFO] Epoch: 38 , batch: 363 , training loss: 4.050138\n",
      "[INFO] Epoch: 38 , batch: 364 , training loss: 4.113290\n",
      "[INFO] Epoch: 38 , batch: 365 , training loss: 4.029845\n",
      "[INFO] Epoch: 38 , batch: 366 , training loss: 4.123743\n",
      "[INFO] Epoch: 38 , batch: 367 , training loss: 4.176987\n",
      "[INFO] Epoch: 38 , batch: 368 , training loss: 4.557996\n",
      "[INFO] Epoch: 38 , batch: 369 , training loss: 4.254591\n",
      "[INFO] Epoch: 38 , batch: 370 , training loss: 4.045208\n",
      "[INFO] Epoch: 38 , batch: 371 , training loss: 4.453846\n",
      "[INFO] Epoch: 38 , batch: 372 , training loss: 4.707452\n",
      "[INFO] Epoch: 38 , batch: 373 , training loss: 4.777357\n",
      "[INFO] Epoch: 38 , batch: 374 , training loss: 4.931163\n",
      "[INFO] Epoch: 38 , batch: 375 , training loss: 4.859055\n",
      "[INFO] Epoch: 38 , batch: 376 , training loss: 4.741597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 38 , batch: 377 , training loss: 4.467439\n",
      "[INFO] Epoch: 38 , batch: 378 , training loss: 4.598678\n",
      "[INFO] Epoch: 38 , batch: 379 , training loss: 4.543349\n",
      "[INFO] Epoch: 38 , batch: 380 , training loss: 4.696840\n",
      "[INFO] Epoch: 38 , batch: 381 , training loss: 4.410990\n",
      "[INFO] Epoch: 38 , batch: 382 , training loss: 4.696966\n",
      "[INFO] Epoch: 38 , batch: 383 , training loss: 4.736222\n",
      "[INFO] Epoch: 38 , batch: 384 , training loss: 4.739931\n",
      "[INFO] Epoch: 38 , batch: 385 , training loss: 4.400185\n",
      "[INFO] Epoch: 38 , batch: 386 , training loss: 4.638112\n",
      "[INFO] Epoch: 38 , batch: 387 , training loss: 4.596667\n",
      "[INFO] Epoch: 38 , batch: 388 , training loss: 4.404340\n",
      "[INFO] Epoch: 38 , batch: 389 , training loss: 4.216561\n",
      "[INFO] Epoch: 38 , batch: 390 , training loss: 4.227110\n",
      "[INFO] Epoch: 38 , batch: 391 , training loss: 4.254709\n",
      "[INFO] Epoch: 38 , batch: 392 , training loss: 4.627491\n",
      "[INFO] Epoch: 38 , batch: 393 , training loss: 4.528847\n",
      "[INFO] Epoch: 38 , batch: 394 , training loss: 4.602470\n",
      "[INFO] Epoch: 38 , batch: 395 , training loss: 4.424628\n",
      "[INFO] Epoch: 38 , batch: 396 , training loss: 4.250649\n",
      "[INFO] Epoch: 38 , batch: 397 , training loss: 4.374021\n",
      "[INFO] Epoch: 38 , batch: 398 , training loss: 4.241665\n",
      "[INFO] Epoch: 38 , batch: 399 , training loss: 4.337912\n",
      "[INFO] Epoch: 38 , batch: 400 , training loss: 4.285888\n",
      "[INFO] Epoch: 38 , batch: 401 , training loss: 4.739419\n",
      "[INFO] Epoch: 38 , batch: 402 , training loss: 4.460177\n",
      "[INFO] Epoch: 38 , batch: 403 , training loss: 4.282497\n",
      "[INFO] Epoch: 38 , batch: 404 , training loss: 4.442272\n",
      "[INFO] Epoch: 38 , batch: 405 , training loss: 4.521811\n",
      "[INFO] Epoch: 38 , batch: 406 , training loss: 4.407305\n",
      "[INFO] Epoch: 38 , batch: 407 , training loss: 4.442896\n",
      "[INFO] Epoch: 38 , batch: 408 , training loss: 4.403973\n",
      "[INFO] Epoch: 38 , batch: 409 , training loss: 4.443060\n",
      "[INFO] Epoch: 38 , batch: 410 , training loss: 4.500708\n",
      "[INFO] Epoch: 38 , batch: 411 , training loss: 4.677552\n",
      "[INFO] Epoch: 38 , batch: 412 , training loss: 4.494709\n",
      "[INFO] Epoch: 38 , batch: 413 , training loss: 4.353352\n",
      "[INFO] Epoch: 38 , batch: 414 , training loss: 4.397147\n",
      "[INFO] Epoch: 38 , batch: 415 , training loss: 4.455838\n",
      "[INFO] Epoch: 38 , batch: 416 , training loss: 4.497819\n",
      "[INFO] Epoch: 38 , batch: 417 , training loss: 4.417495\n",
      "[INFO] Epoch: 38 , batch: 418 , training loss: 4.472297\n",
      "[INFO] Epoch: 38 , batch: 419 , training loss: 4.444244\n",
      "[INFO] Epoch: 38 , batch: 420 , training loss: 4.428613\n",
      "[INFO] Epoch: 38 , batch: 421 , training loss: 4.391951\n",
      "[INFO] Epoch: 38 , batch: 422 , training loss: 4.234731\n",
      "[INFO] Epoch: 38 , batch: 423 , training loss: 4.482949\n",
      "[INFO] Epoch: 38 , batch: 424 , training loss: 4.645617\n",
      "[INFO] Epoch: 38 , batch: 425 , training loss: 4.489940\n",
      "[INFO] Epoch: 38 , batch: 426 , training loss: 4.231213\n",
      "[INFO] Epoch: 38 , batch: 427 , training loss: 4.488217\n",
      "[INFO] Epoch: 38 , batch: 428 , training loss: 4.353131\n",
      "[INFO] Epoch: 38 , batch: 429 , training loss: 4.234452\n",
      "[INFO] Epoch: 38 , batch: 430 , training loss: 4.480457\n",
      "[INFO] Epoch: 38 , batch: 431 , training loss: 4.096827\n",
      "[INFO] Epoch: 38 , batch: 432 , training loss: 4.144028\n",
      "[INFO] Epoch: 38 , batch: 433 , training loss: 4.175020\n",
      "[INFO] Epoch: 38 , batch: 434 , training loss: 4.057956\n",
      "[INFO] Epoch: 38 , batch: 435 , training loss: 4.410040\n",
      "[INFO] Epoch: 38 , batch: 436 , training loss: 4.445425\n",
      "[INFO] Epoch: 38 , batch: 437 , training loss: 4.239490\n",
      "[INFO] Epoch: 38 , batch: 438 , training loss: 4.104805\n",
      "[INFO] Epoch: 38 , batch: 439 , training loss: 4.343979\n",
      "[INFO] Epoch: 38 , batch: 440 , training loss: 4.466975\n",
      "[INFO] Epoch: 38 , batch: 441 , training loss: 4.535387\n",
      "[INFO] Epoch: 38 , batch: 442 , training loss: 4.320494\n",
      "[INFO] Epoch: 38 , batch: 443 , training loss: 4.505968\n",
      "[INFO] Epoch: 38 , batch: 444 , training loss: 4.090356\n",
      "[INFO] Epoch: 38 , batch: 445 , training loss: 3.994751\n",
      "[INFO] Epoch: 38 , batch: 446 , training loss: 3.936010\n",
      "[INFO] Epoch: 38 , batch: 447 , training loss: 4.121567\n",
      "[INFO] Epoch: 38 , batch: 448 , training loss: 4.264281\n",
      "[INFO] Epoch: 38 , batch: 449 , training loss: 4.633623\n",
      "[INFO] Epoch: 38 , batch: 450 , training loss: 4.711896\n",
      "[INFO] Epoch: 38 , batch: 451 , training loss: 4.589736\n",
      "[INFO] Epoch: 38 , batch: 452 , training loss: 4.418907\n",
      "[INFO] Epoch: 38 , batch: 453 , training loss: 4.180392\n",
      "[INFO] Epoch: 38 , batch: 454 , training loss: 4.324899\n",
      "[INFO] Epoch: 38 , batch: 455 , training loss: 4.382482\n",
      "[INFO] Epoch: 38 , batch: 456 , training loss: 4.373751\n",
      "[INFO] Epoch: 38 , batch: 457 , training loss: 4.436049\n",
      "[INFO] Epoch: 38 , batch: 458 , training loss: 4.202107\n",
      "[INFO] Epoch: 38 , batch: 459 , training loss: 4.163841\n",
      "[INFO] Epoch: 38 , batch: 460 , training loss: 4.294042\n",
      "[INFO] Epoch: 38 , batch: 461 , training loss: 4.260507\n",
      "[INFO] Epoch: 38 , batch: 462 , training loss: 4.297052\n",
      "[INFO] Epoch: 38 , batch: 463 , training loss: 4.221330\n",
      "[INFO] Epoch: 38 , batch: 464 , training loss: 4.402218\n",
      "[INFO] Epoch: 38 , batch: 465 , training loss: 4.345188\n",
      "[INFO] Epoch: 38 , batch: 466 , training loss: 4.451907\n",
      "[INFO] Epoch: 38 , batch: 467 , training loss: 4.411293\n",
      "[INFO] Epoch: 38 , batch: 468 , training loss: 4.350108\n",
      "[INFO] Epoch: 38 , batch: 469 , training loss: 4.379331\n",
      "[INFO] Epoch: 38 , batch: 470 , training loss: 4.224304\n",
      "[INFO] Epoch: 38 , batch: 471 , training loss: 4.314060\n",
      "[INFO] Epoch: 38 , batch: 472 , training loss: 4.381414\n",
      "[INFO] Epoch: 38 , batch: 473 , training loss: 4.278674\n",
      "[INFO] Epoch: 38 , batch: 474 , training loss: 4.062988\n",
      "[INFO] Epoch: 38 , batch: 475 , training loss: 3.941904\n",
      "[INFO] Epoch: 38 , batch: 476 , training loss: 4.358642\n",
      "[INFO] Epoch: 38 , batch: 477 , training loss: 4.453593\n",
      "[INFO] Epoch: 38 , batch: 478 , training loss: 4.477072\n",
      "[INFO] Epoch: 38 , batch: 479 , training loss: 4.434737\n",
      "[INFO] Epoch: 38 , batch: 480 , training loss: 4.555323\n",
      "[INFO] Epoch: 38 , batch: 481 , training loss: 4.433614\n",
      "[INFO] Epoch: 38 , batch: 482 , training loss: 4.562519\n",
      "[INFO] Epoch: 38 , batch: 483 , training loss: 4.397914\n",
      "[INFO] Epoch: 38 , batch: 484 , training loss: 4.193419\n",
      "[INFO] Epoch: 38 , batch: 485 , training loss: 4.287118\n",
      "[INFO] Epoch: 38 , batch: 486 , training loss: 4.190050\n",
      "[INFO] Epoch: 38 , batch: 487 , training loss: 4.168758\n",
      "[INFO] Epoch: 38 , batch: 488 , training loss: 4.355369\n",
      "[INFO] Epoch: 38 , batch: 489 , training loss: 4.258384\n",
      "[INFO] Epoch: 38 , batch: 490 , training loss: 4.321908\n",
      "[INFO] Epoch: 38 , batch: 491 , training loss: 4.244249\n",
      "[INFO] Epoch: 38 , batch: 492 , training loss: 4.215627\n",
      "[INFO] Epoch: 38 , batch: 493 , training loss: 4.376864\n",
      "[INFO] Epoch: 38 , batch: 494 , training loss: 4.283288\n",
      "[INFO] Epoch: 38 , batch: 495 , training loss: 4.441823\n",
      "[INFO] Epoch: 38 , batch: 496 , training loss: 4.311819\n",
      "[INFO] Epoch: 38 , batch: 497 , training loss: 4.368083\n",
      "[INFO] Epoch: 38 , batch: 498 , training loss: 4.343598\n",
      "[INFO] Epoch: 38 , batch: 499 , training loss: 4.405491\n",
      "[INFO] Epoch: 38 , batch: 500 , training loss: 4.555579\n",
      "[INFO] Epoch: 38 , batch: 501 , training loss: 4.909168\n",
      "[INFO] Epoch: 38 , batch: 502 , training loss: 4.905802\n",
      "[INFO] Epoch: 38 , batch: 503 , training loss: 4.558564\n",
      "[INFO] Epoch: 38 , batch: 504 , training loss: 4.703245\n",
      "[INFO] Epoch: 38 , batch: 505 , training loss: 4.662157\n",
      "[INFO] Epoch: 38 , batch: 506 , training loss: 4.626254\n",
      "[INFO] Epoch: 38 , batch: 507 , training loss: 4.688205\n",
      "[INFO] Epoch: 38 , batch: 508 , training loss: 4.618718\n",
      "[INFO] Epoch: 38 , batch: 509 , training loss: 4.423648\n",
      "[INFO] Epoch: 38 , batch: 510 , training loss: 4.531565\n",
      "[INFO] Epoch: 38 , batch: 511 , training loss: 4.438831\n",
      "[INFO] Epoch: 38 , batch: 512 , training loss: 4.508736\n",
      "[INFO] Epoch: 38 , batch: 513 , training loss: 4.780414\n",
      "[INFO] Epoch: 38 , batch: 514 , training loss: 4.405437\n",
      "[INFO] Epoch: 38 , batch: 515 , training loss: 4.672165\n",
      "[INFO] Epoch: 38 , batch: 516 , training loss: 4.476424\n",
      "[INFO] Epoch: 38 , batch: 517 , training loss: 4.438971\n",
      "[INFO] Epoch: 38 , batch: 518 , training loss: 4.409253\n",
      "[INFO] Epoch: 38 , batch: 519 , training loss: 4.251613\n",
      "[INFO] Epoch: 38 , batch: 520 , training loss: 4.492319\n",
      "[INFO] Epoch: 38 , batch: 521 , training loss: 4.470383\n",
      "[INFO] Epoch: 38 , batch: 522 , training loss: 4.554987\n",
      "[INFO] Epoch: 38 , batch: 523 , training loss: 4.454658\n",
      "[INFO] Epoch: 38 , batch: 524 , training loss: 4.747853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 38 , batch: 525 , training loss: 4.638764\n",
      "[INFO] Epoch: 38 , batch: 526 , training loss: 4.411294\n",
      "[INFO] Epoch: 38 , batch: 527 , training loss: 4.448054\n",
      "[INFO] Epoch: 38 , batch: 528 , training loss: 4.455080\n",
      "[INFO] Epoch: 38 , batch: 529 , training loss: 4.430147\n",
      "[INFO] Epoch: 38 , batch: 530 , training loss: 4.301062\n",
      "[INFO] Epoch: 38 , batch: 531 , training loss: 4.429957\n",
      "[INFO] Epoch: 38 , batch: 532 , training loss: 4.341866\n",
      "[INFO] Epoch: 38 , batch: 533 , training loss: 4.477232\n",
      "[INFO] Epoch: 38 , batch: 534 , training loss: 4.490010\n",
      "[INFO] Epoch: 38 , batch: 535 , training loss: 4.470464\n",
      "[INFO] Epoch: 38 , batch: 536 , training loss: 4.321494\n",
      "[INFO] Epoch: 38 , batch: 537 , training loss: 4.296870\n",
      "[INFO] Epoch: 38 , batch: 538 , training loss: 4.391711\n",
      "[INFO] Epoch: 38 , batch: 539 , training loss: 4.513257\n",
      "[INFO] Epoch: 38 , batch: 540 , training loss: 5.100956\n",
      "[INFO] Epoch: 38 , batch: 541 , training loss: 4.890897\n",
      "[INFO] Epoch: 38 , batch: 542 , training loss: 4.731271\n",
      "[INFO] Epoch: 39 , batch: 0 , training loss: 3.848230\n",
      "[INFO] Epoch: 39 , batch: 1 , training loss: 3.628922\n",
      "[INFO] Epoch: 39 , batch: 2 , training loss: 3.801623\n",
      "[INFO] Epoch: 39 , batch: 3 , training loss: 3.710776\n",
      "[INFO] Epoch: 39 , batch: 4 , training loss: 3.991459\n",
      "[INFO] Epoch: 39 , batch: 5 , training loss: 3.684611\n",
      "[INFO] Epoch: 39 , batch: 6 , training loss: 4.097669\n",
      "[INFO] Epoch: 39 , batch: 7 , training loss: 3.990304\n",
      "[INFO] Epoch: 39 , batch: 8 , training loss: 3.660358\n",
      "[INFO] Epoch: 39 , batch: 9 , training loss: 3.902676\n",
      "[INFO] Epoch: 39 , batch: 10 , training loss: 3.842804\n",
      "[INFO] Epoch: 39 , batch: 11 , training loss: 3.771946\n",
      "[INFO] Epoch: 39 , batch: 12 , training loss: 3.661087\n",
      "[INFO] Epoch: 39 , batch: 13 , training loss: 3.672210\n",
      "[INFO] Epoch: 39 , batch: 14 , training loss: 3.565411\n",
      "[INFO] Epoch: 39 , batch: 15 , training loss: 3.775278\n",
      "[INFO] Epoch: 39 , batch: 16 , training loss: 3.652762\n",
      "[INFO] Epoch: 39 , batch: 17 , training loss: 3.783370\n",
      "[INFO] Epoch: 39 , batch: 18 , training loss: 3.721124\n",
      "[INFO] Epoch: 39 , batch: 19 , training loss: 3.485410\n",
      "[INFO] Epoch: 39 , batch: 20 , training loss: 3.446216\n",
      "[INFO] Epoch: 39 , batch: 21 , training loss: 3.578040\n",
      "[INFO] Epoch: 39 , batch: 22 , training loss: 3.513073\n",
      "[INFO] Epoch: 39 , batch: 23 , training loss: 3.679216\n",
      "[INFO] Epoch: 39 , batch: 24 , training loss: 3.518119\n",
      "[INFO] Epoch: 39 , batch: 25 , training loss: 3.649540\n",
      "[INFO] Epoch: 39 , batch: 26 , training loss: 3.513848\n",
      "[INFO] Epoch: 39 , batch: 27 , training loss: 3.476399\n",
      "[INFO] Epoch: 39 , batch: 28 , training loss: 3.700402\n",
      "[INFO] Epoch: 39 , batch: 29 , training loss: 3.501505\n",
      "[INFO] Epoch: 39 , batch: 30 , training loss: 3.536757\n",
      "[INFO] Epoch: 39 , batch: 31 , training loss: 3.636600\n",
      "[INFO] Epoch: 39 , batch: 32 , training loss: 3.599140\n",
      "[INFO] Epoch: 39 , batch: 33 , training loss: 3.644796\n",
      "[INFO] Epoch: 39 , batch: 34 , training loss: 3.640411\n",
      "[INFO] Epoch: 39 , batch: 35 , training loss: 3.571025\n",
      "[INFO] Epoch: 39 , batch: 36 , training loss: 3.664955\n",
      "[INFO] Epoch: 39 , batch: 37 , training loss: 3.493479\n",
      "[INFO] Epoch: 39 , batch: 38 , training loss: 3.606091\n",
      "[INFO] Epoch: 39 , batch: 39 , training loss: 3.425727\n",
      "[INFO] Epoch: 39 , batch: 40 , training loss: 3.616038\n",
      "[INFO] Epoch: 39 , batch: 41 , training loss: 3.619099\n",
      "[INFO] Epoch: 39 , batch: 42 , training loss: 4.125735\n",
      "[INFO] Epoch: 39 , batch: 43 , training loss: 3.910615\n",
      "[INFO] Epoch: 39 , batch: 44 , training loss: 4.141846\n",
      "[INFO] Epoch: 39 , batch: 45 , training loss: 4.130939\n",
      "[INFO] Epoch: 39 , batch: 46 , training loss: 4.080402\n",
      "[INFO] Epoch: 39 , batch: 47 , training loss: 3.657528\n",
      "[INFO] Epoch: 39 , batch: 48 , training loss: 3.685009\n",
      "[INFO] Epoch: 39 , batch: 49 , training loss: 3.895124\n",
      "[INFO] Epoch: 39 , batch: 50 , training loss: 3.622014\n",
      "[INFO] Epoch: 39 , batch: 51 , training loss: 3.866289\n",
      "[INFO] Epoch: 39 , batch: 52 , training loss: 3.673170\n",
      "[INFO] Epoch: 39 , batch: 53 , training loss: 3.807333\n",
      "[INFO] Epoch: 39 , batch: 54 , training loss: 3.820798\n",
      "[INFO] Epoch: 39 , batch: 55 , training loss: 3.887199\n",
      "[INFO] Epoch: 39 , batch: 56 , training loss: 3.707003\n",
      "[INFO] Epoch: 39 , batch: 57 , training loss: 3.642622\n",
      "[INFO] Epoch: 39 , batch: 58 , training loss: 3.695629\n",
      "[INFO] Epoch: 39 , batch: 59 , training loss: 3.797793\n",
      "[INFO] Epoch: 39 , batch: 60 , training loss: 3.710076\n",
      "[INFO] Epoch: 39 , batch: 61 , training loss: 3.789996\n",
      "[INFO] Epoch: 39 , batch: 62 , training loss: 3.688282\n",
      "[INFO] Epoch: 39 , batch: 63 , training loss: 3.850660\n",
      "[INFO] Epoch: 39 , batch: 64 , training loss: 4.073012\n",
      "[INFO] Epoch: 39 , batch: 65 , training loss: 3.750369\n",
      "[INFO] Epoch: 39 , batch: 66 , training loss: 3.635912\n",
      "[INFO] Epoch: 39 , batch: 67 , training loss: 3.614006\n",
      "[INFO] Epoch: 39 , batch: 68 , training loss: 3.822010\n",
      "[INFO] Epoch: 39 , batch: 69 , training loss: 3.716131\n",
      "[INFO] Epoch: 39 , batch: 70 , training loss: 3.981062\n",
      "[INFO] Epoch: 39 , batch: 71 , training loss: 3.808545\n",
      "[INFO] Epoch: 39 , batch: 72 , training loss: 3.873790\n",
      "[INFO] Epoch: 39 , batch: 73 , training loss: 3.797668\n",
      "[INFO] Epoch: 39 , batch: 74 , training loss: 3.943014\n",
      "[INFO] Epoch: 39 , batch: 75 , training loss: 3.772998\n",
      "[INFO] Epoch: 39 , batch: 76 , training loss: 3.883203\n",
      "[INFO] Epoch: 39 , batch: 77 , training loss: 3.814852\n",
      "[INFO] Epoch: 39 , batch: 78 , training loss: 3.922887\n",
      "[INFO] Epoch: 39 , batch: 79 , training loss: 3.765058\n",
      "[INFO] Epoch: 39 , batch: 80 , training loss: 3.967876\n",
      "[INFO] Epoch: 39 , batch: 81 , training loss: 3.886231\n",
      "[INFO] Epoch: 39 , batch: 82 , training loss: 3.859906\n",
      "[INFO] Epoch: 39 , batch: 83 , training loss: 3.935319\n",
      "[INFO] Epoch: 39 , batch: 84 , training loss: 3.921052\n",
      "[INFO] Epoch: 39 , batch: 85 , training loss: 4.029998\n",
      "[INFO] Epoch: 39 , batch: 86 , training loss: 3.928999\n",
      "[INFO] Epoch: 39 , batch: 87 , training loss: 3.895731\n",
      "[INFO] Epoch: 39 , batch: 88 , training loss: 4.052315\n",
      "[INFO] Epoch: 39 , batch: 89 , training loss: 3.831346\n",
      "[INFO] Epoch: 39 , batch: 90 , training loss: 3.925569\n",
      "[INFO] Epoch: 39 , batch: 91 , training loss: 3.836781\n",
      "[INFO] Epoch: 39 , batch: 92 , training loss: 3.868448\n",
      "[INFO] Epoch: 39 , batch: 93 , training loss: 3.961103\n",
      "[INFO] Epoch: 39 , batch: 94 , training loss: 4.128685\n",
      "[INFO] Epoch: 39 , batch: 95 , training loss: 3.877162\n",
      "[INFO] Epoch: 39 , batch: 96 , training loss: 3.864263\n",
      "[INFO] Epoch: 39 , batch: 97 , training loss: 3.813356\n",
      "[INFO] Epoch: 39 , batch: 98 , training loss: 3.750455\n",
      "[INFO] Epoch: 39 , batch: 99 , training loss: 3.868938\n",
      "[INFO] Epoch: 39 , batch: 100 , training loss: 3.758942\n",
      "[INFO] Epoch: 39 , batch: 101 , training loss: 3.770622\n",
      "[INFO] Epoch: 39 , batch: 102 , training loss: 3.981660\n",
      "[INFO] Epoch: 39 , batch: 103 , training loss: 3.745997\n",
      "[INFO] Epoch: 39 , batch: 104 , training loss: 3.688236\n",
      "[INFO] Epoch: 39 , batch: 105 , training loss: 3.939325\n",
      "[INFO] Epoch: 39 , batch: 106 , training loss: 3.965317\n",
      "[INFO] Epoch: 39 , batch: 107 , training loss: 3.817763\n",
      "[INFO] Epoch: 39 , batch: 108 , training loss: 3.737459\n",
      "[INFO] Epoch: 39 , batch: 109 , training loss: 3.670842\n",
      "[INFO] Epoch: 39 , batch: 110 , training loss: 3.864854\n",
      "[INFO] Epoch: 39 , batch: 111 , training loss: 3.921058\n",
      "[INFO] Epoch: 39 , batch: 112 , training loss: 3.862064\n",
      "[INFO] Epoch: 39 , batch: 113 , training loss: 3.855627\n",
      "[INFO] Epoch: 39 , batch: 114 , training loss: 3.816612\n",
      "[INFO] Epoch: 39 , batch: 115 , training loss: 3.836457\n",
      "[INFO] Epoch: 39 , batch: 116 , training loss: 3.762804\n",
      "[INFO] Epoch: 39 , batch: 117 , training loss: 3.961948\n",
      "[INFO] Epoch: 39 , batch: 118 , training loss: 3.949666\n",
      "[INFO] Epoch: 39 , batch: 119 , training loss: 4.113876\n",
      "[INFO] Epoch: 39 , batch: 120 , training loss: 4.037093\n",
      "[INFO] Epoch: 39 , batch: 121 , training loss: 3.932640\n",
      "[INFO] Epoch: 39 , batch: 122 , training loss: 3.828783\n",
      "[INFO] Epoch: 39 , batch: 123 , training loss: 3.821454\n",
      "[INFO] Epoch: 39 , batch: 124 , training loss: 3.928844\n",
      "[INFO] Epoch: 39 , batch: 125 , training loss: 3.727906\n",
      "[INFO] Epoch: 39 , batch: 126 , training loss: 3.762529\n",
      "[INFO] Epoch: 39 , batch: 127 , training loss: 3.780632\n",
      "[INFO] Epoch: 39 , batch: 128 , training loss: 3.927458\n",
      "[INFO] Epoch: 39 , batch: 129 , training loss: 3.850280\n",
      "[INFO] Epoch: 39 , batch: 130 , training loss: 3.881857\n",
      "[INFO] Epoch: 39 , batch: 131 , training loss: 3.872176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 39 , batch: 132 , training loss: 3.883475\n",
      "[INFO] Epoch: 39 , batch: 133 , training loss: 3.829739\n",
      "[INFO] Epoch: 39 , batch: 134 , training loss: 3.607854\n",
      "[INFO] Epoch: 39 , batch: 135 , training loss: 3.682206\n",
      "[INFO] Epoch: 39 , batch: 136 , training loss: 3.948277\n",
      "[INFO] Epoch: 39 , batch: 137 , training loss: 3.883797\n",
      "[INFO] Epoch: 39 , batch: 138 , training loss: 3.956850\n",
      "[INFO] Epoch: 39 , batch: 139 , training loss: 4.514145\n",
      "[INFO] Epoch: 39 , batch: 140 , training loss: 4.287838\n",
      "[INFO] Epoch: 39 , batch: 141 , training loss: 4.089301\n",
      "[INFO] Epoch: 39 , batch: 142 , training loss: 3.765368\n",
      "[INFO] Epoch: 39 , batch: 143 , training loss: 3.890159\n",
      "[INFO] Epoch: 39 , batch: 144 , training loss: 3.761717\n",
      "[INFO] Epoch: 39 , batch: 145 , training loss: 3.829260\n",
      "[INFO] Epoch: 39 , batch: 146 , training loss: 4.045646\n",
      "[INFO] Epoch: 39 , batch: 147 , training loss: 3.704217\n",
      "[INFO] Epoch: 39 , batch: 148 , training loss: 3.678098\n",
      "[INFO] Epoch: 39 , batch: 149 , training loss: 3.752000\n",
      "[INFO] Epoch: 39 , batch: 150 , training loss: 4.029547\n",
      "[INFO] Epoch: 39 , batch: 151 , training loss: 3.849065\n",
      "[INFO] Epoch: 39 , batch: 152 , training loss: 3.862212\n",
      "[INFO] Epoch: 39 , batch: 153 , training loss: 3.890461\n",
      "[INFO] Epoch: 39 , batch: 154 , training loss: 3.976903\n",
      "[INFO] Epoch: 39 , batch: 155 , training loss: 4.211739\n",
      "[INFO] Epoch: 39 , batch: 156 , training loss: 3.950799\n",
      "[INFO] Epoch: 39 , batch: 157 , training loss: 3.892435\n",
      "[INFO] Epoch: 39 , batch: 158 , training loss: 4.029330\n",
      "[INFO] Epoch: 39 , batch: 159 , training loss: 3.953352\n",
      "[INFO] Epoch: 39 , batch: 160 , training loss: 4.240839\n",
      "[INFO] Epoch: 39 , batch: 161 , training loss: 4.225441\n",
      "[INFO] Epoch: 39 , batch: 162 , training loss: 4.240017\n",
      "[INFO] Epoch: 39 , batch: 163 , training loss: 4.384984\n",
      "[INFO] Epoch: 39 , batch: 164 , training loss: 4.321939\n",
      "[INFO] Epoch: 39 , batch: 165 , training loss: 4.283851\n",
      "[INFO] Epoch: 39 , batch: 166 , training loss: 4.148086\n",
      "[INFO] Epoch: 39 , batch: 167 , training loss: 4.276552\n",
      "[INFO] Epoch: 39 , batch: 168 , training loss: 3.892520\n",
      "[INFO] Epoch: 39 , batch: 169 , training loss: 3.918445\n",
      "[INFO] Epoch: 39 , batch: 170 , training loss: 4.111429\n",
      "[INFO] Epoch: 39 , batch: 171 , training loss: 3.538457\n",
      "[INFO] Epoch: 39 , batch: 172 , training loss: 3.747778\n",
      "[INFO] Epoch: 39 , batch: 173 , training loss: 4.093785\n",
      "[INFO] Epoch: 39 , batch: 174 , training loss: 4.512507\n",
      "[INFO] Epoch: 39 , batch: 175 , training loss: 4.782087\n",
      "[INFO] Epoch: 39 , batch: 176 , training loss: 4.423098\n",
      "[INFO] Epoch: 39 , batch: 177 , training loss: 4.042301\n",
      "[INFO] Epoch: 39 , batch: 178 , training loss: 4.075464\n",
      "[INFO] Epoch: 39 , batch: 179 , training loss: 4.118217\n",
      "[INFO] Epoch: 39 , batch: 180 , training loss: 4.067982\n",
      "[INFO] Epoch: 39 , batch: 181 , training loss: 4.346903\n",
      "[INFO] Epoch: 39 , batch: 182 , training loss: 4.319274\n",
      "[INFO] Epoch: 39 , batch: 183 , training loss: 4.285218\n",
      "[INFO] Epoch: 39 , batch: 184 , training loss: 4.159337\n",
      "[INFO] Epoch: 39 , batch: 185 , training loss: 4.129803\n",
      "[INFO] Epoch: 39 , batch: 186 , training loss: 4.291470\n",
      "[INFO] Epoch: 39 , batch: 187 , training loss: 4.376153\n",
      "[INFO] Epoch: 39 , batch: 188 , training loss: 4.373695\n",
      "[INFO] Epoch: 39 , batch: 189 , training loss: 4.287932\n",
      "[INFO] Epoch: 39 , batch: 190 , training loss: 4.324248\n",
      "[INFO] Epoch: 39 , batch: 191 , training loss: 4.411170\n",
      "[INFO] Epoch: 39 , batch: 192 , training loss: 4.283203\n",
      "[INFO] Epoch: 39 , batch: 193 , training loss: 4.358570\n",
      "[INFO] Epoch: 39 , batch: 194 , training loss: 4.287496\n",
      "[INFO] Epoch: 39 , batch: 195 , training loss: 4.238107\n",
      "[INFO] Epoch: 39 , batch: 196 , training loss: 4.094942\n",
      "[INFO] Epoch: 39 , batch: 197 , training loss: 4.188048\n",
      "[INFO] Epoch: 39 , batch: 198 , training loss: 4.098788\n",
      "[INFO] Epoch: 39 , batch: 199 , training loss: 4.183730\n",
      "[INFO] Epoch: 39 , batch: 200 , training loss: 4.128877\n",
      "[INFO] Epoch: 39 , batch: 201 , training loss: 4.021712\n",
      "[INFO] Epoch: 39 , batch: 202 , training loss: 4.034478\n",
      "[INFO] Epoch: 39 , batch: 203 , training loss: 4.162349\n",
      "[INFO] Epoch: 39 , batch: 204 , training loss: 4.248369\n",
      "[INFO] Epoch: 39 , batch: 205 , training loss: 3.844832\n",
      "[INFO] Epoch: 39 , batch: 206 , training loss: 3.762192\n",
      "[INFO] Epoch: 39 , batch: 207 , training loss: 3.775850\n",
      "[INFO] Epoch: 39 , batch: 208 , training loss: 4.088981\n",
      "[INFO] Epoch: 39 , batch: 209 , training loss: 4.070323\n",
      "[INFO] Epoch: 39 , batch: 210 , training loss: 4.067284\n",
      "[INFO] Epoch: 39 , batch: 211 , training loss: 4.075083\n",
      "[INFO] Epoch: 39 , batch: 212 , training loss: 4.183923\n",
      "[INFO] Epoch: 39 , batch: 213 , training loss: 4.133880\n",
      "[INFO] Epoch: 39 , batch: 214 , training loss: 4.219848\n",
      "[INFO] Epoch: 39 , batch: 215 , training loss: 4.390315\n",
      "[INFO] Epoch: 39 , batch: 216 , training loss: 4.118517\n",
      "[INFO] Epoch: 39 , batch: 217 , training loss: 4.057054\n",
      "[INFO] Epoch: 39 , batch: 218 , training loss: 4.048985\n",
      "[INFO] Epoch: 39 , batch: 219 , training loss: 4.149385\n",
      "[INFO] Epoch: 39 , batch: 220 , training loss: 3.965223\n",
      "[INFO] Epoch: 39 , batch: 221 , training loss: 3.992323\n",
      "[INFO] Epoch: 39 , batch: 222 , training loss: 4.138468\n",
      "[INFO] Epoch: 39 , batch: 223 , training loss: 4.234785\n",
      "[INFO] Epoch: 39 , batch: 224 , training loss: 4.288370\n",
      "[INFO] Epoch: 39 , batch: 225 , training loss: 4.160353\n",
      "[INFO] Epoch: 39 , batch: 226 , training loss: 4.295633\n",
      "[INFO] Epoch: 39 , batch: 227 , training loss: 4.264029\n",
      "[INFO] Epoch: 39 , batch: 228 , training loss: 4.279974\n",
      "[INFO] Epoch: 39 , batch: 229 , training loss: 4.135719\n",
      "[INFO] Epoch: 39 , batch: 230 , training loss: 3.997799\n",
      "[INFO] Epoch: 39 , batch: 231 , training loss: 3.863034\n",
      "[INFO] Epoch: 39 , batch: 232 , training loss: 4.010406\n",
      "[INFO] Epoch: 39 , batch: 233 , training loss: 4.035231\n",
      "[INFO] Epoch: 39 , batch: 234 , training loss: 3.718237\n",
      "[INFO] Epoch: 39 , batch: 235 , training loss: 3.825318\n",
      "[INFO] Epoch: 39 , batch: 236 , training loss: 3.939823\n",
      "[INFO] Epoch: 39 , batch: 237 , training loss: 4.161621\n",
      "[INFO] Epoch: 39 , batch: 238 , training loss: 3.942098\n",
      "[INFO] Epoch: 39 , batch: 239 , training loss: 3.993929\n",
      "[INFO] Epoch: 39 , batch: 240 , training loss: 4.013371\n",
      "[INFO] Epoch: 39 , batch: 241 , training loss: 3.837020\n",
      "[INFO] Epoch: 39 , batch: 242 , training loss: 3.850462\n",
      "[INFO] Epoch: 39 , batch: 243 , training loss: 4.136254\n",
      "[INFO] Epoch: 39 , batch: 244 , training loss: 4.071422\n",
      "[INFO] Epoch: 39 , batch: 245 , training loss: 4.060311\n",
      "[INFO] Epoch: 39 , batch: 246 , training loss: 3.761078\n",
      "[INFO] Epoch: 39 , batch: 247 , training loss: 3.923781\n",
      "[INFO] Epoch: 39 , batch: 248 , training loss: 3.989357\n",
      "[INFO] Epoch: 39 , batch: 249 , training loss: 3.968446\n",
      "[INFO] Epoch: 39 , batch: 250 , training loss: 3.785843\n",
      "[INFO] Epoch: 39 , batch: 251 , training loss: 4.221415\n",
      "[INFO] Epoch: 39 , batch: 252 , training loss: 3.933666\n",
      "[INFO] Epoch: 39 , batch: 253 , training loss: 3.832711\n",
      "[INFO] Epoch: 39 , batch: 254 , training loss: 4.101142\n",
      "[INFO] Epoch: 39 , batch: 255 , training loss: 4.086216\n",
      "[INFO] Epoch: 39 , batch: 256 , training loss: 4.071868\n",
      "[INFO] Epoch: 39 , batch: 257 , training loss: 4.260790\n",
      "[INFO] Epoch: 39 , batch: 258 , training loss: 4.254640\n",
      "[INFO] Epoch: 39 , batch: 259 , training loss: 4.297979\n",
      "[INFO] Epoch: 39 , batch: 260 , training loss: 4.068573\n",
      "[INFO] Epoch: 39 , batch: 261 , training loss: 4.241788\n",
      "[INFO] Epoch: 39 , batch: 262 , training loss: 4.378531\n",
      "[INFO] Epoch: 39 , batch: 263 , training loss: 4.560542\n",
      "[INFO] Epoch: 39 , batch: 264 , training loss: 3.908896\n",
      "[INFO] Epoch: 39 , batch: 265 , training loss: 4.032016\n",
      "[INFO] Epoch: 39 , batch: 266 , training loss: 4.420383\n",
      "[INFO] Epoch: 39 , batch: 267 , training loss: 4.165120\n",
      "[INFO] Epoch: 39 , batch: 268 , training loss: 4.101894\n",
      "[INFO] Epoch: 39 , batch: 269 , training loss: 4.062732\n",
      "[INFO] Epoch: 39 , batch: 270 , training loss: 4.101887\n",
      "[INFO] Epoch: 39 , batch: 271 , training loss: 4.131162\n",
      "[INFO] Epoch: 39 , batch: 272 , training loss: 4.117708\n",
      "[INFO] Epoch: 39 , batch: 273 , training loss: 4.130998\n",
      "[INFO] Epoch: 39 , batch: 274 , training loss: 4.217568\n",
      "[INFO] Epoch: 39 , batch: 275 , training loss: 4.065146\n",
      "[INFO] Epoch: 39 , batch: 276 , training loss: 4.134370\n",
      "[INFO] Epoch: 39 , batch: 277 , training loss: 4.315684\n",
      "[INFO] Epoch: 39 , batch: 278 , training loss: 4.001954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 39 , batch: 279 , training loss: 3.988086\n",
      "[INFO] Epoch: 39 , batch: 280 , training loss: 3.970098\n",
      "[INFO] Epoch: 39 , batch: 281 , training loss: 4.090488\n",
      "[INFO] Epoch: 39 , batch: 282 , training loss: 4.008225\n",
      "[INFO] Epoch: 39 , batch: 283 , training loss: 3.996948\n",
      "[INFO] Epoch: 39 , batch: 284 , training loss: 4.044653\n",
      "[INFO] Epoch: 39 , batch: 285 , training loss: 3.970055\n",
      "[INFO] Epoch: 39 , batch: 286 , training loss: 4.002169\n",
      "[INFO] Epoch: 39 , batch: 287 , training loss: 3.945544\n",
      "[INFO] Epoch: 39 , batch: 288 , training loss: 3.883554\n",
      "[INFO] Epoch: 39 , batch: 289 , training loss: 3.968888\n",
      "[INFO] Epoch: 39 , batch: 290 , training loss: 3.759567\n",
      "[INFO] Epoch: 39 , batch: 291 , training loss: 3.745245\n",
      "[INFO] Epoch: 39 , batch: 292 , training loss: 3.840506\n",
      "[INFO] Epoch: 39 , batch: 293 , training loss: 3.778267\n",
      "[INFO] Epoch: 39 , batch: 294 , training loss: 4.428946\n",
      "[INFO] Epoch: 39 , batch: 295 , training loss: 4.216960\n",
      "[INFO] Epoch: 39 , batch: 296 , training loss: 4.141948\n",
      "[INFO] Epoch: 39 , batch: 297 , training loss: 4.088541\n",
      "[INFO] Epoch: 39 , batch: 298 , training loss: 3.936019\n",
      "[INFO] Epoch: 39 , batch: 299 , training loss: 3.984680\n",
      "[INFO] Epoch: 39 , batch: 300 , training loss: 3.975739\n",
      "[INFO] Epoch: 39 , batch: 301 , training loss: 3.891835\n",
      "[INFO] Epoch: 39 , batch: 302 , training loss: 4.066059\n",
      "[INFO] Epoch: 39 , batch: 303 , training loss: 4.070111\n",
      "[INFO] Epoch: 39 , batch: 304 , training loss: 4.217531\n",
      "[INFO] Epoch: 39 , batch: 305 , training loss: 4.052797\n",
      "[INFO] Epoch: 39 , batch: 306 , training loss: 4.144103\n",
      "[INFO] Epoch: 39 , batch: 307 , training loss: 4.147597\n",
      "[INFO] Epoch: 39 , batch: 308 , training loss: 3.976683\n",
      "[INFO] Epoch: 39 , batch: 309 , training loss: 3.963336\n",
      "[INFO] Epoch: 39 , batch: 310 , training loss: 3.908696\n",
      "[INFO] Epoch: 39 , batch: 311 , training loss: 3.900436\n",
      "[INFO] Epoch: 39 , batch: 312 , training loss: 3.808776\n",
      "[INFO] Epoch: 39 , batch: 313 , training loss: 3.906632\n",
      "[INFO] Epoch: 39 , batch: 314 , training loss: 3.984137\n",
      "[INFO] Epoch: 39 , batch: 315 , training loss: 4.049155\n",
      "[INFO] Epoch: 39 , batch: 316 , training loss: 4.293889\n",
      "[INFO] Epoch: 39 , batch: 317 , training loss: 4.651888\n",
      "[INFO] Epoch: 39 , batch: 318 , training loss: 4.756491\n",
      "[INFO] Epoch: 39 , batch: 319 , training loss: 4.461978\n",
      "[INFO] Epoch: 39 , batch: 320 , training loss: 4.014888\n",
      "[INFO] Epoch: 39 , batch: 321 , training loss: 3.815131\n",
      "[INFO] Epoch: 39 , batch: 322 , training loss: 3.917606\n",
      "[INFO] Epoch: 39 , batch: 323 , training loss: 3.971292\n",
      "[INFO] Epoch: 39 , batch: 324 , training loss: 3.943918\n",
      "[INFO] Epoch: 39 , batch: 325 , training loss: 4.052165\n",
      "[INFO] Epoch: 39 , batch: 326 , training loss: 4.114072\n",
      "[INFO] Epoch: 39 , batch: 327 , training loss: 4.063431\n",
      "[INFO] Epoch: 39 , batch: 328 , training loss: 4.044242\n",
      "[INFO] Epoch: 39 , batch: 329 , training loss: 3.975641\n",
      "[INFO] Epoch: 39 , batch: 330 , training loss: 3.954201\n",
      "[INFO] Epoch: 39 , batch: 331 , training loss: 4.094201\n",
      "[INFO] Epoch: 39 , batch: 332 , training loss: 3.955145\n",
      "[INFO] Epoch: 39 , batch: 333 , training loss: 3.916063\n",
      "[INFO] Epoch: 39 , batch: 334 , training loss: 3.945373\n",
      "[INFO] Epoch: 39 , batch: 335 , training loss: 4.083143\n",
      "[INFO] Epoch: 39 , batch: 336 , training loss: 4.092915\n",
      "[INFO] Epoch: 39 , batch: 337 , training loss: 4.139454\n",
      "[INFO] Epoch: 39 , batch: 338 , training loss: 4.334359\n",
      "[INFO] Epoch: 39 , batch: 339 , training loss: 4.159015\n",
      "[INFO] Epoch: 39 , batch: 340 , training loss: 4.338656\n",
      "[INFO] Epoch: 39 , batch: 341 , training loss: 4.091920\n",
      "[INFO] Epoch: 39 , batch: 342 , training loss: 3.873682\n",
      "[INFO] Epoch: 39 , batch: 343 , training loss: 3.963459\n",
      "[INFO] Epoch: 39 , batch: 344 , training loss: 3.829397\n",
      "[INFO] Epoch: 39 , batch: 345 , training loss: 3.951342\n",
      "[INFO] Epoch: 39 , batch: 346 , training loss: 4.011735\n",
      "[INFO] Epoch: 39 , batch: 347 , training loss: 3.913087\n",
      "[INFO] Epoch: 39 , batch: 348 , training loss: 4.022496\n",
      "[INFO] Epoch: 39 , batch: 349 , training loss: 4.095217\n",
      "[INFO] Epoch: 39 , batch: 350 , training loss: 3.953163\n",
      "[INFO] Epoch: 39 , batch: 351 , training loss: 4.046652\n",
      "[INFO] Epoch: 39 , batch: 352 , training loss: 4.042718\n",
      "[INFO] Epoch: 39 , batch: 353 , training loss: 4.019962\n",
      "[INFO] Epoch: 39 , batch: 354 , training loss: 4.120073\n",
      "[INFO] Epoch: 39 , batch: 355 , training loss: 4.127244\n",
      "[INFO] Epoch: 39 , batch: 356 , training loss: 3.994277\n",
      "[INFO] Epoch: 39 , batch: 357 , training loss: 4.055307\n",
      "[INFO] Epoch: 39 , batch: 358 , training loss: 3.957919\n",
      "[INFO] Epoch: 39 , batch: 359 , training loss: 3.972417\n",
      "[INFO] Epoch: 39 , batch: 360 , training loss: 4.088930\n",
      "[INFO] Epoch: 39 , batch: 361 , training loss: 4.038132\n",
      "[INFO] Epoch: 39 , batch: 362 , training loss: 4.138939\n",
      "[INFO] Epoch: 39 , batch: 363 , training loss: 4.036604\n",
      "[INFO] Epoch: 39 , batch: 364 , training loss: 4.077521\n",
      "[INFO] Epoch: 39 , batch: 365 , training loss: 4.004417\n",
      "[INFO] Epoch: 39 , batch: 366 , training loss: 4.100501\n",
      "[INFO] Epoch: 39 , batch: 367 , training loss: 4.141442\n",
      "[INFO] Epoch: 39 , batch: 368 , training loss: 4.523676\n",
      "[INFO] Epoch: 39 , batch: 369 , training loss: 4.221948\n",
      "[INFO] Epoch: 39 , batch: 370 , training loss: 3.994373\n",
      "[INFO] Epoch: 39 , batch: 371 , training loss: 4.434713\n",
      "[INFO] Epoch: 39 , batch: 372 , training loss: 4.672910\n",
      "[INFO] Epoch: 39 , batch: 373 , training loss: 4.706496\n",
      "[INFO] Epoch: 39 , batch: 374 , training loss: 4.857659\n",
      "[INFO] Epoch: 39 , batch: 375 , training loss: 4.807782\n",
      "[INFO] Epoch: 39 , batch: 376 , training loss: 4.706121\n",
      "[INFO] Epoch: 39 , batch: 377 , training loss: 4.436586\n",
      "[INFO] Epoch: 39 , batch: 378 , training loss: 4.565274\n",
      "[INFO] Epoch: 39 , batch: 379 , training loss: 4.518006\n",
      "[INFO] Epoch: 39 , batch: 380 , training loss: 4.688499\n",
      "[INFO] Epoch: 39 , batch: 381 , training loss: 4.391428\n",
      "[INFO] Epoch: 39 , batch: 382 , training loss: 4.666812\n",
      "[INFO] Epoch: 39 , batch: 383 , training loss: 4.696457\n",
      "[INFO] Epoch: 39 , batch: 384 , training loss: 4.685582\n",
      "[INFO] Epoch: 39 , batch: 385 , training loss: 4.358552\n",
      "[INFO] Epoch: 39 , batch: 386 , training loss: 4.621555\n",
      "[INFO] Epoch: 39 , batch: 387 , training loss: 4.545746\n",
      "[INFO] Epoch: 39 , batch: 388 , training loss: 4.390365\n",
      "[INFO] Epoch: 39 , batch: 389 , training loss: 4.201887\n",
      "[INFO] Epoch: 39 , batch: 390 , training loss: 4.202844\n",
      "[INFO] Epoch: 39 , batch: 391 , training loss: 4.223477\n",
      "[INFO] Epoch: 39 , batch: 392 , training loss: 4.612520\n",
      "[INFO] Epoch: 39 , batch: 393 , training loss: 4.491851\n",
      "[INFO] Epoch: 39 , batch: 394 , training loss: 4.582320\n",
      "[INFO] Epoch: 39 , batch: 395 , training loss: 4.400475\n",
      "[INFO] Epoch: 39 , batch: 396 , training loss: 4.214634\n",
      "[INFO] Epoch: 39 , batch: 397 , training loss: 4.367334\n",
      "[INFO] Epoch: 39 , batch: 398 , training loss: 4.216504\n",
      "[INFO] Epoch: 39 , batch: 399 , training loss: 4.312202\n",
      "[INFO] Epoch: 39 , batch: 400 , training loss: 4.268888\n",
      "[INFO] Epoch: 39 , batch: 401 , training loss: 4.711284\n",
      "[INFO] Epoch: 39 , batch: 402 , training loss: 4.437486\n",
      "[INFO] Epoch: 39 , batch: 403 , training loss: 4.243999\n",
      "[INFO] Epoch: 39 , batch: 404 , training loss: 4.418407\n",
      "[INFO] Epoch: 39 , batch: 405 , training loss: 4.487278\n",
      "[INFO] Epoch: 39 , batch: 406 , training loss: 4.385527\n",
      "[INFO] Epoch: 39 , batch: 407 , training loss: 4.428551\n",
      "[INFO] Epoch: 39 , batch: 408 , training loss: 4.363904\n",
      "[INFO] Epoch: 39 , batch: 409 , training loss: 4.399837\n",
      "[INFO] Epoch: 39 , batch: 410 , training loss: 4.471187\n",
      "[INFO] Epoch: 39 , batch: 411 , training loss: 4.639339\n",
      "[INFO] Epoch: 39 , batch: 412 , training loss: 4.466632\n",
      "[INFO] Epoch: 39 , batch: 413 , training loss: 4.324108\n",
      "[INFO] Epoch: 39 , batch: 414 , training loss: 4.355685\n",
      "[INFO] Epoch: 39 , batch: 415 , training loss: 4.426588\n",
      "[INFO] Epoch: 39 , batch: 416 , training loss: 4.457577\n",
      "[INFO] Epoch: 39 , batch: 417 , training loss: 4.396421\n",
      "[INFO] Epoch: 39 , batch: 418 , training loss: 4.453167\n",
      "[INFO] Epoch: 39 , batch: 419 , training loss: 4.423556\n",
      "[INFO] Epoch: 39 , batch: 420 , training loss: 4.388895\n",
      "[INFO] Epoch: 39 , batch: 421 , training loss: 4.356296\n",
      "[INFO] Epoch: 39 , batch: 422 , training loss: 4.198477\n",
      "[INFO] Epoch: 39 , batch: 423 , training loss: 4.442705\n",
      "[INFO] Epoch: 39 , batch: 424 , training loss: 4.620876\n",
      "[INFO] Epoch: 39 , batch: 425 , training loss: 4.448211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 39 , batch: 426 , training loss: 4.208983\n",
      "[INFO] Epoch: 39 , batch: 427 , training loss: 4.467167\n",
      "[INFO] Epoch: 39 , batch: 428 , training loss: 4.318598\n",
      "[INFO] Epoch: 39 , batch: 429 , training loss: 4.215936\n",
      "[INFO] Epoch: 39 , batch: 430 , training loss: 4.448986\n",
      "[INFO] Epoch: 39 , batch: 431 , training loss: 4.057014\n",
      "[INFO] Epoch: 39 , batch: 432 , training loss: 4.113441\n",
      "[INFO] Epoch: 39 , batch: 433 , training loss: 4.147572\n",
      "[INFO] Epoch: 39 , batch: 434 , training loss: 4.031970\n",
      "[INFO] Epoch: 39 , batch: 435 , training loss: 4.383222\n",
      "[INFO] Epoch: 39 , batch: 436 , training loss: 4.419603\n",
      "[INFO] Epoch: 39 , batch: 437 , training loss: 4.224200\n",
      "[INFO] Epoch: 39 , batch: 438 , training loss: 4.081543\n",
      "[INFO] Epoch: 39 , batch: 439 , training loss: 4.322114\n",
      "[INFO] Epoch: 39 , batch: 440 , training loss: 4.440900\n",
      "[INFO] Epoch: 39 , batch: 441 , training loss: 4.509222\n",
      "[INFO] Epoch: 39 , batch: 442 , training loss: 4.300539\n",
      "[INFO] Epoch: 39 , batch: 443 , training loss: 4.479578\n",
      "[INFO] Epoch: 39 , batch: 444 , training loss: 4.057621\n",
      "[INFO] Epoch: 39 , batch: 445 , training loss: 3.976162\n",
      "[INFO] Epoch: 39 , batch: 446 , training loss: 3.913353\n",
      "[INFO] Epoch: 39 , batch: 447 , training loss: 4.098306\n",
      "[INFO] Epoch: 39 , batch: 448 , training loss: 4.234628\n",
      "[INFO] Epoch: 39 , batch: 449 , training loss: 4.620183\n",
      "[INFO] Epoch: 39 , batch: 450 , training loss: 4.683173\n",
      "[INFO] Epoch: 39 , batch: 451 , training loss: 4.554713\n",
      "[INFO] Epoch: 39 , batch: 452 , training loss: 4.400095\n",
      "[INFO] Epoch: 39 , batch: 453 , training loss: 4.161227\n",
      "[INFO] Epoch: 39 , batch: 454 , training loss: 4.298642\n",
      "[INFO] Epoch: 39 , batch: 455 , training loss: 4.363928\n",
      "[INFO] Epoch: 39 , batch: 456 , training loss: 4.347300\n",
      "[INFO] Epoch: 39 , batch: 457 , training loss: 4.412470\n",
      "[INFO] Epoch: 39 , batch: 458 , training loss: 4.178452\n",
      "[INFO] Epoch: 39 , batch: 459 , training loss: 4.148502\n",
      "[INFO] Epoch: 39 , batch: 460 , training loss: 4.254463\n",
      "[INFO] Epoch: 39 , batch: 461 , training loss: 4.232659\n",
      "[INFO] Epoch: 39 , batch: 462 , training loss: 4.281420\n",
      "[INFO] Epoch: 39 , batch: 463 , training loss: 4.203816\n",
      "[INFO] Epoch: 39 , batch: 464 , training loss: 4.375896\n",
      "[INFO] Epoch: 39 , batch: 465 , training loss: 4.323937\n",
      "[INFO] Epoch: 39 , batch: 466 , training loss: 4.424174\n",
      "[INFO] Epoch: 39 , batch: 467 , training loss: 4.378197\n",
      "[INFO] Epoch: 39 , batch: 468 , training loss: 4.336484\n",
      "[INFO] Epoch: 39 , batch: 469 , training loss: 4.360443\n",
      "[INFO] Epoch: 39 , batch: 470 , training loss: 4.187813\n",
      "[INFO] Epoch: 39 , batch: 471 , training loss: 4.292203\n",
      "[INFO] Epoch: 39 , batch: 472 , training loss: 4.359365\n",
      "[INFO] Epoch: 39 , batch: 473 , training loss: 4.253035\n",
      "[INFO] Epoch: 39 , batch: 474 , training loss: 4.037857\n",
      "[INFO] Epoch: 39 , batch: 475 , training loss: 3.920637\n",
      "[INFO] Epoch: 39 , batch: 476 , training loss: 4.333688\n",
      "[INFO] Epoch: 39 , batch: 477 , training loss: 4.437811\n",
      "[INFO] Epoch: 39 , batch: 478 , training loss: 4.439910\n",
      "[INFO] Epoch: 39 , batch: 479 , training loss: 4.419854\n",
      "[INFO] Epoch: 39 , batch: 480 , training loss: 4.543817\n",
      "[INFO] Epoch: 39 , batch: 481 , training loss: 4.418442\n",
      "[INFO] Epoch: 39 , batch: 482 , training loss: 4.535904\n",
      "[INFO] Epoch: 39 , batch: 483 , training loss: 4.370156\n",
      "[INFO] Epoch: 39 , batch: 484 , training loss: 4.174580\n",
      "[INFO] Epoch: 39 , batch: 485 , training loss: 4.265258\n",
      "[INFO] Epoch: 39 , batch: 486 , training loss: 4.168831\n",
      "[INFO] Epoch: 39 , batch: 487 , training loss: 4.149802\n",
      "[INFO] Epoch: 39 , batch: 488 , training loss: 4.328176\n",
      "[INFO] Epoch: 39 , batch: 489 , training loss: 4.235899\n",
      "[INFO] Epoch: 39 , batch: 490 , training loss: 4.290113\n",
      "[INFO] Epoch: 39 , batch: 491 , training loss: 4.207825\n",
      "[INFO] Epoch: 39 , batch: 492 , training loss: 4.187812\n",
      "[INFO] Epoch: 39 , batch: 493 , training loss: 4.359950\n",
      "[INFO] Epoch: 39 , batch: 494 , training loss: 4.260907\n",
      "[INFO] Epoch: 39 , batch: 495 , training loss: 4.422677\n",
      "[INFO] Epoch: 39 , batch: 496 , training loss: 4.300967\n",
      "[INFO] Epoch: 39 , batch: 497 , training loss: 4.350317\n",
      "[INFO] Epoch: 39 , batch: 498 , training loss: 4.319288\n",
      "[INFO] Epoch: 39 , batch: 499 , training loss: 4.387487\n",
      "[INFO] Epoch: 39 , batch: 500 , training loss: 4.529626\n",
      "[INFO] Epoch: 39 , batch: 501 , training loss: 4.871719\n",
      "[INFO] Epoch: 39 , batch: 502 , training loss: 4.885083\n",
      "[INFO] Epoch: 39 , batch: 503 , training loss: 4.528226\n",
      "[INFO] Epoch: 39 , batch: 504 , training loss: 4.679822\n",
      "[INFO] Epoch: 39 , batch: 505 , training loss: 4.640594\n",
      "[INFO] Epoch: 39 , batch: 506 , training loss: 4.627471\n",
      "[INFO] Epoch: 39 , batch: 507 , training loss: 4.657128\n",
      "[INFO] Epoch: 39 , batch: 508 , training loss: 4.584781\n",
      "[INFO] Epoch: 39 , batch: 509 , training loss: 4.396588\n",
      "[INFO] Epoch: 39 , batch: 510 , training loss: 4.492418\n",
      "[INFO] Epoch: 39 , batch: 511 , training loss: 4.414088\n",
      "[INFO] Epoch: 39 , batch: 512 , training loss: 4.502595\n",
      "[INFO] Epoch: 39 , batch: 513 , training loss: 4.766918\n",
      "[INFO] Epoch: 39 , batch: 514 , training loss: 4.393539\n",
      "[INFO] Epoch: 39 , batch: 515 , training loss: 4.637626\n",
      "[INFO] Epoch: 39 , batch: 516 , training loss: 4.447400\n",
      "[INFO] Epoch: 39 , batch: 517 , training loss: 4.417735\n",
      "[INFO] Epoch: 39 , batch: 518 , training loss: 4.377131\n",
      "[INFO] Epoch: 39 , batch: 519 , training loss: 4.222838\n",
      "[INFO] Epoch: 39 , batch: 520 , training loss: 4.459225\n",
      "[INFO] Epoch: 39 , batch: 521 , training loss: 4.440140\n",
      "[INFO] Epoch: 39 , batch: 522 , training loss: 4.517256\n",
      "[INFO] Epoch: 39 , batch: 523 , training loss: 4.420264\n",
      "[INFO] Epoch: 39 , batch: 524 , training loss: 4.725895\n",
      "[INFO] Epoch: 39 , batch: 525 , training loss: 4.596803\n",
      "[INFO] Epoch: 39 , batch: 526 , training loss: 4.392446\n",
      "[INFO] Epoch: 39 , batch: 527 , training loss: 4.418474\n",
      "[INFO] Epoch: 39 , batch: 528 , training loss: 4.438817\n",
      "[INFO] Epoch: 39 , batch: 529 , training loss: 4.427474\n",
      "[INFO] Epoch: 39 , batch: 530 , training loss: 4.287221\n",
      "[INFO] Epoch: 39 , batch: 531 , training loss: 4.416811\n",
      "[INFO] Epoch: 39 , batch: 532 , training loss: 4.324485\n",
      "[INFO] Epoch: 39 , batch: 533 , training loss: 4.444887\n",
      "[INFO] Epoch: 39 , batch: 534 , training loss: 4.457547\n",
      "[INFO] Epoch: 39 , batch: 535 , training loss: 4.455542\n",
      "[INFO] Epoch: 39 , batch: 536 , training loss: 4.304125\n",
      "[INFO] Epoch: 39 , batch: 537 , training loss: 4.278068\n",
      "[INFO] Epoch: 39 , batch: 538 , training loss: 4.368701\n",
      "[INFO] Epoch: 39 , batch: 539 , training loss: 4.472030\n",
      "[INFO] Epoch: 39 , batch: 540 , training loss: 5.007401\n",
      "[INFO] Epoch: 39 , batch: 541 , training loss: 4.804896\n",
      "[INFO] Epoch: 39 , batch: 542 , training loss: 4.700455\n",
      "[INFO] Epoch: 40 , batch: 0 , training loss: 3.715062\n",
      "[INFO] Epoch: 40 , batch: 1 , training loss: 3.572605\n",
      "[INFO] Epoch: 40 , batch: 2 , training loss: 3.712618\n",
      "[INFO] Epoch: 40 , batch: 3 , training loss: 3.657389\n",
      "[INFO] Epoch: 40 , batch: 4 , training loss: 3.966082\n",
      "[INFO] Epoch: 40 , batch: 5 , training loss: 3.656917\n",
      "[INFO] Epoch: 40 , batch: 6 , training loss: 3.980551\n",
      "[INFO] Epoch: 40 , batch: 7 , training loss: 3.921073\n",
      "[INFO] Epoch: 40 , batch: 8 , training loss: 3.603372\n",
      "[INFO] Epoch: 40 , batch: 9 , training loss: 3.880629\n",
      "[INFO] Epoch: 40 , batch: 10 , training loss: 3.811496\n",
      "[INFO] Epoch: 40 , batch: 11 , training loss: 3.756287\n",
      "[INFO] Epoch: 40 , batch: 12 , training loss: 3.648081\n",
      "[INFO] Epoch: 40 , batch: 13 , training loss: 3.659330\n",
      "[INFO] Epoch: 40 , batch: 14 , training loss: 3.540600\n",
      "[INFO] Epoch: 40 , batch: 15 , training loss: 3.758058\n",
      "[INFO] Epoch: 40 , batch: 16 , training loss: 3.627841\n",
      "[INFO] Epoch: 40 , batch: 17 , training loss: 3.779503\n",
      "[INFO] Epoch: 40 , batch: 18 , training loss: 3.703491\n",
      "[INFO] Epoch: 40 , batch: 19 , training loss: 3.492425\n",
      "[INFO] Epoch: 40 , batch: 20 , training loss: 3.431577\n",
      "[INFO] Epoch: 40 , batch: 21 , training loss: 3.568125\n",
      "[INFO] Epoch: 40 , batch: 22 , training loss: 3.485398\n",
      "[INFO] Epoch: 40 , batch: 23 , training loss: 3.660957\n",
      "[INFO] Epoch: 40 , batch: 24 , training loss: 3.520437\n",
      "[INFO] Epoch: 40 , batch: 25 , training loss: 3.628265\n",
      "[INFO] Epoch: 40 , batch: 26 , training loss: 3.487706\n",
      "[INFO] Epoch: 40 , batch: 27 , training loss: 3.471739\n",
      "[INFO] Epoch: 40 , batch: 28 , training loss: 3.685507\n",
      "[INFO] Epoch: 40 , batch: 29 , training loss: 3.478170\n",
      "[INFO] Epoch: 40 , batch: 30 , training loss: 3.513027\n",
      "[INFO] Epoch: 40 , batch: 31 , training loss: 3.614708\n",
      "[INFO] Epoch: 40 , batch: 32 , training loss: 3.599615\n",
      "[INFO] Epoch: 40 , batch: 33 , training loss: 3.646398\n",
      "[INFO] Epoch: 40 , batch: 34 , training loss: 3.585021\n",
      "[INFO] Epoch: 40 , batch: 35 , training loss: 3.573216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 40 , batch: 36 , training loss: 3.644316\n",
      "[INFO] Epoch: 40 , batch: 37 , training loss: 3.507462\n",
      "[INFO] Epoch: 40 , batch: 38 , training loss: 3.581134\n",
      "[INFO] Epoch: 40 , batch: 39 , training loss: 3.429216\n",
      "[INFO] Epoch: 40 , batch: 40 , training loss: 3.614465\n",
      "[INFO] Epoch: 40 , batch: 41 , training loss: 3.580711\n",
      "[INFO] Epoch: 40 , batch: 42 , training loss: 4.107711\n",
      "[INFO] Epoch: 40 , batch: 43 , training loss: 3.824532\n",
      "[INFO] Epoch: 40 , batch: 44 , training loss: 4.132149\n",
      "[INFO] Epoch: 40 , batch: 45 , training loss: 4.077847\n",
      "[INFO] Epoch: 40 , batch: 46 , training loss: 4.084975\n",
      "[INFO] Epoch: 40 , batch: 47 , training loss: 3.636250\n",
      "[INFO] Epoch: 40 , batch: 48 , training loss: 3.645258\n",
      "[INFO] Epoch: 40 , batch: 49 , training loss: 3.866989\n",
      "[INFO] Epoch: 40 , batch: 50 , training loss: 3.662930\n",
      "[INFO] Epoch: 40 , batch: 51 , training loss: 3.843980\n",
      "[INFO] Epoch: 40 , batch: 52 , training loss: 3.653132\n",
      "[INFO] Epoch: 40 , batch: 53 , training loss: 3.806917\n",
      "[INFO] Epoch: 40 , batch: 54 , training loss: 3.803904\n",
      "[INFO] Epoch: 40 , batch: 55 , training loss: 3.891498\n",
      "[INFO] Epoch: 40 , batch: 56 , training loss: 3.701738\n",
      "[INFO] Epoch: 40 , batch: 57 , training loss: 3.631431\n",
      "[INFO] Epoch: 40 , batch: 58 , training loss: 3.669566\n",
      "[INFO] Epoch: 40 , batch: 59 , training loss: 3.771581\n",
      "[INFO] Epoch: 40 , batch: 60 , training loss: 3.691389\n",
      "[INFO] Epoch: 40 , batch: 61 , training loss: 3.773035\n",
      "[INFO] Epoch: 40 , batch: 62 , training loss: 3.664291\n",
      "[INFO] Epoch: 40 , batch: 63 , training loss: 3.834641\n",
      "[INFO] Epoch: 40 , batch: 64 , training loss: 4.063869\n",
      "[INFO] Epoch: 40 , batch: 65 , training loss: 3.744350\n",
      "[INFO] Epoch: 40 , batch: 66 , training loss: 3.624602\n",
      "[INFO] Epoch: 40 , batch: 67 , training loss: 3.628875\n",
      "[INFO] Epoch: 40 , batch: 68 , training loss: 3.825609\n",
      "[INFO] Epoch: 40 , batch: 69 , training loss: 3.716114\n",
      "[INFO] Epoch: 40 , batch: 70 , training loss: 3.948052\n",
      "[INFO] Epoch: 40 , batch: 71 , training loss: 3.800860\n",
      "[INFO] Epoch: 40 , batch: 72 , training loss: 3.844149\n",
      "[INFO] Epoch: 40 , batch: 73 , training loss: 3.808013\n",
      "[INFO] Epoch: 40 , batch: 74 , training loss: 3.938883\n",
      "[INFO] Epoch: 40 , batch: 75 , training loss: 3.763335\n",
      "[INFO] Epoch: 40 , batch: 76 , training loss: 3.876698\n",
      "[INFO] Epoch: 40 , batch: 77 , training loss: 3.831908\n",
      "[INFO] Epoch: 40 , batch: 78 , training loss: 3.897074\n",
      "[INFO] Epoch: 40 , batch: 79 , training loss: 3.760248\n",
      "[INFO] Epoch: 40 , batch: 80 , training loss: 3.970426\n",
      "[INFO] Epoch: 40 , batch: 81 , training loss: 3.882199\n",
      "[INFO] Epoch: 40 , batch: 82 , training loss: 3.872136\n",
      "[INFO] Epoch: 40 , batch: 83 , training loss: 3.930336\n",
      "[INFO] Epoch: 40 , batch: 84 , training loss: 3.937279\n",
      "[INFO] Epoch: 40 , batch: 85 , training loss: 4.022509\n",
      "[INFO] Epoch: 40 , batch: 86 , training loss: 3.919685\n",
      "[INFO] Epoch: 40 , batch: 87 , training loss: 3.890264\n",
      "[INFO] Epoch: 40 , batch: 88 , training loss: 4.023017\n",
      "[INFO] Epoch: 40 , batch: 89 , training loss: 3.824935\n",
      "[INFO] Epoch: 40 , batch: 90 , training loss: 3.932083\n",
      "[INFO] Epoch: 40 , batch: 91 , training loss: 3.846609\n",
      "[INFO] Epoch: 40 , batch: 92 , training loss: 3.875514\n",
      "[INFO] Epoch: 40 , batch: 93 , training loss: 3.953573\n",
      "[INFO] Epoch: 40 , batch: 94 , training loss: 4.116533\n",
      "[INFO] Epoch: 40 , batch: 95 , training loss: 3.871317\n",
      "[INFO] Epoch: 40 , batch: 96 , training loss: 3.845722\n",
      "[INFO] Epoch: 40 , batch: 97 , training loss: 3.789772\n",
      "[INFO] Epoch: 40 , batch: 98 , training loss: 3.714929\n",
      "[INFO] Epoch: 40 , batch: 99 , training loss: 3.857914\n",
      "[INFO] Epoch: 40 , batch: 100 , training loss: 3.765673\n",
      "[INFO] Epoch: 40 , batch: 101 , training loss: 3.778994\n",
      "[INFO] Epoch: 40 , batch: 102 , training loss: 3.967859\n",
      "[INFO] Epoch: 40 , batch: 103 , training loss: 3.740763\n",
      "[INFO] Epoch: 40 , batch: 104 , training loss: 3.688906\n",
      "[INFO] Epoch: 40 , batch: 105 , training loss: 3.937901\n",
      "[INFO] Epoch: 40 , batch: 106 , training loss: 3.986477\n",
      "[INFO] Epoch: 40 , batch: 107 , training loss: 3.809991\n",
      "[INFO] Epoch: 40 , batch: 108 , training loss: 3.769213\n",
      "[INFO] Epoch: 40 , batch: 109 , training loss: 3.698766\n",
      "[INFO] Epoch: 40 , batch: 110 , training loss: 3.853831\n",
      "[INFO] Epoch: 40 , batch: 111 , training loss: 3.911801\n",
      "[INFO] Epoch: 40 , batch: 112 , training loss: 3.838115\n",
      "[INFO] Epoch: 40 , batch: 113 , training loss: 3.871123\n",
      "[INFO] Epoch: 40 , batch: 114 , training loss: 3.823060\n",
      "[INFO] Epoch: 40 , batch: 115 , training loss: 3.842891\n",
      "[INFO] Epoch: 40 , batch: 116 , training loss: 3.739220\n",
      "[INFO] Epoch: 40 , batch: 117 , training loss: 3.942743\n",
      "[INFO] Epoch: 40 , batch: 118 , training loss: 3.945324\n",
      "[INFO] Epoch: 40 , batch: 119 , training loss: 4.101988\n",
      "[INFO] Epoch: 40 , batch: 120 , training loss: 4.066900\n",
      "[INFO] Epoch: 40 , batch: 121 , training loss: 3.932291\n",
      "[INFO] Epoch: 40 , batch: 122 , training loss: 3.823249\n",
      "[INFO] Epoch: 40 , batch: 123 , training loss: 3.797103\n",
      "[INFO] Epoch: 40 , batch: 124 , training loss: 3.908111\n",
      "[INFO] Epoch: 40 , batch: 125 , training loss: 3.737753\n",
      "[INFO] Epoch: 40 , batch: 126 , training loss: 3.774869\n",
      "[INFO] Epoch: 40 , batch: 127 , training loss: 3.796086\n",
      "[INFO] Epoch: 40 , batch: 128 , training loss: 3.942046\n",
      "[INFO] Epoch: 40 , batch: 129 , training loss: 3.855533\n",
      "[INFO] Epoch: 40 , batch: 130 , training loss: 3.875953\n",
      "[INFO] Epoch: 40 , batch: 131 , training loss: 3.851233\n",
      "[INFO] Epoch: 40 , batch: 132 , training loss: 3.883840\n",
      "[INFO] Epoch: 40 , batch: 133 , training loss: 3.850324\n",
      "[INFO] Epoch: 40 , batch: 134 , training loss: 3.583973\n",
      "[INFO] Epoch: 40 , batch: 135 , training loss: 3.692744\n",
      "[INFO] Epoch: 40 , batch: 136 , training loss: 3.955540\n",
      "[INFO] Epoch: 40 , batch: 137 , training loss: 3.885700\n",
      "[INFO] Epoch: 40 , batch: 138 , training loss: 3.911876\n",
      "[INFO] Epoch: 40 , batch: 139 , training loss: 4.450976\n",
      "[INFO] Epoch: 40 , batch: 140 , training loss: 4.266451\n",
      "[INFO] Epoch: 40 , batch: 141 , training loss: 4.071674\n",
      "[INFO] Epoch: 40 , batch: 142 , training loss: 3.800689\n",
      "[INFO] Epoch: 40 , batch: 143 , training loss: 3.916825\n",
      "[INFO] Epoch: 40 , batch: 144 , training loss: 3.759155\n",
      "[INFO] Epoch: 40 , batch: 145 , training loss: 3.837833\n",
      "[INFO] Epoch: 40 , batch: 146 , training loss: 4.046862\n",
      "[INFO] Epoch: 40 , batch: 147 , training loss: 3.666379\n",
      "[INFO] Epoch: 40 , batch: 148 , training loss: 3.669138\n",
      "[INFO] Epoch: 40 , batch: 149 , training loss: 3.748440\n",
      "[INFO] Epoch: 40 , batch: 150 , training loss: 4.009554\n",
      "[INFO] Epoch: 40 , batch: 151 , training loss: 3.845596\n",
      "[INFO] Epoch: 40 , batch: 152 , training loss: 3.865548\n",
      "[INFO] Epoch: 40 , batch: 153 , training loss: 3.889359\n",
      "[INFO] Epoch: 40 , batch: 154 , training loss: 3.954784\n",
      "[INFO] Epoch: 40 , batch: 155 , training loss: 4.152760\n",
      "[INFO] Epoch: 40 , batch: 156 , training loss: 3.936033\n",
      "[INFO] Epoch: 40 , batch: 157 , training loss: 3.885099\n",
      "[INFO] Epoch: 40 , batch: 158 , training loss: 3.991130\n",
      "[INFO] Epoch: 40 , batch: 159 , training loss: 3.925779\n",
      "[INFO] Epoch: 40 , batch: 160 , training loss: 4.198003\n",
      "[INFO] Epoch: 40 , batch: 161 , training loss: 4.234509\n",
      "[INFO] Epoch: 40 , batch: 162 , training loss: 4.206164\n",
      "[INFO] Epoch: 40 , batch: 163 , training loss: 4.377288\n",
      "[INFO] Epoch: 40 , batch: 164 , training loss: 4.309991\n",
      "[INFO] Epoch: 40 , batch: 165 , training loss: 4.253952\n",
      "[INFO] Epoch: 40 , batch: 166 , training loss: 4.130472\n",
      "[INFO] Epoch: 40 , batch: 167 , training loss: 4.210032\n",
      "[INFO] Epoch: 40 , batch: 168 , training loss: 3.836232\n",
      "[INFO] Epoch: 40 , batch: 169 , training loss: 3.834215\n",
      "[INFO] Epoch: 40 , batch: 170 , training loss: 4.057539\n",
      "[INFO] Epoch: 40 , batch: 171 , training loss: 3.468447\n",
      "[INFO] Epoch: 40 , batch: 172 , training loss: 3.714478\n",
      "[INFO] Epoch: 40 , batch: 173 , training loss: 4.035402\n",
      "[INFO] Epoch: 40 , batch: 174 , training loss: 4.520127\n",
      "[INFO] Epoch: 40 , batch: 175 , training loss: 4.775468\n",
      "[INFO] Epoch: 40 , batch: 176 , training loss: 4.415363\n",
      "[INFO] Epoch: 40 , batch: 177 , training loss: 4.060939\n",
      "[INFO] Epoch: 40 , batch: 178 , training loss: 4.087674\n",
      "[INFO] Epoch: 40 , batch: 179 , training loss: 4.137900\n",
      "[INFO] Epoch: 40 , batch: 180 , training loss: 4.044952\n",
      "[INFO] Epoch: 40 , batch: 181 , training loss: 4.341105\n",
      "[INFO] Epoch: 40 , batch: 182 , training loss: 4.307097\n",
      "[INFO] Epoch: 40 , batch: 183 , training loss: 4.274466\n",
      "[INFO] Epoch: 40 , batch: 184 , training loss: 4.164620\n",
      "[INFO] Epoch: 40 , batch: 185 , training loss: 4.140925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 40 , batch: 186 , training loss: 4.305072\n",
      "[INFO] Epoch: 40 , batch: 187 , training loss: 4.372703\n",
      "[INFO] Epoch: 40 , batch: 188 , training loss: 4.398574\n",
      "[INFO] Epoch: 40 , batch: 189 , training loss: 4.292228\n",
      "[INFO] Epoch: 40 , batch: 190 , training loss: 4.316269\n",
      "[INFO] Epoch: 40 , batch: 191 , training loss: 4.420483\n",
      "[INFO] Epoch: 40 , batch: 192 , training loss: 4.258284\n",
      "[INFO] Epoch: 40 , batch: 193 , training loss: 4.342587\n",
      "[INFO] Epoch: 40 , batch: 194 , training loss: 4.303509\n",
      "[INFO] Epoch: 40 , batch: 195 , training loss: 4.235786\n",
      "[INFO] Epoch: 40 , batch: 196 , training loss: 4.067243\n",
      "[INFO] Epoch: 40 , batch: 197 , training loss: 4.172729\n",
      "[INFO] Epoch: 40 , batch: 198 , training loss: 4.094281\n",
      "[INFO] Epoch: 40 , batch: 199 , training loss: 4.217321\n",
      "[INFO] Epoch: 40 , batch: 200 , training loss: 4.127196\n",
      "[INFO] Epoch: 40 , batch: 201 , training loss: 4.022899\n",
      "[INFO] Epoch: 40 , batch: 202 , training loss: 4.026203\n",
      "[INFO] Epoch: 40 , batch: 203 , training loss: 4.170974\n",
      "[INFO] Epoch: 40 , batch: 204 , training loss: 4.228141\n",
      "[INFO] Epoch: 40 , batch: 205 , training loss: 3.864089\n",
      "[INFO] Epoch: 40 , batch: 206 , training loss: 3.777867\n",
      "[INFO] Epoch: 40 , batch: 207 , training loss: 3.764220\n",
      "[INFO] Epoch: 40 , batch: 208 , training loss: 4.098813\n",
      "[INFO] Epoch: 40 , batch: 209 , training loss: 4.080894\n",
      "[INFO] Epoch: 40 , batch: 210 , training loss: 4.075090\n",
      "[INFO] Epoch: 40 , batch: 211 , training loss: 4.083707\n",
      "[INFO] Epoch: 40 , batch: 212 , training loss: 4.189966\n",
      "[INFO] Epoch: 40 , batch: 213 , training loss: 4.126353\n",
      "[INFO] Epoch: 40 , batch: 214 , training loss: 4.201087\n",
      "[INFO] Epoch: 40 , batch: 215 , training loss: 4.378618\n",
      "[INFO] Epoch: 40 , batch: 216 , training loss: 4.109168\n",
      "[INFO] Epoch: 40 , batch: 217 , training loss: 4.056544\n",
      "[INFO] Epoch: 40 , batch: 218 , training loss: 4.050961\n",
      "[INFO] Epoch: 40 , batch: 219 , training loss: 4.174232\n",
      "[INFO] Epoch: 40 , batch: 220 , training loss: 3.965731\n",
      "[INFO] Epoch: 40 , batch: 221 , training loss: 3.989291\n",
      "[INFO] Epoch: 40 , batch: 222 , training loss: 4.124929\n",
      "[INFO] Epoch: 40 , batch: 223 , training loss: 4.226067\n",
      "[INFO] Epoch: 40 , batch: 224 , training loss: 4.270887\n",
      "[INFO] Epoch: 40 , batch: 225 , training loss: 4.162090\n",
      "[INFO] Epoch: 40 , batch: 226 , training loss: 4.310775\n",
      "[INFO] Epoch: 40 , batch: 227 , training loss: 4.265748\n",
      "[INFO] Epoch: 40 , batch: 228 , training loss: 4.265740\n",
      "[INFO] Epoch: 40 , batch: 229 , training loss: 4.146057\n",
      "[INFO] Epoch: 40 , batch: 230 , training loss: 4.009223\n",
      "[INFO] Epoch: 40 , batch: 231 , training loss: 3.868897\n",
      "[INFO] Epoch: 40 , batch: 232 , training loss: 4.012344\n",
      "[INFO] Epoch: 40 , batch: 233 , training loss: 4.045523\n",
      "[INFO] Epoch: 40 , batch: 234 , training loss: 3.735441\n",
      "[INFO] Epoch: 40 , batch: 235 , training loss: 3.850335\n",
      "[INFO] Epoch: 40 , batch: 236 , training loss: 3.946771\n",
      "[INFO] Epoch: 40 , batch: 237 , training loss: 4.174469\n",
      "[INFO] Epoch: 40 , batch: 238 , training loss: 3.955317\n",
      "[INFO] Epoch: 40 , batch: 239 , training loss: 3.991164\n",
      "[INFO] Epoch: 40 , batch: 240 , training loss: 4.008764\n",
      "[INFO] Epoch: 40 , batch: 241 , training loss: 3.829313\n",
      "[INFO] Epoch: 40 , batch: 242 , training loss: 3.852364\n",
      "[INFO] Epoch: 40 , batch: 243 , training loss: 4.133663\n",
      "[INFO] Epoch: 40 , batch: 244 , training loss: 4.081299\n",
      "[INFO] Epoch: 40 , batch: 245 , training loss: 4.062857\n",
      "[INFO] Epoch: 40 , batch: 246 , training loss: 3.755655\n",
      "[INFO] Epoch: 40 , batch: 247 , training loss: 3.927686\n",
      "[INFO] Epoch: 40 , batch: 248 , training loss: 3.996552\n",
      "[INFO] Epoch: 40 , batch: 249 , training loss: 3.965236\n",
      "[INFO] Epoch: 40 , batch: 250 , training loss: 3.784048\n",
      "[INFO] Epoch: 40 , batch: 251 , training loss: 4.221560\n",
      "[INFO] Epoch: 40 , batch: 252 , training loss: 3.935751\n",
      "[INFO] Epoch: 40 , batch: 253 , training loss: 3.841363\n",
      "[INFO] Epoch: 40 , batch: 254 , training loss: 4.098936\n",
      "[INFO] Epoch: 40 , batch: 255 , training loss: 4.106531\n",
      "[INFO] Epoch: 40 , batch: 256 , training loss: 4.066116\n",
      "[INFO] Epoch: 40 , batch: 257 , training loss: 4.237828\n",
      "[INFO] Epoch: 40 , batch: 258 , training loss: 4.257193\n",
      "[INFO] Epoch: 40 , batch: 259 , training loss: 4.287600\n",
      "[INFO] Epoch: 40 , batch: 260 , training loss: 4.069073\n",
      "[INFO] Epoch: 40 , batch: 261 , training loss: 4.261557\n",
      "[INFO] Epoch: 40 , batch: 262 , training loss: 4.379967\n",
      "[INFO] Epoch: 40 , batch: 263 , training loss: 4.547341\n",
      "[INFO] Epoch: 40 , batch: 264 , training loss: 3.916504\n",
      "[INFO] Epoch: 40 , batch: 265 , training loss: 4.023892\n",
      "[INFO] Epoch: 40 , batch: 266 , training loss: 4.416645\n",
      "[INFO] Epoch: 40 , batch: 267 , training loss: 4.151674\n",
      "[INFO] Epoch: 40 , batch: 268 , training loss: 4.076315\n",
      "[INFO] Epoch: 40 , batch: 269 , training loss: 4.058893\n",
      "[INFO] Epoch: 40 , batch: 270 , training loss: 4.101226\n",
      "[INFO] Epoch: 40 , batch: 271 , training loss: 4.122433\n",
      "[INFO] Epoch: 40 , batch: 272 , training loss: 4.121198\n",
      "[INFO] Epoch: 40 , batch: 273 , training loss: 4.139105\n",
      "[INFO] Epoch: 40 , batch: 274 , training loss: 4.213363\n",
      "[INFO] Epoch: 40 , batch: 275 , training loss: 4.072958\n",
      "[INFO] Epoch: 40 , batch: 276 , training loss: 4.145720\n",
      "[INFO] Epoch: 40 , batch: 277 , training loss: 4.315841\n",
      "[INFO] Epoch: 40 , batch: 278 , training loss: 3.992404\n",
      "[INFO] Epoch: 40 , batch: 279 , training loss: 3.994673\n",
      "[INFO] Epoch: 40 , batch: 280 , training loss: 3.991170\n",
      "[INFO] Epoch: 40 , batch: 281 , training loss: 4.098166\n",
      "[INFO] Epoch: 40 , batch: 282 , training loss: 3.999821\n",
      "[INFO] Epoch: 40 , batch: 283 , training loss: 4.015762\n",
      "[INFO] Epoch: 40 , batch: 284 , training loss: 4.033121\n",
      "[INFO] Epoch: 40 , batch: 285 , training loss: 3.974258\n",
      "[INFO] Epoch: 40 , batch: 286 , training loss: 3.998227\n",
      "[INFO] Epoch: 40 , batch: 287 , training loss: 3.947481\n",
      "[INFO] Epoch: 40 , batch: 288 , training loss: 3.896465\n",
      "[INFO] Epoch: 40 , batch: 289 , training loss: 3.955276\n",
      "[INFO] Epoch: 40 , batch: 290 , training loss: 3.749815\n",
      "[INFO] Epoch: 40 , batch: 291 , training loss: 3.753472\n",
      "[INFO] Epoch: 40 , batch: 292 , training loss: 3.828129\n",
      "[INFO] Epoch: 40 , batch: 293 , training loss: 3.776451\n",
      "[INFO] Epoch: 40 , batch: 294 , training loss: 4.422964\n",
      "[INFO] Epoch: 40 , batch: 295 , training loss: 4.230648\n",
      "[INFO] Epoch: 40 , batch: 296 , training loss: 4.144678\n",
      "[INFO] Epoch: 40 , batch: 297 , training loss: 4.083374\n",
      "[INFO] Epoch: 40 , batch: 298 , training loss: 3.918347\n",
      "[INFO] Epoch: 40 , batch: 299 , training loss: 3.974367\n",
      "[INFO] Epoch: 40 , batch: 300 , training loss: 3.978309\n",
      "[INFO] Epoch: 40 , batch: 301 , training loss: 3.881568\n",
      "[INFO] Epoch: 40 , batch: 302 , training loss: 4.071756\n",
      "[INFO] Epoch: 40 , batch: 303 , training loss: 4.066913\n",
      "[INFO] Epoch: 40 , batch: 304 , training loss: 4.208963\n",
      "[INFO] Epoch: 40 , batch: 305 , training loss: 4.038466\n",
      "[INFO] Epoch: 40 , batch: 306 , training loss: 4.157086\n",
      "[INFO] Epoch: 40 , batch: 307 , training loss: 4.170896\n",
      "[INFO] Epoch: 40 , batch: 308 , training loss: 3.974963\n",
      "[INFO] Epoch: 40 , batch: 309 , training loss: 3.968271\n",
      "[INFO] Epoch: 40 , batch: 310 , training loss: 3.904641\n",
      "[INFO] Epoch: 40 , batch: 311 , training loss: 3.917019\n",
      "[INFO] Epoch: 40 , batch: 312 , training loss: 3.801933\n",
      "[INFO] Epoch: 40 , batch: 313 , training loss: 3.920223\n",
      "[INFO] Epoch: 40 , batch: 314 , training loss: 3.977462\n",
      "[INFO] Epoch: 40 , batch: 315 , training loss: 4.058084\n",
      "[INFO] Epoch: 40 , batch: 316 , training loss: 4.280133\n",
      "[INFO] Epoch: 40 , batch: 317 , training loss: 4.635448\n",
      "[INFO] Epoch: 40 , batch: 318 , training loss: 4.768530\n",
      "[INFO] Epoch: 40 , batch: 319 , training loss: 4.460978\n",
      "[INFO] Epoch: 40 , batch: 320 , training loss: 4.016928\n",
      "[INFO] Epoch: 40 , batch: 321 , training loss: 3.836274\n",
      "[INFO] Epoch: 40 , batch: 322 , training loss: 3.920287\n",
      "[INFO] Epoch: 40 , batch: 323 , training loss: 3.974976\n",
      "[INFO] Epoch: 40 , batch: 324 , training loss: 3.948112\n",
      "[INFO] Epoch: 40 , batch: 325 , training loss: 4.065416\n",
      "[INFO] Epoch: 40 , batch: 326 , training loss: 4.138874\n",
      "[INFO] Epoch: 40 , batch: 327 , training loss: 4.058664\n",
      "[INFO] Epoch: 40 , batch: 328 , training loss: 4.047373\n",
      "[INFO] Epoch: 40 , batch: 329 , training loss: 3.960709\n",
      "[INFO] Epoch: 40 , batch: 330 , training loss: 3.946363\n",
      "[INFO] Epoch: 40 , batch: 331 , training loss: 4.106260\n",
      "[INFO] Epoch: 40 , batch: 332 , training loss: 3.963715\n",
      "[INFO] Epoch: 40 , batch: 333 , training loss: 3.935048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 40 , batch: 334 , training loss: 3.955671\n",
      "[INFO] Epoch: 40 , batch: 335 , training loss: 4.077829\n",
      "[INFO] Epoch: 40 , batch: 336 , training loss: 4.079340\n",
      "[INFO] Epoch: 40 , batch: 337 , training loss: 4.127500\n",
      "[INFO] Epoch: 40 , batch: 338 , training loss: 4.333267\n",
      "[INFO] Epoch: 40 , batch: 339 , training loss: 4.151210\n",
      "[INFO] Epoch: 40 , batch: 340 , training loss: 4.340765\n",
      "[INFO] Epoch: 40 , batch: 341 , training loss: 4.085073\n",
      "[INFO] Epoch: 40 , batch: 342 , training loss: 3.870212\n",
      "[INFO] Epoch: 40 , batch: 343 , training loss: 3.944657\n",
      "[INFO] Epoch: 40 , batch: 344 , training loss: 3.829668\n",
      "[INFO] Epoch: 40 , batch: 345 , training loss: 3.967127\n",
      "[INFO] Epoch: 40 , batch: 346 , training loss: 3.998647\n",
      "[INFO] Epoch: 40 , batch: 347 , training loss: 3.912192\n",
      "[INFO] Epoch: 40 , batch: 348 , training loss: 4.019652\n",
      "[INFO] Epoch: 40 , batch: 349 , training loss: 4.101336\n",
      "[INFO] Epoch: 40 , batch: 350 , training loss: 3.947180\n",
      "[INFO] Epoch: 40 , batch: 351 , training loss: 4.034224\n",
      "[INFO] Epoch: 40 , batch: 352 , training loss: 4.046804\n",
      "[INFO] Epoch: 40 , batch: 353 , training loss: 4.031013\n",
      "[INFO] Epoch: 40 , batch: 354 , training loss: 4.131661\n",
      "[INFO] Epoch: 40 , batch: 355 , training loss: 4.128542\n",
      "[INFO] Epoch: 40 , batch: 356 , training loss: 3.986080\n",
      "[INFO] Epoch: 40 , batch: 357 , training loss: 4.054959\n",
      "[INFO] Epoch: 40 , batch: 358 , training loss: 3.957380\n",
      "[INFO] Epoch: 40 , batch: 359 , training loss: 3.954641\n",
      "[INFO] Epoch: 40 , batch: 360 , training loss: 4.078562\n",
      "[INFO] Epoch: 40 , batch: 361 , training loss: 4.056074\n",
      "[INFO] Epoch: 40 , batch: 362 , training loss: 4.133572\n",
      "[INFO] Epoch: 40 , batch: 363 , training loss: 4.029852\n",
      "[INFO] Epoch: 40 , batch: 364 , training loss: 4.071432\n",
      "[INFO] Epoch: 40 , batch: 365 , training loss: 4.005247\n",
      "[INFO] Epoch: 40 , batch: 366 , training loss: 4.101807\n",
      "[INFO] Epoch: 40 , batch: 367 , training loss: 4.154988\n",
      "[INFO] Epoch: 40 , batch: 368 , training loss: 4.529047\n",
      "[INFO] Epoch: 40 , batch: 369 , training loss: 4.218499\n",
      "[INFO] Epoch: 40 , batch: 370 , training loss: 3.988547\n",
      "[INFO] Epoch: 40 , batch: 371 , training loss: 4.409565\n",
      "[INFO] Epoch: 40 , batch: 372 , training loss: 4.643241\n",
      "[INFO] Epoch: 40 , batch: 373 , training loss: 4.695279\n",
      "[INFO] Epoch: 40 , batch: 374 , training loss: 4.863097\n",
      "[INFO] Epoch: 40 , batch: 375 , training loss: 4.817042\n",
      "[INFO] Epoch: 40 , batch: 376 , training loss: 4.673168\n",
      "[INFO] Epoch: 40 , batch: 377 , training loss: 4.430915\n",
      "[INFO] Epoch: 40 , batch: 378 , training loss: 4.553611\n",
      "[INFO] Epoch: 40 , batch: 379 , training loss: 4.507460\n",
      "[INFO] Epoch: 40 , batch: 380 , training loss: 4.678914\n",
      "[INFO] Epoch: 40 , batch: 381 , training loss: 4.377000\n",
      "[INFO] Epoch: 40 , batch: 382 , training loss: 4.649498\n",
      "[INFO] Epoch: 40 , batch: 383 , training loss: 4.707065\n",
      "[INFO] Epoch: 40 , batch: 384 , training loss: 4.661033\n",
      "[INFO] Epoch: 40 , batch: 385 , training loss: 4.324970\n",
      "[INFO] Epoch: 40 , batch: 386 , training loss: 4.593629\n",
      "[INFO] Epoch: 40 , batch: 387 , training loss: 4.540414\n",
      "[INFO] Epoch: 40 , batch: 388 , training loss: 4.367548\n",
      "[INFO] Epoch: 40 , batch: 389 , training loss: 4.165781\n",
      "[INFO] Epoch: 40 , batch: 390 , training loss: 4.185999\n",
      "[INFO] Epoch: 40 , batch: 391 , training loss: 4.226132\n",
      "[INFO] Epoch: 40 , batch: 392 , training loss: 4.604525\n",
      "[INFO] Epoch: 40 , batch: 393 , training loss: 4.467556\n",
      "[INFO] Epoch: 40 , batch: 394 , training loss: 4.564699\n",
      "[INFO] Epoch: 40 , batch: 395 , training loss: 4.398861\n",
      "[INFO] Epoch: 40 , batch: 396 , training loss: 4.215891\n",
      "[INFO] Epoch: 40 , batch: 397 , training loss: 4.347796\n",
      "[INFO] Epoch: 40 , batch: 398 , training loss: 4.214697\n",
      "[INFO] Epoch: 40 , batch: 399 , training loss: 4.290034\n",
      "[INFO] Epoch: 40 , batch: 400 , training loss: 4.247355\n",
      "[INFO] Epoch: 40 , batch: 401 , training loss: 4.703738\n",
      "[INFO] Epoch: 40 , batch: 402 , training loss: 4.424248\n",
      "[INFO] Epoch: 40 , batch: 403 , training loss: 4.243885\n",
      "[INFO] Epoch: 40 , batch: 404 , training loss: 4.409981\n",
      "[INFO] Epoch: 40 , batch: 405 , training loss: 4.481905\n",
      "[INFO] Epoch: 40 , batch: 406 , training loss: 4.376053\n",
      "[INFO] Epoch: 40 , batch: 407 , training loss: 4.411753\n",
      "[INFO] Epoch: 40 , batch: 408 , training loss: 4.379492\n",
      "[INFO] Epoch: 40 , batch: 409 , training loss: 4.392499\n",
      "[INFO] Epoch: 40 , batch: 410 , training loss: 4.460217\n",
      "[INFO] Epoch: 40 , batch: 411 , training loss: 4.640788\n",
      "[INFO] Epoch: 40 , batch: 412 , training loss: 4.460889\n",
      "[INFO] Epoch: 40 , batch: 413 , training loss: 4.321162\n",
      "[INFO] Epoch: 40 , batch: 414 , training loss: 4.376343\n",
      "[INFO] Epoch: 40 , batch: 415 , training loss: 4.418910\n",
      "[INFO] Epoch: 40 , batch: 416 , training loss: 4.477567\n",
      "[INFO] Epoch: 40 , batch: 417 , training loss: 4.374705\n",
      "[INFO] Epoch: 40 , batch: 418 , training loss: 4.437035\n",
      "[INFO] Epoch: 40 , batch: 419 , training loss: 4.415260\n",
      "[INFO] Epoch: 40 , batch: 420 , training loss: 4.384646\n",
      "[INFO] Epoch: 40 , batch: 421 , training loss: 4.366152\n",
      "[INFO] Epoch: 40 , batch: 422 , training loss: 4.193704\n",
      "[INFO] Epoch: 40 , batch: 423 , training loss: 4.450046\n",
      "[INFO] Epoch: 40 , batch: 424 , training loss: 4.613328\n",
      "[INFO] Epoch: 40 , batch: 425 , training loss: 4.453464\n",
      "[INFO] Epoch: 40 , batch: 426 , training loss: 4.210615\n",
      "[INFO] Epoch: 40 , batch: 427 , training loss: 4.440124\n",
      "[INFO] Epoch: 40 , batch: 428 , training loss: 4.310279\n",
      "[INFO] Epoch: 40 , batch: 429 , training loss: 4.209318\n",
      "[INFO] Epoch: 40 , batch: 430 , training loss: 4.434760\n",
      "[INFO] Epoch: 40 , batch: 431 , training loss: 4.066257\n",
      "[INFO] Epoch: 40 , batch: 432 , training loss: 4.120650\n",
      "[INFO] Epoch: 40 , batch: 433 , training loss: 4.140946\n",
      "[INFO] Epoch: 40 , batch: 434 , training loss: 4.038040\n",
      "[INFO] Epoch: 40 , batch: 435 , training loss: 4.385867\n",
      "[INFO] Epoch: 40 , batch: 436 , training loss: 4.425985\n",
      "[INFO] Epoch: 40 , batch: 437 , training loss: 4.214337\n",
      "[INFO] Epoch: 40 , batch: 438 , training loss: 4.088170\n",
      "[INFO] Epoch: 40 , batch: 439 , training loss: 4.323922\n",
      "[INFO] Epoch: 40 , batch: 440 , training loss: 4.431327\n",
      "[INFO] Epoch: 40 , batch: 441 , training loss: 4.509436\n",
      "[INFO] Epoch: 40 , batch: 442 , training loss: 4.280053\n",
      "[INFO] Epoch: 40 , batch: 443 , training loss: 4.468327\n",
      "[INFO] Epoch: 40 , batch: 444 , training loss: 4.076180\n",
      "[INFO] Epoch: 40 , batch: 445 , training loss: 3.981256\n",
      "[INFO] Epoch: 40 , batch: 446 , training loss: 3.912198\n",
      "[INFO] Epoch: 40 , batch: 447 , training loss: 4.096679\n",
      "[INFO] Epoch: 40 , batch: 448 , training loss: 4.234527\n",
      "[INFO] Epoch: 40 , batch: 449 , training loss: 4.621082\n",
      "[INFO] Epoch: 40 , batch: 450 , training loss: 4.692891\n",
      "[INFO] Epoch: 40 , batch: 451 , training loss: 4.558521\n",
      "[INFO] Epoch: 40 , batch: 452 , training loss: 4.395647\n",
      "[INFO] Epoch: 40 , batch: 453 , training loss: 4.159494\n",
      "[INFO] Epoch: 40 , batch: 454 , training loss: 4.292757\n",
      "[INFO] Epoch: 40 , batch: 455 , training loss: 4.373005\n",
      "[INFO] Epoch: 40 , batch: 456 , training loss: 4.360996\n",
      "[INFO] Epoch: 40 , batch: 457 , training loss: 4.413858\n",
      "[INFO] Epoch: 40 , batch: 458 , training loss: 4.161486\n",
      "[INFO] Epoch: 40 , batch: 459 , training loss: 4.148617\n",
      "[INFO] Epoch: 40 , batch: 460 , training loss: 4.256416\n",
      "[INFO] Epoch: 40 , batch: 461 , training loss: 4.228061\n",
      "[INFO] Epoch: 40 , batch: 462 , training loss: 4.264139\n",
      "[INFO] Epoch: 40 , batch: 463 , training loss: 4.202457\n",
      "[INFO] Epoch: 40 , batch: 464 , training loss: 4.366358\n",
      "[INFO] Epoch: 40 , batch: 465 , training loss: 4.317411\n",
      "[INFO] Epoch: 40 , batch: 466 , training loss: 4.429426\n",
      "[INFO] Epoch: 40 , batch: 467 , training loss: 4.369039\n",
      "[INFO] Epoch: 40 , batch: 468 , training loss: 4.329335\n",
      "[INFO] Epoch: 40 , batch: 469 , training loss: 4.355949\n",
      "[INFO] Epoch: 40 , batch: 470 , training loss: 4.182938\n",
      "[INFO] Epoch: 40 , batch: 471 , training loss: 4.293210\n",
      "[INFO] Epoch: 40 , batch: 472 , training loss: 4.353720\n",
      "[INFO] Epoch: 40 , batch: 473 , training loss: 4.261678\n",
      "[INFO] Epoch: 40 , batch: 474 , training loss: 4.033699\n",
      "[INFO] Epoch: 40 , batch: 475 , training loss: 3.913624\n",
      "[INFO] Epoch: 40 , batch: 476 , training loss: 4.333041\n",
      "[INFO] Epoch: 40 , batch: 477 , training loss: 4.449065\n",
      "[INFO] Epoch: 40 , batch: 478 , training loss: 4.438562\n",
      "[INFO] Epoch: 40 , batch: 479 , training loss: 4.414287\n",
      "[INFO] Epoch: 40 , batch: 480 , training loss: 4.542025\n",
      "[INFO] Epoch: 40 , batch: 481 , training loss: 4.419153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 40 , batch: 482 , training loss: 4.532619\n",
      "[INFO] Epoch: 40 , batch: 483 , training loss: 4.372852\n",
      "[INFO] Epoch: 40 , batch: 484 , training loss: 4.170910\n",
      "[INFO] Epoch: 40 , batch: 485 , training loss: 4.275576\n",
      "[INFO] Epoch: 40 , batch: 486 , training loss: 4.171430\n",
      "[INFO] Epoch: 40 , batch: 487 , training loss: 4.144297\n",
      "[INFO] Epoch: 40 , batch: 488 , training loss: 4.333218\n",
      "[INFO] Epoch: 40 , batch: 489 , training loss: 4.249922\n",
      "[INFO] Epoch: 40 , batch: 490 , training loss: 4.307158\n",
      "[INFO] Epoch: 40 , batch: 491 , training loss: 4.215880\n",
      "[INFO] Epoch: 40 , batch: 492 , training loss: 4.200532\n",
      "[INFO] Epoch: 40 , batch: 493 , training loss: 4.354593\n",
      "[INFO] Epoch: 40 , batch: 494 , training loss: 4.265782\n",
      "[INFO] Epoch: 40 , batch: 495 , training loss: 4.427502\n",
      "[INFO] Epoch: 40 , batch: 496 , training loss: 4.310777\n",
      "[INFO] Epoch: 40 , batch: 497 , training loss: 4.331920\n",
      "[INFO] Epoch: 40 , batch: 498 , training loss: 4.321319\n",
      "[INFO] Epoch: 40 , batch: 499 , training loss: 4.398398\n",
      "[INFO] Epoch: 40 , batch: 500 , training loss: 4.534009\n",
      "[INFO] Epoch: 40 , batch: 501 , training loss: 4.872425\n",
      "[INFO] Epoch: 40 , batch: 502 , training loss: 4.878788\n",
      "[INFO] Epoch: 40 , batch: 503 , training loss: 4.524705\n",
      "[INFO] Epoch: 40 , batch: 504 , training loss: 4.685198\n",
      "[INFO] Epoch: 40 , batch: 505 , training loss: 4.635021\n",
      "[INFO] Epoch: 40 , batch: 506 , training loss: 4.629200\n",
      "[INFO] Epoch: 40 , batch: 507 , training loss: 4.666851\n",
      "[INFO] Epoch: 40 , batch: 508 , training loss: 4.592274\n",
      "[INFO] Epoch: 40 , batch: 509 , training loss: 4.388477\n",
      "[INFO] Epoch: 40 , batch: 510 , training loss: 4.496584\n",
      "[INFO] Epoch: 40 , batch: 511 , training loss: 4.414365\n",
      "[INFO] Epoch: 40 , batch: 512 , training loss: 4.500372\n",
      "[INFO] Epoch: 40 , batch: 513 , training loss: 4.765501\n",
      "[INFO] Epoch: 40 , batch: 514 , training loss: 4.392564\n",
      "[INFO] Epoch: 40 , batch: 515 , training loss: 4.667603\n",
      "[INFO] Epoch: 40 , batch: 516 , training loss: 4.454477\n",
      "[INFO] Epoch: 40 , batch: 517 , training loss: 4.407860\n",
      "[INFO] Epoch: 40 , batch: 518 , training loss: 4.380553\n",
      "[INFO] Epoch: 40 , batch: 519 , training loss: 4.236192\n",
      "[INFO] Epoch: 40 , batch: 520 , training loss: 4.461083\n",
      "[INFO] Epoch: 40 , batch: 521 , training loss: 4.455400\n",
      "[INFO] Epoch: 40 , batch: 522 , training loss: 4.538826\n",
      "[INFO] Epoch: 40 , batch: 523 , training loss: 4.441136\n",
      "[INFO] Epoch: 40 , batch: 524 , training loss: 4.735053\n",
      "[INFO] Epoch: 40 , batch: 525 , training loss: 4.610321\n",
      "[INFO] Epoch: 40 , batch: 526 , training loss: 4.388524\n",
      "[INFO] Epoch: 40 , batch: 527 , training loss: 4.443523\n",
      "[INFO] Epoch: 40 , batch: 528 , training loss: 4.441141\n",
      "[INFO] Epoch: 40 , batch: 529 , training loss: 4.427589\n",
      "[INFO] Epoch: 40 , batch: 530 , training loss: 4.288594\n",
      "[INFO] Epoch: 40 , batch: 531 , training loss: 4.426919\n",
      "[INFO] Epoch: 40 , batch: 532 , training loss: 4.333018\n",
      "[INFO] Epoch: 40 , batch: 533 , training loss: 4.471931\n",
      "[INFO] Epoch: 40 , batch: 534 , training loss: 4.478807\n",
      "[INFO] Epoch: 40 , batch: 535 , training loss: 4.454600\n",
      "[INFO] Epoch: 40 , batch: 536 , training loss: 4.317290\n",
      "[INFO] Epoch: 40 , batch: 537 , training loss: 4.278164\n",
      "[INFO] Epoch: 40 , batch: 538 , training loss: 4.383677\n",
      "[INFO] Epoch: 40 , batch: 539 , training loss: 4.490977\n",
      "[INFO] Epoch: 40 , batch: 540 , training loss: 5.039300\n",
      "[INFO] Epoch: 40 , batch: 541 , training loss: 4.851870\n",
      "[INFO] Epoch: 40 , batch: 542 , training loss: 4.726349\n",
      "[INFO] Epoch: 41 , batch: 0 , training loss: 3.804072\n",
      "[INFO] Epoch: 41 , batch: 1 , training loss: 3.585955\n",
      "[INFO] Epoch: 41 , batch: 2 , training loss: 3.753150\n",
      "[INFO] Epoch: 41 , batch: 3 , training loss: 3.670015\n",
      "[INFO] Epoch: 41 , batch: 4 , training loss: 3.968577\n",
      "[INFO] Epoch: 41 , batch: 5 , training loss: 3.646084\n",
      "[INFO] Epoch: 41 , batch: 6 , training loss: 4.014326\n",
      "[INFO] Epoch: 41 , batch: 7 , training loss: 3.920154\n",
      "[INFO] Epoch: 41 , batch: 8 , training loss: 3.635183\n",
      "[INFO] Epoch: 41 , batch: 9 , training loss: 3.869106\n",
      "[INFO] Epoch: 41 , batch: 10 , training loss: 3.854867\n",
      "[INFO] Epoch: 41 , batch: 11 , training loss: 3.790374\n",
      "[INFO] Epoch: 41 , batch: 12 , training loss: 3.634813\n",
      "[INFO] Epoch: 41 , batch: 13 , training loss: 3.684243\n",
      "[INFO] Epoch: 41 , batch: 14 , training loss: 3.572705\n",
      "[INFO] Epoch: 41 , batch: 15 , training loss: 3.771457\n",
      "[INFO] Epoch: 41 , batch: 16 , training loss: 3.621651\n",
      "[INFO] Epoch: 41 , batch: 17 , training loss: 3.774189\n",
      "[INFO] Epoch: 41 , batch: 18 , training loss: 3.737003\n",
      "[INFO] Epoch: 41 , batch: 19 , training loss: 3.488802\n",
      "[INFO] Epoch: 41 , batch: 20 , training loss: 3.442233\n",
      "[INFO] Epoch: 41 , batch: 21 , training loss: 3.602150\n",
      "[INFO] Epoch: 41 , batch: 22 , training loss: 3.518757\n",
      "[INFO] Epoch: 41 , batch: 23 , training loss: 3.701807\n",
      "[INFO] Epoch: 41 , batch: 24 , training loss: 3.521212\n",
      "[INFO] Epoch: 41 , batch: 25 , training loss: 3.638407\n",
      "[INFO] Epoch: 41 , batch: 26 , training loss: 3.510820\n",
      "[INFO] Epoch: 41 , batch: 27 , training loss: 3.462574\n",
      "[INFO] Epoch: 41 , batch: 28 , training loss: 3.711590\n",
      "[INFO] Epoch: 41 , batch: 29 , training loss: 3.475849\n",
      "[INFO] Epoch: 41 , batch: 30 , training loss: 3.519399\n",
      "[INFO] Epoch: 41 , batch: 31 , training loss: 3.620449\n",
      "[INFO] Epoch: 41 , batch: 32 , training loss: 3.588117\n",
      "[INFO] Epoch: 41 , batch: 33 , training loss: 3.610060\n",
      "[INFO] Epoch: 41 , batch: 34 , training loss: 3.599158\n",
      "[INFO] Epoch: 41 , batch: 35 , training loss: 3.563864\n",
      "[INFO] Epoch: 41 , batch: 36 , training loss: 3.652285\n",
      "[INFO] Epoch: 41 , batch: 37 , training loss: 3.512799\n",
      "[INFO] Epoch: 41 , batch: 38 , training loss: 3.618211\n",
      "[INFO] Epoch: 41 , batch: 39 , training loss: 3.413636\n",
      "[INFO] Epoch: 41 , batch: 40 , training loss: 3.597839\n",
      "[INFO] Epoch: 41 , batch: 41 , training loss: 3.568853\n",
      "[INFO] Epoch: 41 , batch: 42 , training loss: 4.035199\n",
      "[INFO] Epoch: 41 , batch: 43 , training loss: 3.823678\n",
      "[INFO] Epoch: 41 , batch: 44 , training loss: 4.183552\n",
      "[INFO] Epoch: 41 , batch: 45 , training loss: 4.046105\n",
      "[INFO] Epoch: 41 , batch: 46 , training loss: 4.052761\n",
      "[INFO] Epoch: 41 , batch: 47 , training loss: 3.599315\n",
      "[INFO] Epoch: 41 , batch: 48 , training loss: 3.640224\n",
      "[INFO] Epoch: 41 , batch: 49 , training loss: 3.846226\n",
      "[INFO] Epoch: 41 , batch: 50 , training loss: 3.619516\n",
      "[INFO] Epoch: 41 , batch: 51 , training loss: 3.865434\n",
      "[INFO] Epoch: 41 , batch: 52 , training loss: 3.683176\n",
      "[INFO] Epoch: 41 , batch: 53 , training loss: 3.793508\n",
      "[INFO] Epoch: 41 , batch: 54 , training loss: 3.803937\n",
      "[INFO] Epoch: 41 , batch: 55 , training loss: 3.868406\n",
      "[INFO] Epoch: 41 , batch: 56 , training loss: 3.682998\n",
      "[INFO] Epoch: 41 , batch: 57 , training loss: 3.631827\n",
      "[INFO] Epoch: 41 , batch: 58 , training loss: 3.666345\n",
      "[INFO] Epoch: 41 , batch: 59 , training loss: 3.772198\n",
      "[INFO] Epoch: 41 , batch: 60 , training loss: 3.675721\n",
      "[INFO] Epoch: 41 , batch: 61 , training loss: 3.755146\n",
      "[INFO] Epoch: 41 , batch: 62 , training loss: 3.644164\n",
      "[INFO] Epoch: 41 , batch: 63 , training loss: 3.824086\n",
      "[INFO] Epoch: 41 , batch: 64 , training loss: 4.056752\n",
      "[INFO] Epoch: 41 , batch: 65 , training loss: 3.738066\n",
      "[INFO] Epoch: 41 , batch: 66 , training loss: 3.611485\n",
      "[INFO] Epoch: 41 , batch: 67 , training loss: 3.608441\n",
      "[INFO] Epoch: 41 , batch: 68 , training loss: 3.781101\n",
      "[INFO] Epoch: 41 , batch: 69 , training loss: 3.709713\n",
      "[INFO] Epoch: 41 , batch: 70 , training loss: 3.949162\n",
      "[INFO] Epoch: 41 , batch: 71 , training loss: 3.789631\n",
      "[INFO] Epoch: 41 , batch: 72 , training loss: 3.859221\n",
      "[INFO] Epoch: 41 , batch: 73 , training loss: 3.800083\n",
      "[INFO] Epoch: 41 , batch: 74 , training loss: 3.916099\n",
      "[INFO] Epoch: 41 , batch: 75 , training loss: 3.768782\n",
      "[INFO] Epoch: 41 , batch: 76 , training loss: 3.873527\n",
      "[INFO] Epoch: 41 , batch: 77 , training loss: 3.810035\n",
      "[INFO] Epoch: 41 , batch: 78 , training loss: 3.889158\n",
      "[INFO] Epoch: 41 , batch: 79 , training loss: 3.742617\n",
      "[INFO] Epoch: 41 , batch: 80 , training loss: 3.979793\n",
      "[INFO] Epoch: 41 , batch: 81 , training loss: 3.873755\n",
      "[INFO] Epoch: 41 , batch: 82 , training loss: 3.842386\n",
      "[INFO] Epoch: 41 , batch: 83 , training loss: 3.903247\n",
      "[INFO] Epoch: 41 , batch: 84 , training loss: 3.905774\n",
      "[INFO] Epoch: 41 , batch: 85 , training loss: 4.015887\n",
      "[INFO] Epoch: 41 , batch: 86 , training loss: 3.932124\n",
      "[INFO] Epoch: 41 , batch: 87 , training loss: 3.883605\n",
      "[INFO] Epoch: 41 , batch: 88 , training loss: 4.025203\n",
      "[INFO] Epoch: 41 , batch: 89 , training loss: 3.818915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 41 , batch: 90 , training loss: 3.913788\n",
      "[INFO] Epoch: 41 , batch: 91 , training loss: 3.832295\n",
      "[INFO] Epoch: 41 , batch: 92 , training loss: 3.856183\n",
      "[INFO] Epoch: 41 , batch: 93 , training loss: 3.945772\n",
      "[INFO] Epoch: 41 , batch: 94 , training loss: 4.063206\n",
      "[INFO] Epoch: 41 , batch: 95 , training loss: 3.873293\n",
      "[INFO] Epoch: 41 , batch: 96 , training loss: 3.854133\n",
      "[INFO] Epoch: 41 , batch: 97 , training loss: 3.770893\n",
      "[INFO] Epoch: 41 , batch: 98 , training loss: 3.741205\n",
      "[INFO] Epoch: 41 , batch: 99 , training loss: 3.822652\n",
      "[INFO] Epoch: 41 , batch: 100 , training loss: 3.743883\n",
      "[INFO] Epoch: 41 , batch: 101 , training loss: 3.759873\n",
      "[INFO] Epoch: 41 , batch: 102 , training loss: 3.939426\n",
      "[INFO] Epoch: 41 , batch: 103 , training loss: 3.720029\n",
      "[INFO] Epoch: 41 , batch: 104 , training loss: 3.667220\n",
      "[INFO] Epoch: 41 , batch: 105 , training loss: 3.892759\n",
      "[INFO] Epoch: 41 , batch: 106 , training loss: 3.955668\n",
      "[INFO] Epoch: 41 , batch: 107 , training loss: 3.792554\n",
      "[INFO] Epoch: 41 , batch: 108 , training loss: 3.724594\n",
      "[INFO] Epoch: 41 , batch: 109 , training loss: 3.693278\n",
      "[INFO] Epoch: 41 , batch: 110 , training loss: 3.840903\n",
      "[INFO] Epoch: 41 , batch: 111 , training loss: 3.912275\n",
      "[INFO] Epoch: 41 , batch: 112 , training loss: 3.860736\n",
      "[INFO] Epoch: 41 , batch: 113 , training loss: 3.875257\n",
      "[INFO] Epoch: 41 , batch: 114 , training loss: 3.838324\n",
      "[INFO] Epoch: 41 , batch: 115 , training loss: 3.884419\n",
      "[INFO] Epoch: 41 , batch: 116 , training loss: 3.762011\n",
      "[INFO] Epoch: 41 , batch: 117 , training loss: 3.982814\n",
      "[INFO] Epoch: 41 , batch: 118 , training loss: 3.988510\n",
      "[INFO] Epoch: 41 , batch: 119 , training loss: 4.147387\n",
      "[INFO] Epoch: 41 , batch: 120 , training loss: 4.110089\n",
      "[INFO] Epoch: 41 , batch: 121 , training loss: 3.961429\n",
      "[INFO] Epoch: 41 , batch: 122 , training loss: 3.868233\n",
      "[INFO] Epoch: 41 , batch: 123 , training loss: 3.898451\n",
      "[INFO] Epoch: 41 , batch: 124 , training loss: 3.956959\n",
      "[INFO] Epoch: 41 , batch: 125 , training loss: 3.770387\n",
      "[INFO] Epoch: 41 , batch: 126 , training loss: 3.821671\n",
      "[INFO] Epoch: 41 , batch: 127 , training loss: 3.834100\n",
      "[INFO] Epoch: 41 , batch: 128 , training loss: 3.987745\n",
      "[INFO] Epoch: 41 , batch: 129 , training loss: 3.894135\n",
      "[INFO] Epoch: 41 , batch: 130 , training loss: 3.929022\n",
      "[INFO] Epoch: 41 , batch: 131 , training loss: 3.906168\n",
      "[INFO] Epoch: 41 , batch: 132 , training loss: 3.919697\n",
      "[INFO] Epoch: 41 , batch: 133 , training loss: 3.873423\n",
      "[INFO] Epoch: 41 , batch: 134 , training loss: 3.666202\n",
      "[INFO] Epoch: 41 , batch: 135 , training loss: 3.727619\n",
      "[INFO] Epoch: 41 , batch: 136 , training loss: 3.986290\n",
      "[INFO] Epoch: 41 , batch: 137 , training loss: 3.927419\n",
      "[INFO] Epoch: 41 , batch: 138 , training loss: 3.992561\n",
      "[INFO] Epoch: 41 , batch: 139 , training loss: 4.540658\n",
      "[INFO] Epoch: 41 , batch: 140 , training loss: 4.375732\n",
      "[INFO] Epoch: 41 , batch: 141 , training loss: 4.135127\n",
      "[INFO] Epoch: 41 , batch: 142 , training loss: 3.818656\n",
      "[INFO] Epoch: 41 , batch: 143 , training loss: 3.961642\n",
      "[INFO] Epoch: 41 , batch: 144 , training loss: 3.802655\n",
      "[INFO] Epoch: 41 , batch: 145 , training loss: 3.877256\n",
      "[INFO] Epoch: 41 , batch: 146 , training loss: 4.096600\n",
      "[INFO] Epoch: 41 , batch: 147 , training loss: 3.703532\n",
      "[INFO] Epoch: 41 , batch: 148 , training loss: 3.695848\n",
      "[INFO] Epoch: 41 , batch: 149 , training loss: 3.776548\n",
      "[INFO] Epoch: 41 , batch: 150 , training loss: 4.056181\n",
      "[INFO] Epoch: 41 , batch: 151 , training loss: 3.871452\n",
      "[INFO] Epoch: 41 , batch: 152 , training loss: 3.871249\n",
      "[INFO] Epoch: 41 , batch: 153 , training loss: 3.950151\n",
      "[INFO] Epoch: 41 , batch: 154 , training loss: 4.012193\n",
      "[INFO] Epoch: 41 , batch: 155 , training loss: 4.279123\n",
      "[INFO] Epoch: 41 , batch: 156 , training loss: 3.975328\n",
      "[INFO] Epoch: 41 , batch: 157 , training loss: 3.916121\n",
      "[INFO] Epoch: 41 , batch: 158 , training loss: 4.047208\n",
      "[INFO] Epoch: 41 , batch: 159 , training loss: 4.013118\n",
      "[INFO] Epoch: 41 , batch: 160 , training loss: 4.243397\n",
      "[INFO] Epoch: 41 , batch: 161 , training loss: 4.254776\n",
      "[INFO] Epoch: 41 , batch: 162 , training loss: 4.231780\n",
      "[INFO] Epoch: 41 , batch: 163 , training loss: 4.418378\n",
      "[INFO] Epoch: 41 , batch: 164 , training loss: 4.360376\n",
      "[INFO] Epoch: 41 , batch: 165 , training loss: 4.288927\n",
      "[INFO] Epoch: 41 , batch: 166 , training loss: 4.173539\n",
      "[INFO] Epoch: 41 , batch: 167 , training loss: 4.276058\n",
      "[INFO] Epoch: 41 , batch: 168 , training loss: 3.916564\n",
      "[INFO] Epoch: 41 , batch: 169 , training loss: 3.918665\n",
      "[INFO] Epoch: 41 , batch: 170 , training loss: 4.101528\n",
      "[INFO] Epoch: 41 , batch: 171 , training loss: 3.550141\n",
      "[INFO] Epoch: 41 , batch: 172 , training loss: 3.753972\n",
      "[INFO] Epoch: 41 , batch: 173 , training loss: 4.077286\n",
      "[INFO] Epoch: 41 , batch: 174 , training loss: 4.568062\n",
      "[INFO] Epoch: 41 , batch: 175 , training loss: 4.790599\n",
      "[INFO] Epoch: 41 , batch: 176 , training loss: 4.479275\n",
      "[INFO] Epoch: 41 , batch: 177 , training loss: 4.094897\n",
      "[INFO] Epoch: 41 , batch: 178 , training loss: 4.081201\n",
      "[INFO] Epoch: 41 , batch: 179 , training loss: 4.124315\n",
      "[INFO] Epoch: 41 , batch: 180 , training loss: 4.073974\n",
      "[INFO] Epoch: 41 , batch: 181 , training loss: 4.361665\n",
      "[INFO] Epoch: 41 , batch: 182 , training loss: 4.310571\n",
      "[INFO] Epoch: 41 , batch: 183 , training loss: 4.278396\n",
      "[INFO] Epoch: 41 , batch: 184 , training loss: 4.174971\n",
      "[INFO] Epoch: 41 , batch: 185 , training loss: 4.117290\n",
      "[INFO] Epoch: 41 , batch: 186 , training loss: 4.281456\n",
      "[INFO] Epoch: 41 , batch: 187 , training loss: 4.370385\n",
      "[INFO] Epoch: 41 , batch: 188 , training loss: 4.390908\n",
      "[INFO] Epoch: 41 , batch: 189 , training loss: 4.290646\n",
      "[INFO] Epoch: 41 , batch: 190 , training loss: 4.320128\n",
      "[INFO] Epoch: 41 , batch: 191 , training loss: 4.415641\n",
      "[INFO] Epoch: 41 , batch: 192 , training loss: 4.248474\n",
      "[INFO] Epoch: 41 , batch: 193 , training loss: 4.365706\n",
      "[INFO] Epoch: 41 , batch: 194 , training loss: 4.311494\n",
      "[INFO] Epoch: 41 , batch: 195 , training loss: 4.251494\n",
      "[INFO] Epoch: 41 , batch: 196 , training loss: 4.093410\n",
      "[INFO] Epoch: 41 , batch: 197 , training loss: 4.173842\n",
      "[INFO] Epoch: 41 , batch: 198 , training loss: 4.096646\n",
      "[INFO] Epoch: 41 , batch: 199 , training loss: 4.222859\n",
      "[INFO] Epoch: 41 , batch: 200 , training loss: 4.151247\n",
      "[INFO] Epoch: 41 , batch: 201 , training loss: 4.035158\n",
      "[INFO] Epoch: 41 , batch: 202 , training loss: 4.024253\n",
      "[INFO] Epoch: 41 , batch: 203 , training loss: 4.166900\n",
      "[INFO] Epoch: 41 , batch: 204 , training loss: 4.255320\n",
      "[INFO] Epoch: 41 , batch: 205 , training loss: 3.842469\n",
      "[INFO] Epoch: 41 , batch: 206 , training loss: 3.781613\n",
      "[INFO] Epoch: 41 , batch: 207 , training loss: 3.780081\n",
      "[INFO] Epoch: 41 , batch: 208 , training loss: 4.091369\n",
      "[INFO] Epoch: 41 , batch: 209 , training loss: 4.081478\n",
      "[INFO] Epoch: 41 , batch: 210 , training loss: 4.091697\n",
      "[INFO] Epoch: 41 , batch: 211 , training loss: 4.086454\n",
      "[INFO] Epoch: 41 , batch: 212 , training loss: 4.181486\n",
      "[INFO] Epoch: 41 , batch: 213 , training loss: 4.150450\n",
      "[INFO] Epoch: 41 , batch: 214 , training loss: 4.213289\n",
      "[INFO] Epoch: 41 , batch: 215 , training loss: 4.362928\n",
      "[INFO] Epoch: 41 , batch: 216 , training loss: 4.118989\n",
      "[INFO] Epoch: 41 , batch: 217 , training loss: 4.050017\n",
      "[INFO] Epoch: 41 , batch: 218 , training loss: 4.059706\n",
      "[INFO] Epoch: 41 , batch: 219 , training loss: 4.150855\n",
      "[INFO] Epoch: 41 , batch: 220 , training loss: 3.996928\n",
      "[INFO] Epoch: 41 , batch: 221 , training loss: 3.986909\n",
      "[INFO] Epoch: 41 , batch: 222 , training loss: 4.120810\n",
      "[INFO] Epoch: 41 , batch: 223 , training loss: 4.256513\n",
      "[INFO] Epoch: 41 , batch: 224 , training loss: 4.279255\n",
      "[INFO] Epoch: 41 , batch: 225 , training loss: 4.159956\n",
      "[INFO] Epoch: 41 , batch: 226 , training loss: 4.298254\n",
      "[INFO] Epoch: 41 , batch: 227 , training loss: 4.261773\n",
      "[INFO] Epoch: 41 , batch: 228 , training loss: 4.264303\n",
      "[INFO] Epoch: 41 , batch: 229 , training loss: 4.174320\n",
      "[INFO] Epoch: 41 , batch: 230 , training loss: 4.025850\n",
      "[INFO] Epoch: 41 , batch: 231 , training loss: 3.870005\n",
      "[INFO] Epoch: 41 , batch: 232 , training loss: 4.009469\n",
      "[INFO] Epoch: 41 , batch: 233 , training loss: 4.048263\n",
      "[INFO] Epoch: 41 , batch: 234 , training loss: 3.725806\n",
      "[INFO] Epoch: 41 , batch: 235 , training loss: 3.848361\n",
      "[INFO] Epoch: 41 , batch: 236 , training loss: 3.954040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 41 , batch: 237 , training loss: 4.167872\n",
      "[INFO] Epoch: 41 , batch: 238 , training loss: 3.958684\n",
      "[INFO] Epoch: 41 , batch: 239 , training loss: 3.992081\n",
      "[INFO] Epoch: 41 , batch: 240 , training loss: 4.029525\n",
      "[INFO] Epoch: 41 , batch: 241 , training loss: 3.843428\n",
      "[INFO] Epoch: 41 , batch: 242 , training loss: 3.855335\n",
      "[INFO] Epoch: 41 , batch: 243 , training loss: 4.138950\n",
      "[INFO] Epoch: 41 , batch: 244 , training loss: 4.063415\n",
      "[INFO] Epoch: 41 , batch: 245 , training loss: 4.053849\n",
      "[INFO] Epoch: 41 , batch: 246 , training loss: 3.752563\n",
      "[INFO] Epoch: 41 , batch: 247 , training loss: 3.939966\n",
      "[INFO] Epoch: 41 , batch: 248 , training loss: 4.002109\n",
      "[INFO] Epoch: 41 , batch: 249 , training loss: 3.981691\n",
      "[INFO] Epoch: 41 , batch: 250 , training loss: 3.777805\n",
      "[INFO] Epoch: 41 , batch: 251 , training loss: 4.224478\n",
      "[INFO] Epoch: 41 , batch: 252 , training loss: 3.945656\n",
      "[INFO] Epoch: 41 , batch: 253 , training loss: 3.855256\n",
      "[INFO] Epoch: 41 , batch: 254 , training loss: 4.117136\n",
      "[INFO] Epoch: 41 , batch: 255 , training loss: 4.092914\n",
      "[INFO] Epoch: 41 , batch: 256 , training loss: 4.069138\n",
      "[INFO] Epoch: 41 , batch: 257 , training loss: 4.253959\n",
      "[INFO] Epoch: 41 , batch: 258 , training loss: 4.243634\n",
      "[INFO] Epoch: 41 , batch: 259 , training loss: 4.294455\n",
      "[INFO] Epoch: 41 , batch: 260 , training loss: 4.076023\n",
      "[INFO] Epoch: 41 , batch: 261 , training loss: 4.266530\n",
      "[INFO] Epoch: 41 , batch: 262 , training loss: 4.372822\n",
      "[INFO] Epoch: 41 , batch: 263 , training loss: 4.553833\n",
      "[INFO] Epoch: 41 , batch: 264 , training loss: 3.908560\n",
      "[INFO] Epoch: 41 , batch: 265 , training loss: 4.021978\n",
      "[INFO] Epoch: 41 , batch: 266 , training loss: 4.432196\n",
      "[INFO] Epoch: 41 , batch: 267 , training loss: 4.176988\n",
      "[INFO] Epoch: 41 , batch: 268 , training loss: 4.094600\n",
      "[INFO] Epoch: 41 , batch: 269 , training loss: 4.073041\n",
      "[INFO] Epoch: 41 , batch: 270 , training loss: 4.095273\n",
      "[INFO] Epoch: 41 , batch: 271 , training loss: 4.125145\n",
      "[INFO] Epoch: 41 , batch: 272 , training loss: 4.112740\n",
      "[INFO] Epoch: 41 , batch: 273 , training loss: 4.127651\n",
      "[INFO] Epoch: 41 , batch: 274 , training loss: 4.222644\n",
      "[INFO] Epoch: 41 , batch: 275 , training loss: 4.084264\n",
      "[INFO] Epoch: 41 , batch: 276 , training loss: 4.144725\n",
      "[INFO] Epoch: 41 , batch: 277 , training loss: 4.310564\n",
      "[INFO] Epoch: 41 , batch: 278 , training loss: 3.993658\n",
      "[INFO] Epoch: 41 , batch: 279 , training loss: 4.001836\n",
      "[INFO] Epoch: 41 , batch: 280 , training loss: 3.990955\n",
      "[INFO] Epoch: 41 , batch: 281 , training loss: 4.099860\n",
      "[INFO] Epoch: 41 , batch: 282 , training loss: 4.017778\n",
      "[INFO] Epoch: 41 , batch: 283 , training loss: 4.007824\n",
      "[INFO] Epoch: 41 , batch: 284 , training loss: 4.050093\n",
      "[INFO] Epoch: 41 , batch: 285 , training loss: 3.978574\n",
      "[INFO] Epoch: 41 , batch: 286 , training loss: 4.001291\n",
      "[INFO] Epoch: 41 , batch: 287 , training loss: 3.937727\n",
      "[INFO] Epoch: 41 , batch: 288 , training loss: 3.909971\n",
      "[INFO] Epoch: 41 , batch: 289 , training loss: 3.977408\n",
      "[INFO] Epoch: 41 , batch: 290 , training loss: 3.749727\n",
      "[INFO] Epoch: 41 , batch: 291 , training loss: 3.744750\n",
      "[INFO] Epoch: 41 , batch: 292 , training loss: 3.831910\n",
      "[INFO] Epoch: 41 , batch: 293 , training loss: 3.779243\n",
      "[INFO] Epoch: 41 , batch: 294 , training loss: 4.432717\n",
      "[INFO] Epoch: 41 , batch: 295 , training loss: 4.203265\n",
      "[INFO] Epoch: 41 , batch: 296 , training loss: 4.136823\n",
      "[INFO] Epoch: 41 , batch: 297 , training loss: 4.086237\n",
      "[INFO] Epoch: 41 , batch: 298 , training loss: 3.937726\n",
      "[INFO] Epoch: 41 , batch: 299 , training loss: 3.976441\n",
      "[INFO] Epoch: 41 , batch: 300 , training loss: 3.977609\n",
      "[INFO] Epoch: 41 , batch: 301 , training loss: 3.878283\n",
      "[INFO] Epoch: 41 , batch: 302 , training loss: 4.077626\n",
      "[INFO] Epoch: 41 , batch: 303 , training loss: 4.077459\n",
      "[INFO] Epoch: 41 , batch: 304 , training loss: 4.208783\n",
      "[INFO] Epoch: 41 , batch: 305 , training loss: 4.052492\n",
      "[INFO] Epoch: 41 , batch: 306 , training loss: 4.150704\n",
      "[INFO] Epoch: 41 , batch: 307 , training loss: 4.172419\n",
      "[INFO] Epoch: 41 , batch: 308 , training loss: 3.956586\n",
      "[INFO] Epoch: 41 , batch: 309 , training loss: 3.970922\n",
      "[INFO] Epoch: 41 , batch: 310 , training loss: 3.919518\n",
      "[INFO] Epoch: 41 , batch: 311 , training loss: 3.914608\n",
      "[INFO] Epoch: 41 , batch: 312 , training loss: 3.804627\n",
      "[INFO] Epoch: 41 , batch: 313 , training loss: 3.911694\n",
      "[INFO] Epoch: 41 , batch: 314 , training loss: 3.962658\n",
      "[INFO] Epoch: 41 , batch: 315 , training loss: 4.063979\n",
      "[INFO] Epoch: 41 , batch: 316 , training loss: 4.298379\n",
      "[INFO] Epoch: 41 , batch: 317 , training loss: 4.640904\n",
      "[INFO] Epoch: 41 , batch: 318 , training loss: 4.782142\n",
      "[INFO] Epoch: 41 , batch: 319 , training loss: 4.474558\n",
      "[INFO] Epoch: 41 , batch: 320 , training loss: 4.027069\n",
      "[INFO] Epoch: 41 , batch: 321 , training loss: 3.827518\n",
      "[INFO] Epoch: 41 , batch: 322 , training loss: 3.924929\n",
      "[INFO] Epoch: 41 , batch: 323 , training loss: 3.979559\n",
      "[INFO] Epoch: 41 , batch: 324 , training loss: 3.950463\n",
      "[INFO] Epoch: 41 , batch: 325 , training loss: 4.045177\n",
      "[INFO] Epoch: 41 , batch: 326 , training loss: 4.133677\n",
      "[INFO] Epoch: 41 , batch: 327 , training loss: 4.056329\n",
      "[INFO] Epoch: 41 , batch: 328 , training loss: 4.075366\n",
      "[INFO] Epoch: 41 , batch: 329 , training loss: 3.962927\n",
      "[INFO] Epoch: 41 , batch: 330 , training loss: 3.952485\n",
      "[INFO] Epoch: 41 , batch: 331 , training loss: 4.122638\n",
      "[INFO] Epoch: 41 , batch: 332 , training loss: 3.966233\n",
      "[INFO] Epoch: 41 , batch: 333 , training loss: 3.926860\n",
      "[INFO] Epoch: 41 , batch: 334 , training loss: 3.952954\n",
      "[INFO] Epoch: 41 , batch: 335 , training loss: 4.073952\n",
      "[INFO] Epoch: 41 , batch: 336 , training loss: 4.091627\n",
      "[INFO] Epoch: 41 , batch: 337 , training loss: 4.111754\n",
      "[INFO] Epoch: 41 , batch: 338 , training loss: 4.334593\n",
      "[INFO] Epoch: 41 , batch: 339 , training loss: 4.155406\n",
      "[INFO] Epoch: 41 , batch: 340 , training loss: 4.334957\n",
      "[INFO] Epoch: 41 , batch: 341 , training loss: 4.086396\n",
      "[INFO] Epoch: 41 , batch: 342 , training loss: 3.888517\n",
      "[INFO] Epoch: 41 , batch: 343 , training loss: 3.961072\n",
      "[INFO] Epoch: 41 , batch: 344 , training loss: 3.837390\n",
      "[INFO] Epoch: 41 , batch: 345 , training loss: 3.949908\n",
      "[INFO] Epoch: 41 , batch: 346 , training loss: 3.998872\n",
      "[INFO] Epoch: 41 , batch: 347 , training loss: 3.912715\n",
      "[INFO] Epoch: 41 , batch: 348 , training loss: 4.022243\n",
      "[INFO] Epoch: 41 , batch: 349 , training loss: 4.100925\n",
      "[INFO] Epoch: 41 , batch: 350 , training loss: 3.958664\n",
      "[INFO] Epoch: 41 , batch: 351 , training loss: 4.042184\n",
      "[INFO] Epoch: 41 , batch: 352 , training loss: 4.051852\n",
      "[INFO] Epoch: 41 , batch: 353 , training loss: 4.040469\n",
      "[INFO] Epoch: 41 , batch: 354 , training loss: 4.127724\n",
      "[INFO] Epoch: 41 , batch: 355 , training loss: 4.122942\n",
      "[INFO] Epoch: 41 , batch: 356 , training loss: 3.981313\n",
      "[INFO] Epoch: 41 , batch: 357 , training loss: 4.060266\n",
      "[INFO] Epoch: 41 , batch: 358 , training loss: 3.967494\n",
      "[INFO] Epoch: 41 , batch: 359 , training loss: 3.973114\n",
      "[INFO] Epoch: 41 , batch: 360 , training loss: 4.090536\n",
      "[INFO] Epoch: 41 , batch: 361 , training loss: 4.048890\n",
      "[INFO] Epoch: 41 , batch: 362 , training loss: 4.132232\n",
      "[INFO] Epoch: 41 , batch: 363 , training loss: 4.031098\n",
      "[INFO] Epoch: 41 , batch: 364 , training loss: 4.080616\n",
      "[INFO] Epoch: 41 , batch: 365 , training loss: 4.017374\n",
      "[INFO] Epoch: 41 , batch: 366 , training loss: 4.116397\n",
      "[INFO] Epoch: 41 , batch: 367 , training loss: 4.162907\n",
      "[INFO] Epoch: 41 , batch: 368 , training loss: 4.559947\n",
      "[INFO] Epoch: 41 , batch: 369 , training loss: 4.234061\n",
      "[INFO] Epoch: 41 , batch: 370 , training loss: 3.997563\n",
      "[INFO] Epoch: 41 , batch: 371 , training loss: 4.432302\n",
      "[INFO] Epoch: 41 , batch: 372 , training loss: 4.693668\n",
      "[INFO] Epoch: 41 , batch: 373 , training loss: 4.695742\n",
      "[INFO] Epoch: 41 , batch: 374 , training loss: 4.837022\n",
      "[INFO] Epoch: 41 , batch: 375 , training loss: 4.811739\n",
      "[INFO] Epoch: 41 , batch: 376 , training loss: 4.698100\n",
      "[INFO] Epoch: 41 , batch: 377 , training loss: 4.447252\n",
      "[INFO] Epoch: 41 , batch: 378 , training loss: 4.567463\n",
      "[INFO] Epoch: 41 , batch: 379 , training loss: 4.510792\n",
      "[INFO] Epoch: 41 , batch: 380 , training loss: 4.692297\n",
      "[INFO] Epoch: 41 , batch: 381 , training loss: 4.398249\n",
      "[INFO] Epoch: 41 , batch: 382 , training loss: 4.650822\n",
      "[INFO] Epoch: 41 , batch: 383 , training loss: 4.706240\n",
      "[INFO] Epoch: 41 , batch: 384 , training loss: 4.671420\n",
      "[INFO] Epoch: 41 , batch: 385 , training loss: 4.347546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 41 , batch: 386 , training loss: 4.613366\n",
      "[INFO] Epoch: 41 , batch: 387 , training loss: 4.547829\n",
      "[INFO] Epoch: 41 , batch: 388 , training loss: 4.367192\n",
      "[INFO] Epoch: 41 , batch: 389 , training loss: 4.182867\n",
      "[INFO] Epoch: 41 , batch: 390 , training loss: 4.209369\n",
      "[INFO] Epoch: 41 , batch: 391 , training loss: 4.235665\n",
      "[INFO] Epoch: 41 , batch: 392 , training loss: 4.625567\n",
      "[INFO] Epoch: 41 , batch: 393 , training loss: 4.476885\n",
      "[INFO] Epoch: 41 , batch: 394 , training loss: 4.579690\n",
      "[INFO] Epoch: 41 , batch: 395 , training loss: 4.416328\n",
      "[INFO] Epoch: 41 , batch: 396 , training loss: 4.235492\n",
      "[INFO] Epoch: 41 , batch: 397 , training loss: 4.366096\n",
      "[INFO] Epoch: 41 , batch: 398 , training loss: 4.220299\n",
      "[INFO] Epoch: 41 , batch: 399 , training loss: 4.308951\n",
      "[INFO] Epoch: 41 , batch: 400 , training loss: 4.260149\n",
      "[INFO] Epoch: 41 , batch: 401 , training loss: 4.712337\n",
      "[INFO] Epoch: 41 , batch: 402 , training loss: 4.436053\n",
      "[INFO] Epoch: 41 , batch: 403 , training loss: 4.258879\n",
      "[INFO] Epoch: 41 , batch: 404 , training loss: 4.418623\n",
      "[INFO] Epoch: 41 , batch: 405 , training loss: 4.489065\n",
      "[INFO] Epoch: 41 , batch: 406 , training loss: 4.389301\n",
      "[INFO] Epoch: 41 , batch: 407 , training loss: 4.436203\n",
      "[INFO] Epoch: 41 , batch: 408 , training loss: 4.385149\n",
      "[INFO] Epoch: 41 , batch: 409 , training loss: 4.403386\n",
      "[INFO] Epoch: 41 , batch: 410 , training loss: 4.464840\n",
      "[INFO] Epoch: 41 , batch: 411 , training loss: 4.626509\n",
      "[INFO] Epoch: 41 , batch: 412 , training loss: 4.455932\n",
      "[INFO] Epoch: 41 , batch: 413 , training loss: 4.331422\n",
      "[INFO] Epoch: 41 , batch: 414 , training loss: 4.374519\n",
      "[INFO] Epoch: 41 , batch: 415 , training loss: 4.429174\n",
      "[INFO] Epoch: 41 , batch: 416 , training loss: 4.479584\n",
      "[INFO] Epoch: 41 , batch: 417 , training loss: 4.388600\n",
      "[INFO] Epoch: 41 , batch: 418 , training loss: 4.449245\n",
      "[INFO] Epoch: 41 , batch: 419 , training loss: 4.423612\n",
      "[INFO] Epoch: 41 , batch: 420 , training loss: 4.392445\n",
      "[INFO] Epoch: 41 , batch: 421 , training loss: 4.359525\n",
      "[INFO] Epoch: 41 , batch: 422 , training loss: 4.199964\n",
      "[INFO] Epoch: 41 , batch: 423 , training loss: 4.441678\n",
      "[INFO] Epoch: 41 , batch: 424 , training loss: 4.613575\n",
      "[INFO] Epoch: 41 , batch: 425 , training loss: 4.476817\n",
      "[INFO] Epoch: 41 , batch: 426 , training loss: 4.224442\n",
      "[INFO] Epoch: 41 , batch: 427 , training loss: 4.433289\n",
      "[INFO] Epoch: 41 , batch: 428 , training loss: 4.332330\n",
      "[INFO] Epoch: 41 , batch: 429 , training loss: 4.219622\n",
      "[INFO] Epoch: 41 , batch: 430 , training loss: 4.436604\n",
      "[INFO] Epoch: 41 , batch: 431 , training loss: 4.075027\n",
      "[INFO] Epoch: 41 , batch: 432 , training loss: 4.127392\n",
      "[INFO] Epoch: 41 , batch: 433 , training loss: 4.150132\n",
      "[INFO] Epoch: 41 , batch: 434 , training loss: 4.038166\n",
      "[INFO] Epoch: 41 , batch: 435 , training loss: 4.385305\n",
      "[INFO] Epoch: 41 , batch: 436 , training loss: 4.422208\n",
      "[INFO] Epoch: 41 , batch: 437 , training loss: 4.224849\n",
      "[INFO] Epoch: 41 , batch: 438 , training loss: 4.073647\n",
      "[INFO] Epoch: 41 , batch: 439 , training loss: 4.334410\n",
      "[INFO] Epoch: 41 , batch: 440 , training loss: 4.433364\n",
      "[INFO] Epoch: 41 , batch: 441 , training loss: 4.509316\n",
      "[INFO] Epoch: 41 , batch: 442 , training loss: 4.294425\n",
      "[INFO] Epoch: 41 , batch: 443 , training loss: 4.467497\n",
      "[INFO] Epoch: 41 , batch: 444 , training loss: 4.070628\n",
      "[INFO] Epoch: 41 , batch: 445 , training loss: 3.995282\n",
      "[INFO] Epoch: 41 , batch: 446 , training loss: 3.907036\n",
      "[INFO] Epoch: 41 , batch: 447 , training loss: 4.106575\n",
      "[INFO] Epoch: 41 , batch: 448 , training loss: 4.239640\n",
      "[INFO] Epoch: 41 , batch: 449 , training loss: 4.610015\n",
      "[INFO] Epoch: 41 , batch: 450 , training loss: 4.685202\n",
      "[INFO] Epoch: 41 , batch: 451 , training loss: 4.567830\n",
      "[INFO] Epoch: 41 , batch: 452 , training loss: 4.396456\n",
      "[INFO] Epoch: 41 , batch: 453 , training loss: 4.162344\n",
      "[INFO] Epoch: 41 , batch: 454 , training loss: 4.293324\n",
      "[INFO] Epoch: 41 , batch: 455 , training loss: 4.357831\n",
      "[INFO] Epoch: 41 , batch: 456 , training loss: 4.343515\n",
      "[INFO] Epoch: 41 , batch: 457 , training loss: 4.420121\n",
      "[INFO] Epoch: 41 , batch: 458 , training loss: 4.166126\n",
      "[INFO] Epoch: 41 , batch: 459 , training loss: 4.152346\n",
      "[INFO] Epoch: 41 , batch: 460 , training loss: 4.257386\n",
      "[INFO] Epoch: 41 , batch: 461 , training loss: 4.224633\n",
      "[INFO] Epoch: 41 , batch: 462 , training loss: 4.275312\n",
      "[INFO] Epoch: 41 , batch: 463 , training loss: 4.188808\n",
      "[INFO] Epoch: 41 , batch: 464 , training loss: 4.369908\n",
      "[INFO] Epoch: 41 , batch: 465 , training loss: 4.317255\n",
      "[INFO] Epoch: 41 , batch: 466 , training loss: 4.422168\n",
      "[INFO] Epoch: 41 , batch: 467 , training loss: 4.367765\n",
      "[INFO] Epoch: 41 , batch: 468 , training loss: 4.337355\n",
      "[INFO] Epoch: 41 , batch: 469 , training loss: 4.372728\n",
      "[INFO] Epoch: 41 , batch: 470 , training loss: 4.185267\n",
      "[INFO] Epoch: 41 , batch: 471 , training loss: 4.285210\n",
      "[INFO] Epoch: 41 , batch: 472 , training loss: 4.339299\n",
      "[INFO] Epoch: 41 , batch: 473 , training loss: 4.253150\n",
      "[INFO] Epoch: 41 , batch: 474 , training loss: 4.037834\n",
      "[INFO] Epoch: 41 , batch: 475 , training loss: 3.921259\n",
      "[INFO] Epoch: 41 , batch: 476 , training loss: 4.332516\n",
      "[INFO] Epoch: 41 , batch: 477 , training loss: 4.437327\n",
      "[INFO] Epoch: 41 , batch: 478 , training loss: 4.443255\n",
      "[INFO] Epoch: 41 , batch: 479 , training loss: 4.417798\n",
      "[INFO] Epoch: 41 , batch: 480 , training loss: 4.543837\n",
      "[INFO] Epoch: 41 , batch: 481 , training loss: 4.423834\n",
      "[INFO] Epoch: 41 , batch: 482 , training loss: 4.530045\n",
      "[INFO] Epoch: 41 , batch: 483 , training loss: 4.372013\n",
      "[INFO] Epoch: 41 , batch: 484 , training loss: 4.182131\n",
      "[INFO] Epoch: 41 , batch: 485 , training loss: 4.262074\n",
      "[INFO] Epoch: 41 , batch: 486 , training loss: 4.169878\n",
      "[INFO] Epoch: 41 , batch: 487 , training loss: 4.146544\n",
      "[INFO] Epoch: 41 , batch: 488 , training loss: 4.325875\n",
      "[INFO] Epoch: 41 , batch: 489 , training loss: 4.239687\n",
      "[INFO] Epoch: 41 , batch: 490 , training loss: 4.299921\n",
      "[INFO] Epoch: 41 , batch: 491 , training loss: 4.216407\n",
      "[INFO] Epoch: 41 , batch: 492 , training loss: 4.194844\n",
      "[INFO] Epoch: 41 , batch: 493 , training loss: 4.348372\n",
      "[INFO] Epoch: 41 , batch: 494 , training loss: 4.264513\n",
      "[INFO] Epoch: 41 , batch: 495 , training loss: 4.443100\n",
      "[INFO] Epoch: 41 , batch: 496 , training loss: 4.299413\n",
      "[INFO] Epoch: 41 , batch: 497 , training loss: 4.324808\n",
      "[INFO] Epoch: 41 , batch: 498 , training loss: 4.319407\n",
      "[INFO] Epoch: 41 , batch: 499 , training loss: 4.379735\n",
      "[INFO] Epoch: 41 , batch: 500 , training loss: 4.518625\n",
      "[INFO] Epoch: 41 , batch: 501 , training loss: 4.822457\n",
      "[INFO] Epoch: 41 , batch: 502 , training loss: 4.867436\n",
      "[INFO] Epoch: 41 , batch: 503 , training loss: 4.512868\n",
      "[INFO] Epoch: 41 , batch: 504 , training loss: 4.677184\n",
      "[INFO] Epoch: 41 , batch: 505 , training loss: 4.631287\n",
      "[INFO] Epoch: 41 , batch: 506 , training loss: 4.632297\n",
      "[INFO] Epoch: 41 , batch: 507 , training loss: 4.644073\n",
      "[INFO] Epoch: 41 , batch: 508 , training loss: 4.573527\n",
      "[INFO] Epoch: 41 , batch: 509 , training loss: 4.388578\n",
      "[INFO] Epoch: 41 , batch: 510 , training loss: 4.493531\n",
      "[INFO] Epoch: 41 , batch: 511 , training loss: 4.419850\n",
      "[INFO] Epoch: 41 , batch: 512 , training loss: 4.499833\n",
      "[INFO] Epoch: 41 , batch: 513 , training loss: 4.758166\n",
      "[INFO] Epoch: 41 , batch: 514 , training loss: 4.379756\n",
      "[INFO] Epoch: 41 , batch: 515 , training loss: 4.648023\n",
      "[INFO] Epoch: 41 , batch: 516 , training loss: 4.451002\n",
      "[INFO] Epoch: 41 , batch: 517 , training loss: 4.417587\n",
      "[INFO] Epoch: 41 , batch: 518 , training loss: 4.369530\n",
      "[INFO] Epoch: 41 , batch: 519 , training loss: 4.221391\n",
      "[INFO] Epoch: 41 , batch: 520 , training loss: 4.462450\n",
      "[INFO] Epoch: 41 , batch: 521 , training loss: 4.436946\n",
      "[INFO] Epoch: 41 , batch: 522 , training loss: 4.519357\n",
      "[INFO] Epoch: 41 , batch: 523 , training loss: 4.427423\n",
      "[INFO] Epoch: 41 , batch: 524 , training loss: 4.725594\n",
      "[INFO] Epoch: 41 , batch: 525 , training loss: 4.597408\n",
      "[INFO] Epoch: 41 , batch: 526 , training loss: 4.376163\n",
      "[INFO] Epoch: 41 , batch: 527 , training loss: 4.425661\n",
      "[INFO] Epoch: 41 , batch: 528 , training loss: 4.420993\n",
      "[INFO] Epoch: 41 , batch: 529 , training loss: 4.408069\n",
      "[INFO] Epoch: 41 , batch: 530 , training loss: 4.280670\n",
      "[INFO] Epoch: 41 , batch: 531 , training loss: 4.414721\n",
      "[INFO] Epoch: 41 , batch: 532 , training loss: 4.332162\n",
      "[INFO] Epoch: 41 , batch: 533 , training loss: 4.451625\n",
      "[INFO] Epoch: 41 , batch: 534 , training loss: 4.458035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 41 , batch: 535 , training loss: 4.437058\n",
      "[INFO] Epoch: 41 , batch: 536 , training loss: 4.300030\n",
      "[INFO] Epoch: 41 , batch: 537 , training loss: 4.268120\n",
      "[INFO] Epoch: 41 , batch: 538 , training loss: 4.368634\n",
      "[INFO] Epoch: 41 , batch: 539 , training loss: 4.471135\n",
      "[INFO] Epoch: 41 , batch: 540 , training loss: 5.044400\n",
      "[INFO] Epoch: 41 , batch: 541 , training loss: 4.837068\n",
      "[INFO] Epoch: 41 , batch: 542 , training loss: 4.695778\n",
      "[INFO] Epoch: 42 , batch: 0 , training loss: 3.664309\n",
      "[INFO] Epoch: 42 , batch: 1 , training loss: 3.500419\n",
      "[INFO] Epoch: 42 , batch: 2 , training loss: 3.732588\n",
      "[INFO] Epoch: 42 , batch: 3 , training loss: 3.619916\n",
      "[INFO] Epoch: 42 , batch: 4 , training loss: 3.946817\n",
      "[INFO] Epoch: 42 , batch: 5 , training loss: 3.609532\n",
      "[INFO] Epoch: 42 , batch: 6 , training loss: 3.956455\n",
      "[INFO] Epoch: 42 , batch: 7 , training loss: 3.870358\n",
      "[INFO] Epoch: 42 , batch: 8 , training loss: 3.578564\n",
      "[INFO] Epoch: 42 , batch: 9 , training loss: 3.833469\n",
      "[INFO] Epoch: 42 , batch: 10 , training loss: 3.800562\n",
      "[INFO] Epoch: 42 , batch: 11 , training loss: 3.765367\n",
      "[INFO] Epoch: 42 , batch: 12 , training loss: 3.617246\n",
      "[INFO] Epoch: 42 , batch: 13 , training loss: 3.638352\n",
      "[INFO] Epoch: 42 , batch: 14 , training loss: 3.548159\n",
      "[INFO] Epoch: 42 , batch: 15 , training loss: 3.758122\n",
      "[INFO] Epoch: 42 , batch: 16 , training loss: 3.596802\n",
      "[INFO] Epoch: 42 , batch: 17 , training loss: 3.758501\n",
      "[INFO] Epoch: 42 , batch: 18 , training loss: 3.689441\n",
      "[INFO] Epoch: 42 , batch: 19 , training loss: 3.461606\n",
      "[INFO] Epoch: 42 , batch: 20 , training loss: 3.411829\n",
      "[INFO] Epoch: 42 , batch: 21 , training loss: 3.555581\n",
      "[INFO] Epoch: 42 , batch: 22 , training loss: 3.479257\n",
      "[INFO] Epoch: 42 , batch: 23 , training loss: 3.648668\n",
      "[INFO] Epoch: 42 , batch: 24 , training loss: 3.481103\n",
      "[INFO] Epoch: 42 , batch: 25 , training loss: 3.609459\n",
      "[INFO] Epoch: 42 , batch: 26 , training loss: 3.495857\n",
      "[INFO] Epoch: 42 , batch: 27 , training loss: 3.456564\n",
      "[INFO] Epoch: 42 , batch: 28 , training loss: 3.645617\n",
      "[INFO] Epoch: 42 , batch: 29 , training loss: 3.468426\n",
      "[INFO] Epoch: 42 , batch: 30 , training loss: 3.483807\n",
      "[INFO] Epoch: 42 , batch: 31 , training loss: 3.579326\n",
      "[INFO] Epoch: 42 , batch: 32 , training loss: 3.544955\n",
      "[INFO] Epoch: 42 , batch: 33 , training loss: 3.573264\n",
      "[INFO] Epoch: 42 , batch: 34 , training loss: 3.585286\n",
      "[INFO] Epoch: 42 , batch: 35 , training loss: 3.525684\n",
      "[INFO] Epoch: 42 , batch: 36 , training loss: 3.624764\n",
      "[INFO] Epoch: 42 , batch: 37 , training loss: 3.451782\n",
      "[INFO] Epoch: 42 , batch: 38 , training loss: 3.544409\n",
      "[INFO] Epoch: 42 , batch: 39 , training loss: 3.405981\n",
      "[INFO] Epoch: 42 , batch: 40 , training loss: 3.554428\n",
      "[INFO] Epoch: 42 , batch: 41 , training loss: 3.498839\n",
      "[INFO] Epoch: 42 , batch: 42 , training loss: 3.999417\n",
      "[INFO] Epoch: 42 , batch: 43 , training loss: 3.728358\n",
      "[INFO] Epoch: 42 , batch: 44 , training loss: 4.124423\n",
      "[INFO] Epoch: 42 , batch: 45 , training loss: 4.018752\n",
      "[INFO] Epoch: 42 , batch: 46 , training loss: 4.004537\n",
      "[INFO] Epoch: 42 , batch: 47 , training loss: 3.565749\n",
      "[INFO] Epoch: 42 , batch: 48 , training loss: 3.578379\n",
      "[INFO] Epoch: 42 , batch: 49 , training loss: 3.804923\n",
      "[INFO] Epoch: 42 , batch: 50 , training loss: 3.557042\n",
      "[INFO] Epoch: 42 , batch: 51 , training loss: 3.796538\n",
      "[INFO] Epoch: 42 , batch: 52 , training loss: 3.662028\n",
      "[INFO] Epoch: 42 , batch: 53 , training loss: 3.735992\n",
      "[INFO] Epoch: 42 , batch: 54 , training loss: 3.787766\n",
      "[INFO] Epoch: 42 , batch: 55 , training loss: 3.839516\n",
      "[INFO] Epoch: 42 , batch: 56 , training loss: 3.662729\n",
      "[INFO] Epoch: 42 , batch: 57 , training loss: 3.606684\n",
      "[INFO] Epoch: 42 , batch: 58 , training loss: 3.641718\n",
      "[INFO] Epoch: 42 , batch: 59 , training loss: 3.750086\n",
      "[INFO] Epoch: 42 , batch: 60 , training loss: 3.649536\n",
      "[INFO] Epoch: 42 , batch: 61 , training loss: 3.752366\n",
      "[INFO] Epoch: 42 , batch: 62 , training loss: 3.627830\n",
      "[INFO] Epoch: 42 , batch: 63 , training loss: 3.807355\n",
      "[INFO] Epoch: 42 , batch: 64 , training loss: 4.025452\n",
      "[INFO] Epoch: 42 , batch: 65 , training loss: 3.728044\n",
      "[INFO] Epoch: 42 , batch: 66 , training loss: 3.579509\n",
      "[INFO] Epoch: 42 , batch: 67 , training loss: 3.598749\n",
      "[INFO] Epoch: 42 , batch: 68 , training loss: 3.775160\n",
      "[INFO] Epoch: 42 , batch: 69 , training loss: 3.682110\n",
      "[INFO] Epoch: 42 , batch: 70 , training loss: 3.932657\n",
      "[INFO] Epoch: 42 , batch: 71 , training loss: 3.763466\n",
      "[INFO] Epoch: 42 , batch: 72 , training loss: 3.831928\n",
      "[INFO] Epoch: 42 , batch: 73 , training loss: 3.763695\n",
      "[INFO] Epoch: 42 , batch: 74 , training loss: 3.869668\n",
      "[INFO] Epoch: 42 , batch: 75 , training loss: 3.721465\n",
      "[INFO] Epoch: 42 , batch: 76 , training loss: 3.874161\n",
      "[INFO] Epoch: 42 , batch: 77 , training loss: 3.788682\n",
      "[INFO] Epoch: 42 , batch: 78 , training loss: 3.877069\n",
      "[INFO] Epoch: 42 , batch: 79 , training loss: 3.737325\n",
      "[INFO] Epoch: 42 , batch: 80 , training loss: 3.934102\n",
      "[INFO] Epoch: 42 , batch: 81 , training loss: 3.859034\n",
      "[INFO] Epoch: 42 , batch: 82 , training loss: 3.821110\n",
      "[INFO] Epoch: 42 , batch: 83 , training loss: 3.888798\n",
      "[INFO] Epoch: 42 , batch: 84 , training loss: 3.906061\n",
      "[INFO] Epoch: 42 , batch: 85 , training loss: 4.007581\n",
      "[INFO] Epoch: 42 , batch: 86 , training loss: 3.936637\n",
      "[INFO] Epoch: 42 , batch: 87 , training loss: 3.864703\n",
      "[INFO] Epoch: 42 , batch: 88 , training loss: 4.013559\n",
      "[INFO] Epoch: 42 , batch: 89 , training loss: 3.810369\n",
      "[INFO] Epoch: 42 , batch: 90 , training loss: 3.913543\n",
      "[INFO] Epoch: 42 , batch: 91 , training loss: 3.827704\n",
      "[INFO] Epoch: 42 , batch: 92 , training loss: 3.852910\n",
      "[INFO] Epoch: 42 , batch: 93 , training loss: 3.942157\n",
      "[INFO] Epoch: 42 , batch: 94 , training loss: 4.074513\n",
      "[INFO] Epoch: 42 , batch: 95 , training loss: 3.872015\n",
      "[INFO] Epoch: 42 , batch: 96 , training loss: 3.836439\n",
      "[INFO] Epoch: 42 , batch: 97 , training loss: 3.778199\n",
      "[INFO] Epoch: 42 , batch: 98 , training loss: 3.708832\n",
      "[INFO] Epoch: 42 , batch: 99 , training loss: 3.840096\n",
      "[INFO] Epoch: 42 , batch: 100 , training loss: 3.762985\n",
      "[INFO] Epoch: 42 , batch: 101 , training loss: 3.794566\n",
      "[INFO] Epoch: 42 , batch: 102 , training loss: 3.938137\n",
      "[INFO] Epoch: 42 , batch: 103 , training loss: 3.712854\n",
      "[INFO] Epoch: 42 , batch: 104 , training loss: 3.663045\n",
      "[INFO] Epoch: 42 , batch: 105 , training loss: 3.921078\n",
      "[INFO] Epoch: 42 , batch: 106 , training loss: 3.945040\n",
      "[INFO] Epoch: 42 , batch: 107 , training loss: 3.807173\n",
      "[INFO] Epoch: 42 , batch: 108 , training loss: 3.749147\n",
      "[INFO] Epoch: 42 , batch: 109 , training loss: 3.668332\n",
      "[INFO] Epoch: 42 , batch: 110 , training loss: 3.830080\n",
      "[INFO] Epoch: 42 , batch: 111 , training loss: 3.902898\n",
      "[INFO] Epoch: 42 , batch: 112 , training loss: 3.825724\n",
      "[INFO] Epoch: 42 , batch: 113 , training loss: 3.822213\n",
      "[INFO] Epoch: 42 , batch: 114 , training loss: 3.791431\n",
      "[INFO] Epoch: 42 , batch: 115 , training loss: 3.834837\n",
      "[INFO] Epoch: 42 , batch: 116 , training loss: 3.744655\n",
      "[INFO] Epoch: 42 , batch: 117 , training loss: 3.946307\n",
      "[INFO] Epoch: 42 , batch: 118 , training loss: 3.920246\n",
      "[INFO] Epoch: 42 , batch: 119 , training loss: 4.091065\n",
      "[INFO] Epoch: 42 , batch: 120 , training loss: 4.034543\n",
      "[INFO] Epoch: 42 , batch: 121 , training loss: 3.886721\n",
      "[INFO] Epoch: 42 , batch: 122 , training loss: 3.802034\n",
      "[INFO] Epoch: 42 , batch: 123 , training loss: 3.806988\n",
      "[INFO] Epoch: 42 , batch: 124 , training loss: 3.869935\n",
      "[INFO] Epoch: 42 , batch: 125 , training loss: 3.712360\n",
      "[INFO] Epoch: 42 , batch: 126 , training loss: 3.744433\n",
      "[INFO] Epoch: 42 , batch: 127 , training loss: 3.770313\n",
      "[INFO] Epoch: 42 , batch: 128 , training loss: 3.890205\n",
      "[INFO] Epoch: 42 , batch: 129 , training loss: 3.841021\n",
      "[INFO] Epoch: 42 , batch: 130 , training loss: 3.859885\n",
      "[INFO] Epoch: 42 , batch: 131 , training loss: 3.845694\n",
      "[INFO] Epoch: 42 , batch: 132 , training loss: 3.867636\n",
      "[INFO] Epoch: 42 , batch: 133 , training loss: 3.803583\n",
      "[INFO] Epoch: 42 , batch: 134 , training loss: 3.588536\n",
      "[INFO] Epoch: 42 , batch: 135 , training loss: 3.654605\n",
      "[INFO] Epoch: 42 , batch: 136 , training loss: 3.929758\n",
      "[INFO] Epoch: 42 , batch: 137 , training loss: 3.858588\n",
      "[INFO] Epoch: 42 , batch: 138 , training loss: 3.903936\n",
      "[INFO] Epoch: 42 , batch: 139 , training loss: 4.474031\n",
      "[INFO] Epoch: 42 , batch: 140 , training loss: 4.252825\n",
      "[INFO] Epoch: 42 , batch: 141 , training loss: 4.049927\n",
      "[INFO] Epoch: 42 , batch: 142 , training loss: 3.749022\n",
      "[INFO] Epoch: 42 , batch: 143 , training loss: 3.896477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 42 , batch: 144 , training loss: 3.767254\n",
      "[INFO] Epoch: 42 , batch: 145 , training loss: 3.809467\n",
      "[INFO] Epoch: 42 , batch: 146 , training loss: 4.011044\n",
      "[INFO] Epoch: 42 , batch: 147 , training loss: 3.660017\n",
      "[INFO] Epoch: 42 , batch: 148 , training loss: 3.640027\n",
      "[INFO] Epoch: 42 , batch: 149 , training loss: 3.732972\n",
      "[INFO] Epoch: 42 , batch: 150 , training loss: 3.993710\n",
      "[INFO] Epoch: 42 , batch: 151 , training loss: 3.841755\n",
      "[INFO] Epoch: 42 , batch: 152 , training loss: 3.872593\n",
      "[INFO] Epoch: 42 , batch: 153 , training loss: 3.876779\n",
      "[INFO] Epoch: 42 , batch: 154 , training loss: 3.975023\n",
      "[INFO] Epoch: 42 , batch: 155 , training loss: 4.170116\n",
      "[INFO] Epoch: 42 , batch: 156 , training loss: 3.883820\n",
      "[INFO] Epoch: 42 , batch: 157 , training loss: 3.891573\n",
      "[INFO] Epoch: 42 , batch: 158 , training loss: 3.998102\n",
      "[INFO] Epoch: 42 , batch: 159 , training loss: 3.928526\n",
      "[INFO] Epoch: 42 , batch: 160 , training loss: 4.142729\n",
      "[INFO] Epoch: 42 , batch: 161 , training loss: 4.187283\n",
      "[INFO] Epoch: 42 , batch: 162 , training loss: 4.182737\n",
      "[INFO] Epoch: 42 , batch: 163 , training loss: 4.367714\n",
      "[INFO] Epoch: 42 , batch: 164 , training loss: 4.295625\n",
      "[INFO] Epoch: 42 , batch: 165 , training loss: 4.228765\n",
      "[INFO] Epoch: 42 , batch: 166 , training loss: 4.110140\n",
      "[INFO] Epoch: 42 , batch: 167 , training loss: 4.179414\n",
      "[INFO] Epoch: 42 , batch: 168 , training loss: 3.819619\n",
      "[INFO] Epoch: 42 , batch: 169 , training loss: 3.771885\n",
      "[INFO] Epoch: 42 , batch: 170 , training loss: 4.007045\n",
      "[INFO] Epoch: 42 , batch: 171 , training loss: 3.427102\n",
      "[INFO] Epoch: 42 , batch: 172 , training loss: 3.668003\n",
      "[INFO] Epoch: 42 , batch: 173 , training loss: 3.990374\n",
      "[INFO] Epoch: 42 , batch: 174 , training loss: 4.518577\n",
      "[INFO] Epoch: 42 , batch: 175 , training loss: 4.753744\n",
      "[INFO] Epoch: 42 , batch: 176 , training loss: 4.411446\n",
      "[INFO] Epoch: 42 , batch: 177 , training loss: 4.011749\n",
      "[INFO] Epoch: 42 , batch: 178 , training loss: 4.033996\n",
      "[INFO] Epoch: 42 , batch: 179 , training loss: 4.114383\n",
      "[INFO] Epoch: 42 , batch: 180 , training loss: 4.046111\n",
      "[INFO] Epoch: 42 , batch: 181 , training loss: 4.339202\n",
      "[INFO] Epoch: 42 , batch: 182 , training loss: 4.295891\n",
      "[INFO] Epoch: 42 , batch: 183 , training loss: 4.270390\n",
      "[INFO] Epoch: 42 , batch: 184 , training loss: 4.171093\n",
      "[INFO] Epoch: 42 , batch: 185 , training loss: 4.111115\n",
      "[INFO] Epoch: 42 , batch: 186 , training loss: 4.267368\n",
      "[INFO] Epoch: 42 , batch: 187 , training loss: 4.365909\n",
      "[INFO] Epoch: 42 , batch: 188 , training loss: 4.368134\n",
      "[INFO] Epoch: 42 , batch: 189 , training loss: 4.291284\n",
      "[INFO] Epoch: 42 , batch: 190 , training loss: 4.320997\n",
      "[INFO] Epoch: 42 , batch: 191 , training loss: 4.421619\n",
      "[INFO] Epoch: 42 , batch: 192 , training loss: 4.235137\n",
      "[INFO] Epoch: 42 , batch: 193 , training loss: 4.344771\n",
      "[INFO] Epoch: 42 , batch: 194 , training loss: 4.276022\n",
      "[INFO] Epoch: 42 , batch: 195 , training loss: 4.200993\n",
      "[INFO] Epoch: 42 , batch: 196 , training loss: 4.064338\n",
      "[INFO] Epoch: 42 , batch: 197 , training loss: 4.183532\n",
      "[INFO] Epoch: 42 , batch: 198 , training loss: 4.075817\n",
      "[INFO] Epoch: 42 , batch: 199 , training loss: 4.223520\n",
      "[INFO] Epoch: 42 , batch: 200 , training loss: 4.153512\n",
      "[INFO] Epoch: 42 , batch: 201 , training loss: 4.040073\n",
      "[INFO] Epoch: 42 , batch: 202 , training loss: 4.030243\n",
      "[INFO] Epoch: 42 , batch: 203 , training loss: 4.154170\n",
      "[INFO] Epoch: 42 , batch: 204 , training loss: 4.266401\n",
      "[INFO] Epoch: 42 , batch: 205 , training loss: 3.856337\n",
      "[INFO] Epoch: 42 , batch: 206 , training loss: 3.778073\n",
      "[INFO] Epoch: 42 , batch: 207 , training loss: 3.788779\n",
      "[INFO] Epoch: 42 , batch: 208 , training loss: 4.083033\n",
      "[INFO] Epoch: 42 , batch: 209 , training loss: 4.074672\n",
      "[INFO] Epoch: 42 , batch: 210 , training loss: 4.071260\n",
      "[INFO] Epoch: 42 , batch: 211 , training loss: 4.095343\n",
      "[INFO] Epoch: 42 , batch: 212 , training loss: 4.172803\n",
      "[INFO] Epoch: 42 , batch: 213 , training loss: 4.128802\n",
      "[INFO] Epoch: 42 , batch: 214 , training loss: 4.211186\n",
      "[INFO] Epoch: 42 , batch: 215 , training loss: 4.385106\n",
      "[INFO] Epoch: 42 , batch: 216 , training loss: 4.120810\n",
      "[INFO] Epoch: 42 , batch: 217 , training loss: 4.055886\n",
      "[INFO] Epoch: 42 , batch: 218 , training loss: 4.041351\n",
      "[INFO] Epoch: 42 , batch: 219 , training loss: 4.147943\n",
      "[INFO] Epoch: 42 , batch: 220 , training loss: 3.961341\n",
      "[INFO] Epoch: 42 , batch: 221 , training loss: 3.979828\n",
      "[INFO] Epoch: 42 , batch: 222 , training loss: 4.111779\n",
      "[INFO] Epoch: 42 , batch: 223 , training loss: 4.244383\n",
      "[INFO] Epoch: 42 , batch: 224 , training loss: 4.276189\n",
      "[INFO] Epoch: 42 , batch: 225 , training loss: 4.157487\n",
      "[INFO] Epoch: 42 , batch: 226 , training loss: 4.300332\n",
      "[INFO] Epoch: 42 , batch: 227 , training loss: 4.262286\n",
      "[INFO] Epoch: 42 , batch: 228 , training loss: 4.293117\n",
      "[INFO] Epoch: 42 , batch: 229 , training loss: 4.150709\n",
      "[INFO] Epoch: 42 , batch: 230 , training loss: 3.995155\n",
      "[INFO] Epoch: 42 , batch: 231 , training loss: 3.867221\n",
      "[INFO] Epoch: 42 , batch: 232 , training loss: 4.004326\n",
      "[INFO] Epoch: 42 , batch: 233 , training loss: 4.046728\n",
      "[INFO] Epoch: 42 , batch: 234 , training loss: 3.719033\n",
      "[INFO] Epoch: 42 , batch: 235 , training loss: 3.840985\n",
      "[INFO] Epoch: 42 , batch: 236 , training loss: 3.960353\n",
      "[INFO] Epoch: 42 , batch: 237 , training loss: 4.149101\n",
      "[INFO] Epoch: 42 , batch: 238 , training loss: 3.957351\n",
      "[INFO] Epoch: 42 , batch: 239 , training loss: 3.990016\n",
      "[INFO] Epoch: 42 , batch: 240 , training loss: 4.009476\n",
      "[INFO] Epoch: 42 , batch: 241 , training loss: 3.833560\n",
      "[INFO] Epoch: 42 , batch: 242 , training loss: 3.823792\n",
      "[INFO] Epoch: 42 , batch: 243 , training loss: 4.143230\n",
      "[INFO] Epoch: 42 , batch: 244 , training loss: 4.074080\n",
      "[INFO] Epoch: 42 , batch: 245 , training loss: 4.044554\n",
      "[INFO] Epoch: 42 , batch: 246 , training loss: 3.766366\n",
      "[INFO] Epoch: 42 , batch: 247 , training loss: 3.931790\n",
      "[INFO] Epoch: 42 , batch: 248 , training loss: 4.003963\n",
      "[INFO] Epoch: 42 , batch: 249 , training loss: 3.969914\n",
      "[INFO] Epoch: 42 , batch: 250 , training loss: 3.790832\n",
      "[INFO] Epoch: 42 , batch: 251 , training loss: 4.224526\n",
      "[INFO] Epoch: 42 , batch: 252 , training loss: 3.932950\n",
      "[INFO] Epoch: 42 , batch: 253 , training loss: 3.838856\n",
      "[INFO] Epoch: 42 , batch: 254 , training loss: 4.109834\n",
      "[INFO] Epoch: 42 , batch: 255 , training loss: 4.101703\n",
      "[INFO] Epoch: 42 , batch: 256 , training loss: 4.074790\n",
      "[INFO] Epoch: 42 , batch: 257 , training loss: 4.249913\n",
      "[INFO] Epoch: 42 , batch: 258 , training loss: 4.253740\n",
      "[INFO] Epoch: 42 , batch: 259 , training loss: 4.296191\n",
      "[INFO] Epoch: 42 , batch: 260 , training loss: 4.075872\n",
      "[INFO] Epoch: 42 , batch: 261 , training loss: 4.223905\n",
      "[INFO] Epoch: 42 , batch: 262 , training loss: 4.360288\n",
      "[INFO] Epoch: 42 , batch: 263 , training loss: 4.574040\n",
      "[INFO] Epoch: 42 , batch: 264 , training loss: 3.911919\n",
      "[INFO] Epoch: 42 , batch: 265 , training loss: 4.006200\n",
      "[INFO] Epoch: 42 , batch: 266 , training loss: 4.418936\n",
      "[INFO] Epoch: 42 , batch: 267 , training loss: 4.165933\n",
      "[INFO] Epoch: 42 , batch: 268 , training loss: 4.110076\n",
      "[INFO] Epoch: 42 , batch: 269 , training loss: 4.087104\n",
      "[INFO] Epoch: 42 , batch: 270 , training loss: 4.095893\n",
      "[INFO] Epoch: 42 , batch: 271 , training loss: 4.116068\n",
      "[INFO] Epoch: 42 , batch: 272 , training loss: 4.111171\n",
      "[INFO] Epoch: 42 , batch: 273 , training loss: 4.112701\n",
      "[INFO] Epoch: 42 , batch: 274 , training loss: 4.212683\n",
      "[INFO] Epoch: 42 , batch: 275 , training loss: 4.093667\n",
      "[INFO] Epoch: 42 , batch: 276 , training loss: 4.138815\n",
      "[INFO] Epoch: 42 , batch: 277 , training loss: 4.310979\n",
      "[INFO] Epoch: 42 , batch: 278 , training loss: 3.987465\n",
      "[INFO] Epoch: 42 , batch: 279 , training loss: 4.011712\n",
      "[INFO] Epoch: 42 , batch: 280 , training loss: 3.993098\n",
      "[INFO] Epoch: 42 , batch: 281 , training loss: 4.100574\n",
      "[INFO] Epoch: 42 , batch: 282 , training loss: 4.000956\n",
      "[INFO] Epoch: 42 , batch: 283 , training loss: 3.998347\n",
      "[INFO] Epoch: 42 , batch: 284 , training loss: 4.042042\n",
      "[INFO] Epoch: 42 , batch: 285 , training loss: 3.988451\n",
      "[INFO] Epoch: 42 , batch: 286 , training loss: 4.001522\n",
      "[INFO] Epoch: 42 , batch: 287 , training loss: 3.928656\n",
      "[INFO] Epoch: 42 , batch: 288 , training loss: 3.891108\n",
      "[INFO] Epoch: 42 , batch: 289 , training loss: 3.975524\n",
      "[INFO] Epoch: 42 , batch: 290 , training loss: 3.754197\n",
      "[INFO] Epoch: 42 , batch: 291 , training loss: 3.728954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 42 , batch: 292 , training loss: 3.832247\n",
      "[INFO] Epoch: 42 , batch: 293 , training loss: 3.768555\n",
      "[INFO] Epoch: 42 , batch: 294 , training loss: 4.422789\n",
      "[INFO] Epoch: 42 , batch: 295 , training loss: 4.205281\n",
      "[INFO] Epoch: 42 , batch: 296 , training loss: 4.150624\n",
      "[INFO] Epoch: 42 , batch: 297 , training loss: 4.082201\n",
      "[INFO] Epoch: 42 , batch: 298 , training loss: 3.925601\n",
      "[INFO] Epoch: 42 , batch: 299 , training loss: 3.983822\n",
      "[INFO] Epoch: 42 , batch: 300 , training loss: 3.970894\n",
      "[INFO] Epoch: 42 , batch: 301 , training loss: 3.862197\n",
      "[INFO] Epoch: 42 , batch: 302 , training loss: 4.064104\n",
      "[INFO] Epoch: 42 , batch: 303 , training loss: 4.062557\n",
      "[INFO] Epoch: 42 , batch: 304 , training loss: 4.199172\n",
      "[INFO] Epoch: 42 , batch: 305 , training loss: 4.044349\n",
      "[INFO] Epoch: 42 , batch: 306 , training loss: 4.161148\n",
      "[INFO] Epoch: 42 , batch: 307 , training loss: 4.170910\n",
      "[INFO] Epoch: 42 , batch: 308 , training loss: 3.974020\n",
      "[INFO] Epoch: 42 , batch: 309 , training loss: 3.963260\n",
      "[INFO] Epoch: 42 , batch: 310 , training loss: 3.905351\n",
      "[INFO] Epoch: 42 , batch: 311 , training loss: 3.916256\n",
      "[INFO] Epoch: 42 , batch: 312 , training loss: 3.800301\n",
      "[INFO] Epoch: 42 , batch: 313 , training loss: 3.907956\n",
      "[INFO] Epoch: 42 , batch: 314 , training loss: 3.971131\n",
      "[INFO] Epoch: 42 , batch: 315 , training loss: 4.061836\n",
      "[INFO] Epoch: 42 , batch: 316 , training loss: 4.310231\n",
      "[INFO] Epoch: 42 , batch: 317 , training loss: 4.650410\n",
      "[INFO] Epoch: 42 , batch: 318 , training loss: 4.750066\n",
      "[INFO] Epoch: 42 , batch: 319 , training loss: 4.458289\n",
      "[INFO] Epoch: 42 , batch: 320 , training loss: 4.015593\n",
      "[INFO] Epoch: 42 , batch: 321 , training loss: 3.830056\n",
      "[INFO] Epoch: 42 , batch: 322 , training loss: 3.931158\n",
      "[INFO] Epoch: 42 , batch: 323 , training loss: 3.972070\n",
      "[INFO] Epoch: 42 , batch: 324 , training loss: 3.935219\n",
      "[INFO] Epoch: 42 , batch: 325 , training loss: 4.050061\n",
      "[INFO] Epoch: 42 , batch: 326 , training loss: 4.115600\n",
      "[INFO] Epoch: 42 , batch: 327 , training loss: 4.055296\n",
      "[INFO] Epoch: 42 , batch: 328 , training loss: 4.072796\n",
      "[INFO] Epoch: 42 , batch: 329 , training loss: 3.958758\n",
      "[INFO] Epoch: 42 , batch: 330 , training loss: 3.945563\n",
      "[INFO] Epoch: 42 , batch: 331 , training loss: 4.111784\n",
      "[INFO] Epoch: 42 , batch: 332 , training loss: 3.962862\n",
      "[INFO] Epoch: 42 , batch: 333 , training loss: 3.925775\n",
      "[INFO] Epoch: 42 , batch: 334 , training loss: 3.953643\n",
      "[INFO] Epoch: 42 , batch: 335 , training loss: 4.071432\n",
      "[INFO] Epoch: 42 , batch: 336 , training loss: 4.099198\n",
      "[INFO] Epoch: 42 , batch: 337 , training loss: 4.120602\n",
      "[INFO] Epoch: 42 , batch: 338 , training loss: 4.332953\n",
      "[INFO] Epoch: 42 , batch: 339 , training loss: 4.141142\n",
      "[INFO] Epoch: 42 , batch: 340 , training loss: 4.330670\n",
      "[INFO] Epoch: 42 , batch: 341 , training loss: 4.097921\n",
      "[INFO] Epoch: 42 , batch: 342 , training loss: 3.889745\n",
      "[INFO] Epoch: 42 , batch: 343 , training loss: 3.957972\n",
      "[INFO] Epoch: 42 , batch: 344 , training loss: 3.820459\n",
      "[INFO] Epoch: 42 , batch: 345 , training loss: 3.952385\n",
      "[INFO] Epoch: 42 , batch: 346 , training loss: 3.994040\n",
      "[INFO] Epoch: 42 , batch: 347 , training loss: 3.906940\n",
      "[INFO] Epoch: 42 , batch: 348 , training loss: 4.016449\n",
      "[INFO] Epoch: 42 , batch: 349 , training loss: 4.108402\n",
      "[INFO] Epoch: 42 , batch: 350 , training loss: 3.955331\n",
      "[INFO] Epoch: 42 , batch: 351 , training loss: 4.036343\n",
      "[INFO] Epoch: 42 , batch: 352 , training loss: 4.047592\n",
      "[INFO] Epoch: 42 , batch: 353 , training loss: 4.019917\n",
      "[INFO] Epoch: 42 , batch: 354 , training loss: 4.129893\n",
      "[INFO] Epoch: 42 , batch: 355 , training loss: 4.122237\n",
      "[INFO] Epoch: 42 , batch: 356 , training loss: 3.976193\n",
      "[INFO] Epoch: 42 , batch: 357 , training loss: 4.055800\n",
      "[INFO] Epoch: 42 , batch: 358 , training loss: 3.956425\n",
      "[INFO] Epoch: 42 , batch: 359 , training loss: 3.963154\n",
      "[INFO] Epoch: 42 , batch: 360 , training loss: 4.086443\n",
      "[INFO] Epoch: 42 , batch: 361 , training loss: 4.041717\n",
      "[INFO] Epoch: 42 , batch: 362 , training loss: 4.142761\n",
      "[INFO] Epoch: 42 , batch: 363 , training loss: 4.033798\n",
      "[INFO] Epoch: 42 , batch: 364 , training loss: 4.076737\n",
      "[INFO] Epoch: 42 , batch: 365 , training loss: 3.999327\n",
      "[INFO] Epoch: 42 , batch: 366 , training loss: 4.099155\n",
      "[INFO] Epoch: 42 , batch: 367 , training loss: 4.140564\n",
      "[INFO] Epoch: 42 , batch: 368 , training loss: 4.534205\n",
      "[INFO] Epoch: 42 , batch: 369 , training loss: 4.219061\n",
      "[INFO] Epoch: 42 , batch: 370 , training loss: 3.999378\n",
      "[INFO] Epoch: 42 , batch: 371 , training loss: 4.434896\n",
      "[INFO] Epoch: 42 , batch: 372 , training loss: 4.673335\n",
      "[INFO] Epoch: 42 , batch: 373 , training loss: 4.702940\n",
      "[INFO] Epoch: 42 , batch: 374 , training loss: 4.843735\n",
      "[INFO] Epoch: 42 , batch: 375 , training loss: 4.832571\n",
      "[INFO] Epoch: 42 , batch: 376 , training loss: 4.710446\n",
      "[INFO] Epoch: 42 , batch: 377 , training loss: 4.453759\n",
      "[INFO] Epoch: 42 , batch: 378 , training loss: 4.573090\n",
      "[INFO] Epoch: 42 , batch: 379 , training loss: 4.514432\n",
      "[INFO] Epoch: 42 , batch: 380 , training loss: 4.690238\n",
      "[INFO] Epoch: 42 , batch: 381 , training loss: 4.380984\n",
      "[INFO] Epoch: 42 , batch: 382 , training loss: 4.646656\n",
      "[INFO] Epoch: 42 , batch: 383 , training loss: 4.694670\n",
      "[INFO] Epoch: 42 , batch: 384 , training loss: 4.679074\n",
      "[INFO] Epoch: 42 , batch: 385 , training loss: 4.330803\n",
      "[INFO] Epoch: 42 , batch: 386 , training loss: 4.623509\n",
      "[INFO] Epoch: 42 , batch: 387 , training loss: 4.545727\n",
      "[INFO] Epoch: 42 , batch: 388 , training loss: 4.386251\n",
      "[INFO] Epoch: 42 , batch: 389 , training loss: 4.182690\n",
      "[INFO] Epoch: 42 , batch: 390 , training loss: 4.205365\n",
      "[INFO] Epoch: 42 , batch: 391 , training loss: 4.228504\n",
      "[INFO] Epoch: 42 , batch: 392 , training loss: 4.604065\n",
      "[INFO] Epoch: 42 , batch: 393 , training loss: 4.501374\n",
      "[INFO] Epoch: 42 , batch: 394 , training loss: 4.586286\n",
      "[INFO] Epoch: 42 , batch: 395 , training loss: 4.410866\n",
      "[INFO] Epoch: 42 , batch: 396 , training loss: 4.224174\n",
      "[INFO] Epoch: 42 , batch: 397 , training loss: 4.368254\n",
      "[INFO] Epoch: 42 , batch: 398 , training loss: 4.224347\n",
      "[INFO] Epoch: 42 , batch: 399 , training loss: 4.296047\n",
      "[INFO] Epoch: 42 , batch: 400 , training loss: 4.267619\n",
      "[INFO] Epoch: 42 , batch: 401 , training loss: 4.704014\n",
      "[INFO] Epoch: 42 , batch: 402 , training loss: 4.426797\n",
      "[INFO] Epoch: 42 , batch: 403 , training loss: 4.263987\n",
      "[INFO] Epoch: 42 , batch: 404 , training loss: 4.427980\n",
      "[INFO] Epoch: 42 , batch: 405 , training loss: 4.491144\n",
      "[INFO] Epoch: 42 , batch: 406 , training loss: 4.369308\n",
      "[INFO] Epoch: 42 , batch: 407 , training loss: 4.422757\n",
      "[INFO] Epoch: 42 , batch: 408 , training loss: 4.389463\n",
      "[INFO] Epoch: 42 , batch: 409 , training loss: 4.403477\n",
      "[INFO] Epoch: 42 , batch: 410 , training loss: 4.466992\n",
      "[INFO] Epoch: 42 , batch: 411 , training loss: 4.639521\n",
      "[INFO] Epoch: 42 , batch: 412 , training loss: 4.464042\n",
      "[INFO] Epoch: 42 , batch: 413 , training loss: 4.329646\n",
      "[INFO] Epoch: 42 , batch: 414 , training loss: 4.376268\n",
      "[INFO] Epoch: 42 , batch: 415 , training loss: 4.423306\n",
      "[INFO] Epoch: 42 , batch: 416 , training loss: 4.474012\n",
      "[INFO] Epoch: 42 , batch: 417 , training loss: 4.397223\n",
      "[INFO] Epoch: 42 , batch: 418 , training loss: 4.453891\n",
      "[INFO] Epoch: 42 , batch: 419 , training loss: 4.411478\n",
      "[INFO] Epoch: 42 , batch: 420 , training loss: 4.400797\n",
      "[INFO] Epoch: 42 , batch: 421 , training loss: 4.365741\n",
      "[INFO] Epoch: 42 , batch: 422 , training loss: 4.217760\n",
      "[INFO] Epoch: 42 , batch: 423 , training loss: 4.453170\n",
      "[INFO] Epoch: 42 , batch: 424 , training loss: 4.617191\n",
      "[INFO] Epoch: 42 , batch: 425 , training loss: 4.481002\n",
      "[INFO] Epoch: 42 , batch: 426 , training loss: 4.217637\n",
      "[INFO] Epoch: 42 , batch: 427 , training loss: 4.457989\n",
      "[INFO] Epoch: 42 , batch: 428 , training loss: 4.317866\n",
      "[INFO] Epoch: 42 , batch: 429 , training loss: 4.225039\n",
      "[INFO] Epoch: 42 , batch: 430 , training loss: 4.442650\n",
      "[INFO] Epoch: 42 , batch: 431 , training loss: 4.059855\n",
      "[INFO] Epoch: 42 , batch: 432 , training loss: 4.120354\n",
      "[INFO] Epoch: 42 , batch: 433 , training loss: 4.153172\n",
      "[INFO] Epoch: 42 , batch: 434 , training loss: 4.025230\n",
      "[INFO] Epoch: 42 , batch: 435 , training loss: 4.384805\n",
      "[INFO] Epoch: 42 , batch: 436 , training loss: 4.426710\n",
      "[INFO] Epoch: 42 , batch: 437 , training loss: 4.229293\n",
      "[INFO] Epoch: 42 , batch: 438 , training loss: 4.075097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 42 , batch: 439 , training loss: 4.332915\n",
      "[INFO] Epoch: 42 , batch: 440 , training loss: 4.435271\n",
      "[INFO] Epoch: 42 , batch: 441 , training loss: 4.519481\n",
      "[INFO] Epoch: 42 , batch: 442 , training loss: 4.292665\n",
      "[INFO] Epoch: 42 , batch: 443 , training loss: 4.463495\n",
      "[INFO] Epoch: 42 , batch: 444 , training loss: 4.072833\n",
      "[INFO] Epoch: 42 , batch: 445 , training loss: 3.979360\n",
      "[INFO] Epoch: 42 , batch: 446 , training loss: 3.922320\n",
      "[INFO] Epoch: 42 , batch: 447 , training loss: 4.100696\n",
      "[INFO] Epoch: 42 , batch: 448 , training loss: 4.234749\n",
      "[INFO] Epoch: 42 , batch: 449 , training loss: 4.608967\n",
      "[INFO] Epoch: 42 , batch: 450 , training loss: 4.697653\n",
      "[INFO] Epoch: 42 , batch: 451 , training loss: 4.558113\n",
      "[INFO] Epoch: 42 , batch: 452 , training loss: 4.391050\n",
      "[INFO] Epoch: 42 , batch: 453 , training loss: 4.157310\n",
      "[INFO] Epoch: 42 , batch: 454 , training loss: 4.303176\n",
      "[INFO] Epoch: 42 , batch: 455 , training loss: 4.373178\n",
      "[INFO] Epoch: 42 , batch: 456 , training loss: 4.350189\n",
      "[INFO] Epoch: 42 , batch: 457 , training loss: 4.432877\n",
      "[INFO] Epoch: 42 , batch: 458 , training loss: 4.176462\n",
      "[INFO] Epoch: 42 , batch: 459 , training loss: 4.137097\n",
      "[INFO] Epoch: 42 , batch: 460 , training loss: 4.247478\n",
      "[INFO] Epoch: 42 , batch: 461 , training loss: 4.209697\n",
      "[INFO] Epoch: 42 , batch: 462 , training loss: 4.278777\n",
      "[INFO] Epoch: 42 , batch: 463 , training loss: 4.194835\n",
      "[INFO] Epoch: 42 , batch: 464 , training loss: 4.382817\n",
      "[INFO] Epoch: 42 , batch: 465 , training loss: 4.324393\n",
      "[INFO] Epoch: 42 , batch: 466 , training loss: 4.416036\n",
      "[INFO] Epoch: 42 , batch: 467 , training loss: 4.373331\n",
      "[INFO] Epoch: 42 , batch: 468 , training loss: 4.335268\n",
      "[INFO] Epoch: 42 , batch: 469 , training loss: 4.372223\n",
      "[INFO] Epoch: 42 , batch: 470 , training loss: 4.175901\n",
      "[INFO] Epoch: 42 , batch: 471 , training loss: 4.299517\n",
      "[INFO] Epoch: 42 , batch: 472 , training loss: 4.348981\n",
      "[INFO] Epoch: 42 , batch: 473 , training loss: 4.259954\n",
      "[INFO] Epoch: 42 , batch: 474 , training loss: 4.041364\n",
      "[INFO] Epoch: 42 , batch: 475 , training loss: 3.915465\n",
      "[INFO] Epoch: 42 , batch: 476 , training loss: 4.336372\n",
      "[INFO] Epoch: 42 , batch: 477 , training loss: 4.440368\n",
      "[INFO] Epoch: 42 , batch: 478 , training loss: 4.430912\n",
      "[INFO] Epoch: 42 , batch: 479 , training loss: 4.422585\n",
      "[INFO] Epoch: 42 , batch: 480 , training loss: 4.536228\n",
      "[INFO] Epoch: 42 , batch: 481 , training loss: 4.429307\n",
      "[INFO] Epoch: 42 , batch: 482 , training loss: 4.523106\n",
      "[INFO] Epoch: 42 , batch: 483 , training loss: 4.380054\n",
      "[INFO] Epoch: 42 , batch: 484 , training loss: 4.174644\n",
      "[INFO] Epoch: 42 , batch: 485 , training loss: 4.273587\n",
      "[INFO] Epoch: 42 , batch: 486 , training loss: 4.181532\n",
      "[INFO] Epoch: 42 , batch: 487 , training loss: 4.151694\n",
      "[INFO] Epoch: 42 , batch: 488 , training loss: 4.331438\n",
      "[INFO] Epoch: 42 , batch: 489 , training loss: 4.240649\n",
      "[INFO] Epoch: 42 , batch: 490 , training loss: 4.299456\n",
      "[INFO] Epoch: 42 , batch: 491 , training loss: 4.211849\n",
      "[INFO] Epoch: 42 , batch: 492 , training loss: 4.190585\n",
      "[INFO] Epoch: 42 , batch: 493 , training loss: 4.352590\n",
      "[INFO] Epoch: 42 , batch: 494 , training loss: 4.270457\n",
      "[INFO] Epoch: 42 , batch: 495 , training loss: 4.420031\n",
      "[INFO] Epoch: 42 , batch: 496 , training loss: 4.291244\n",
      "[INFO] Epoch: 42 , batch: 497 , training loss: 4.328189\n",
      "[INFO] Epoch: 42 , batch: 498 , training loss: 4.321361\n",
      "[INFO] Epoch: 42 , batch: 499 , training loss: 4.395159\n",
      "[INFO] Epoch: 42 , batch: 500 , training loss: 4.523355\n",
      "[INFO] Epoch: 42 , batch: 501 , training loss: 4.833773\n",
      "[INFO] Epoch: 42 , batch: 502 , training loss: 4.845778\n",
      "[INFO] Epoch: 42 , batch: 503 , training loss: 4.512950\n",
      "[INFO] Epoch: 42 , batch: 504 , training loss: 4.666563\n",
      "[INFO] Epoch: 42 , batch: 505 , training loss: 4.647015\n",
      "[INFO] Epoch: 42 , batch: 506 , training loss: 4.630250\n",
      "[INFO] Epoch: 42 , batch: 507 , training loss: 4.651657\n",
      "[INFO] Epoch: 42 , batch: 508 , training loss: 4.576177\n",
      "[INFO] Epoch: 42 , batch: 509 , training loss: 4.382215\n",
      "[INFO] Epoch: 42 , batch: 510 , training loss: 4.480011\n",
      "[INFO] Epoch: 42 , batch: 511 , training loss: 4.413246\n",
      "[INFO] Epoch: 42 , batch: 512 , training loss: 4.490617\n",
      "[INFO] Epoch: 42 , batch: 513 , training loss: 4.772845\n",
      "[INFO] Epoch: 42 , batch: 514 , training loss: 4.381377\n",
      "[INFO] Epoch: 42 , batch: 515 , training loss: 4.643025\n",
      "[INFO] Epoch: 42 , batch: 516 , training loss: 4.453847\n",
      "[INFO] Epoch: 42 , batch: 517 , training loss: 4.409083\n",
      "[INFO] Epoch: 42 , batch: 518 , training loss: 4.361144\n",
      "[INFO] Epoch: 42 , batch: 519 , training loss: 4.219959\n",
      "[INFO] Epoch: 42 , batch: 520 , training loss: 4.450441\n",
      "[INFO] Epoch: 42 , batch: 521 , training loss: 4.439641\n",
      "[INFO] Epoch: 42 , batch: 522 , training loss: 4.516119\n",
      "[INFO] Epoch: 42 , batch: 523 , training loss: 4.426173\n",
      "[INFO] Epoch: 42 , batch: 524 , training loss: 4.708121\n",
      "[INFO] Epoch: 42 , batch: 525 , training loss: 4.596334\n",
      "[INFO] Epoch: 42 , batch: 526 , training loss: 4.380440\n",
      "[INFO] Epoch: 42 , batch: 527 , training loss: 4.427204\n",
      "[INFO] Epoch: 42 , batch: 528 , training loss: 4.446977\n",
      "[INFO] Epoch: 42 , batch: 529 , training loss: 4.421989\n",
      "[INFO] Epoch: 42 , batch: 530 , training loss: 4.288065\n",
      "[INFO] Epoch: 42 , batch: 531 , training loss: 4.421135\n",
      "[INFO] Epoch: 42 , batch: 532 , training loss: 4.334795\n",
      "[INFO] Epoch: 42 , batch: 533 , training loss: 4.440709\n",
      "[INFO] Epoch: 42 , batch: 534 , training loss: 4.461205\n",
      "[INFO] Epoch: 42 , batch: 535 , training loss: 4.457957\n",
      "[INFO] Epoch: 42 , batch: 536 , training loss: 4.308152\n",
      "[INFO] Epoch: 42 , batch: 537 , training loss: 4.273772\n",
      "[INFO] Epoch: 42 , batch: 538 , training loss: 4.370400\n",
      "[INFO] Epoch: 42 , batch: 539 , training loss: 4.470079\n",
      "[INFO] Epoch: 42 , batch: 540 , training loss: 5.018688\n",
      "[INFO] Epoch: 42 , batch: 541 , training loss: 4.818163\n",
      "[INFO] Epoch: 42 , batch: 542 , training loss: 4.693959\n",
      "[INFO] Epoch: 43 , batch: 0 , training loss: 3.617518\n",
      "[INFO] Epoch: 43 , batch: 1 , training loss: 3.579784\n",
      "[INFO] Epoch: 43 , batch: 2 , training loss: 3.759899\n",
      "[INFO] Epoch: 43 , batch: 3 , training loss: 3.641412\n",
      "[INFO] Epoch: 43 , batch: 4 , training loss: 3.896403\n",
      "[INFO] Epoch: 43 , batch: 5 , training loss: 3.608103\n",
      "[INFO] Epoch: 43 , batch: 6 , training loss: 3.953860\n",
      "[INFO] Epoch: 43 , batch: 7 , training loss: 3.879052\n",
      "[INFO] Epoch: 43 , batch: 8 , training loss: 3.574154\n",
      "[INFO] Epoch: 43 , batch: 9 , training loss: 3.863559\n",
      "[INFO] Epoch: 43 , batch: 10 , training loss: 3.793842\n",
      "[INFO] Epoch: 43 , batch: 11 , training loss: 3.735578\n",
      "[INFO] Epoch: 43 , batch: 12 , training loss: 3.611888\n",
      "[INFO] Epoch: 43 , batch: 13 , training loss: 3.625404\n",
      "[INFO] Epoch: 43 , batch: 14 , training loss: 3.527554\n",
      "[INFO] Epoch: 43 , batch: 15 , training loss: 3.765796\n",
      "[INFO] Epoch: 43 , batch: 16 , training loss: 3.605524\n",
      "[INFO] Epoch: 43 , batch: 17 , training loss: 3.740832\n",
      "[INFO] Epoch: 43 , batch: 18 , training loss: 3.681449\n",
      "[INFO] Epoch: 43 , batch: 19 , training loss: 3.443407\n",
      "[INFO] Epoch: 43 , batch: 20 , training loss: 3.413344\n",
      "[INFO] Epoch: 43 , batch: 21 , training loss: 3.554523\n",
      "[INFO] Epoch: 43 , batch: 22 , training loss: 3.476938\n",
      "[INFO] Epoch: 43 , batch: 23 , training loss: 3.653778\n",
      "[INFO] Epoch: 43 , batch: 24 , training loss: 3.485829\n",
      "[INFO] Epoch: 43 , batch: 25 , training loss: 3.590315\n",
      "[INFO] Epoch: 43 , batch: 26 , training loss: 3.483046\n",
      "[INFO] Epoch: 43 , batch: 27 , training loss: 3.484335\n",
      "[INFO] Epoch: 43 , batch: 28 , training loss: 3.639507\n",
      "[INFO] Epoch: 43 , batch: 29 , training loss: 3.460190\n",
      "[INFO] Epoch: 43 , batch: 30 , training loss: 3.489035\n",
      "[INFO] Epoch: 43 , batch: 31 , training loss: 3.572494\n",
      "[INFO] Epoch: 43 , batch: 32 , training loss: 3.522763\n",
      "[INFO] Epoch: 43 , batch: 33 , training loss: 3.562118\n",
      "[INFO] Epoch: 43 , batch: 34 , training loss: 3.582681\n",
      "[INFO] Epoch: 43 , batch: 35 , training loss: 3.536611\n",
      "[INFO] Epoch: 43 , batch: 36 , training loss: 3.595035\n",
      "[INFO] Epoch: 43 , batch: 37 , training loss: 3.490275\n",
      "[INFO] Epoch: 43 , batch: 38 , training loss: 3.574285\n",
      "[INFO] Epoch: 43 , batch: 39 , training loss: 3.377321\n",
      "[INFO] Epoch: 43 , batch: 40 , training loss: 3.554460\n",
      "[INFO] Epoch: 43 , batch: 41 , training loss: 3.515910\n",
      "[INFO] Epoch: 43 , batch: 42 , training loss: 4.009707\n",
      "[INFO] Epoch: 43 , batch: 43 , training loss: 3.778597\n",
      "[INFO] Epoch: 43 , batch: 44 , training loss: 4.090643\n",
      "[INFO] Epoch: 43 , batch: 45 , training loss: 4.006011\n",
      "[INFO] Epoch: 43 , batch: 46 , training loss: 3.926217\n",
      "[INFO] Epoch: 43 , batch: 47 , training loss: 3.554976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 43 , batch: 48 , training loss: 3.583407\n",
      "[INFO] Epoch: 43 , batch: 49 , training loss: 3.810515\n",
      "[INFO] Epoch: 43 , batch: 50 , training loss: 3.570544\n",
      "[INFO] Epoch: 43 , batch: 51 , training loss: 3.803909\n",
      "[INFO] Epoch: 43 , batch: 52 , training loss: 3.657049\n",
      "[INFO] Epoch: 43 , batch: 53 , training loss: 3.755496\n",
      "[INFO] Epoch: 43 , batch: 54 , training loss: 3.773462\n",
      "[INFO] Epoch: 43 , batch: 55 , training loss: 3.838010\n",
      "[INFO] Epoch: 43 , batch: 56 , training loss: 3.662352\n",
      "[INFO] Epoch: 43 , batch: 57 , training loss: 3.633958\n",
      "[INFO] Epoch: 43 , batch: 58 , training loss: 3.642540\n",
      "[INFO] Epoch: 43 , batch: 59 , training loss: 3.765549\n",
      "[INFO] Epoch: 43 , batch: 60 , training loss: 3.680578\n",
      "[INFO] Epoch: 43 , batch: 61 , training loss: 3.746468\n",
      "[INFO] Epoch: 43 , batch: 62 , training loss: 3.617994\n",
      "[INFO] Epoch: 43 , batch: 63 , training loss: 3.814971\n",
      "[INFO] Epoch: 43 , batch: 64 , training loss: 4.045072\n",
      "[INFO] Epoch: 43 , batch: 65 , training loss: 3.724810\n",
      "[INFO] Epoch: 43 , batch: 66 , training loss: 3.603876\n",
      "[INFO] Epoch: 43 , batch: 67 , training loss: 3.610791\n",
      "[INFO] Epoch: 43 , batch: 68 , training loss: 3.769252\n",
      "[INFO] Epoch: 43 , batch: 69 , training loss: 3.691674\n",
      "[INFO] Epoch: 43 , batch: 70 , training loss: 3.923284\n",
      "[INFO] Epoch: 43 , batch: 71 , training loss: 3.784467\n",
      "[INFO] Epoch: 43 , batch: 72 , training loss: 3.847277\n",
      "[INFO] Epoch: 43 , batch: 73 , training loss: 3.793952\n",
      "[INFO] Epoch: 43 , batch: 74 , training loss: 3.893965\n",
      "[INFO] Epoch: 43 , batch: 75 , training loss: 3.749700\n",
      "[INFO] Epoch: 43 , batch: 76 , training loss: 3.887836\n",
      "[INFO] Epoch: 43 , batch: 77 , training loss: 3.799678\n",
      "[INFO] Epoch: 43 , batch: 78 , training loss: 3.903854\n",
      "[INFO] Epoch: 43 , batch: 79 , training loss: 3.765334\n",
      "[INFO] Epoch: 43 , batch: 80 , training loss: 3.935374\n",
      "[INFO] Epoch: 43 , batch: 81 , training loss: 3.870399\n",
      "[INFO] Epoch: 43 , batch: 82 , training loss: 3.832998\n",
      "[INFO] Epoch: 43 , batch: 83 , training loss: 3.889347\n",
      "[INFO] Epoch: 43 , batch: 84 , training loss: 3.937865\n",
      "[INFO] Epoch: 43 , batch: 85 , training loss: 3.988771\n",
      "[INFO] Epoch: 43 , batch: 86 , training loss: 3.911474\n",
      "[INFO] Epoch: 43 , batch: 87 , training loss: 3.873271\n",
      "[INFO] Epoch: 43 , batch: 88 , training loss: 4.004254\n",
      "[INFO] Epoch: 43 , batch: 89 , training loss: 3.807951\n",
      "[INFO] Epoch: 43 , batch: 90 , training loss: 3.885586\n",
      "[INFO] Epoch: 43 , batch: 91 , training loss: 3.822340\n",
      "[INFO] Epoch: 43 , batch: 92 , training loss: 3.833961\n",
      "[INFO] Epoch: 43 , batch: 93 , training loss: 3.955089\n",
      "[INFO] Epoch: 43 , batch: 94 , training loss: 4.064974\n",
      "[INFO] Epoch: 43 , batch: 95 , training loss: 3.863020\n",
      "[INFO] Epoch: 43 , batch: 96 , training loss: 3.837554\n",
      "[INFO] Epoch: 43 , batch: 97 , training loss: 3.780627\n",
      "[INFO] Epoch: 43 , batch: 98 , training loss: 3.708646\n",
      "[INFO] Epoch: 43 , batch: 99 , training loss: 3.835829\n",
      "[INFO] Epoch: 43 , batch: 100 , training loss: 3.736740\n",
      "[INFO] Epoch: 43 , batch: 101 , training loss: 3.759553\n",
      "[INFO] Epoch: 43 , batch: 102 , training loss: 3.933758\n",
      "[INFO] Epoch: 43 , batch: 103 , training loss: 3.721641\n",
      "[INFO] Epoch: 43 , batch: 104 , training loss: 3.668786\n",
      "[INFO] Epoch: 43 , batch: 105 , training loss: 3.900971\n",
      "[INFO] Epoch: 43 , batch: 106 , training loss: 3.934713\n",
      "[INFO] Epoch: 43 , batch: 107 , training loss: 3.791321\n",
      "[INFO] Epoch: 43 , batch: 108 , training loss: 3.723477\n",
      "[INFO] Epoch: 43 , batch: 109 , training loss: 3.669324\n",
      "[INFO] Epoch: 43 , batch: 110 , training loss: 3.826434\n",
      "[INFO] Epoch: 43 , batch: 111 , training loss: 3.902315\n",
      "[INFO] Epoch: 43 , batch: 112 , training loss: 3.829767\n",
      "[INFO] Epoch: 43 , batch: 113 , training loss: 3.825725\n",
      "[INFO] Epoch: 43 , batch: 114 , training loss: 3.810654\n",
      "[INFO] Epoch: 43 , batch: 115 , training loss: 3.824950\n",
      "[INFO] Epoch: 43 , batch: 116 , training loss: 3.723421\n",
      "[INFO] Epoch: 43 , batch: 117 , training loss: 3.915799\n",
      "[INFO] Epoch: 43 , batch: 118 , training loss: 3.920467\n",
      "[INFO] Epoch: 43 , batch: 119 , training loss: 4.057145\n",
      "[INFO] Epoch: 43 , batch: 120 , training loss: 4.037528\n",
      "[INFO] Epoch: 43 , batch: 121 , training loss: 3.904135\n",
      "[INFO] Epoch: 43 , batch: 122 , training loss: 3.822943\n",
      "[INFO] Epoch: 43 , batch: 123 , training loss: 3.789784\n",
      "[INFO] Epoch: 43 , batch: 124 , training loss: 3.876411\n",
      "[INFO] Epoch: 43 , batch: 125 , training loss: 3.721322\n",
      "[INFO] Epoch: 43 , batch: 126 , training loss: 3.752459\n",
      "[INFO] Epoch: 43 , batch: 127 , training loss: 3.751983\n",
      "[INFO] Epoch: 43 , batch: 128 , training loss: 3.893027\n",
      "[INFO] Epoch: 43 , batch: 129 , training loss: 3.843552\n",
      "[INFO] Epoch: 43 , batch: 130 , training loss: 3.845685\n",
      "[INFO] Epoch: 43 , batch: 131 , training loss: 3.816756\n",
      "[INFO] Epoch: 43 , batch: 132 , training loss: 3.852713\n",
      "[INFO] Epoch: 43 , batch: 133 , training loss: 3.809571\n",
      "[INFO] Epoch: 43 , batch: 134 , training loss: 3.590371\n",
      "[INFO] Epoch: 43 , batch: 135 , training loss: 3.664238\n",
      "[INFO] Epoch: 43 , batch: 136 , training loss: 3.894233\n",
      "[INFO] Epoch: 43 , batch: 137 , training loss: 3.867463\n",
      "[INFO] Epoch: 43 , batch: 138 , training loss: 3.894356\n",
      "[INFO] Epoch: 43 , batch: 139 , training loss: 4.453631\n",
      "[INFO] Epoch: 43 , batch: 140 , training loss: 4.231039\n",
      "[INFO] Epoch: 43 , batch: 141 , training loss: 4.031008\n",
      "[INFO] Epoch: 43 , batch: 142 , training loss: 3.737097\n",
      "[INFO] Epoch: 43 , batch: 143 , training loss: 3.908913\n",
      "[INFO] Epoch: 43 , batch: 144 , training loss: 3.736109\n",
      "[INFO] Epoch: 43 , batch: 145 , training loss: 3.810244\n",
      "[INFO] Epoch: 43 , batch: 146 , training loss: 4.043413\n",
      "[INFO] Epoch: 43 , batch: 147 , training loss: 3.664542\n",
      "[INFO] Epoch: 43 , batch: 148 , training loss: 3.654066\n",
      "[INFO] Epoch: 43 , batch: 149 , training loss: 3.721645\n",
      "[INFO] Epoch: 43 , batch: 150 , training loss: 4.008261\n",
      "[INFO] Epoch: 43 , batch: 151 , training loss: 3.844781\n",
      "[INFO] Epoch: 43 , batch: 152 , training loss: 3.873869\n",
      "[INFO] Epoch: 43 , batch: 153 , training loss: 3.918825\n",
      "[INFO] Epoch: 43 , batch: 154 , training loss: 3.992362\n",
      "[INFO] Epoch: 43 , batch: 155 , training loss: 4.208174\n",
      "[INFO] Epoch: 43 , batch: 156 , training loss: 3.990317\n",
      "[INFO] Epoch: 43 , batch: 157 , training loss: 3.952511\n",
      "[INFO] Epoch: 43 , batch: 158 , training loss: 4.067753\n",
      "[INFO] Epoch: 43 , batch: 159 , training loss: 3.986441\n",
      "[INFO] Epoch: 43 , batch: 160 , training loss: 4.267007\n",
      "[INFO] Epoch: 43 , batch: 161 , training loss: 4.232132\n",
      "[INFO] Epoch: 43 , batch: 162 , training loss: 4.264034\n",
      "[INFO] Epoch: 43 , batch: 163 , training loss: 4.455913\n",
      "[INFO] Epoch: 43 , batch: 164 , training loss: 4.377717\n",
      "[INFO] Epoch: 43 , batch: 165 , training loss: 4.330153\n",
      "[INFO] Epoch: 43 , batch: 166 , training loss: 4.239423\n",
      "[INFO] Epoch: 43 , batch: 167 , training loss: 4.340814\n",
      "[INFO] Epoch: 43 , batch: 168 , training loss: 3.955697\n",
      "[INFO] Epoch: 43 , batch: 169 , training loss: 3.967147\n",
      "[INFO] Epoch: 43 , batch: 170 , training loss: 4.129076\n",
      "[INFO] Epoch: 43 , batch: 171 , training loss: 3.567987\n",
      "[INFO] Epoch: 43 , batch: 172 , training loss: 3.823706\n",
      "[INFO] Epoch: 43 , batch: 173 , training loss: 4.075072\n",
      "[INFO] Epoch: 43 , batch: 174 , training loss: 4.540981\n",
      "[INFO] Epoch: 43 , batch: 175 , training loss: 4.808187\n",
      "[INFO] Epoch: 43 , batch: 176 , training loss: 4.507265\n",
      "[INFO] Epoch: 43 , batch: 177 , training loss: 4.097473\n",
      "[INFO] Epoch: 43 , batch: 178 , training loss: 4.099844\n",
      "[INFO] Epoch: 43 , batch: 179 , training loss: 4.149129\n",
      "[INFO] Epoch: 43 , batch: 180 , training loss: 4.113678\n",
      "[INFO] Epoch: 43 , batch: 181 , training loss: 4.375335\n",
      "[INFO] Epoch: 43 , batch: 182 , training loss: 4.343464\n",
      "[INFO] Epoch: 43 , batch: 183 , training loss: 4.316289\n",
      "[INFO] Epoch: 43 , batch: 184 , training loss: 4.182715\n",
      "[INFO] Epoch: 43 , batch: 185 , training loss: 4.158361\n",
      "[INFO] Epoch: 43 , batch: 186 , training loss: 4.303566\n",
      "[INFO] Epoch: 43 , batch: 187 , training loss: 4.364989\n",
      "[INFO] Epoch: 43 , batch: 188 , training loss: 4.384657\n",
      "[INFO] Epoch: 43 , batch: 189 , training loss: 4.280072\n",
      "[INFO] Epoch: 43 , batch: 190 , training loss: 4.343200\n",
      "[INFO] Epoch: 43 , batch: 191 , training loss: 4.451908\n",
      "[INFO] Epoch: 43 , batch: 192 , training loss: 4.249787\n",
      "[INFO] Epoch: 43 , batch: 193 , training loss: 4.363983\n",
      "[INFO] Epoch: 43 , batch: 194 , training loss: 4.302576\n",
      "[INFO] Epoch: 43 , batch: 195 , training loss: 4.253986\n",
      "[INFO] Epoch: 43 , batch: 196 , training loss: 4.101130\n",
      "[INFO] Epoch: 43 , batch: 197 , training loss: 4.200074\n",
      "[INFO] Epoch: 43 , batch: 198 , training loss: 4.088235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 43 , batch: 199 , training loss: 4.227091\n",
      "[INFO] Epoch: 43 , batch: 200 , training loss: 4.138012\n",
      "[INFO] Epoch: 43 , batch: 201 , training loss: 4.035495\n",
      "[INFO] Epoch: 43 , batch: 202 , training loss: 4.028935\n",
      "[INFO] Epoch: 43 , batch: 203 , training loss: 4.171982\n",
      "[INFO] Epoch: 43 , batch: 204 , training loss: 4.260121\n",
      "[INFO] Epoch: 43 , batch: 205 , training loss: 3.857214\n",
      "[INFO] Epoch: 43 , batch: 206 , training loss: 3.784853\n",
      "[INFO] Epoch: 43 , batch: 207 , training loss: 3.794343\n",
      "[INFO] Epoch: 43 , batch: 208 , training loss: 4.101849\n",
      "[INFO] Epoch: 43 , batch: 209 , training loss: 4.086834\n",
      "[INFO] Epoch: 43 , batch: 210 , training loss: 4.101584\n",
      "[INFO] Epoch: 43 , batch: 211 , training loss: 4.119031\n",
      "[INFO] Epoch: 43 , batch: 212 , training loss: 4.197448\n",
      "[INFO] Epoch: 43 , batch: 213 , training loss: 4.171241\n",
      "[INFO] Epoch: 43 , batch: 214 , training loss: 4.237406\n",
      "[INFO] Epoch: 43 , batch: 215 , training loss: 4.398181\n",
      "[INFO] Epoch: 43 , batch: 216 , training loss: 4.126005\n",
      "[INFO] Epoch: 43 , batch: 217 , training loss: 4.047686\n",
      "[INFO] Epoch: 43 , batch: 218 , training loss: 4.065884\n",
      "[INFO] Epoch: 43 , batch: 219 , training loss: 4.147918\n",
      "[INFO] Epoch: 43 , batch: 220 , training loss: 3.972286\n",
      "[INFO] Epoch: 43 , batch: 221 , training loss: 4.003452\n",
      "[INFO] Epoch: 43 , batch: 222 , training loss: 4.134439\n",
      "[INFO] Epoch: 43 , batch: 223 , training loss: 4.287992\n",
      "[INFO] Epoch: 43 , batch: 224 , training loss: 4.295050\n",
      "[INFO] Epoch: 43 , batch: 225 , training loss: 4.170938\n",
      "[INFO] Epoch: 43 , batch: 226 , training loss: 4.300009\n",
      "[INFO] Epoch: 43 , batch: 227 , training loss: 4.275318\n",
      "[INFO] Epoch: 43 , batch: 228 , training loss: 4.295314\n",
      "[INFO] Epoch: 43 , batch: 229 , training loss: 4.171710\n",
      "[INFO] Epoch: 43 , batch: 230 , training loss: 4.031630\n",
      "[INFO] Epoch: 43 , batch: 231 , training loss: 3.883010\n",
      "[INFO] Epoch: 43 , batch: 232 , training loss: 4.026743\n",
      "[INFO] Epoch: 43 , batch: 233 , training loss: 4.062690\n",
      "[INFO] Epoch: 43 , batch: 234 , training loss: 3.737309\n",
      "[INFO] Epoch: 43 , batch: 235 , training loss: 3.860837\n",
      "[INFO] Epoch: 43 , batch: 236 , training loss: 3.952409\n",
      "[INFO] Epoch: 43 , batch: 237 , training loss: 4.158648\n",
      "[INFO] Epoch: 43 , batch: 238 , training loss: 3.976113\n",
      "[INFO] Epoch: 43 , batch: 239 , training loss: 3.984255\n",
      "[INFO] Epoch: 43 , batch: 240 , training loss: 4.022070\n",
      "[INFO] Epoch: 43 , batch: 241 , training loss: 3.842150\n",
      "[INFO] Epoch: 43 , batch: 242 , training loss: 3.846703\n",
      "[INFO] Epoch: 43 , batch: 243 , training loss: 4.149496\n",
      "[INFO] Epoch: 43 , batch: 244 , training loss: 4.089210\n",
      "[INFO] Epoch: 43 , batch: 245 , training loss: 4.067238\n",
      "[INFO] Epoch: 43 , batch: 246 , training loss: 3.764121\n",
      "[INFO] Epoch: 43 , batch: 247 , training loss: 3.940689\n",
      "[INFO] Epoch: 43 , batch: 248 , training loss: 4.004171\n",
      "[INFO] Epoch: 43 , batch: 249 , training loss: 3.994358\n",
      "[INFO] Epoch: 43 , batch: 250 , training loss: 3.795503\n",
      "[INFO] Epoch: 43 , batch: 251 , training loss: 4.228426\n",
      "[INFO] Epoch: 43 , batch: 252 , training loss: 3.948687\n",
      "[INFO] Epoch: 43 , batch: 253 , training loss: 3.858156\n",
      "[INFO] Epoch: 43 , batch: 254 , training loss: 4.129375\n",
      "[INFO] Epoch: 43 , batch: 255 , training loss: 4.131865\n",
      "[INFO] Epoch: 43 , batch: 256 , training loss: 4.085716\n",
      "[INFO] Epoch: 43 , batch: 257 , training loss: 4.280396\n",
      "[INFO] Epoch: 43 , batch: 258 , training loss: 4.255152\n",
      "[INFO] Epoch: 43 , batch: 259 , training loss: 4.326299\n",
      "[INFO] Epoch: 43 , batch: 260 , training loss: 4.090375\n",
      "[INFO] Epoch: 43 , batch: 261 , training loss: 4.252480\n",
      "[INFO] Epoch: 43 , batch: 262 , training loss: 4.375946\n",
      "[INFO] Epoch: 43 , batch: 263 , training loss: 4.580327\n",
      "[INFO] Epoch: 43 , batch: 264 , training loss: 3.904859\n",
      "[INFO] Epoch: 43 , batch: 265 , training loss: 4.024707\n",
      "[INFO] Epoch: 43 , batch: 266 , training loss: 4.435629\n",
      "[INFO] Epoch: 43 , batch: 267 , training loss: 4.170234\n",
      "[INFO] Epoch: 43 , batch: 268 , training loss: 4.105575\n",
      "[INFO] Epoch: 43 , batch: 269 , training loss: 4.076066\n",
      "[INFO] Epoch: 43 , batch: 270 , training loss: 4.094505\n",
      "[INFO] Epoch: 43 , batch: 271 , training loss: 4.132849\n",
      "[INFO] Epoch: 43 , batch: 272 , training loss: 4.118245\n",
      "[INFO] Epoch: 43 , batch: 273 , training loss: 4.118895\n",
      "[INFO] Epoch: 43 , batch: 274 , training loss: 4.212453\n",
      "[INFO] Epoch: 43 , batch: 275 , training loss: 4.100549\n",
      "[INFO] Epoch: 43 , batch: 276 , training loss: 4.133717\n",
      "[INFO] Epoch: 43 , batch: 277 , training loss: 4.310340\n",
      "[INFO] Epoch: 43 , batch: 278 , training loss: 3.992114\n",
      "[INFO] Epoch: 43 , batch: 279 , training loss: 3.998987\n",
      "[INFO] Epoch: 43 , batch: 280 , training loss: 3.999430\n",
      "[INFO] Epoch: 43 , batch: 281 , training loss: 4.102308\n",
      "[INFO] Epoch: 43 , batch: 282 , training loss: 4.026173\n",
      "[INFO] Epoch: 43 , batch: 283 , training loss: 4.015138\n",
      "[INFO] Epoch: 43 , batch: 284 , training loss: 4.053970\n",
      "[INFO] Epoch: 43 , batch: 285 , training loss: 3.990283\n",
      "[INFO] Epoch: 43 , batch: 286 , training loss: 4.005549\n",
      "[INFO] Epoch: 43 , batch: 287 , training loss: 3.957172\n",
      "[INFO] Epoch: 43 , batch: 288 , training loss: 3.925639\n",
      "[INFO] Epoch: 43 , batch: 289 , training loss: 3.991297\n",
      "[INFO] Epoch: 43 , batch: 290 , training loss: 3.771074\n",
      "[INFO] Epoch: 43 , batch: 291 , training loss: 3.763913\n",
      "[INFO] Epoch: 43 , batch: 292 , training loss: 3.853247\n",
      "[INFO] Epoch: 43 , batch: 293 , training loss: 3.779067\n",
      "[INFO] Epoch: 43 , batch: 294 , training loss: 4.434434\n",
      "[INFO] Epoch: 43 , batch: 295 , training loss: 4.228764\n",
      "[INFO] Epoch: 43 , batch: 296 , training loss: 4.144861\n",
      "[INFO] Epoch: 43 , batch: 297 , training loss: 4.094625\n",
      "[INFO] Epoch: 43 , batch: 298 , training loss: 3.936606\n",
      "[INFO] Epoch: 43 , batch: 299 , training loss: 3.988898\n",
      "[INFO] Epoch: 43 , batch: 300 , training loss: 3.982252\n",
      "[INFO] Epoch: 43 , batch: 301 , training loss: 3.880239\n",
      "[INFO] Epoch: 43 , batch: 302 , training loss: 4.078996\n",
      "[INFO] Epoch: 43 , batch: 303 , training loss: 4.078751\n",
      "[INFO] Epoch: 43 , batch: 304 , training loss: 4.214381\n",
      "[INFO] Epoch: 43 , batch: 305 , training loss: 4.038942\n",
      "[INFO] Epoch: 43 , batch: 306 , training loss: 4.168760\n",
      "[INFO] Epoch: 43 , batch: 307 , training loss: 4.180382\n",
      "[INFO] Epoch: 43 , batch: 308 , training loss: 3.990136\n",
      "[INFO] Epoch: 43 , batch: 309 , training loss: 3.974482\n",
      "[INFO] Epoch: 43 , batch: 310 , training loss: 3.910391\n",
      "[INFO] Epoch: 43 , batch: 311 , training loss: 3.900367\n",
      "[INFO] Epoch: 43 , batch: 312 , training loss: 3.812988\n",
      "[INFO] Epoch: 43 , batch: 313 , training loss: 3.922731\n",
      "[INFO] Epoch: 43 , batch: 314 , training loss: 3.977614\n",
      "[INFO] Epoch: 43 , batch: 315 , training loss: 4.071571\n",
      "[INFO] Epoch: 43 , batch: 316 , training loss: 4.294319\n",
      "[INFO] Epoch: 43 , batch: 317 , training loss: 4.687549\n",
      "[INFO] Epoch: 43 , batch: 318 , training loss: 4.788609\n",
      "[INFO] Epoch: 43 , batch: 319 , training loss: 4.458071\n",
      "[INFO] Epoch: 43 , batch: 320 , training loss: 4.029696\n",
      "[INFO] Epoch: 43 , batch: 321 , training loss: 3.854119\n",
      "[INFO] Epoch: 43 , batch: 322 , training loss: 3.952428\n",
      "[INFO] Epoch: 43 , batch: 323 , training loss: 3.976989\n",
      "[INFO] Epoch: 43 , batch: 324 , training loss: 3.956173\n",
      "[INFO] Epoch: 43 , batch: 325 , training loss: 4.070817\n",
      "[INFO] Epoch: 43 , batch: 326 , training loss: 4.139967\n",
      "[INFO] Epoch: 43 , batch: 327 , training loss: 4.072287\n",
      "[INFO] Epoch: 43 , batch: 328 , training loss: 4.088562\n",
      "[INFO] Epoch: 43 , batch: 329 , training loss: 3.982042\n",
      "[INFO] Epoch: 43 , batch: 330 , training loss: 3.957047\n",
      "[INFO] Epoch: 43 , batch: 331 , training loss: 4.122499\n",
      "[INFO] Epoch: 43 , batch: 332 , training loss: 3.981273\n",
      "[INFO] Epoch: 43 , batch: 333 , training loss: 3.936090\n",
      "[INFO] Epoch: 43 , batch: 334 , training loss: 3.947585\n",
      "[INFO] Epoch: 43 , batch: 335 , training loss: 4.085855\n",
      "[INFO] Epoch: 43 , batch: 336 , training loss: 4.095826\n",
      "[INFO] Epoch: 43 , batch: 337 , training loss: 4.133423\n",
      "[INFO] Epoch: 43 , batch: 338 , training loss: 4.339777\n",
      "[INFO] Epoch: 43 , batch: 339 , training loss: 4.166652\n",
      "[INFO] Epoch: 43 , batch: 340 , training loss: 4.342649\n",
      "[INFO] Epoch: 43 , batch: 341 , training loss: 4.113703\n",
      "[INFO] Epoch: 43 , batch: 342 , training loss: 3.896763\n",
      "[INFO] Epoch: 43 , batch: 343 , training loss: 3.978115\n",
      "[INFO] Epoch: 43 , batch: 344 , training loss: 3.839169\n",
      "[INFO] Epoch: 43 , batch: 345 , training loss: 3.958883\n",
      "[INFO] Epoch: 43 , batch: 346 , training loss: 3.990563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 43 , batch: 347 , training loss: 3.919128\n",
      "[INFO] Epoch: 43 , batch: 348 , training loss: 4.034727\n",
      "[INFO] Epoch: 43 , batch: 349 , training loss: 4.112951\n",
      "[INFO] Epoch: 43 , batch: 350 , training loss: 3.953278\n",
      "[INFO] Epoch: 43 , batch: 351 , training loss: 4.054953\n",
      "[INFO] Epoch: 43 , batch: 352 , training loss: 4.059086\n",
      "[INFO] Epoch: 43 , batch: 353 , training loss: 4.049063\n",
      "[INFO] Epoch: 43 , batch: 354 , training loss: 4.143483\n",
      "[INFO] Epoch: 43 , batch: 355 , training loss: 4.129113\n",
      "[INFO] Epoch: 43 , batch: 356 , training loss: 3.990499\n",
      "[INFO] Epoch: 43 , batch: 357 , training loss: 4.068197\n",
      "[INFO] Epoch: 43 , batch: 358 , training loss: 3.968598\n",
      "[INFO] Epoch: 43 , batch: 359 , training loss: 3.991387\n",
      "[INFO] Epoch: 43 , batch: 360 , training loss: 4.069403\n",
      "[INFO] Epoch: 43 , batch: 361 , training loss: 4.052797\n",
      "[INFO] Epoch: 43 , batch: 362 , training loss: 4.148405\n",
      "[INFO] Epoch: 43 , batch: 363 , training loss: 4.038948\n",
      "[INFO] Epoch: 43 , batch: 364 , training loss: 4.099900\n",
      "[INFO] Epoch: 43 , batch: 365 , training loss: 4.008541\n",
      "[INFO] Epoch: 43 , batch: 366 , training loss: 4.112981\n",
      "[INFO] Epoch: 43 , batch: 367 , training loss: 4.163261\n",
      "[INFO] Epoch: 43 , batch: 368 , training loss: 4.560426\n",
      "[INFO] Epoch: 43 , batch: 369 , training loss: 4.231169\n",
      "[INFO] Epoch: 43 , batch: 370 , training loss: 4.003953\n",
      "[INFO] Epoch: 43 , batch: 371 , training loss: 4.436811\n",
      "[INFO] Epoch: 43 , batch: 372 , training loss: 4.709683\n",
      "[INFO] Epoch: 43 , batch: 373 , training loss: 4.724249\n",
      "[INFO] Epoch: 43 , batch: 374 , training loss: 4.848180\n",
      "[INFO] Epoch: 43 , batch: 375 , training loss: 4.827027\n",
      "[INFO] Epoch: 43 , batch: 376 , training loss: 4.694563\n",
      "[INFO] Epoch: 43 , batch: 377 , training loss: 4.447251\n",
      "[INFO] Epoch: 43 , batch: 378 , training loss: 4.552121\n",
      "[INFO] Epoch: 43 , batch: 379 , training loss: 4.516739\n",
      "[INFO] Epoch: 43 , batch: 380 , training loss: 4.695789\n",
      "[INFO] Epoch: 43 , batch: 381 , training loss: 4.405416\n",
      "[INFO] Epoch: 43 , batch: 382 , training loss: 4.654291\n",
      "[INFO] Epoch: 43 , batch: 383 , training loss: 4.733293\n",
      "[INFO] Epoch: 43 , batch: 384 , training loss: 4.672677\n",
      "[INFO] Epoch: 43 , batch: 385 , training loss: 4.363773\n",
      "[INFO] Epoch: 43 , batch: 386 , training loss: 4.616547\n",
      "[INFO] Epoch: 43 , batch: 387 , training loss: 4.558080\n",
      "[INFO] Epoch: 43 , batch: 388 , training loss: 4.375753\n",
      "[INFO] Epoch: 43 , batch: 389 , training loss: 4.182698\n",
      "[INFO] Epoch: 43 , batch: 390 , training loss: 4.217842\n",
      "[INFO] Epoch: 43 , batch: 391 , training loss: 4.241838\n",
      "[INFO] Epoch: 43 , batch: 392 , training loss: 4.603987\n",
      "[INFO] Epoch: 43 , batch: 393 , training loss: 4.498363\n",
      "[INFO] Epoch: 43 , batch: 394 , training loss: 4.586111\n",
      "[INFO] Epoch: 43 , batch: 395 , training loss: 4.404710\n",
      "[INFO] Epoch: 43 , batch: 396 , training loss: 4.212707\n",
      "[INFO] Epoch: 43 , batch: 397 , training loss: 4.350751\n",
      "[INFO] Epoch: 43 , batch: 398 , training loss: 4.214743\n",
      "[INFO] Epoch: 43 , batch: 399 , training loss: 4.288586\n",
      "[INFO] Epoch: 43 , batch: 400 , training loss: 4.283900\n",
      "[INFO] Epoch: 43 , batch: 401 , training loss: 4.697519\n",
      "[INFO] Epoch: 43 , batch: 402 , training loss: 4.417404\n",
      "[INFO] Epoch: 43 , batch: 403 , training loss: 4.265433\n",
      "[INFO] Epoch: 43 , batch: 404 , training loss: 4.414429\n",
      "[INFO] Epoch: 43 , batch: 405 , training loss: 4.503061\n",
      "[INFO] Epoch: 43 , batch: 406 , training loss: 4.382235\n",
      "[INFO] Epoch: 43 , batch: 407 , training loss: 4.404955\n",
      "[INFO] Epoch: 43 , batch: 408 , training loss: 4.382011\n",
      "[INFO] Epoch: 43 , batch: 409 , training loss: 4.407537\n",
      "[INFO] Epoch: 43 , batch: 410 , training loss: 4.469028\n",
      "[INFO] Epoch: 43 , batch: 411 , training loss: 4.627786\n",
      "[INFO] Epoch: 43 , batch: 412 , training loss: 4.469427\n",
      "[INFO] Epoch: 43 , batch: 413 , training loss: 4.341313\n",
      "[INFO] Epoch: 43 , batch: 414 , training loss: 4.361409\n",
      "[INFO] Epoch: 43 , batch: 415 , training loss: 4.434684\n",
      "[INFO] Epoch: 43 , batch: 416 , training loss: 4.471264\n",
      "[INFO] Epoch: 43 , batch: 417 , training loss: 4.410506\n",
      "[INFO] Epoch: 43 , batch: 418 , training loss: 4.464566\n",
      "[INFO] Epoch: 43 , batch: 419 , training loss: 4.430329\n",
      "[INFO] Epoch: 43 , batch: 420 , training loss: 4.398671\n",
      "[INFO] Epoch: 43 , batch: 421 , training loss: 4.359192\n",
      "[INFO] Epoch: 43 , batch: 422 , training loss: 4.203072\n",
      "[INFO] Epoch: 43 , batch: 423 , training loss: 4.444365\n",
      "[INFO] Epoch: 43 , batch: 424 , training loss: 4.626916\n",
      "[INFO] Epoch: 43 , batch: 425 , training loss: 4.473359\n",
      "[INFO] Epoch: 43 , batch: 426 , training loss: 4.219683\n",
      "[INFO] Epoch: 43 , batch: 427 , training loss: 4.461794\n",
      "[INFO] Epoch: 43 , batch: 428 , training loss: 4.331306\n",
      "[INFO] Epoch: 43 , batch: 429 , training loss: 4.229719\n",
      "[INFO] Epoch: 43 , batch: 430 , training loss: 4.443578\n",
      "[INFO] Epoch: 43 , batch: 431 , training loss: 4.072937\n",
      "[INFO] Epoch: 43 , batch: 432 , training loss: 4.115215\n",
      "[INFO] Epoch: 43 , batch: 433 , training loss: 4.153381\n",
      "[INFO] Epoch: 43 , batch: 434 , training loss: 4.035679\n",
      "[INFO] Epoch: 43 , batch: 435 , training loss: 4.382314\n",
      "[INFO] Epoch: 43 , batch: 436 , training loss: 4.428629\n",
      "[INFO] Epoch: 43 , batch: 437 , training loss: 4.218070\n",
      "[INFO] Epoch: 43 , batch: 438 , training loss: 4.089586\n",
      "[INFO] Epoch: 43 , batch: 439 , training loss: 4.331999\n",
      "[INFO] Epoch: 43 , batch: 440 , training loss: 4.438348\n",
      "[INFO] Epoch: 43 , batch: 441 , training loss: 4.521644\n",
      "[INFO] Epoch: 43 , batch: 442 , training loss: 4.289411\n",
      "[INFO] Epoch: 43 , batch: 443 , training loss: 4.449233\n",
      "[INFO] Epoch: 43 , batch: 444 , training loss: 4.079639\n",
      "[INFO] Epoch: 43 , batch: 445 , training loss: 3.969765\n",
      "[INFO] Epoch: 43 , batch: 446 , training loss: 3.909145\n",
      "[INFO] Epoch: 43 , batch: 447 , training loss: 4.114677\n",
      "[INFO] Epoch: 43 , batch: 448 , training loss: 4.238769\n",
      "[INFO] Epoch: 43 , batch: 449 , training loss: 4.620384\n",
      "[INFO] Epoch: 43 , batch: 450 , training loss: 4.681414\n",
      "[INFO] Epoch: 43 , batch: 451 , training loss: 4.567385\n",
      "[INFO] Epoch: 43 , batch: 452 , training loss: 4.392414\n",
      "[INFO] Epoch: 43 , batch: 453 , training loss: 4.167354\n",
      "[INFO] Epoch: 43 , batch: 454 , training loss: 4.310475\n",
      "[INFO] Epoch: 43 , batch: 455 , training loss: 4.369842\n",
      "[INFO] Epoch: 43 , batch: 456 , training loss: 4.344876\n",
      "[INFO] Epoch: 43 , batch: 457 , training loss: 4.418514\n",
      "[INFO] Epoch: 43 , batch: 458 , training loss: 4.161767\n",
      "[INFO] Epoch: 43 , batch: 459 , training loss: 4.151793\n",
      "[INFO] Epoch: 43 , batch: 460 , training loss: 4.258955\n",
      "[INFO] Epoch: 43 , batch: 461 , training loss: 4.214797\n",
      "[INFO] Epoch: 43 , batch: 462 , training loss: 4.290858\n",
      "[INFO] Epoch: 43 , batch: 463 , training loss: 4.206586\n",
      "[INFO] Epoch: 43 , batch: 464 , training loss: 4.376480\n",
      "[INFO] Epoch: 43 , batch: 465 , training loss: 4.310566\n",
      "[INFO] Epoch: 43 , batch: 466 , training loss: 4.420242\n",
      "[INFO] Epoch: 43 , batch: 467 , training loss: 4.373000\n",
      "[INFO] Epoch: 43 , batch: 468 , training loss: 4.333478\n",
      "[INFO] Epoch: 43 , batch: 469 , training loss: 4.355802\n",
      "[INFO] Epoch: 43 , batch: 470 , training loss: 4.189883\n",
      "[INFO] Epoch: 43 , batch: 471 , training loss: 4.275865\n",
      "[INFO] Epoch: 43 , batch: 472 , training loss: 4.350284\n",
      "[INFO] Epoch: 43 , batch: 473 , training loss: 4.257168\n",
      "[INFO] Epoch: 43 , batch: 474 , training loss: 4.035821\n",
      "[INFO] Epoch: 43 , batch: 475 , training loss: 3.914566\n",
      "[INFO] Epoch: 43 , batch: 476 , training loss: 4.327944\n",
      "[INFO] Epoch: 43 , batch: 477 , training loss: 4.442251\n",
      "[INFO] Epoch: 43 , batch: 478 , training loss: 4.440279\n",
      "[INFO] Epoch: 43 , batch: 479 , training loss: 4.424448\n",
      "[INFO] Epoch: 43 , batch: 480 , training loss: 4.562653\n",
      "[INFO] Epoch: 43 , batch: 481 , training loss: 4.421484\n",
      "[INFO] Epoch: 43 , batch: 482 , training loss: 4.524409\n",
      "[INFO] Epoch: 43 , batch: 483 , training loss: 4.372427\n",
      "[INFO] Epoch: 43 , batch: 484 , training loss: 4.171269\n",
      "[INFO] Epoch: 43 , batch: 485 , training loss: 4.271878\n",
      "[INFO] Epoch: 43 , batch: 486 , training loss: 4.167620\n",
      "[INFO] Epoch: 43 , batch: 487 , training loss: 4.152372\n",
      "[INFO] Epoch: 43 , batch: 488 , training loss: 4.329382\n",
      "[INFO] Epoch: 43 , batch: 489 , training loss: 4.230103\n",
      "[INFO] Epoch: 43 , batch: 490 , training loss: 4.309480\n",
      "[INFO] Epoch: 43 , batch: 491 , training loss: 4.202055\n",
      "[INFO] Epoch: 43 , batch: 492 , training loss: 4.197085\n",
      "[INFO] Epoch: 43 , batch: 493 , training loss: 4.353365\n",
      "[INFO] Epoch: 43 , batch: 494 , training loss: 4.274189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 43 , batch: 495 , training loss: 4.437093\n",
      "[INFO] Epoch: 43 , batch: 496 , training loss: 4.295676\n",
      "[INFO] Epoch: 43 , batch: 497 , training loss: 4.328935\n",
      "[INFO] Epoch: 43 , batch: 498 , training loss: 4.312044\n",
      "[INFO] Epoch: 43 , batch: 499 , training loss: 4.384213\n",
      "[INFO] Epoch: 43 , batch: 500 , training loss: 4.540695\n",
      "[INFO] Epoch: 43 , batch: 501 , training loss: 4.842070\n",
      "[INFO] Epoch: 43 , batch: 502 , training loss: 4.875947\n",
      "[INFO] Epoch: 43 , batch: 503 , training loss: 4.504588\n",
      "[INFO] Epoch: 43 , batch: 504 , training loss: 4.664537\n",
      "[INFO] Epoch: 43 , batch: 505 , training loss: 4.639269\n",
      "[INFO] Epoch: 43 , batch: 506 , training loss: 4.616595\n",
      "[INFO] Epoch: 43 , batch: 507 , training loss: 4.664185\n",
      "[INFO] Epoch: 43 , batch: 508 , training loss: 4.574562\n",
      "[INFO] Epoch: 43 , batch: 509 , training loss: 4.388742\n",
      "[INFO] Epoch: 43 , batch: 510 , training loss: 4.486927\n",
      "[INFO] Epoch: 43 , batch: 511 , training loss: 4.399128\n",
      "[INFO] Epoch: 43 , batch: 512 , training loss: 4.495615\n",
      "[INFO] Epoch: 43 , batch: 513 , training loss: 4.766671\n",
      "[INFO] Epoch: 43 , batch: 514 , training loss: 4.378285\n",
      "[INFO] Epoch: 43 , batch: 515 , training loss: 4.647195\n",
      "[INFO] Epoch: 43 , batch: 516 , training loss: 4.448857\n",
      "[INFO] Epoch: 43 , batch: 517 , training loss: 4.405377\n",
      "[INFO] Epoch: 43 , batch: 518 , training loss: 4.358272\n",
      "[INFO] Epoch: 43 , batch: 519 , training loss: 4.222402\n",
      "[INFO] Epoch: 43 , batch: 520 , training loss: 4.441594\n",
      "[INFO] Epoch: 43 , batch: 521 , training loss: 4.437385\n",
      "[INFO] Epoch: 43 , batch: 522 , training loss: 4.526175\n",
      "[INFO] Epoch: 43 , batch: 523 , training loss: 4.428050\n",
      "[INFO] Epoch: 43 , batch: 524 , training loss: 4.717665\n",
      "[INFO] Epoch: 43 , batch: 525 , training loss: 4.608546\n",
      "[INFO] Epoch: 43 , batch: 526 , training loss: 4.366983\n",
      "[INFO] Epoch: 43 , batch: 527 , training loss: 4.433469\n",
      "[INFO] Epoch: 43 , batch: 528 , training loss: 4.443997\n",
      "[INFO] Epoch: 43 , batch: 529 , training loss: 4.426266\n",
      "[INFO] Epoch: 43 , batch: 530 , training loss: 4.277721\n",
      "[INFO] Epoch: 43 , batch: 531 , training loss: 4.413703\n",
      "[INFO] Epoch: 43 , batch: 532 , training loss: 4.336503\n",
      "[INFO] Epoch: 43 , batch: 533 , training loss: 4.448514\n",
      "[INFO] Epoch: 43 , batch: 534 , training loss: 4.471117\n",
      "[INFO] Epoch: 43 , batch: 535 , training loss: 4.450423\n",
      "[INFO] Epoch: 43 , batch: 536 , training loss: 4.306905\n",
      "[INFO] Epoch: 43 , batch: 537 , training loss: 4.279047\n",
      "[INFO] Epoch: 43 , batch: 538 , training loss: 4.383400\n",
      "[INFO] Epoch: 43 , batch: 539 , training loss: 4.477996\n",
      "[INFO] Epoch: 43 , batch: 540 , training loss: 5.006558\n",
      "[INFO] Epoch: 43 , batch: 541 , training loss: 4.808977\n",
      "[INFO] Epoch: 43 , batch: 542 , training loss: 4.700744\n",
      "[INFO] Epoch: 44 , batch: 0 , training loss: 3.740594\n",
      "[INFO] Epoch: 44 , batch: 1 , training loss: 3.542812\n",
      "[INFO] Epoch: 44 , batch: 2 , training loss: 3.709601\n",
      "[INFO] Epoch: 44 , batch: 3 , training loss: 3.596727\n",
      "[INFO] Epoch: 44 , batch: 4 , training loss: 3.967942\n",
      "[INFO] Epoch: 44 , batch: 5 , training loss: 3.658246\n",
      "[INFO] Epoch: 44 , batch: 6 , training loss: 4.004311\n",
      "[INFO] Epoch: 44 , batch: 7 , training loss: 3.893800\n",
      "[INFO] Epoch: 44 , batch: 8 , training loss: 3.598761\n",
      "[INFO] Epoch: 44 , batch: 9 , training loss: 3.851876\n",
      "[INFO] Epoch: 44 , batch: 10 , training loss: 3.808645\n",
      "[INFO] Epoch: 44 , batch: 11 , training loss: 3.756040\n",
      "[INFO] Epoch: 44 , batch: 12 , training loss: 3.620690\n",
      "[INFO] Epoch: 44 , batch: 13 , training loss: 3.649663\n",
      "[INFO] Epoch: 44 , batch: 14 , training loss: 3.539843\n",
      "[INFO] Epoch: 44 , batch: 15 , training loss: 3.757978\n",
      "[INFO] Epoch: 44 , batch: 16 , training loss: 3.610546\n",
      "[INFO] Epoch: 44 , batch: 17 , training loss: 3.750543\n",
      "[INFO] Epoch: 44 , batch: 18 , training loss: 3.713550\n",
      "[INFO] Epoch: 44 , batch: 19 , training loss: 3.473797\n",
      "[INFO] Epoch: 44 , batch: 20 , training loss: 3.435999\n",
      "[INFO] Epoch: 44 , batch: 21 , training loss: 3.569519\n",
      "[INFO] Epoch: 44 , batch: 22 , training loss: 3.480312\n",
      "[INFO] Epoch: 44 , batch: 23 , training loss: 3.680278\n",
      "[INFO] Epoch: 44 , batch: 24 , training loss: 3.506180\n",
      "[INFO] Epoch: 44 , batch: 25 , training loss: 3.613761\n",
      "[INFO] Epoch: 44 , batch: 26 , training loss: 3.508441\n",
      "[INFO] Epoch: 44 , batch: 27 , training loss: 3.477110\n",
      "[INFO] Epoch: 44 , batch: 28 , training loss: 3.663007\n",
      "[INFO] Epoch: 44 , batch: 29 , training loss: 3.456952\n",
      "[INFO] Epoch: 44 , batch: 30 , training loss: 3.488692\n",
      "[INFO] Epoch: 44 , batch: 31 , training loss: 3.600549\n",
      "[INFO] Epoch: 44 , batch: 32 , training loss: 3.545867\n",
      "[INFO] Epoch: 44 , batch: 33 , training loss: 3.579992\n",
      "[INFO] Epoch: 44 , batch: 34 , training loss: 3.579310\n",
      "[INFO] Epoch: 44 , batch: 35 , training loss: 3.555689\n",
      "[INFO] Epoch: 44 , batch: 36 , training loss: 3.617850\n",
      "[INFO] Epoch: 44 , batch: 37 , training loss: 3.486797\n",
      "[INFO] Epoch: 44 , batch: 38 , training loss: 3.572442\n",
      "[INFO] Epoch: 44 , batch: 39 , training loss: 3.385908\n",
      "[INFO] Epoch: 44 , batch: 40 , training loss: 3.577744\n",
      "[INFO] Epoch: 44 , batch: 41 , training loss: 3.525877\n",
      "[INFO] Epoch: 44 , batch: 42 , training loss: 4.007823\n",
      "[INFO] Epoch: 44 , batch: 43 , training loss: 3.722509\n",
      "[INFO] Epoch: 44 , batch: 44 , training loss: 4.081982\n",
      "[INFO] Epoch: 44 , batch: 45 , training loss: 4.029559\n",
      "[INFO] Epoch: 44 , batch: 46 , training loss: 3.961097\n",
      "[INFO] Epoch: 44 , batch: 47 , training loss: 3.596425\n",
      "[INFO] Epoch: 44 , batch: 48 , training loss: 3.590005\n",
      "[INFO] Epoch: 44 , batch: 49 , training loss: 3.833957\n",
      "[INFO] Epoch: 44 , batch: 50 , training loss: 3.594160\n",
      "[INFO] Epoch: 44 , batch: 51 , training loss: 3.823427\n",
      "[INFO] Epoch: 44 , batch: 52 , training loss: 3.641373\n",
      "[INFO] Epoch: 44 , batch: 53 , training loss: 3.750987\n",
      "[INFO] Epoch: 44 , batch: 54 , training loss: 3.763035\n",
      "[INFO] Epoch: 44 , batch: 55 , training loss: 3.846260\n",
      "[INFO] Epoch: 44 , batch: 56 , training loss: 3.662885\n",
      "[INFO] Epoch: 44 , batch: 57 , training loss: 3.593358\n",
      "[INFO] Epoch: 44 , batch: 58 , training loss: 3.631792\n",
      "[INFO] Epoch: 44 , batch: 59 , training loss: 3.738850\n",
      "[INFO] Epoch: 44 , batch: 60 , training loss: 3.671440\n",
      "[INFO] Epoch: 44 , batch: 61 , training loss: 3.744143\n",
      "[INFO] Epoch: 44 , batch: 62 , training loss: 3.628413\n",
      "[INFO] Epoch: 44 , batch: 63 , training loss: 3.810673\n",
      "[INFO] Epoch: 44 , batch: 64 , training loss: 4.021835\n",
      "[INFO] Epoch: 44 , batch: 65 , training loss: 3.739913\n",
      "[INFO] Epoch: 44 , batch: 66 , training loss: 3.579237\n",
      "[INFO] Epoch: 44 , batch: 67 , training loss: 3.589846\n",
      "[INFO] Epoch: 44 , batch: 68 , training loss: 3.749785\n",
      "[INFO] Epoch: 44 , batch: 69 , training loss: 3.697227\n",
      "[INFO] Epoch: 44 , batch: 70 , training loss: 3.934261\n",
      "[INFO] Epoch: 44 , batch: 71 , training loss: 3.763712\n",
      "[INFO] Epoch: 44 , batch: 72 , training loss: 3.842822\n",
      "[INFO] Epoch: 44 , batch: 73 , training loss: 3.777076\n",
      "[INFO] Epoch: 44 , batch: 74 , training loss: 3.883660\n",
      "[INFO] Epoch: 44 , batch: 75 , training loss: 3.758364\n",
      "[INFO] Epoch: 44 , batch: 76 , training loss: 3.855284\n",
      "[INFO] Epoch: 44 , batch: 77 , training loss: 3.781920\n",
      "[INFO] Epoch: 44 , batch: 78 , training loss: 3.885159\n",
      "[INFO] Epoch: 44 , batch: 79 , training loss: 3.728107\n",
      "[INFO] Epoch: 44 , batch: 80 , training loss: 3.957310\n",
      "[INFO] Epoch: 44 , batch: 81 , training loss: 3.851183\n",
      "[INFO] Epoch: 44 , batch: 82 , training loss: 3.838609\n",
      "[INFO] Epoch: 44 , batch: 83 , training loss: 3.892010\n",
      "[INFO] Epoch: 44 , batch: 84 , training loss: 3.883168\n",
      "[INFO] Epoch: 44 , batch: 85 , training loss: 3.980966\n",
      "[INFO] Epoch: 44 , batch: 86 , training loss: 3.919439\n",
      "[INFO] Epoch: 44 , batch: 87 , training loss: 3.844279\n",
      "[INFO] Epoch: 44 , batch: 88 , training loss: 4.010313\n",
      "[INFO] Epoch: 44 , batch: 89 , training loss: 3.796201\n",
      "[INFO] Epoch: 44 , batch: 90 , training loss: 3.890049\n",
      "[INFO] Epoch: 44 , batch: 91 , training loss: 3.808819\n",
      "[INFO] Epoch: 44 , batch: 92 , training loss: 3.829146\n",
      "[INFO] Epoch: 44 , batch: 93 , training loss: 3.964692\n",
      "[INFO] Epoch: 44 , batch: 94 , training loss: 4.067850\n",
      "[INFO] Epoch: 44 , batch: 95 , training loss: 3.845542\n",
      "[INFO] Epoch: 44 , batch: 96 , training loss: 3.840351\n",
      "[INFO] Epoch: 44 , batch: 97 , training loss: 3.767980\n",
      "[INFO] Epoch: 44 , batch: 98 , training loss: 3.667664\n",
      "[INFO] Epoch: 44 , batch: 99 , training loss: 3.828059\n",
      "[INFO] Epoch: 44 , batch: 100 , training loss: 3.742043\n",
      "[INFO] Epoch: 44 , batch: 101 , training loss: 3.720597\n",
      "[INFO] Epoch: 44 , batch: 102 , training loss: 3.909260\n",
      "[INFO] Epoch: 44 , batch: 103 , training loss: 3.713693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 44 , batch: 104 , training loss: 3.665237\n",
      "[INFO] Epoch: 44 , batch: 105 , training loss: 3.896258\n",
      "[INFO] Epoch: 44 , batch: 106 , training loss: 3.930190\n",
      "[INFO] Epoch: 44 , batch: 107 , training loss: 3.769424\n",
      "[INFO] Epoch: 44 , batch: 108 , training loss: 3.713797\n",
      "[INFO] Epoch: 44 , batch: 109 , training loss: 3.634322\n",
      "[INFO] Epoch: 44 , batch: 110 , training loss: 3.812593\n",
      "[INFO] Epoch: 44 , batch: 111 , training loss: 3.874165\n",
      "[INFO] Epoch: 44 , batch: 112 , training loss: 3.792797\n",
      "[INFO] Epoch: 44 , batch: 113 , training loss: 3.820277\n",
      "[INFO] Epoch: 44 , batch: 114 , training loss: 3.782302\n",
      "[INFO] Epoch: 44 , batch: 115 , training loss: 3.816821\n",
      "[INFO] Epoch: 44 , batch: 116 , training loss: 3.719252\n",
      "[INFO] Epoch: 44 , batch: 117 , training loss: 3.915402\n",
      "[INFO] Epoch: 44 , batch: 118 , training loss: 3.873439\n",
      "[INFO] Epoch: 44 , batch: 119 , training loss: 4.040656\n",
      "[INFO] Epoch: 44 , batch: 120 , training loss: 4.014151\n",
      "[INFO] Epoch: 44 , batch: 121 , training loss: 3.892930\n",
      "[INFO] Epoch: 44 , batch: 122 , training loss: 3.801725\n",
      "[INFO] Epoch: 44 , batch: 123 , training loss: 3.780654\n",
      "[INFO] Epoch: 44 , batch: 124 , training loss: 3.846900\n",
      "[INFO] Epoch: 44 , batch: 125 , training loss: 3.718325\n",
      "[INFO] Epoch: 44 , batch: 126 , training loss: 3.735972\n",
      "[INFO] Epoch: 44 , batch: 127 , training loss: 3.756517\n",
      "[INFO] Epoch: 44 , batch: 128 , training loss: 3.860575\n",
      "[INFO] Epoch: 44 , batch: 129 , training loss: 3.828233\n",
      "[INFO] Epoch: 44 , batch: 130 , training loss: 3.839163\n",
      "[INFO] Epoch: 44 , batch: 131 , training loss: 3.818064\n",
      "[INFO] Epoch: 44 , batch: 132 , training loss: 3.857059\n",
      "[INFO] Epoch: 44 , batch: 133 , training loss: 3.795212\n",
      "[INFO] Epoch: 44 , batch: 134 , training loss: 3.583654\n",
      "[INFO] Epoch: 44 , batch: 135 , training loss: 3.660711\n",
      "[INFO] Epoch: 44 , batch: 136 , training loss: 3.911665\n",
      "[INFO] Epoch: 44 , batch: 137 , training loss: 3.846540\n",
      "[INFO] Epoch: 44 , batch: 138 , training loss: 3.896961\n",
      "[INFO] Epoch: 44 , batch: 139 , training loss: 4.429008\n",
      "[INFO] Epoch: 44 , batch: 140 , training loss: 4.205577\n",
      "[INFO] Epoch: 44 , batch: 141 , training loss: 4.014561\n",
      "[INFO] Epoch: 44 , batch: 142 , training loss: 3.719065\n",
      "[INFO] Epoch: 44 , batch: 143 , training loss: 3.883072\n",
      "[INFO] Epoch: 44 , batch: 144 , training loss: 3.721315\n",
      "[INFO] Epoch: 44 , batch: 145 , training loss: 3.795538\n",
      "[INFO] Epoch: 44 , batch: 146 , training loss: 3.986891\n",
      "[INFO] Epoch: 44 , batch: 147 , training loss: 3.631521\n",
      "[INFO] Epoch: 44 , batch: 148 , training loss: 3.602980\n",
      "[INFO] Epoch: 44 , batch: 149 , training loss: 3.702417\n",
      "[INFO] Epoch: 44 , batch: 150 , training loss: 3.969858\n",
      "[INFO] Epoch: 44 , batch: 151 , training loss: 3.812689\n",
      "[INFO] Epoch: 44 , batch: 152 , training loss: 3.835630\n",
      "[INFO] Epoch: 44 , batch: 153 , training loss: 3.879458\n",
      "[INFO] Epoch: 44 , batch: 154 , training loss: 3.949557\n",
      "[INFO] Epoch: 44 , batch: 155 , training loss: 4.142924\n",
      "[INFO] Epoch: 44 , batch: 156 , training loss: 3.912903\n",
      "[INFO] Epoch: 44 , batch: 157 , training loss: 3.863336\n",
      "[INFO] Epoch: 44 , batch: 158 , training loss: 3.953501\n",
      "[INFO] Epoch: 44 , batch: 159 , training loss: 3.888053\n",
      "[INFO] Epoch: 44 , batch: 160 , training loss: 4.173125\n",
      "[INFO] Epoch: 44 , batch: 161 , training loss: 4.175791\n",
      "[INFO] Epoch: 44 , batch: 162 , training loss: 4.191115\n",
      "[INFO] Epoch: 44 , batch: 163 , training loss: 4.355199\n",
      "[INFO] Epoch: 44 , batch: 164 , training loss: 4.292739\n",
      "[INFO] Epoch: 44 , batch: 165 , training loss: 4.258490\n",
      "[INFO] Epoch: 44 , batch: 166 , training loss: 4.119556\n",
      "[INFO] Epoch: 44 , batch: 167 , training loss: 4.163012\n",
      "[INFO] Epoch: 44 , batch: 168 , training loss: 3.794930\n",
      "[INFO] Epoch: 44 , batch: 169 , training loss: 3.787806\n",
      "[INFO] Epoch: 44 , batch: 170 , training loss: 4.011698\n",
      "[INFO] Epoch: 44 , batch: 171 , training loss: 3.439837\n",
      "[INFO] Epoch: 44 , batch: 172 , training loss: 3.686770\n",
      "[INFO] Epoch: 44 , batch: 173 , training loss: 3.997745\n",
      "[INFO] Epoch: 44 , batch: 174 , training loss: 4.477885\n",
      "[INFO] Epoch: 44 , batch: 175 , training loss: 4.722468\n",
      "[INFO] Epoch: 44 , batch: 176 , training loss: 4.391844\n",
      "[INFO] Epoch: 44 , batch: 177 , training loss: 4.000128\n",
      "[INFO] Epoch: 44 , batch: 178 , training loss: 4.022009\n",
      "[INFO] Epoch: 44 , batch: 179 , training loss: 4.085581\n",
      "[INFO] Epoch: 44 , batch: 180 , training loss: 4.021531\n",
      "[INFO] Epoch: 44 , batch: 181 , training loss: 4.315973\n",
      "[INFO] Epoch: 44 , batch: 182 , training loss: 4.258605\n",
      "[INFO] Epoch: 44 , batch: 183 , training loss: 4.252806\n",
      "[INFO] Epoch: 44 , batch: 184 , training loss: 4.134571\n",
      "[INFO] Epoch: 44 , batch: 185 , training loss: 4.099422\n",
      "[INFO] Epoch: 44 , batch: 186 , training loss: 4.256963\n",
      "[INFO] Epoch: 44 , batch: 187 , training loss: 4.325169\n",
      "[INFO] Epoch: 44 , batch: 188 , training loss: 4.336717\n",
      "[INFO] Epoch: 44 , batch: 189 , training loss: 4.246309\n",
      "[INFO] Epoch: 44 , batch: 190 , training loss: 4.294733\n",
      "[INFO] Epoch: 44 , batch: 191 , training loss: 4.383109\n",
      "[INFO] Epoch: 44 , batch: 192 , training loss: 4.220544\n",
      "[INFO] Epoch: 44 , batch: 193 , training loss: 4.329270\n",
      "[INFO] Epoch: 44 , batch: 194 , training loss: 4.281965\n",
      "[INFO] Epoch: 44 , batch: 195 , training loss: 4.200670\n",
      "[INFO] Epoch: 44 , batch: 196 , training loss: 4.058767\n",
      "[INFO] Epoch: 44 , batch: 197 , training loss: 4.143514\n",
      "[INFO] Epoch: 44 , batch: 198 , training loss: 4.061988\n",
      "[INFO] Epoch: 44 , batch: 199 , training loss: 4.183308\n",
      "[INFO] Epoch: 44 , batch: 200 , training loss: 4.106274\n",
      "[INFO] Epoch: 44 , batch: 201 , training loss: 4.008906\n",
      "[INFO] Epoch: 44 , batch: 202 , training loss: 4.003183\n",
      "[INFO] Epoch: 44 , batch: 203 , training loss: 4.142889\n",
      "[INFO] Epoch: 44 , batch: 204 , training loss: 4.226540\n",
      "[INFO] Epoch: 44 , batch: 205 , training loss: 3.828631\n",
      "[INFO] Epoch: 44 , batch: 206 , training loss: 3.761379\n",
      "[INFO] Epoch: 44 , batch: 207 , training loss: 3.748944\n",
      "[INFO] Epoch: 44 , batch: 208 , training loss: 4.069926\n",
      "[INFO] Epoch: 44 , batch: 209 , training loss: 4.075938\n",
      "[INFO] Epoch: 44 , batch: 210 , training loss: 4.058022\n",
      "[INFO] Epoch: 44 , batch: 211 , training loss: 4.066244\n",
      "[INFO] Epoch: 44 , batch: 212 , training loss: 4.145552\n",
      "[INFO] Epoch: 44 , batch: 213 , training loss: 4.116107\n",
      "[INFO] Epoch: 44 , batch: 214 , training loss: 4.218202\n",
      "[INFO] Epoch: 44 , batch: 215 , training loss: 4.376206\n",
      "[INFO] Epoch: 44 , batch: 216 , training loss: 4.088356\n",
      "[INFO] Epoch: 44 , batch: 217 , training loss: 4.015501\n",
      "[INFO] Epoch: 44 , batch: 218 , training loss: 4.037749\n",
      "[INFO] Epoch: 44 , batch: 219 , training loss: 4.140142\n",
      "[INFO] Epoch: 44 , batch: 220 , training loss: 3.953046\n",
      "[INFO] Epoch: 44 , batch: 221 , training loss: 3.962044\n",
      "[INFO] Epoch: 44 , batch: 222 , training loss: 4.090562\n",
      "[INFO] Epoch: 44 , batch: 223 , training loss: 4.232727\n",
      "[INFO] Epoch: 44 , batch: 224 , training loss: 4.281108\n",
      "[INFO] Epoch: 44 , batch: 225 , training loss: 4.149781\n",
      "[INFO] Epoch: 44 , batch: 226 , training loss: 4.277634\n",
      "[INFO] Epoch: 44 , batch: 227 , training loss: 4.242373\n",
      "[INFO] Epoch: 44 , batch: 228 , training loss: 4.265716\n",
      "[INFO] Epoch: 44 , batch: 229 , training loss: 4.134550\n",
      "[INFO] Epoch: 44 , batch: 230 , training loss: 4.002092\n",
      "[INFO] Epoch: 44 , batch: 231 , training loss: 3.853317\n",
      "[INFO] Epoch: 44 , batch: 232 , training loss: 3.984764\n",
      "[INFO] Epoch: 44 , batch: 233 , training loss: 4.035793\n",
      "[INFO] Epoch: 44 , batch: 234 , training loss: 3.714880\n",
      "[INFO] Epoch: 44 , batch: 235 , training loss: 3.829622\n",
      "[INFO] Epoch: 44 , batch: 236 , training loss: 3.932852\n",
      "[INFO] Epoch: 44 , batch: 237 , training loss: 4.158997\n",
      "[INFO] Epoch: 44 , batch: 238 , training loss: 3.938621\n",
      "[INFO] Epoch: 44 , batch: 239 , training loss: 3.954604\n",
      "[INFO] Epoch: 44 , batch: 240 , training loss: 3.991688\n",
      "[INFO] Epoch: 44 , batch: 241 , training loss: 3.824454\n",
      "[INFO] Epoch: 44 , batch: 242 , training loss: 3.832018\n",
      "[INFO] Epoch: 44 , batch: 243 , training loss: 4.098086\n",
      "[INFO] Epoch: 44 , batch: 244 , training loss: 4.056598\n",
      "[INFO] Epoch: 44 , batch: 245 , training loss: 4.027517\n",
      "[INFO] Epoch: 44 , batch: 246 , training loss: 3.739594\n",
      "[INFO] Epoch: 44 , batch: 247 , training loss: 3.900443\n",
      "[INFO] Epoch: 44 , batch: 248 , training loss: 3.980495\n",
      "[INFO] Epoch: 44 , batch: 249 , training loss: 3.951413\n",
      "[INFO] Epoch: 44 , batch: 250 , training loss: 3.781566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 44 , batch: 251 , training loss: 4.191169\n",
      "[INFO] Epoch: 44 , batch: 252 , training loss: 3.917352\n",
      "[INFO] Epoch: 44 , batch: 253 , training loss: 3.838393\n",
      "[INFO] Epoch: 44 , batch: 254 , training loss: 4.091787\n",
      "[INFO] Epoch: 44 , batch: 255 , training loss: 4.070030\n",
      "[INFO] Epoch: 44 , batch: 256 , training loss: 4.056110\n",
      "[INFO] Epoch: 44 , batch: 257 , training loss: 4.234103\n",
      "[INFO] Epoch: 44 , batch: 258 , training loss: 4.237196\n",
      "[INFO] Epoch: 44 , batch: 259 , training loss: 4.270636\n",
      "[INFO] Epoch: 44 , batch: 260 , training loss: 4.056900\n",
      "[INFO] Epoch: 44 , batch: 261 , training loss: 4.233602\n",
      "[INFO] Epoch: 44 , batch: 262 , training loss: 4.344142\n",
      "[INFO] Epoch: 44 , batch: 263 , training loss: 4.532402\n",
      "[INFO] Epoch: 44 , batch: 264 , training loss: 3.883313\n",
      "[INFO] Epoch: 44 , batch: 265 , training loss: 4.000831\n",
      "[INFO] Epoch: 44 , batch: 266 , training loss: 4.394902\n",
      "[INFO] Epoch: 44 , batch: 267 , training loss: 4.137129\n",
      "[INFO] Epoch: 44 , batch: 268 , training loss: 4.070611\n",
      "[INFO] Epoch: 44 , batch: 269 , training loss: 4.048666\n",
      "[INFO] Epoch: 44 , batch: 270 , training loss: 4.070873\n",
      "[INFO] Epoch: 44 , batch: 271 , training loss: 4.104455\n",
      "[INFO] Epoch: 44 , batch: 272 , training loss: 4.098050\n",
      "[INFO] Epoch: 44 , batch: 273 , training loss: 4.110812\n",
      "[INFO] Epoch: 44 , batch: 274 , training loss: 4.187901\n",
      "[INFO] Epoch: 44 , batch: 275 , training loss: 4.064566\n",
      "[INFO] Epoch: 44 , batch: 276 , training loss: 4.113377\n",
      "[INFO] Epoch: 44 , batch: 277 , training loss: 4.278264\n",
      "[INFO] Epoch: 44 , batch: 278 , training loss: 3.974215\n",
      "[INFO] Epoch: 44 , batch: 279 , training loss: 3.982333\n",
      "[INFO] Epoch: 44 , batch: 280 , training loss: 3.967017\n",
      "[INFO] Epoch: 44 , batch: 281 , training loss: 4.072067\n",
      "[INFO] Epoch: 44 , batch: 282 , training loss: 4.006172\n",
      "[INFO] Epoch: 44 , batch: 283 , training loss: 3.980538\n",
      "[INFO] Epoch: 44 , batch: 284 , training loss: 4.017838\n",
      "[INFO] Epoch: 44 , batch: 285 , training loss: 3.952601\n",
      "[INFO] Epoch: 44 , batch: 286 , training loss: 3.975620\n",
      "[INFO] Epoch: 44 , batch: 287 , training loss: 3.936832\n",
      "[INFO] Epoch: 44 , batch: 288 , training loss: 3.883001\n",
      "[INFO] Epoch: 44 , batch: 289 , training loss: 3.955766\n",
      "[INFO] Epoch: 44 , batch: 290 , training loss: 3.736889\n",
      "[INFO] Epoch: 44 , batch: 291 , training loss: 3.742873\n",
      "[INFO] Epoch: 44 , batch: 292 , training loss: 3.832981\n",
      "[INFO] Epoch: 44 , batch: 293 , training loss: 3.758274\n",
      "[INFO] Epoch: 44 , batch: 294 , training loss: 4.406422\n",
      "[INFO] Epoch: 44 , batch: 295 , training loss: 4.207959\n",
      "[INFO] Epoch: 44 , batch: 296 , training loss: 4.125683\n",
      "[INFO] Epoch: 44 , batch: 297 , training loss: 4.072348\n",
      "[INFO] Epoch: 44 , batch: 298 , training loss: 3.897233\n",
      "[INFO] Epoch: 44 , batch: 299 , training loss: 3.964991\n",
      "[INFO] Epoch: 44 , batch: 300 , training loss: 3.956351\n",
      "[INFO] Epoch: 44 , batch: 301 , training loss: 3.872262\n",
      "[INFO] Epoch: 44 , batch: 302 , training loss: 4.060889\n",
      "[INFO] Epoch: 44 , batch: 303 , training loss: 4.053922\n",
      "[INFO] Epoch: 44 , batch: 304 , training loss: 4.183383\n",
      "[INFO] Epoch: 44 , batch: 305 , training loss: 4.019058\n",
      "[INFO] Epoch: 44 , batch: 306 , training loss: 4.131742\n",
      "[INFO] Epoch: 44 , batch: 307 , training loss: 4.163892\n",
      "[INFO] Epoch: 44 , batch: 308 , training loss: 3.956505\n",
      "[INFO] Epoch: 44 , batch: 309 , training loss: 3.951478\n",
      "[INFO] Epoch: 44 , batch: 310 , training loss: 3.891779\n",
      "[INFO] Epoch: 44 , batch: 311 , training loss: 3.879772\n",
      "[INFO] Epoch: 44 , batch: 312 , training loss: 3.783355\n",
      "[INFO] Epoch: 44 , batch: 313 , training loss: 3.889723\n",
      "[INFO] Epoch: 44 , batch: 314 , training loss: 3.954584\n",
      "[INFO] Epoch: 44 , batch: 315 , training loss: 4.043619\n",
      "[INFO] Epoch: 44 , batch: 316 , training loss: 4.277925\n",
      "[INFO] Epoch: 44 , batch: 317 , training loss: 4.630644\n",
      "[INFO] Epoch: 44 , batch: 318 , training loss: 4.729916\n",
      "[INFO] Epoch: 44 , batch: 319 , training loss: 4.434973\n",
      "[INFO] Epoch: 44 , batch: 320 , training loss: 3.990537\n",
      "[INFO] Epoch: 44 , batch: 321 , training loss: 3.815319\n",
      "[INFO] Epoch: 44 , batch: 322 , training loss: 3.932880\n",
      "[INFO] Epoch: 44 , batch: 323 , training loss: 3.958098\n",
      "[INFO] Epoch: 44 , batch: 324 , training loss: 3.922915\n",
      "[INFO] Epoch: 44 , batch: 325 , training loss: 4.034400\n",
      "[INFO] Epoch: 44 , batch: 326 , training loss: 4.089890\n",
      "[INFO] Epoch: 44 , batch: 327 , training loss: 4.057946\n",
      "[INFO] Epoch: 44 , batch: 328 , training loss: 4.029515\n",
      "[INFO] Epoch: 44 , batch: 329 , training loss: 3.950891\n",
      "[INFO] Epoch: 44 , batch: 330 , training loss: 3.944142\n",
      "[INFO] Epoch: 44 , batch: 331 , training loss: 4.083171\n",
      "[INFO] Epoch: 44 , batch: 332 , training loss: 3.952823\n",
      "[INFO] Epoch: 44 , batch: 333 , training loss: 3.915286\n",
      "[INFO] Epoch: 44 , batch: 334 , training loss: 3.931682\n",
      "[INFO] Epoch: 44 , batch: 335 , training loss: 4.062093\n",
      "[INFO] Epoch: 44 , batch: 336 , training loss: 4.069477\n",
      "[INFO] Epoch: 44 , batch: 337 , training loss: 4.111939\n",
      "[INFO] Epoch: 44 , batch: 338 , training loss: 4.310341\n",
      "[INFO] Epoch: 44 , batch: 339 , training loss: 4.138706\n",
      "[INFO] Epoch: 44 , batch: 340 , training loss: 4.317312\n",
      "[INFO] Epoch: 44 , batch: 341 , training loss: 4.087785\n",
      "[INFO] Epoch: 44 , batch: 342 , training loss: 3.871730\n",
      "[INFO] Epoch: 44 , batch: 343 , training loss: 3.961773\n",
      "[INFO] Epoch: 44 , batch: 344 , training loss: 3.813381\n",
      "[INFO] Epoch: 44 , batch: 345 , training loss: 3.923143\n",
      "[INFO] Epoch: 44 , batch: 346 , training loss: 3.977232\n",
      "[INFO] Epoch: 44 , batch: 347 , training loss: 3.890013\n",
      "[INFO] Epoch: 44 , batch: 348 , training loss: 3.992084\n",
      "[INFO] Epoch: 44 , batch: 349 , training loss: 4.081128\n",
      "[INFO] Epoch: 44 , batch: 350 , training loss: 3.948309\n",
      "[INFO] Epoch: 44 , batch: 351 , training loss: 4.031692\n",
      "[INFO] Epoch: 44 , batch: 352 , training loss: 4.032961\n",
      "[INFO] Epoch: 44 , batch: 353 , training loss: 4.019238\n",
      "[INFO] Epoch: 44 , batch: 354 , training loss: 4.114321\n",
      "[INFO] Epoch: 44 , batch: 355 , training loss: 4.096342\n",
      "[INFO] Epoch: 44 , batch: 356 , training loss: 3.969206\n",
      "[INFO] Epoch: 44 , batch: 357 , training loss: 4.040372\n",
      "[INFO] Epoch: 44 , batch: 358 , training loss: 3.950315\n",
      "[INFO] Epoch: 44 , batch: 359 , training loss: 3.959501\n",
      "[INFO] Epoch: 44 , batch: 360 , training loss: 4.072958\n",
      "[INFO] Epoch: 44 , batch: 361 , training loss: 4.042724\n",
      "[INFO] Epoch: 44 , batch: 362 , training loss: 4.131526\n",
      "[INFO] Epoch: 44 , batch: 363 , training loss: 4.013796\n",
      "[INFO] Epoch: 44 , batch: 364 , training loss: 4.080010\n",
      "[INFO] Epoch: 44 , batch: 365 , training loss: 3.987474\n",
      "[INFO] Epoch: 44 , batch: 366 , training loss: 4.098489\n",
      "[INFO] Epoch: 44 , batch: 367 , training loss: 4.142525\n",
      "[INFO] Epoch: 44 , batch: 368 , training loss: 4.528728\n",
      "[INFO] Epoch: 44 , batch: 369 , training loss: 4.208645\n",
      "[INFO] Epoch: 44 , batch: 370 , training loss: 3.995983\n",
      "[INFO] Epoch: 44 , batch: 371 , training loss: 4.395365\n",
      "[INFO] Epoch: 44 , batch: 372 , training loss: 4.663883\n",
      "[INFO] Epoch: 44 , batch: 373 , training loss: 4.659554\n",
      "[INFO] Epoch: 44 , batch: 374 , training loss: 4.799671\n",
      "[INFO] Epoch: 44 , batch: 375 , training loss: 4.810670\n",
      "[INFO] Epoch: 44 , batch: 376 , training loss: 4.659821\n",
      "[INFO] Epoch: 44 , batch: 377 , training loss: 4.416354\n",
      "[INFO] Epoch: 44 , batch: 378 , training loss: 4.526247\n",
      "[INFO] Epoch: 44 , batch: 379 , training loss: 4.493894\n",
      "[INFO] Epoch: 44 , batch: 380 , training loss: 4.677175\n",
      "[INFO] Epoch: 44 , batch: 381 , training loss: 4.371362\n",
      "[INFO] Epoch: 44 , batch: 382 , training loss: 4.623957\n",
      "[INFO] Epoch: 44 , batch: 383 , training loss: 4.667821\n",
      "[INFO] Epoch: 44 , batch: 384 , training loss: 4.632731\n",
      "[INFO] Epoch: 44 , batch: 385 , training loss: 4.302667\n",
      "[INFO] Epoch: 44 , batch: 386 , training loss: 4.572650\n",
      "[INFO] Epoch: 44 , batch: 387 , training loss: 4.525384\n",
      "[INFO] Epoch: 44 , batch: 388 , training loss: 4.344307\n",
      "[INFO] Epoch: 44 , batch: 389 , training loss: 4.148875\n",
      "[INFO] Epoch: 44 , batch: 390 , training loss: 4.190690\n",
      "[INFO] Epoch: 44 , batch: 391 , training loss: 4.211994\n",
      "[INFO] Epoch: 44 , batch: 392 , training loss: 4.576257\n",
      "[INFO] Epoch: 44 , batch: 393 , training loss: 4.451969\n",
      "[INFO] Epoch: 44 , batch: 394 , training loss: 4.565261\n",
      "[INFO] Epoch: 44 , batch: 395 , training loss: 4.374001\n",
      "[INFO] Epoch: 44 , batch: 396 , training loss: 4.183440\n",
      "[INFO] Epoch: 44 , batch: 397 , training loss: 4.331146\n",
      "[INFO] Epoch: 44 , batch: 398 , training loss: 4.193101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 44 , batch: 399 , training loss: 4.285588\n",
      "[INFO] Epoch: 44 , batch: 400 , training loss: 4.246871\n",
      "[INFO] Epoch: 44 , batch: 401 , training loss: 4.694816\n",
      "[INFO] Epoch: 44 , batch: 402 , training loss: 4.390719\n",
      "[INFO] Epoch: 44 , batch: 403 , training loss: 4.233034\n",
      "[INFO] Epoch: 44 , batch: 404 , training loss: 4.391929\n",
      "[INFO] Epoch: 44 , batch: 405 , training loss: 4.456439\n",
      "[INFO] Epoch: 44 , batch: 406 , training loss: 4.356699\n",
      "[INFO] Epoch: 44 , batch: 407 , training loss: 4.388775\n",
      "[INFO] Epoch: 44 , batch: 408 , training loss: 4.372667\n",
      "[INFO] Epoch: 44 , batch: 409 , training loss: 4.381068\n",
      "[INFO] Epoch: 44 , batch: 410 , training loss: 4.446373\n",
      "[INFO] Epoch: 44 , batch: 411 , training loss: 4.600433\n",
      "[INFO] Epoch: 44 , batch: 412 , training loss: 4.420599\n",
      "[INFO] Epoch: 44 , batch: 413 , training loss: 4.315169\n",
      "[INFO] Epoch: 44 , batch: 414 , training loss: 4.337389\n",
      "[INFO] Epoch: 44 , batch: 415 , training loss: 4.412773\n",
      "[INFO] Epoch: 44 , batch: 416 , training loss: 4.455173\n",
      "[INFO] Epoch: 44 , batch: 417 , training loss: 4.375497\n",
      "[INFO] Epoch: 44 , batch: 418 , training loss: 4.418565\n",
      "[INFO] Epoch: 44 , batch: 419 , training loss: 4.403544\n",
      "[INFO] Epoch: 44 , batch: 420 , training loss: 4.373023\n",
      "[INFO] Epoch: 44 , batch: 421 , training loss: 4.344868\n",
      "[INFO] Epoch: 44 , batch: 422 , training loss: 4.170403\n",
      "[INFO] Epoch: 44 , batch: 423 , training loss: 4.422142\n",
      "[INFO] Epoch: 44 , batch: 424 , training loss: 4.582135\n",
      "[INFO] Epoch: 44 , batch: 425 , training loss: 4.455630\n",
      "[INFO] Epoch: 44 , batch: 426 , training loss: 4.203574\n",
      "[INFO] Epoch: 44 , batch: 427 , training loss: 4.417379\n",
      "[INFO] Epoch: 44 , batch: 428 , training loss: 4.298772\n",
      "[INFO] Epoch: 44 , batch: 429 , training loss: 4.203870\n",
      "[INFO] Epoch: 44 , batch: 430 , training loss: 4.420732\n",
      "[INFO] Epoch: 44 , batch: 431 , training loss: 4.053854\n",
      "[INFO] Epoch: 44 , batch: 432 , training loss: 4.093089\n",
      "[INFO] Epoch: 44 , batch: 433 , training loss: 4.142066\n",
      "[INFO] Epoch: 44 , batch: 434 , training loss: 4.023545\n",
      "[INFO] Epoch: 44 , batch: 435 , training loss: 4.367243\n",
      "[INFO] Epoch: 44 , batch: 436 , training loss: 4.406366\n",
      "[INFO] Epoch: 44 , batch: 437 , training loss: 4.194240\n",
      "[INFO] Epoch: 44 , batch: 438 , training loss: 4.071060\n",
      "[INFO] Epoch: 44 , batch: 439 , training loss: 4.309527\n",
      "[INFO] Epoch: 44 , batch: 440 , training loss: 4.408565\n",
      "[INFO] Epoch: 44 , batch: 441 , training loss: 4.525913\n",
      "[INFO] Epoch: 44 , batch: 442 , training loss: 4.267380\n",
      "[INFO] Epoch: 44 , batch: 443 , training loss: 4.437763\n",
      "[INFO] Epoch: 44 , batch: 444 , training loss: 4.068862\n",
      "[INFO] Epoch: 44 , batch: 445 , training loss: 3.956214\n",
      "[INFO] Epoch: 44 , batch: 446 , training loss: 3.907706\n",
      "[INFO] Epoch: 44 , batch: 447 , training loss: 4.088382\n",
      "[INFO] Epoch: 44 , batch: 448 , training loss: 4.212463\n",
      "[INFO] Epoch: 44 , batch: 449 , training loss: 4.586966\n",
      "[INFO] Epoch: 44 , batch: 450 , training loss: 4.658598\n",
      "[INFO] Epoch: 44 , batch: 451 , training loss: 4.549154\n",
      "[INFO] Epoch: 44 , batch: 452 , training loss: 4.376070\n",
      "[INFO] Epoch: 44 , batch: 453 , training loss: 4.139778\n",
      "[INFO] Epoch: 44 , batch: 454 , training loss: 4.289028\n",
      "[INFO] Epoch: 44 , batch: 455 , training loss: 4.351507\n",
      "[INFO] Epoch: 44 , batch: 456 , training loss: 4.327537\n",
      "[INFO] Epoch: 44 , batch: 457 , training loss: 4.403476\n",
      "[INFO] Epoch: 44 , batch: 458 , training loss: 4.151311\n",
      "[INFO] Epoch: 44 , batch: 459 , training loss: 4.137950\n",
      "[INFO] Epoch: 44 , batch: 460 , training loss: 4.249498\n",
      "[INFO] Epoch: 44 , batch: 461 , training loss: 4.208782\n",
      "[INFO] Epoch: 44 , batch: 462 , training loss: 4.256531\n",
      "[INFO] Epoch: 44 , batch: 463 , training loss: 4.191362\n",
      "[INFO] Epoch: 44 , batch: 464 , training loss: 4.368204\n",
      "[INFO] Epoch: 44 , batch: 465 , training loss: 4.306482\n",
      "[INFO] Epoch: 44 , batch: 466 , training loss: 4.410407\n",
      "[INFO] Epoch: 44 , batch: 467 , training loss: 4.350303\n",
      "[INFO] Epoch: 44 , batch: 468 , training loss: 4.319765\n",
      "[INFO] Epoch: 44 , batch: 469 , training loss: 4.344205\n",
      "[INFO] Epoch: 44 , batch: 470 , training loss: 4.164829\n",
      "[INFO] Epoch: 44 , batch: 471 , training loss: 4.262470\n",
      "[INFO] Epoch: 44 , batch: 472 , training loss: 4.317775\n",
      "[INFO] Epoch: 44 , batch: 473 , training loss: 4.249557\n",
      "[INFO] Epoch: 44 , batch: 474 , training loss: 4.021741\n",
      "[INFO] Epoch: 44 , batch: 475 , training loss: 3.909749\n",
      "[INFO] Epoch: 44 , batch: 476 , training loss: 4.321909\n",
      "[INFO] Epoch: 44 , batch: 477 , training loss: 4.432071\n",
      "[INFO] Epoch: 44 , batch: 478 , training loss: 4.418579\n",
      "[INFO] Epoch: 44 , batch: 479 , training loss: 4.404865\n",
      "[INFO] Epoch: 44 , batch: 480 , training loss: 4.529739\n",
      "[INFO] Epoch: 44 , batch: 481 , training loss: 4.411131\n",
      "[INFO] Epoch: 44 , batch: 482 , training loss: 4.506244\n",
      "[INFO] Epoch: 44 , batch: 483 , training loss: 4.352504\n",
      "[INFO] Epoch: 44 , batch: 484 , training loss: 4.147729\n",
      "[INFO] Epoch: 44 , batch: 485 , training loss: 4.256290\n",
      "[INFO] Epoch: 44 , batch: 486 , training loss: 4.150550\n",
      "[INFO] Epoch: 44 , batch: 487 , training loss: 4.135939\n",
      "[INFO] Epoch: 44 , batch: 488 , training loss: 4.306134\n",
      "[INFO] Epoch: 44 , batch: 489 , training loss: 4.228693\n",
      "[INFO] Epoch: 44 , batch: 490 , training loss: 4.280778\n",
      "[INFO] Epoch: 44 , batch: 491 , training loss: 4.195935\n",
      "[INFO] Epoch: 44 , batch: 492 , training loss: 4.190922\n",
      "[INFO] Epoch: 44 , batch: 493 , training loss: 4.331528\n",
      "[INFO] Epoch: 44 , batch: 494 , training loss: 4.245311\n",
      "[INFO] Epoch: 44 , batch: 495 , training loss: 4.426674\n",
      "[INFO] Epoch: 44 , batch: 496 , training loss: 4.293124\n",
      "[INFO] Epoch: 44 , batch: 497 , training loss: 4.317897\n",
      "[INFO] Epoch: 44 , batch: 498 , training loss: 4.296775\n",
      "[INFO] Epoch: 44 , batch: 499 , training loss: 4.375496\n",
      "[INFO] Epoch: 44 , batch: 500 , training loss: 4.518094\n",
      "[INFO] Epoch: 44 , batch: 501 , training loss: 4.813320\n",
      "[INFO] Epoch: 44 , batch: 502 , training loss: 4.841768\n",
      "[INFO] Epoch: 44 , batch: 503 , training loss: 4.484948\n",
      "[INFO] Epoch: 44 , batch: 504 , training loss: 4.642998\n",
      "[INFO] Epoch: 44 , batch: 505 , training loss: 4.609775\n",
      "[INFO] Epoch: 44 , batch: 506 , training loss: 4.593143\n",
      "[INFO] Epoch: 44 , batch: 507 , training loss: 4.625729\n",
      "[INFO] Epoch: 44 , batch: 508 , training loss: 4.544654\n",
      "[INFO] Epoch: 44 , batch: 509 , training loss: 4.380294\n",
      "[INFO] Epoch: 44 , batch: 510 , training loss: 4.468016\n",
      "[INFO] Epoch: 44 , batch: 511 , training loss: 4.384283\n",
      "[INFO] Epoch: 44 , batch: 512 , training loss: 4.452574\n",
      "[INFO] Epoch: 44 , batch: 513 , training loss: 4.722878\n",
      "[INFO] Epoch: 44 , batch: 514 , training loss: 4.352379\n",
      "[INFO] Epoch: 44 , batch: 515 , training loss: 4.628003\n",
      "[INFO] Epoch: 44 , batch: 516 , training loss: 4.426571\n",
      "[INFO] Epoch: 44 , batch: 517 , training loss: 4.393736\n",
      "[INFO] Epoch: 44 , batch: 518 , training loss: 4.355315\n",
      "[INFO] Epoch: 44 , batch: 519 , training loss: 4.211989\n",
      "[INFO] Epoch: 44 , batch: 520 , training loss: 4.439316\n",
      "[INFO] Epoch: 44 , batch: 521 , training loss: 4.412448\n",
      "[INFO] Epoch: 44 , batch: 522 , training loss: 4.507952\n",
      "[INFO] Epoch: 44 , batch: 523 , training loss: 4.405920\n",
      "[INFO] Epoch: 44 , batch: 524 , training loss: 4.698368\n",
      "[INFO] Epoch: 44 , batch: 525 , training loss: 4.573387\n",
      "[INFO] Epoch: 44 , batch: 526 , training loss: 4.374311\n",
      "[INFO] Epoch: 44 , batch: 527 , training loss: 4.410029\n",
      "[INFO] Epoch: 44 , batch: 528 , training loss: 4.411241\n",
      "[INFO] Epoch: 44 , batch: 529 , training loss: 4.395213\n",
      "[INFO] Epoch: 44 , batch: 530 , training loss: 4.249310\n",
      "[INFO] Epoch: 44 , batch: 531 , training loss: 4.395979\n",
      "[INFO] Epoch: 44 , batch: 532 , training loss: 4.302686\n",
      "[INFO] Epoch: 44 , batch: 533 , training loss: 4.418317\n",
      "[INFO] Epoch: 44 , batch: 534 , training loss: 4.445522\n",
      "[INFO] Epoch: 44 , batch: 535 , training loss: 4.428633\n",
      "[INFO] Epoch: 44 , batch: 536 , training loss: 4.277331\n",
      "[INFO] Epoch: 44 , batch: 537 , training loss: 4.261912\n",
      "[INFO] Epoch: 44 , batch: 538 , training loss: 4.359599\n",
      "[INFO] Epoch: 44 , batch: 539 , training loss: 4.452341\n",
      "[INFO] Epoch: 44 , batch: 540 , training loss: 4.968191\n",
      "[INFO] Epoch: 44 , batch: 541 , training loss: 4.788569\n",
      "[INFO] Epoch: 44 , batch: 542 , training loss: 4.690228\n",
      "[INFO] Epoch: 45 , batch: 0 , training loss: 3.577197\n",
      "[INFO] Epoch: 45 , batch: 1 , training loss: 3.458794\n",
      "[INFO] Epoch: 45 , batch: 2 , training loss: 3.666632\n",
      "[INFO] Epoch: 45 , batch: 3 , training loss: 3.582418\n",
      "[INFO] Epoch: 45 , batch: 4 , training loss: 3.889099\n",
      "[INFO] Epoch: 45 , batch: 5 , training loss: 3.569871\n",
      "[INFO] Epoch: 45 , batch: 6 , training loss: 3.946128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 45 , batch: 7 , training loss: 3.865589\n",
      "[INFO] Epoch: 45 , batch: 8 , training loss: 3.548468\n",
      "[INFO] Epoch: 45 , batch: 9 , training loss: 3.813029\n",
      "[INFO] Epoch: 45 , batch: 10 , training loss: 3.792333\n",
      "[INFO] Epoch: 45 , batch: 11 , training loss: 3.716592\n",
      "[INFO] Epoch: 45 , batch: 12 , training loss: 3.605608\n",
      "[INFO] Epoch: 45 , batch: 13 , training loss: 3.613806\n",
      "[INFO] Epoch: 45 , batch: 14 , training loss: 3.531250\n",
      "[INFO] Epoch: 45 , batch: 15 , training loss: 3.749313\n",
      "[INFO] Epoch: 45 , batch: 16 , training loss: 3.591177\n",
      "[INFO] Epoch: 45 , batch: 17 , training loss: 3.732184\n",
      "[INFO] Epoch: 45 , batch: 18 , training loss: 3.665259\n",
      "[INFO] Epoch: 45 , batch: 19 , training loss: 3.457680\n",
      "[INFO] Epoch: 45 , batch: 20 , training loss: 3.412862\n",
      "[INFO] Epoch: 45 , batch: 21 , training loss: 3.543865\n",
      "[INFO] Epoch: 45 , batch: 22 , training loss: 3.430664\n",
      "[INFO] Epoch: 45 , batch: 23 , training loss: 3.679651\n",
      "[INFO] Epoch: 45 , batch: 24 , training loss: 3.497438\n",
      "[INFO] Epoch: 45 , batch: 25 , training loss: 3.578453\n",
      "[INFO] Epoch: 45 , batch: 26 , training loss: 3.485847\n",
      "[INFO] Epoch: 45 , batch: 27 , training loss: 3.459366\n",
      "[INFO] Epoch: 45 , batch: 28 , training loss: 3.643600\n",
      "[INFO] Epoch: 45 , batch: 29 , training loss: 3.428242\n",
      "[INFO] Epoch: 45 , batch: 30 , training loss: 3.476345\n",
      "[INFO] Epoch: 45 , batch: 31 , training loss: 3.601981\n",
      "[INFO] Epoch: 45 , batch: 32 , training loss: 3.546232\n",
      "[INFO] Epoch: 45 , batch: 33 , training loss: 3.556718\n",
      "[INFO] Epoch: 45 , batch: 34 , training loss: 3.581417\n",
      "[INFO] Epoch: 45 , batch: 35 , training loss: 3.545054\n",
      "[INFO] Epoch: 45 , batch: 36 , training loss: 3.629193\n",
      "[INFO] Epoch: 45 , batch: 37 , training loss: 3.461931\n",
      "[INFO] Epoch: 45 , batch: 38 , training loss: 3.541654\n",
      "[INFO] Epoch: 45 , batch: 39 , training loss: 3.385859\n",
      "[INFO] Epoch: 45 , batch: 40 , training loss: 3.584624\n",
      "[INFO] Epoch: 45 , batch: 41 , training loss: 3.516062\n",
      "[INFO] Epoch: 45 , batch: 42 , training loss: 3.987758\n",
      "[INFO] Epoch: 45 , batch: 43 , training loss: 3.695987\n",
      "[INFO] Epoch: 45 , batch: 44 , training loss: 4.089577\n",
      "[INFO] Epoch: 45 , batch: 45 , training loss: 3.994581\n",
      "[INFO] Epoch: 45 , batch: 46 , training loss: 3.913583\n",
      "[INFO] Epoch: 45 , batch: 47 , training loss: 3.541240\n",
      "[INFO] Epoch: 45 , batch: 48 , training loss: 3.542887\n",
      "[INFO] Epoch: 45 , batch: 49 , training loss: 3.793147\n",
      "[INFO] Epoch: 45 , batch: 50 , training loss: 3.552170\n",
      "[INFO] Epoch: 45 , batch: 51 , training loss: 3.802255\n",
      "[INFO] Epoch: 45 , batch: 52 , training loss: 3.636388\n",
      "[INFO] Epoch: 45 , batch: 53 , training loss: 3.722365\n",
      "[INFO] Epoch: 45 , batch: 54 , training loss: 3.772634\n",
      "[INFO] Epoch: 45 , batch: 55 , training loss: 3.843928\n",
      "[INFO] Epoch: 45 , batch: 56 , training loss: 3.625710\n",
      "[INFO] Epoch: 45 , batch: 57 , training loss: 3.574986\n",
      "[INFO] Epoch: 45 , batch: 58 , training loss: 3.624696\n",
      "[INFO] Epoch: 45 , batch: 59 , training loss: 3.724443\n",
      "[INFO] Epoch: 45 , batch: 60 , training loss: 3.648430\n",
      "[INFO] Epoch: 45 , batch: 61 , training loss: 3.735469\n",
      "[INFO] Epoch: 45 , batch: 62 , training loss: 3.588195\n",
      "[INFO] Epoch: 45 , batch: 63 , training loss: 3.811031\n",
      "[INFO] Epoch: 45 , batch: 64 , training loss: 4.012938\n",
      "[INFO] Epoch: 45 , batch: 65 , training loss: 3.728663\n",
      "[INFO] Epoch: 45 , batch: 66 , training loss: 3.591509\n",
      "[INFO] Epoch: 45 , batch: 67 , training loss: 3.586652\n",
      "[INFO] Epoch: 45 , batch: 68 , training loss: 3.744076\n",
      "[INFO] Epoch: 45 , batch: 69 , training loss: 3.670596\n",
      "[INFO] Epoch: 45 , batch: 70 , training loss: 3.895727\n",
      "[INFO] Epoch: 45 , batch: 71 , training loss: 3.762914\n",
      "[INFO] Epoch: 45 , batch: 72 , training loss: 3.824799\n",
      "[INFO] Epoch: 45 , batch: 73 , training loss: 3.775489\n",
      "[INFO] Epoch: 45 , batch: 74 , training loss: 3.873754\n",
      "[INFO] Epoch: 45 , batch: 75 , training loss: 3.741580\n",
      "[INFO] Epoch: 45 , batch: 76 , training loss: 3.836090\n",
      "[INFO] Epoch: 45 , batch: 77 , training loss: 3.782364\n",
      "[INFO] Epoch: 45 , batch: 78 , training loss: 3.855255\n",
      "[INFO] Epoch: 45 , batch: 79 , training loss: 3.728596\n",
      "[INFO] Epoch: 45 , batch: 80 , training loss: 3.920927\n",
      "[INFO] Epoch: 45 , batch: 81 , training loss: 3.847041\n",
      "[INFO] Epoch: 45 , batch: 82 , training loss: 3.806533\n",
      "[INFO] Epoch: 45 , batch: 83 , training loss: 3.885984\n",
      "[INFO] Epoch: 45 , batch: 84 , training loss: 3.875733\n",
      "[INFO] Epoch: 45 , batch: 85 , training loss: 3.953210\n",
      "[INFO] Epoch: 45 , batch: 86 , training loss: 3.895890\n",
      "[INFO] Epoch: 45 , batch: 87 , training loss: 3.840950\n",
      "[INFO] Epoch: 45 , batch: 88 , training loss: 3.985445\n",
      "[INFO] Epoch: 45 , batch: 89 , training loss: 3.786106\n",
      "[INFO] Epoch: 45 , batch: 90 , training loss: 3.884825\n",
      "[INFO] Epoch: 45 , batch: 91 , training loss: 3.811388\n",
      "[INFO] Epoch: 45 , batch: 92 , training loss: 3.819413\n",
      "[INFO] Epoch: 45 , batch: 93 , training loss: 3.934301\n",
      "[INFO] Epoch: 45 , batch: 94 , training loss: 4.054027\n",
      "[INFO] Epoch: 45 , batch: 95 , training loss: 3.852865\n",
      "[INFO] Epoch: 45 , batch: 96 , training loss: 3.818661\n",
      "[INFO] Epoch: 45 , batch: 97 , training loss: 3.747256\n",
      "[INFO] Epoch: 45 , batch: 98 , training loss: 3.673156\n",
      "[INFO] Epoch: 45 , batch: 99 , training loss: 3.805000\n",
      "[INFO] Epoch: 45 , batch: 100 , training loss: 3.727365\n",
      "[INFO] Epoch: 45 , batch: 101 , training loss: 3.739956\n",
      "[INFO] Epoch: 45 , batch: 102 , training loss: 3.879568\n",
      "[INFO] Epoch: 45 , batch: 103 , training loss: 3.723273\n",
      "[INFO] Epoch: 45 , batch: 104 , training loss: 3.653572\n",
      "[INFO] Epoch: 45 , batch: 105 , training loss: 3.901124\n",
      "[INFO] Epoch: 45 , batch: 106 , training loss: 3.907099\n",
      "[INFO] Epoch: 45 , batch: 107 , training loss: 3.738512\n",
      "[INFO] Epoch: 45 , batch: 108 , training loss: 3.723161\n",
      "[INFO] Epoch: 45 , batch: 109 , training loss: 3.640220\n",
      "[INFO] Epoch: 45 , batch: 110 , training loss: 3.777126\n",
      "[INFO] Epoch: 45 , batch: 111 , training loss: 3.880798\n",
      "[INFO] Epoch: 45 , batch: 112 , training loss: 3.796667\n",
      "[INFO] Epoch: 45 , batch: 113 , training loss: 3.796011\n",
      "[INFO] Epoch: 45 , batch: 114 , training loss: 3.792705\n",
      "[INFO] Epoch: 45 , batch: 115 , training loss: 3.803566\n",
      "[INFO] Epoch: 45 , batch: 116 , training loss: 3.685514\n",
      "[INFO] Epoch: 45 , batch: 117 , training loss: 3.899825\n",
      "[INFO] Epoch: 45 , batch: 118 , training loss: 3.876734\n",
      "[INFO] Epoch: 45 , batch: 119 , training loss: 4.006408\n",
      "[INFO] Epoch: 45 , batch: 120 , training loss: 4.004914\n",
      "[INFO] Epoch: 45 , batch: 121 , training loss: 3.853688\n",
      "[INFO] Epoch: 45 , batch: 122 , training loss: 3.787907\n",
      "[INFO] Epoch: 45 , batch: 123 , training loss: 3.751823\n",
      "[INFO] Epoch: 45 , batch: 124 , training loss: 3.827195\n",
      "[INFO] Epoch: 45 , batch: 125 , training loss: 3.713224\n",
      "[INFO] Epoch: 45 , batch: 126 , training loss: 3.724976\n",
      "[INFO] Epoch: 45 , batch: 127 , training loss: 3.735057\n",
      "[INFO] Epoch: 45 , batch: 128 , training loss: 3.875207\n",
      "[INFO] Epoch: 45 , batch: 129 , training loss: 3.823303\n",
      "[INFO] Epoch: 45 , batch: 130 , training loss: 3.796294\n",
      "[INFO] Epoch: 45 , batch: 131 , training loss: 3.826475\n",
      "[INFO] Epoch: 45 , batch: 132 , training loss: 3.842878\n",
      "[INFO] Epoch: 45 , batch: 133 , training loss: 3.814021\n",
      "[INFO] Epoch: 45 , batch: 134 , training loss: 3.591808\n",
      "[INFO] Epoch: 45 , batch: 135 , training loss: 3.689166\n",
      "[INFO] Epoch: 45 , batch: 136 , training loss: 3.927817\n",
      "[INFO] Epoch: 45 , batch: 137 , training loss: 3.877759\n",
      "[INFO] Epoch: 45 , batch: 138 , training loss: 3.910664\n",
      "[INFO] Epoch: 45 , batch: 139 , training loss: 4.442660\n",
      "[INFO] Epoch: 45 , batch: 140 , training loss: 4.260237\n",
      "[INFO] Epoch: 45 , batch: 141 , training loss: 4.045401\n",
      "[INFO] Epoch: 45 , batch: 142 , training loss: 3.750075\n",
      "[INFO] Epoch: 45 , batch: 143 , training loss: 3.910785\n",
      "[INFO] Epoch: 45 , batch: 144 , training loss: 3.758018\n",
      "[INFO] Epoch: 45 , batch: 145 , training loss: 3.822484\n",
      "[INFO] Epoch: 45 , batch: 146 , training loss: 4.011261\n",
      "[INFO] Epoch: 45 , batch: 147 , training loss: 3.669383\n",
      "[INFO] Epoch: 45 , batch: 148 , training loss: 3.677438\n",
      "[INFO] Epoch: 45 , batch: 149 , training loss: 3.722558\n",
      "[INFO] Epoch: 45 , batch: 150 , training loss: 4.033542\n",
      "[INFO] Epoch: 45 , batch: 151 , training loss: 3.861798\n",
      "[INFO] Epoch: 45 , batch: 152 , training loss: 3.861607\n",
      "[INFO] Epoch: 45 , batch: 153 , training loss: 3.918496\n",
      "[INFO] Epoch: 45 , batch: 154 , training loss: 3.995707\n",
      "[INFO] Epoch: 45 , batch: 155 , training loss: 4.180410\n",
      "[INFO] Epoch: 45 , batch: 156 , training loss: 3.940601\n",
      "[INFO] Epoch: 45 , batch: 157 , training loss: 3.923657\n",
      "[INFO] Epoch: 45 , batch: 158 , training loss: 4.025593\n",
      "[INFO] Epoch: 45 , batch: 159 , training loss: 3.970842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 45 , batch: 160 , training loss: 4.223408\n",
      "[INFO] Epoch: 45 , batch: 161 , training loss: 4.233848\n",
      "[INFO] Epoch: 45 , batch: 162 , training loss: 4.218527\n",
      "[INFO] Epoch: 45 , batch: 163 , training loss: 4.412282\n",
      "[INFO] Epoch: 45 , batch: 164 , training loss: 4.355298\n",
      "[INFO] Epoch: 45 , batch: 165 , training loss: 4.302806\n",
      "[INFO] Epoch: 45 , batch: 166 , training loss: 4.213691\n",
      "[INFO] Epoch: 45 , batch: 167 , training loss: 4.328332\n",
      "[INFO] Epoch: 45 , batch: 168 , training loss: 4.000281\n",
      "[INFO] Epoch: 45 , batch: 169 , training loss: 3.978591\n",
      "[INFO] Epoch: 45 , batch: 170 , training loss: 4.117597\n",
      "[INFO] Epoch: 45 , batch: 171 , training loss: 3.572212\n",
      "[INFO] Epoch: 45 , batch: 172 , training loss: 3.799228\n",
      "[INFO] Epoch: 45 , batch: 173 , training loss: 4.059609\n",
      "[INFO] Epoch: 45 , batch: 174 , training loss: 4.547973\n",
      "[INFO] Epoch: 45 , batch: 175 , training loss: 4.804126\n",
      "[INFO] Epoch: 45 , batch: 176 , training loss: 4.461813\n",
      "[INFO] Epoch: 45 , batch: 177 , training loss: 4.069951\n",
      "[INFO] Epoch: 45 , batch: 178 , training loss: 4.072945\n",
      "[INFO] Epoch: 45 , batch: 179 , training loss: 4.123270\n",
      "[INFO] Epoch: 45 , batch: 180 , training loss: 4.077819\n",
      "[INFO] Epoch: 45 , batch: 181 , training loss: 4.400087\n",
      "[INFO] Epoch: 45 , batch: 182 , training loss: 4.297472\n",
      "[INFO] Epoch: 45 , batch: 183 , training loss: 4.278345\n",
      "[INFO] Epoch: 45 , batch: 184 , training loss: 4.183918\n",
      "[INFO] Epoch: 45 , batch: 185 , training loss: 4.137988\n",
      "[INFO] Epoch: 45 , batch: 186 , training loss: 4.276108\n",
      "[INFO] Epoch: 45 , batch: 187 , training loss: 4.373895\n",
      "[INFO] Epoch: 45 , batch: 188 , training loss: 4.394057\n",
      "[INFO] Epoch: 45 , batch: 189 , training loss: 4.274210\n",
      "[INFO] Epoch: 45 , batch: 190 , training loss: 4.343413\n",
      "[INFO] Epoch: 45 , batch: 191 , training loss: 4.410042\n",
      "[INFO] Epoch: 45 , batch: 192 , training loss: 4.269871\n",
      "[INFO] Epoch: 45 , batch: 193 , training loss: 4.355586\n",
      "[INFO] Epoch: 45 , batch: 194 , training loss: 4.291485\n",
      "[INFO] Epoch: 45 , batch: 195 , training loss: 4.225618\n",
      "[INFO] Epoch: 45 , batch: 196 , training loss: 4.073607\n",
      "[INFO] Epoch: 45 , batch: 197 , training loss: 4.182753\n",
      "[INFO] Epoch: 45 , batch: 198 , training loss: 4.088892\n",
      "[INFO] Epoch: 45 , batch: 199 , training loss: 4.215529\n",
      "[INFO] Epoch: 45 , batch: 200 , training loss: 4.136784\n",
      "[INFO] Epoch: 45 , batch: 201 , training loss: 4.039278\n",
      "[INFO] Epoch: 45 , batch: 202 , training loss: 4.053218\n",
      "[INFO] Epoch: 45 , batch: 203 , training loss: 4.180109\n",
      "[INFO] Epoch: 45 , batch: 204 , training loss: 4.245818\n",
      "[INFO] Epoch: 45 , batch: 205 , training loss: 3.851670\n",
      "[INFO] Epoch: 45 , batch: 206 , training loss: 3.792249\n",
      "[INFO] Epoch: 45 , batch: 207 , training loss: 3.791025\n",
      "[INFO] Epoch: 45 , batch: 208 , training loss: 4.095958\n",
      "[INFO] Epoch: 45 , batch: 209 , training loss: 4.099336\n",
      "[INFO] Epoch: 45 , batch: 210 , training loss: 4.075546\n",
      "[INFO] Epoch: 45 , batch: 211 , training loss: 4.085514\n",
      "[INFO] Epoch: 45 , batch: 212 , training loss: 4.158029\n",
      "[INFO] Epoch: 45 , batch: 213 , training loss: 4.145640\n",
      "[INFO] Epoch: 45 , batch: 214 , training loss: 4.210652\n",
      "[INFO] Epoch: 45 , batch: 215 , training loss: 4.388861\n",
      "[INFO] Epoch: 45 , batch: 216 , training loss: 4.127188\n",
      "[INFO] Epoch: 45 , batch: 217 , training loss: 4.063324\n",
      "[INFO] Epoch: 45 , batch: 218 , training loss: 4.042198\n",
      "[INFO] Epoch: 45 , batch: 219 , training loss: 4.139052\n",
      "[INFO] Epoch: 45 , batch: 220 , training loss: 3.978655\n",
      "[INFO] Epoch: 45 , batch: 221 , training loss: 3.992216\n",
      "[INFO] Epoch: 45 , batch: 222 , training loss: 4.135193\n",
      "[INFO] Epoch: 45 , batch: 223 , training loss: 4.270150\n",
      "[INFO] Epoch: 45 , batch: 224 , training loss: 4.323772\n",
      "[INFO] Epoch: 45 , batch: 225 , training loss: 4.205231\n",
      "[INFO] Epoch: 45 , batch: 226 , training loss: 4.347412\n",
      "[INFO] Epoch: 45 , batch: 227 , training loss: 4.310635\n",
      "[INFO] Epoch: 45 , batch: 228 , training loss: 4.371463\n",
      "[INFO] Epoch: 45 , batch: 229 , training loss: 4.235475\n",
      "[INFO] Epoch: 45 , batch: 230 , training loss: 4.090994\n",
      "[INFO] Epoch: 45 , batch: 231 , training loss: 3.937365\n",
      "[INFO] Epoch: 45 , batch: 232 , training loss: 4.105510\n",
      "[INFO] Epoch: 45 , batch: 233 , training loss: 4.134359\n",
      "[INFO] Epoch: 45 , batch: 234 , training loss: 3.817732\n",
      "[INFO] Epoch: 45 , batch: 235 , training loss: 3.934230\n",
      "[INFO] Epoch: 45 , batch: 236 , training loss: 4.060393\n",
      "[INFO] Epoch: 45 , batch: 237 , training loss: 4.259072\n",
      "[INFO] Epoch: 45 , batch: 238 , training loss: 4.026909\n",
      "[INFO] Epoch: 45 , batch: 239 , training loss: 4.040099\n",
      "[INFO] Epoch: 45 , batch: 240 , training loss: 4.097366\n",
      "[INFO] Epoch: 45 , batch: 241 , training loss: 3.904304\n",
      "[INFO] Epoch: 45 , batch: 242 , training loss: 3.909383\n",
      "[INFO] Epoch: 45 , batch: 243 , training loss: 4.217395\n",
      "[INFO] Epoch: 45 , batch: 244 , training loss: 4.162064\n",
      "[INFO] Epoch: 45 , batch: 245 , training loss: 4.122032\n",
      "[INFO] Epoch: 45 , batch: 246 , training loss: 3.831063\n",
      "[INFO] Epoch: 45 , batch: 247 , training loss: 4.013544\n",
      "[INFO] Epoch: 45 , batch: 248 , training loss: 4.059247\n",
      "[INFO] Epoch: 45 , batch: 249 , training loss: 4.038314\n",
      "[INFO] Epoch: 45 , batch: 250 , training loss: 3.859357\n",
      "[INFO] Epoch: 45 , batch: 251 , training loss: 4.306504\n",
      "[INFO] Epoch: 45 , batch: 252 , training loss: 4.003131\n",
      "[INFO] Epoch: 45 , batch: 253 , training loss: 3.928407\n",
      "[INFO] Epoch: 45 , batch: 254 , training loss: 4.208672\n",
      "[INFO] Epoch: 45 , batch: 255 , training loss: 4.159359\n",
      "[INFO] Epoch: 45 , batch: 256 , training loss: 4.177019\n",
      "[INFO] Epoch: 45 , batch: 257 , training loss: 4.341500\n",
      "[INFO] Epoch: 45 , batch: 258 , training loss: 4.363689\n",
      "[INFO] Epoch: 45 , batch: 259 , training loss: 4.378198\n",
      "[INFO] Epoch: 45 , batch: 260 , training loss: 4.123754\n",
      "[INFO] Epoch: 45 , batch: 261 , training loss: 4.320580\n",
      "[INFO] Epoch: 45 , batch: 262 , training loss: 4.432995\n",
      "[INFO] Epoch: 45 , batch: 263 , training loss: 4.622879\n",
      "[INFO] Epoch: 45 , batch: 264 , training loss: 3.974866\n",
      "[INFO] Epoch: 45 , batch: 265 , training loss: 4.113144\n",
      "[INFO] Epoch: 45 , batch: 266 , training loss: 4.501327\n",
      "[INFO] Epoch: 45 , batch: 267 , training loss: 4.266721\n",
      "[INFO] Epoch: 45 , batch: 268 , training loss: 4.189011\n",
      "[INFO] Epoch: 45 , batch: 269 , training loss: 4.148373\n",
      "[INFO] Epoch: 45 , batch: 270 , training loss: 4.153419\n",
      "[INFO] Epoch: 45 , batch: 271 , training loss: 4.205781\n",
      "[INFO] Epoch: 45 , batch: 272 , training loss: 4.168863\n",
      "[INFO] Epoch: 45 , batch: 273 , training loss: 4.184269\n",
      "[INFO] Epoch: 45 , batch: 274 , training loss: 4.268808\n",
      "[INFO] Epoch: 45 , batch: 275 , training loss: 4.154821\n",
      "[INFO] Epoch: 45 , batch: 276 , training loss: 4.196858\n",
      "[INFO] Epoch: 45 , batch: 277 , training loss: 4.373166\n",
      "[INFO] Epoch: 45 , batch: 278 , training loss: 4.046565\n",
      "[INFO] Epoch: 45 , batch: 279 , training loss: 4.039931\n",
      "[INFO] Epoch: 45 , batch: 280 , training loss: 4.041488\n",
      "[INFO] Epoch: 45 , batch: 281 , training loss: 4.137054\n",
      "[INFO] Epoch: 45 , batch: 282 , training loss: 4.043552\n",
      "[INFO] Epoch: 45 , batch: 283 , training loss: 4.066263\n",
      "[INFO] Epoch: 45 , batch: 284 , training loss: 4.081512\n",
      "[INFO] Epoch: 45 , batch: 285 , training loss: 4.055108\n",
      "[INFO] Epoch: 45 , batch: 286 , training loss: 4.046131\n",
      "[INFO] Epoch: 45 , batch: 287 , training loss: 3.984625\n",
      "[INFO] Epoch: 45 , batch: 288 , training loss: 3.959060\n",
      "[INFO] Epoch: 45 , batch: 289 , training loss: 4.030734\n",
      "[INFO] Epoch: 45 , batch: 290 , training loss: 3.823484\n",
      "[INFO] Epoch: 45 , batch: 291 , training loss: 3.819233\n",
      "[INFO] Epoch: 45 , batch: 292 , training loss: 3.892459\n",
      "[INFO] Epoch: 45 , batch: 293 , training loss: 3.854207\n",
      "[INFO] Epoch: 45 , batch: 294 , training loss: 4.485849\n",
      "[INFO] Epoch: 45 , batch: 295 , training loss: 4.278081\n",
      "[INFO] Epoch: 45 , batch: 296 , training loss: 4.186100\n",
      "[INFO] Epoch: 45 , batch: 297 , training loss: 4.126424\n",
      "[INFO] Epoch: 45 , batch: 298 , training loss: 3.975054\n",
      "[INFO] Epoch: 45 , batch: 299 , training loss: 4.022822\n",
      "[INFO] Epoch: 45 , batch: 300 , training loss: 4.027043\n",
      "[INFO] Epoch: 45 , batch: 301 , training loss: 3.924725\n",
      "[INFO] Epoch: 45 , batch: 302 , training loss: 4.116037\n",
      "[INFO] Epoch: 45 , batch: 303 , training loss: 4.115203\n",
      "[INFO] Epoch: 45 , batch: 304 , training loss: 4.251222\n",
      "[INFO] Epoch: 45 , batch: 305 , training loss: 4.079555\n",
      "[INFO] Epoch: 45 , batch: 306 , training loss: 4.201353\n",
      "[INFO] Epoch: 45 , batch: 307 , training loss: 4.202763\n",
      "[INFO] Epoch: 45 , batch: 308 , training loss: 4.038258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 45 , batch: 309 , training loss: 4.014546\n",
      "[INFO] Epoch: 45 , batch: 310 , training loss: 3.954191\n",
      "[INFO] Epoch: 45 , batch: 311 , training loss: 3.941497\n",
      "[INFO] Epoch: 45 , batch: 312 , training loss: 3.836480\n",
      "[INFO] Epoch: 45 , batch: 313 , training loss: 3.954282\n",
      "[INFO] Epoch: 45 , batch: 314 , training loss: 4.015917\n",
      "[INFO] Epoch: 45 , batch: 315 , training loss: 4.099567\n",
      "[INFO] Epoch: 45 , batch: 316 , training loss: 4.341292\n",
      "[INFO] Epoch: 45 , batch: 317 , training loss: 4.726079\n",
      "[INFO] Epoch: 45 , batch: 318 , training loss: 4.835278\n",
      "[INFO] Epoch: 45 , batch: 319 , training loss: 4.493306\n",
      "[INFO] Epoch: 45 , batch: 320 , training loss: 4.055757\n",
      "[INFO] Epoch: 45 , batch: 321 , training loss: 3.889257\n",
      "[INFO] Epoch: 45 , batch: 322 , training loss: 3.975420\n",
      "[INFO] Epoch: 45 , batch: 323 , training loss: 4.020139\n",
      "[INFO] Epoch: 45 , batch: 324 , training loss: 4.001577\n",
      "[INFO] Epoch: 45 , batch: 325 , training loss: 4.104047\n",
      "[INFO] Epoch: 45 , batch: 326 , training loss: 4.163820\n",
      "[INFO] Epoch: 45 , batch: 327 , training loss: 4.102477\n",
      "[INFO] Epoch: 45 , batch: 328 , training loss: 4.116963\n",
      "[INFO] Epoch: 45 , batch: 329 , training loss: 4.022617\n",
      "[INFO] Epoch: 45 , batch: 330 , training loss: 3.983004\n",
      "[INFO] Epoch: 45 , batch: 331 , training loss: 4.147425\n",
      "[INFO] Epoch: 45 , batch: 332 , training loss: 3.993824\n",
      "[INFO] Epoch: 45 , batch: 333 , training loss: 3.959945\n",
      "[INFO] Epoch: 45 , batch: 334 , training loss: 4.001477\n",
      "[INFO] Epoch: 45 , batch: 335 , training loss: 4.104762\n",
      "[INFO] Epoch: 45 , batch: 336 , training loss: 4.126716\n",
      "[INFO] Epoch: 45 , batch: 337 , training loss: 4.158881\n",
      "[INFO] Epoch: 45 , batch: 338 , training loss: 4.381030\n",
      "[INFO] Epoch: 45 , batch: 339 , training loss: 4.188937\n",
      "[INFO] Epoch: 45 , batch: 340 , training loss: 4.371815\n",
      "[INFO] Epoch: 45 , batch: 341 , training loss: 4.129083\n",
      "[INFO] Epoch: 45 , batch: 342 , training loss: 3.933323\n",
      "[INFO] Epoch: 45 , batch: 343 , training loss: 4.005243\n",
      "[INFO] Epoch: 45 , batch: 344 , training loss: 3.860071\n",
      "[INFO] Epoch: 45 , batch: 345 , training loss: 3.984282\n",
      "[INFO] Epoch: 45 , batch: 346 , training loss: 4.032715\n",
      "[INFO] Epoch: 45 , batch: 347 , training loss: 3.954745\n",
      "[INFO] Epoch: 45 , batch: 348 , training loss: 4.065894\n",
      "[INFO] Epoch: 45 , batch: 349 , training loss: 4.153522\n",
      "[INFO] Epoch: 45 , batch: 350 , training loss: 3.990368\n",
      "[INFO] Epoch: 45 , batch: 351 , training loss: 4.082936\n",
      "[INFO] Epoch: 45 , batch: 352 , training loss: 4.077083\n",
      "[INFO] Epoch: 45 , batch: 353 , training loss: 4.074411\n",
      "[INFO] Epoch: 45 , batch: 354 , training loss: 4.161856\n",
      "[INFO] Epoch: 45 , batch: 355 , training loss: 4.157768\n",
      "[INFO] Epoch: 45 , batch: 356 , training loss: 4.017526\n",
      "[INFO] Epoch: 45 , batch: 357 , training loss: 4.093960\n",
      "[INFO] Epoch: 45 , batch: 358 , training loss: 4.011247\n",
      "[INFO] Epoch: 45 , batch: 359 , training loss: 4.012782\n",
      "[INFO] Epoch: 45 , batch: 360 , training loss: 4.111598\n",
      "[INFO] Epoch: 45 , batch: 361 , training loss: 4.093540\n",
      "[INFO] Epoch: 45 , batch: 362 , training loss: 4.172402\n",
      "[INFO] Epoch: 45 , batch: 363 , training loss: 4.061262\n",
      "[INFO] Epoch: 45 , batch: 364 , training loss: 4.122123\n",
      "[INFO] Epoch: 45 , batch: 365 , training loss: 4.028853\n",
      "[INFO] Epoch: 45 , batch: 366 , training loss: 4.135723\n",
      "[INFO] Epoch: 45 , batch: 367 , training loss: 4.175692\n",
      "[INFO] Epoch: 45 , batch: 368 , training loss: 4.611442\n",
      "[INFO] Epoch: 45 , batch: 369 , training loss: 4.267616\n",
      "[INFO] Epoch: 45 , batch: 370 , training loss: 4.044200\n",
      "[INFO] Epoch: 45 , batch: 371 , training loss: 4.467170\n",
      "[INFO] Epoch: 45 , batch: 372 , training loss: 4.738425\n",
      "[INFO] Epoch: 45 , batch: 373 , training loss: 4.791514\n",
      "[INFO] Epoch: 45 , batch: 374 , training loss: 4.956919\n",
      "[INFO] Epoch: 45 , batch: 375 , training loss: 4.877975\n",
      "[INFO] Epoch: 45 , batch: 376 , training loss: 4.740050\n",
      "[INFO] Epoch: 45 , batch: 377 , training loss: 4.481030\n",
      "[INFO] Epoch: 45 , batch: 378 , training loss: 4.579942\n",
      "[INFO] Epoch: 45 , batch: 379 , training loss: 4.547968\n",
      "[INFO] Epoch: 45 , batch: 380 , training loss: 4.739779\n",
      "[INFO] Epoch: 45 , batch: 381 , training loss: 4.417094\n",
      "[INFO] Epoch: 45 , batch: 382 , training loss: 4.678667\n",
      "[INFO] Epoch: 45 , batch: 383 , training loss: 4.741377\n",
      "[INFO] Epoch: 45 , batch: 384 , training loss: 4.712691\n",
      "[INFO] Epoch: 45 , batch: 385 , training loss: 4.414683\n",
      "[INFO] Epoch: 45 , batch: 386 , training loss: 4.635867\n",
      "[INFO] Epoch: 45 , batch: 387 , training loss: 4.613892\n",
      "[INFO] Epoch: 45 , batch: 388 , training loss: 4.405238\n",
      "[INFO] Epoch: 45 , batch: 389 , training loss: 4.236221\n",
      "[INFO] Epoch: 45 , batch: 390 , training loss: 4.241505\n",
      "[INFO] Epoch: 45 , batch: 391 , training loss: 4.276837\n",
      "[INFO] Epoch: 45 , batch: 392 , training loss: 4.639476\n",
      "[INFO] Epoch: 45 , batch: 393 , training loss: 4.523659\n",
      "[INFO] Epoch: 45 , batch: 394 , training loss: 4.619143\n",
      "[INFO] Epoch: 45 , batch: 395 , training loss: 4.451105\n",
      "[INFO] Epoch: 45 , batch: 396 , training loss: 4.263089\n",
      "[INFO] Epoch: 45 , batch: 397 , training loss: 4.394408\n",
      "[INFO] Epoch: 45 , batch: 398 , training loss: 4.256775\n",
      "[INFO] Epoch: 45 , batch: 399 , training loss: 4.328640\n",
      "[INFO] Epoch: 45 , batch: 400 , training loss: 4.309780\n",
      "[INFO] Epoch: 45 , batch: 401 , training loss: 4.723261\n",
      "[INFO] Epoch: 45 , batch: 402 , training loss: 4.445031\n",
      "[INFO] Epoch: 45 , batch: 403 , training loss: 4.287715\n",
      "[INFO] Epoch: 45 , batch: 404 , training loss: 4.466051\n",
      "[INFO] Epoch: 45 , batch: 405 , training loss: 4.527794\n",
      "[INFO] Epoch: 45 , batch: 406 , training loss: 4.428430\n",
      "[INFO] Epoch: 45 , batch: 407 , training loss: 4.459428\n",
      "[INFO] Epoch: 45 , batch: 408 , training loss: 4.433935\n",
      "[INFO] Epoch: 45 , batch: 409 , training loss: 4.437588\n",
      "[INFO] Epoch: 45 , batch: 410 , training loss: 4.497540\n",
      "[INFO] Epoch: 45 , batch: 411 , training loss: 4.664116\n",
      "[INFO] Epoch: 45 , batch: 412 , training loss: 4.504804\n",
      "[INFO] Epoch: 45 , batch: 413 , training loss: 4.352324\n",
      "[INFO] Epoch: 45 , batch: 414 , training loss: 4.407340\n",
      "[INFO] Epoch: 45 , batch: 415 , training loss: 4.464543\n",
      "[INFO] Epoch: 45 , batch: 416 , training loss: 4.511398\n",
      "[INFO] Epoch: 45 , batch: 417 , training loss: 4.434293\n",
      "[INFO] Epoch: 45 , batch: 418 , training loss: 4.484382\n",
      "[INFO] Epoch: 45 , batch: 419 , training loss: 4.441126\n",
      "[INFO] Epoch: 45 , batch: 420 , training loss: 4.427027\n",
      "[INFO] Epoch: 45 , batch: 421 , training loss: 4.399101\n",
      "[INFO] Epoch: 45 , batch: 422 , training loss: 4.263022\n",
      "[INFO] Epoch: 45 , batch: 423 , training loss: 4.488354\n",
      "[INFO] Epoch: 45 , batch: 424 , training loss: 4.640279\n",
      "[INFO] Epoch: 45 , batch: 425 , training loss: 4.525049\n",
      "[INFO] Epoch: 45 , batch: 426 , training loss: 4.247818\n",
      "[INFO] Epoch: 45 , batch: 427 , training loss: 4.468249\n",
      "[INFO] Epoch: 45 , batch: 428 , training loss: 4.370619\n",
      "[INFO] Epoch: 45 , batch: 429 , training loss: 4.258285\n",
      "[INFO] Epoch: 45 , batch: 430 , training loss: 4.478299\n",
      "[INFO] Epoch: 45 , batch: 431 , training loss: 4.088721\n",
      "[INFO] Epoch: 45 , batch: 432 , training loss: 4.163344\n",
      "[INFO] Epoch: 45 , batch: 433 , training loss: 4.189586\n",
      "[INFO] Epoch: 45 , batch: 434 , training loss: 4.078253\n",
      "[INFO] Epoch: 45 , batch: 435 , training loss: 4.414782\n",
      "[INFO] Epoch: 45 , batch: 436 , training loss: 4.440948\n",
      "[INFO] Epoch: 45 , batch: 437 , training loss: 4.267644\n",
      "[INFO] Epoch: 45 , batch: 438 , training loss: 4.112701\n",
      "[INFO] Epoch: 45 , batch: 439 , training loss: 4.344580\n",
      "[INFO] Epoch: 45 , batch: 440 , training loss: 4.440451\n",
      "[INFO] Epoch: 45 , batch: 441 , training loss: 4.557184\n",
      "[INFO] Epoch: 45 , batch: 442 , training loss: 4.322483\n",
      "[INFO] Epoch: 45 , batch: 443 , training loss: 4.480532\n",
      "[INFO] Epoch: 45 , batch: 444 , training loss: 4.104558\n",
      "[INFO] Epoch: 45 , batch: 445 , training loss: 4.002570\n",
      "[INFO] Epoch: 45 , batch: 446 , training loss: 3.919752\n",
      "[INFO] Epoch: 45 , batch: 447 , training loss: 4.139872\n",
      "[INFO] Epoch: 45 , batch: 448 , training loss: 4.258663\n",
      "[INFO] Epoch: 45 , batch: 449 , training loss: 4.628730\n",
      "[INFO] Epoch: 45 , batch: 450 , training loss: 4.704896\n",
      "[INFO] Epoch: 45 , batch: 451 , training loss: 4.596008\n",
      "[INFO] Epoch: 45 , batch: 452 , training loss: 4.423806\n",
      "[INFO] Epoch: 45 , batch: 453 , training loss: 4.173300\n",
      "[INFO] Epoch: 45 , batch: 454 , training loss: 4.331496\n",
      "[INFO] Epoch: 45 , batch: 455 , training loss: 4.375922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 45 , batch: 456 , training loss: 4.356412\n",
      "[INFO] Epoch: 45 , batch: 457 , training loss: 4.445422\n",
      "[INFO] Epoch: 45 , batch: 458 , training loss: 4.207502\n",
      "[INFO] Epoch: 45 , batch: 459 , training loss: 4.161188\n",
      "[INFO] Epoch: 45 , batch: 460 , training loss: 4.291676\n",
      "[INFO] Epoch: 45 , batch: 461 , training loss: 4.253842\n",
      "[INFO] Epoch: 45 , batch: 462 , training loss: 4.313892\n",
      "[INFO] Epoch: 45 , batch: 463 , training loss: 4.214789\n",
      "[INFO] Epoch: 45 , batch: 464 , training loss: 4.402586\n",
      "[INFO] Epoch: 45 , batch: 465 , training loss: 4.355025\n",
      "[INFO] Epoch: 45 , batch: 466 , training loss: 4.442850\n",
      "[INFO] Epoch: 45 , batch: 467 , training loss: 4.397533\n",
      "[INFO] Epoch: 45 , batch: 468 , training loss: 4.369713\n",
      "[INFO] Epoch: 45 , batch: 469 , training loss: 4.398428\n",
      "[INFO] Epoch: 45 , batch: 470 , training loss: 4.203987\n",
      "[INFO] Epoch: 45 , batch: 471 , training loss: 4.311377\n",
      "[INFO] Epoch: 45 , batch: 472 , training loss: 4.360980\n",
      "[INFO] Epoch: 45 , batch: 473 , training loss: 4.293255\n",
      "[INFO] Epoch: 45 , batch: 474 , training loss: 4.067678\n",
      "[INFO] Epoch: 45 , batch: 475 , training loss: 3.938725\n",
      "[INFO] Epoch: 45 , batch: 476 , training loss: 4.350741\n",
      "[INFO] Epoch: 45 , batch: 477 , training loss: 4.460267\n",
      "[INFO] Epoch: 45 , batch: 478 , training loss: 4.466935\n",
      "[INFO] Epoch: 45 , batch: 479 , training loss: 4.464078\n",
      "[INFO] Epoch: 45 , batch: 480 , training loss: 4.591459\n",
      "[INFO] Epoch: 45 , batch: 481 , training loss: 4.451759\n",
      "[INFO] Epoch: 45 , batch: 482 , training loss: 4.554055\n",
      "[INFO] Epoch: 45 , batch: 483 , training loss: 4.391415\n",
      "[INFO] Epoch: 45 , batch: 484 , training loss: 4.193337\n",
      "[INFO] Epoch: 45 , batch: 485 , training loss: 4.301534\n",
      "[INFO] Epoch: 45 , batch: 486 , training loss: 4.184735\n",
      "[INFO] Epoch: 45 , batch: 487 , training loss: 4.176976\n",
      "[INFO] Epoch: 45 , batch: 488 , training loss: 4.362030\n",
      "[INFO] Epoch: 45 , batch: 489 , training loss: 4.257526\n",
      "[INFO] Epoch: 45 , batch: 490 , training loss: 4.334473\n",
      "[INFO] Epoch: 45 , batch: 491 , training loss: 4.237530\n",
      "[INFO] Epoch: 45 , batch: 492 , training loss: 4.223418\n",
      "[INFO] Epoch: 45 , batch: 493 , training loss: 4.364841\n",
      "[INFO] Epoch: 45 , batch: 494 , training loss: 4.281385\n",
      "[INFO] Epoch: 45 , batch: 495 , training loss: 4.460927\n",
      "[INFO] Epoch: 45 , batch: 496 , training loss: 4.320992\n",
      "[INFO] Epoch: 45 , batch: 497 , training loss: 4.353252\n",
      "[INFO] Epoch: 45 , batch: 498 , training loss: 4.337093\n",
      "[INFO] Epoch: 45 , batch: 499 , training loss: 4.417435\n",
      "[INFO] Epoch: 45 , batch: 500 , training loss: 4.557707\n",
      "[INFO] Epoch: 45 , batch: 501 , training loss: 4.888530\n",
      "[INFO] Epoch: 45 , batch: 502 , training loss: 4.930038\n",
      "[INFO] Epoch: 45 , batch: 503 , training loss: 4.544026\n",
      "[INFO] Epoch: 45 , batch: 504 , training loss: 4.705370\n",
      "[INFO] Epoch: 45 , batch: 505 , training loss: 4.650728\n",
      "[INFO] Epoch: 45 , batch: 506 , training loss: 4.656845\n",
      "[INFO] Epoch: 45 , batch: 507 , training loss: 4.688040\n",
      "[INFO] Epoch: 45 , batch: 508 , training loss: 4.612211\n",
      "[INFO] Epoch: 45 , batch: 509 , training loss: 4.417244\n",
      "[INFO] Epoch: 45 , batch: 510 , training loss: 4.493151\n",
      "[INFO] Epoch: 45 , batch: 511 , training loss: 4.425328\n",
      "[INFO] Epoch: 45 , batch: 512 , training loss: 4.495018\n",
      "[INFO] Epoch: 45 , batch: 513 , training loss: 4.777067\n",
      "[INFO] Epoch: 45 , batch: 514 , training loss: 4.412126\n",
      "[INFO] Epoch: 45 , batch: 515 , training loss: 4.660893\n",
      "[INFO] Epoch: 45 , batch: 516 , training loss: 4.475544\n",
      "[INFO] Epoch: 45 , batch: 517 , training loss: 4.429149\n",
      "[INFO] Epoch: 45 , batch: 518 , training loss: 4.397816\n",
      "[INFO] Epoch: 45 , batch: 519 , training loss: 4.241124\n",
      "[INFO] Epoch: 45 , batch: 520 , training loss: 4.472418\n",
      "[INFO] Epoch: 45 , batch: 521 , training loss: 4.462412\n",
      "[INFO] Epoch: 45 , batch: 522 , training loss: 4.547040\n",
      "[INFO] Epoch: 45 , batch: 523 , training loss: 4.462464\n",
      "[INFO] Epoch: 45 , batch: 524 , training loss: 4.747098\n",
      "[INFO] Epoch: 45 , batch: 525 , training loss: 4.623817\n",
      "[INFO] Epoch: 45 , batch: 526 , training loss: 4.409068\n",
      "[INFO] Epoch: 45 , batch: 527 , training loss: 4.454965\n",
      "[INFO] Epoch: 45 , batch: 528 , training loss: 4.469583\n",
      "[INFO] Epoch: 45 , batch: 529 , training loss: 4.445760\n",
      "[INFO] Epoch: 45 , batch: 530 , training loss: 4.288356\n",
      "[INFO] Epoch: 45 , batch: 531 , training loss: 4.420221\n",
      "[INFO] Epoch: 45 , batch: 532 , training loss: 4.349442\n",
      "[INFO] Epoch: 45 , batch: 533 , training loss: 4.466746\n",
      "[INFO] Epoch: 45 , batch: 534 , training loss: 4.474077\n",
      "[INFO] Epoch: 45 , batch: 535 , training loss: 4.472919\n",
      "[INFO] Epoch: 45 , batch: 536 , training loss: 4.317021\n",
      "[INFO] Epoch: 45 , batch: 537 , training loss: 4.305415\n",
      "[INFO] Epoch: 45 , batch: 538 , training loss: 4.402539\n",
      "[INFO] Epoch: 45 , batch: 539 , training loss: 4.475958\n",
      "[INFO] Epoch: 45 , batch: 540 , training loss: 5.024587\n",
      "[INFO] Epoch: 45 , batch: 541 , training loss: 4.839786\n",
      "[INFO] Epoch: 45 , batch: 542 , training loss: 4.718948\n",
      "[INFO] Epoch: 46 , batch: 0 , training loss: 3.678204\n",
      "[INFO] Epoch: 46 , batch: 1 , training loss: 3.533398\n",
      "[INFO] Epoch: 46 , batch: 2 , training loss: 3.751140\n",
      "[INFO] Epoch: 46 , batch: 3 , training loss: 3.669557\n",
      "[INFO] Epoch: 46 , batch: 4 , training loss: 4.007180\n",
      "[INFO] Epoch: 46 , batch: 5 , training loss: 3.660583\n",
      "[INFO] Epoch: 46 , batch: 6 , training loss: 4.041431\n",
      "[INFO] Epoch: 46 , batch: 7 , training loss: 3.936839\n",
      "[INFO] Epoch: 46 , batch: 8 , training loss: 3.636440\n",
      "[INFO] Epoch: 46 , batch: 9 , training loss: 3.875531\n",
      "[INFO] Epoch: 46 , batch: 10 , training loss: 3.841705\n",
      "[INFO] Epoch: 46 , batch: 11 , training loss: 3.767766\n",
      "[INFO] Epoch: 46 , batch: 12 , training loss: 3.674311\n",
      "[INFO] Epoch: 46 , batch: 13 , training loss: 3.640482\n",
      "[INFO] Epoch: 46 , batch: 14 , training loss: 3.584651\n",
      "[INFO] Epoch: 46 , batch: 15 , training loss: 3.816039\n",
      "[INFO] Epoch: 46 , batch: 16 , training loss: 3.631640\n",
      "[INFO] Epoch: 46 , batch: 17 , training loss: 3.785814\n",
      "[INFO] Epoch: 46 , batch: 18 , training loss: 3.725550\n",
      "[INFO] Epoch: 46 , batch: 19 , training loss: 3.500762\n",
      "[INFO] Epoch: 46 , batch: 20 , training loss: 3.463641\n",
      "[INFO] Epoch: 46 , batch: 21 , training loss: 3.581036\n",
      "[INFO] Epoch: 46 , batch: 22 , training loss: 3.499733\n",
      "[INFO] Epoch: 46 , batch: 23 , training loss: 3.697190\n",
      "[INFO] Epoch: 46 , batch: 24 , training loss: 3.538147\n",
      "[INFO] Epoch: 46 , batch: 25 , training loss: 3.634782\n",
      "[INFO] Epoch: 46 , batch: 26 , training loss: 3.544206\n",
      "[INFO] Epoch: 46 , batch: 27 , training loss: 3.503612\n",
      "[INFO] Epoch: 46 , batch: 28 , training loss: 3.719389\n",
      "[INFO] Epoch: 46 , batch: 29 , training loss: 3.507604\n",
      "[INFO] Epoch: 46 , batch: 30 , training loss: 3.524078\n",
      "[INFO] Epoch: 46 , batch: 31 , training loss: 3.616489\n",
      "[INFO] Epoch: 46 , batch: 32 , training loss: 3.596663\n",
      "[INFO] Epoch: 46 , batch: 33 , training loss: 3.618343\n",
      "[INFO] Epoch: 46 , batch: 34 , training loss: 3.628608\n",
      "[INFO] Epoch: 46 , batch: 35 , training loss: 3.568070\n",
      "[INFO] Epoch: 46 , batch: 36 , training loss: 3.661776\n",
      "[INFO] Epoch: 46 , batch: 37 , training loss: 3.519652\n",
      "[INFO] Epoch: 46 , batch: 38 , training loss: 3.614055\n",
      "[INFO] Epoch: 46 , batch: 39 , training loss: 3.414320\n",
      "[INFO] Epoch: 46 , batch: 40 , training loss: 3.616455\n",
      "[INFO] Epoch: 46 , batch: 41 , training loss: 3.578378\n",
      "[INFO] Epoch: 46 , batch: 42 , training loss: 4.052740\n",
      "[INFO] Epoch: 46 , batch: 43 , training loss: 3.754837\n",
      "[INFO] Epoch: 46 , batch: 44 , training loss: 4.172053\n",
      "[INFO] Epoch: 46 , batch: 45 , training loss: 4.036480\n",
      "[INFO] Epoch: 46 , batch: 46 , training loss: 4.017240\n",
      "[INFO] Epoch: 46 , batch: 47 , training loss: 3.616280\n",
      "[INFO] Epoch: 46 , batch: 48 , training loss: 3.636371\n",
      "[INFO] Epoch: 46 , batch: 49 , training loss: 3.890836\n",
      "[INFO] Epoch: 46 , batch: 50 , training loss: 3.608294\n",
      "[INFO] Epoch: 46 , batch: 51 , training loss: 3.831436\n",
      "[INFO] Epoch: 46 , batch: 52 , training loss: 3.645690\n",
      "[INFO] Epoch: 46 , batch: 53 , training loss: 3.792547\n",
      "[INFO] Epoch: 46 , batch: 54 , training loss: 3.762441\n",
      "[INFO] Epoch: 46 , batch: 55 , training loss: 3.866107\n",
      "[INFO] Epoch: 46 , batch: 56 , training loss: 3.713729\n",
      "[INFO] Epoch: 46 , batch: 57 , training loss: 3.606129\n",
      "[INFO] Epoch: 46 , batch: 58 , training loss: 3.666016\n",
      "[INFO] Epoch: 46 , batch: 59 , training loss: 3.740466\n",
      "[INFO] Epoch: 46 , batch: 60 , training loss: 3.685491\n",
      "[INFO] Epoch: 46 , batch: 61 , training loss: 3.760494\n",
      "[INFO] Epoch: 46 , batch: 62 , training loss: 3.649636\n",
      "[INFO] Epoch: 46 , batch: 63 , training loss: 3.838400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 46 , batch: 64 , training loss: 4.049160\n",
      "[INFO] Epoch: 46 , batch: 65 , training loss: 3.768816\n",
      "[INFO] Epoch: 46 , batch: 66 , training loss: 3.602958\n",
      "[INFO] Epoch: 46 , batch: 67 , training loss: 3.614525\n",
      "[INFO] Epoch: 46 , batch: 68 , training loss: 3.821553\n",
      "[INFO] Epoch: 46 , batch: 69 , training loss: 3.724070\n",
      "[INFO] Epoch: 46 , batch: 70 , training loss: 3.952426\n",
      "[INFO] Epoch: 46 , batch: 71 , training loss: 3.805868\n",
      "[INFO] Epoch: 46 , batch: 72 , training loss: 3.871870\n",
      "[INFO] Epoch: 46 , batch: 73 , training loss: 3.806147\n",
      "[INFO] Epoch: 46 , batch: 74 , training loss: 3.887966\n",
      "[INFO] Epoch: 46 , batch: 75 , training loss: 3.762388\n",
      "[INFO] Epoch: 46 , batch: 76 , training loss: 3.878595\n",
      "[INFO] Epoch: 46 , batch: 77 , training loss: 3.817190\n",
      "[INFO] Epoch: 46 , batch: 78 , training loss: 3.908852\n",
      "[INFO] Epoch: 46 , batch: 79 , training loss: 3.755573\n",
      "[INFO] Epoch: 46 , batch: 80 , training loss: 3.954885\n",
      "[INFO] Epoch: 46 , batch: 81 , training loss: 3.873285\n",
      "[INFO] Epoch: 46 , batch: 82 , training loss: 3.867402\n",
      "[INFO] Epoch: 46 , batch: 83 , training loss: 3.921692\n",
      "[INFO] Epoch: 46 , batch: 84 , training loss: 3.920227\n",
      "[INFO] Epoch: 46 , batch: 85 , training loss: 4.009569\n",
      "[INFO] Epoch: 46 , batch: 86 , training loss: 3.928567\n",
      "[INFO] Epoch: 46 , batch: 87 , training loss: 3.888130\n",
      "[INFO] Epoch: 46 , batch: 88 , training loss: 4.027775\n",
      "[INFO] Epoch: 46 , batch: 89 , training loss: 3.824424\n",
      "[INFO] Epoch: 46 , batch: 90 , training loss: 3.919834\n",
      "[INFO] Epoch: 46 , batch: 91 , training loss: 3.838431\n",
      "[INFO] Epoch: 46 , batch: 92 , training loss: 3.833764\n",
      "[INFO] Epoch: 46 , batch: 93 , training loss: 3.967323\n",
      "[INFO] Epoch: 46 , batch: 94 , training loss: 4.084194\n",
      "[INFO] Epoch: 46 , batch: 95 , training loss: 3.871272\n",
      "[INFO] Epoch: 46 , batch: 96 , training loss: 3.855671\n",
      "[INFO] Epoch: 46 , batch: 97 , training loss: 3.786739\n",
      "[INFO] Epoch: 46 , batch: 98 , training loss: 3.727749\n",
      "[INFO] Epoch: 46 , batch: 99 , training loss: 3.838184\n",
      "[INFO] Epoch: 46 , batch: 100 , training loss: 3.756651\n",
      "[INFO] Epoch: 46 , batch: 101 , training loss: 3.764453\n",
      "[INFO] Epoch: 46 , batch: 102 , training loss: 3.908838\n",
      "[INFO] Epoch: 46 , batch: 103 , training loss: 3.747200\n",
      "[INFO] Epoch: 46 , batch: 104 , training loss: 3.706364\n",
      "[INFO] Epoch: 46 , batch: 105 , training loss: 3.923087\n",
      "[INFO] Epoch: 46 , batch: 106 , training loss: 3.954381\n",
      "[INFO] Epoch: 46 , batch: 107 , training loss: 3.820974\n",
      "[INFO] Epoch: 46 , batch: 108 , training loss: 3.739550\n",
      "[INFO] Epoch: 46 , batch: 109 , training loss: 3.663794\n",
      "[INFO] Epoch: 46 , batch: 110 , training loss: 3.833771\n",
      "[INFO] Epoch: 46 , batch: 111 , training loss: 3.893719\n",
      "[INFO] Epoch: 46 , batch: 112 , training loss: 3.838803\n",
      "[INFO] Epoch: 46 , batch: 113 , training loss: 3.842530\n",
      "[INFO] Epoch: 46 , batch: 114 , training loss: 3.831635\n",
      "[INFO] Epoch: 46 , batch: 115 , training loss: 3.859631\n",
      "[INFO] Epoch: 46 , batch: 116 , training loss: 3.766588\n",
      "[INFO] Epoch: 46 , batch: 117 , training loss: 3.928169\n",
      "[INFO] Epoch: 46 , batch: 118 , training loss: 3.966931\n",
      "[INFO] Epoch: 46 , batch: 119 , training loss: 4.061475\n",
      "[INFO] Epoch: 46 , batch: 120 , training loss: 4.067646\n",
      "[INFO] Epoch: 46 , batch: 121 , training loss: 3.919455\n",
      "[INFO] Epoch: 46 , batch: 122 , training loss: 3.827497\n",
      "[INFO] Epoch: 46 , batch: 123 , training loss: 3.789240\n",
      "[INFO] Epoch: 46 , batch: 124 , training loss: 3.901605\n",
      "[INFO] Epoch: 46 , batch: 125 , training loss: 3.706953\n",
      "[INFO] Epoch: 46 , batch: 126 , training loss: 3.762275\n",
      "[INFO] Epoch: 46 , batch: 127 , training loss: 3.757241\n",
      "[INFO] Epoch: 46 , batch: 128 , training loss: 3.905350\n",
      "[INFO] Epoch: 46 , batch: 129 , training loss: 3.861979\n",
      "[INFO] Epoch: 46 , batch: 130 , training loss: 3.857236\n",
      "[INFO] Epoch: 46 , batch: 131 , training loss: 3.836967\n",
      "[INFO] Epoch: 46 , batch: 132 , training loss: 3.876596\n",
      "[INFO] Epoch: 46 , batch: 133 , training loss: 3.853823\n",
      "[INFO] Epoch: 46 , batch: 134 , training loss: 3.618834\n",
      "[INFO] Epoch: 46 , batch: 135 , training loss: 3.680507\n",
      "[INFO] Epoch: 46 , batch: 136 , training loss: 3.937870\n",
      "[INFO] Epoch: 46 , batch: 137 , training loss: 3.922022\n",
      "[INFO] Epoch: 46 , batch: 138 , training loss: 3.911314\n",
      "[INFO] Epoch: 46 , batch: 139 , training loss: 4.479574\n",
      "[INFO] Epoch: 46 , batch: 140 , training loss: 4.306114\n",
      "[INFO] Epoch: 46 , batch: 141 , training loss: 4.078070\n",
      "[INFO] Epoch: 46 , batch: 142 , training loss: 3.766091\n",
      "[INFO] Epoch: 46 , batch: 143 , training loss: 3.910277\n",
      "[INFO] Epoch: 46 , batch: 144 , training loss: 3.764162\n",
      "[INFO] Epoch: 46 , batch: 145 , training loss: 3.844100\n",
      "[INFO] Epoch: 46 , batch: 146 , training loss: 4.017588\n",
      "[INFO] Epoch: 46 , batch: 147 , training loss: 3.694889\n",
      "[INFO] Epoch: 46 , batch: 148 , training loss: 3.621643\n",
      "[INFO] Epoch: 46 , batch: 149 , training loss: 3.726421\n",
      "[INFO] Epoch: 46 , batch: 150 , training loss: 3.998372\n",
      "[INFO] Epoch: 46 , batch: 151 , training loss: 3.814767\n",
      "[INFO] Epoch: 46 , batch: 152 , training loss: 3.859120\n",
      "[INFO] Epoch: 46 , batch: 153 , training loss: 3.876879\n",
      "[INFO] Epoch: 46 , batch: 154 , training loss: 3.993804\n",
      "[INFO] Epoch: 46 , batch: 155 , training loss: 4.151880\n",
      "[INFO] Epoch: 46 , batch: 156 , training loss: 3.921517\n",
      "[INFO] Epoch: 46 , batch: 157 , training loss: 3.897718\n",
      "[INFO] Epoch: 46 , batch: 158 , training loss: 4.001882\n",
      "[INFO] Epoch: 46 , batch: 159 , training loss: 3.966336\n",
      "[INFO] Epoch: 46 , batch: 160 , training loss: 4.140216\n",
      "[INFO] Epoch: 46 , batch: 161 , training loss: 4.198220\n",
      "[INFO] Epoch: 46 , batch: 162 , training loss: 4.209755\n",
      "[INFO] Epoch: 46 , batch: 163 , training loss: 4.376337\n",
      "[INFO] Epoch: 46 , batch: 164 , training loss: 4.313727\n",
      "[INFO] Epoch: 46 , batch: 165 , training loss: 4.281453\n",
      "[INFO] Epoch: 46 , batch: 166 , training loss: 4.160478\n",
      "[INFO] Epoch: 46 , batch: 167 , training loss: 4.218574\n",
      "[INFO] Epoch: 46 , batch: 168 , training loss: 3.896292\n",
      "[INFO] Epoch: 46 , batch: 169 , training loss: 3.903990\n",
      "[INFO] Epoch: 46 , batch: 170 , training loss: 4.044764\n",
      "[INFO] Epoch: 46 , batch: 171 , training loss: 3.495042\n",
      "[INFO] Epoch: 46 , batch: 172 , training loss: 3.765023\n",
      "[INFO] Epoch: 46 , batch: 173 , training loss: 4.034596\n",
      "[INFO] Epoch: 46 , batch: 174 , training loss: 4.467283\n",
      "[INFO] Epoch: 46 , batch: 175 , training loss: 4.744326\n",
      "[INFO] Epoch: 46 , batch: 176 , training loss: 4.386229\n",
      "[INFO] Epoch: 46 , batch: 177 , training loss: 4.008947\n",
      "[INFO] Epoch: 46 , batch: 178 , training loss: 4.045435\n",
      "[INFO] Epoch: 46 , batch: 179 , training loss: 4.094862\n",
      "[INFO] Epoch: 46 , batch: 180 , training loss: 4.065384\n",
      "[INFO] Epoch: 46 , batch: 181 , training loss: 4.352593\n",
      "[INFO] Epoch: 46 , batch: 182 , training loss: 4.278041\n",
      "[INFO] Epoch: 46 , batch: 183 , training loss: 4.248118\n",
      "[INFO] Epoch: 46 , batch: 184 , training loss: 4.147888\n",
      "[INFO] Epoch: 46 , batch: 185 , training loss: 4.109244\n",
      "[INFO] Epoch: 46 , batch: 186 , training loss: 4.252645\n",
      "[INFO] Epoch: 46 , batch: 187 , training loss: 4.345969\n",
      "[INFO] Epoch: 46 , batch: 188 , training loss: 4.346422\n",
      "[INFO] Epoch: 46 , batch: 189 , training loss: 4.253625\n",
      "[INFO] Epoch: 46 , batch: 190 , training loss: 4.319307\n",
      "[INFO] Epoch: 46 , batch: 191 , training loss: 4.408024\n",
      "[INFO] Epoch: 46 , batch: 192 , training loss: 4.234859\n",
      "[INFO] Epoch: 46 , batch: 193 , training loss: 4.345412\n",
      "[INFO] Epoch: 46 , batch: 194 , training loss: 4.264025\n",
      "[INFO] Epoch: 46 , batch: 195 , training loss: 4.206429\n",
      "[INFO] Epoch: 46 , batch: 196 , training loss: 4.039290\n",
      "[INFO] Epoch: 46 , batch: 197 , training loss: 4.124724\n",
      "[INFO] Epoch: 46 , batch: 198 , training loss: 4.058003\n",
      "[INFO] Epoch: 46 , batch: 199 , training loss: 4.189986\n",
      "[INFO] Epoch: 46 , batch: 200 , training loss: 4.122790\n",
      "[INFO] Epoch: 46 , batch: 201 , training loss: 4.030188\n",
      "[INFO] Epoch: 46 , batch: 202 , training loss: 4.008493\n",
      "[INFO] Epoch: 46 , batch: 203 , training loss: 4.138606\n",
      "[INFO] Epoch: 46 , batch: 204 , training loss: 4.227224\n",
      "[INFO] Epoch: 46 , batch: 205 , training loss: 3.837400\n",
      "[INFO] Epoch: 46 , batch: 206 , training loss: 3.767302\n",
      "[INFO] Epoch: 46 , batch: 207 , training loss: 3.770505\n",
      "[INFO] Epoch: 46 , batch: 208 , training loss: 4.076570\n",
      "[INFO] Epoch: 46 , batch: 209 , training loss: 4.077527\n",
      "[INFO] Epoch: 46 , batch: 210 , training loss: 4.059988\n",
      "[INFO] Epoch: 46 , batch: 211 , training loss: 4.060421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 46 , batch: 212 , training loss: 4.142693\n",
      "[INFO] Epoch: 46 , batch: 213 , training loss: 4.121246\n",
      "[INFO] Epoch: 46 , batch: 214 , training loss: 4.194067\n",
      "[INFO] Epoch: 46 , batch: 215 , training loss: 4.371464\n",
      "[INFO] Epoch: 46 , batch: 216 , training loss: 4.097270\n",
      "[INFO] Epoch: 46 , batch: 217 , training loss: 4.023808\n",
      "[INFO] Epoch: 46 , batch: 218 , training loss: 4.038309\n",
      "[INFO] Epoch: 46 , batch: 219 , training loss: 4.154564\n",
      "[INFO] Epoch: 46 , batch: 220 , training loss: 3.962392\n",
      "[INFO] Epoch: 46 , batch: 221 , training loss: 3.971924\n",
      "[INFO] Epoch: 46 , batch: 222 , training loss: 4.094746\n",
      "[INFO] Epoch: 46 , batch: 223 , training loss: 4.212735\n",
      "[INFO] Epoch: 46 , batch: 224 , training loss: 4.276827\n",
      "[INFO] Epoch: 46 , batch: 225 , training loss: 4.152712\n",
      "[INFO] Epoch: 46 , batch: 226 , training loss: 4.297818\n",
      "[INFO] Epoch: 46 , batch: 227 , training loss: 4.248156\n",
      "[INFO] Epoch: 46 , batch: 228 , training loss: 4.292593\n",
      "[INFO] Epoch: 46 , batch: 229 , training loss: 4.139981\n",
      "[INFO] Epoch: 46 , batch: 230 , training loss: 3.986274\n",
      "[INFO] Epoch: 46 , batch: 231 , training loss: 3.869561\n",
      "[INFO] Epoch: 46 , batch: 232 , training loss: 4.006428\n",
      "[INFO] Epoch: 46 , batch: 233 , training loss: 4.051110\n",
      "[INFO] Epoch: 46 , batch: 234 , training loss: 3.727822\n",
      "[INFO] Epoch: 46 , batch: 235 , training loss: 3.824430\n",
      "[INFO] Epoch: 46 , batch: 236 , training loss: 3.958456\n",
      "[INFO] Epoch: 46 , batch: 237 , training loss: 4.167418\n",
      "[INFO] Epoch: 46 , batch: 238 , training loss: 3.952154\n",
      "[INFO] Epoch: 46 , batch: 239 , training loss: 3.987470\n",
      "[INFO] Epoch: 46 , batch: 240 , training loss: 4.003250\n",
      "[INFO] Epoch: 46 , batch: 241 , training loss: 3.818463\n",
      "[INFO] Epoch: 46 , batch: 242 , training loss: 3.852227\n",
      "[INFO] Epoch: 46 , batch: 243 , training loss: 4.149896\n",
      "[INFO] Epoch: 46 , batch: 244 , training loss: 4.077922\n",
      "[INFO] Epoch: 46 , batch: 245 , training loss: 4.048548\n",
      "[INFO] Epoch: 46 , batch: 246 , training loss: 3.758978\n",
      "[INFO] Epoch: 46 , batch: 247 , training loss: 3.925238\n",
      "[INFO] Epoch: 46 , batch: 248 , training loss: 4.009320\n",
      "[INFO] Epoch: 46 , batch: 249 , training loss: 3.958498\n",
      "[INFO] Epoch: 46 , batch: 250 , training loss: 3.786935\n",
      "[INFO] Epoch: 46 , batch: 251 , training loss: 4.222412\n",
      "[INFO] Epoch: 46 , batch: 252 , training loss: 3.934185\n",
      "[INFO] Epoch: 46 , batch: 253 , training loss: 3.838077\n",
      "[INFO] Epoch: 46 , batch: 254 , training loss: 4.131479\n",
      "[INFO] Epoch: 46 , batch: 255 , training loss: 4.097142\n",
      "[INFO] Epoch: 46 , batch: 256 , training loss: 4.067420\n",
      "[INFO] Epoch: 46 , batch: 257 , training loss: 4.241710\n",
      "[INFO] Epoch: 46 , batch: 258 , training loss: 4.266359\n",
      "[INFO] Epoch: 46 , batch: 259 , training loss: 4.288486\n",
      "[INFO] Epoch: 46 , batch: 260 , training loss: 4.051429\n",
      "[INFO] Epoch: 46 , batch: 261 , training loss: 4.238625\n",
      "[INFO] Epoch: 46 , batch: 262 , training loss: 4.355675\n",
      "[INFO] Epoch: 46 , batch: 263 , training loss: 4.535446\n",
      "[INFO] Epoch: 46 , batch: 264 , training loss: 3.904029\n",
      "[INFO] Epoch: 46 , batch: 265 , training loss: 4.015884\n",
      "[INFO] Epoch: 46 , batch: 266 , training loss: 4.409201\n",
      "[INFO] Epoch: 46 , batch: 267 , training loss: 4.164304\n",
      "[INFO] Epoch: 46 , batch: 268 , training loss: 4.099562\n",
      "[INFO] Epoch: 46 , batch: 269 , training loss: 4.058804\n",
      "[INFO] Epoch: 46 , batch: 270 , training loss: 4.078820\n",
      "[INFO] Epoch: 46 , batch: 271 , training loss: 4.122564\n",
      "[INFO] Epoch: 46 , batch: 272 , training loss: 4.125957\n",
      "[INFO] Epoch: 46 , batch: 273 , training loss: 4.132112\n",
      "[INFO] Epoch: 46 , batch: 274 , training loss: 4.199599\n",
      "[INFO] Epoch: 46 , batch: 275 , training loss: 4.076216\n",
      "[INFO] Epoch: 46 , batch: 276 , training loss: 4.129638\n",
      "[INFO] Epoch: 46 , batch: 277 , training loss: 4.310006\n",
      "[INFO] Epoch: 46 , batch: 278 , training loss: 3.990459\n",
      "[INFO] Epoch: 46 , batch: 279 , training loss: 3.996291\n",
      "[INFO] Epoch: 46 , batch: 280 , training loss: 3.969354\n",
      "[INFO] Epoch: 46 , batch: 281 , training loss: 4.089156\n",
      "[INFO] Epoch: 46 , batch: 282 , training loss: 4.015099\n",
      "[INFO] Epoch: 46 , batch: 283 , training loss: 4.017817\n",
      "[INFO] Epoch: 46 , batch: 284 , training loss: 4.040155\n",
      "[INFO] Epoch: 46 , batch: 285 , training loss: 3.973778\n",
      "[INFO] Epoch: 46 , batch: 286 , training loss: 3.990052\n",
      "[INFO] Epoch: 46 , batch: 287 , training loss: 3.930455\n",
      "[INFO] Epoch: 46 , batch: 288 , training loss: 3.913407\n",
      "[INFO] Epoch: 46 , batch: 289 , training loss: 3.959100\n",
      "[INFO] Epoch: 46 , batch: 290 , training loss: 3.747439\n",
      "[INFO] Epoch: 46 , batch: 291 , training loss: 3.756265\n",
      "[INFO] Epoch: 46 , batch: 292 , training loss: 3.847460\n",
      "[INFO] Epoch: 46 , batch: 293 , training loss: 3.774302\n",
      "[INFO] Epoch: 46 , batch: 294 , training loss: 4.411579\n",
      "[INFO] Epoch: 46 , batch: 295 , training loss: 4.204206\n",
      "[INFO] Epoch: 46 , batch: 296 , training loss: 4.130665\n",
      "[INFO] Epoch: 46 , batch: 297 , training loss: 4.093809\n",
      "[INFO] Epoch: 46 , batch: 298 , training loss: 3.909810\n",
      "[INFO] Epoch: 46 , batch: 299 , training loss: 3.983796\n",
      "[INFO] Epoch: 46 , batch: 300 , training loss: 3.971749\n",
      "[INFO] Epoch: 46 , batch: 301 , training loss: 3.883207\n",
      "[INFO] Epoch: 46 , batch: 302 , training loss: 4.067613\n",
      "[INFO] Epoch: 46 , batch: 303 , training loss: 4.076189\n",
      "[INFO] Epoch: 46 , batch: 304 , training loss: 4.197276\n",
      "[INFO] Epoch: 46 , batch: 305 , training loss: 4.036840\n",
      "[INFO] Epoch: 46 , batch: 306 , training loss: 4.154254\n",
      "[INFO] Epoch: 46 , batch: 307 , training loss: 4.164654\n",
      "[INFO] Epoch: 46 , batch: 308 , training loss: 3.974140\n",
      "[INFO] Epoch: 46 , batch: 309 , training loss: 3.969275\n",
      "[INFO] Epoch: 46 , batch: 310 , training loss: 3.905948\n",
      "[INFO] Epoch: 46 , batch: 311 , training loss: 3.893051\n",
      "[INFO] Epoch: 46 , batch: 312 , training loss: 3.801476\n",
      "[INFO] Epoch: 46 , batch: 313 , training loss: 3.903982\n",
      "[INFO] Epoch: 46 , batch: 314 , training loss: 3.973762\n",
      "[INFO] Epoch: 46 , batch: 315 , training loss: 4.070731\n",
      "[INFO] Epoch: 46 , batch: 316 , training loss: 4.296592\n",
      "[INFO] Epoch: 46 , batch: 317 , training loss: 4.666426\n",
      "[INFO] Epoch: 46 , batch: 318 , training loss: 4.742167\n",
      "[INFO] Epoch: 46 , batch: 319 , training loss: 4.442998\n",
      "[INFO] Epoch: 46 , batch: 320 , training loss: 4.004183\n",
      "[INFO] Epoch: 46 , batch: 321 , training loss: 3.840114\n",
      "[INFO] Epoch: 46 , batch: 322 , training loss: 3.935270\n",
      "[INFO] Epoch: 46 , batch: 323 , training loss: 3.968622\n",
      "[INFO] Epoch: 46 , batch: 324 , training loss: 3.950631\n",
      "[INFO] Epoch: 46 , batch: 325 , training loss: 4.035263\n",
      "[INFO] Epoch: 46 , batch: 326 , training loss: 4.115101\n",
      "[INFO] Epoch: 46 , batch: 327 , training loss: 4.045973\n",
      "[INFO] Epoch: 46 , batch: 328 , training loss: 4.070427\n",
      "[INFO] Epoch: 46 , batch: 329 , training loss: 3.979546\n",
      "[INFO] Epoch: 46 , batch: 330 , training loss: 3.966134\n",
      "[INFO] Epoch: 46 , batch: 331 , training loss: 4.097181\n",
      "[INFO] Epoch: 46 , batch: 332 , training loss: 3.964413\n",
      "[INFO] Epoch: 46 , batch: 333 , training loss: 3.922398\n",
      "[INFO] Epoch: 46 , batch: 334 , training loss: 3.947262\n",
      "[INFO] Epoch: 46 , batch: 335 , training loss: 4.077399\n",
      "[INFO] Epoch: 46 , batch: 336 , training loss: 4.083521\n",
      "[INFO] Epoch: 46 , batch: 337 , training loss: 4.121063\n",
      "[INFO] Epoch: 46 , batch: 338 , training loss: 4.322841\n",
      "[INFO] Epoch: 46 , batch: 339 , training loss: 4.138438\n",
      "[INFO] Epoch: 46 , batch: 340 , training loss: 4.342897\n",
      "[INFO] Epoch: 46 , batch: 341 , training loss: 4.101921\n",
      "[INFO] Epoch: 46 , batch: 342 , training loss: 3.888509\n",
      "[INFO] Epoch: 46 , batch: 343 , training loss: 3.973149\n",
      "[INFO] Epoch: 46 , batch: 344 , training loss: 3.815008\n",
      "[INFO] Epoch: 46 , batch: 345 , training loss: 3.952050\n",
      "[INFO] Epoch: 46 , batch: 346 , training loss: 3.985395\n",
      "[INFO] Epoch: 46 , batch: 347 , training loss: 3.915012\n",
      "[INFO] Epoch: 46 , batch: 348 , training loss: 4.005506\n",
      "[INFO] Epoch: 46 , batch: 349 , training loss: 4.094304\n",
      "[INFO] Epoch: 46 , batch: 350 , training loss: 3.965962\n",
      "[INFO] Epoch: 46 , batch: 351 , training loss: 4.036872\n",
      "[INFO] Epoch: 46 , batch: 352 , training loss: 4.058431\n",
      "[INFO] Epoch: 46 , batch: 353 , training loss: 4.041881\n",
      "[INFO] Epoch: 46 , batch: 354 , training loss: 4.130105\n",
      "[INFO] Epoch: 46 , batch: 355 , training loss: 4.117980\n",
      "[INFO] Epoch: 46 , batch: 356 , training loss: 3.991919\n",
      "[INFO] Epoch: 46 , batch: 357 , training loss: 4.043866\n",
      "[INFO] Epoch: 46 , batch: 358 , training loss: 3.961210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 46 , batch: 359 , training loss: 3.978185\n",
      "[INFO] Epoch: 46 , batch: 360 , training loss: 4.087867\n",
      "[INFO] Epoch: 46 , batch: 361 , training loss: 4.049159\n",
      "[INFO] Epoch: 46 , batch: 362 , training loss: 4.141613\n",
      "[INFO] Epoch: 46 , batch: 363 , training loss: 4.020756\n",
      "[INFO] Epoch: 46 , batch: 364 , training loss: 4.090300\n",
      "[INFO] Epoch: 46 , batch: 365 , training loss: 4.004445\n",
      "[INFO] Epoch: 46 , batch: 366 , training loss: 4.100435\n",
      "[INFO] Epoch: 46 , batch: 367 , training loss: 4.141784\n",
      "[INFO] Epoch: 46 , batch: 368 , training loss: 4.544741\n",
      "[INFO] Epoch: 46 , batch: 369 , training loss: 4.208873\n",
      "[INFO] Epoch: 46 , batch: 370 , training loss: 3.995488\n",
      "[INFO] Epoch: 46 , batch: 371 , training loss: 4.403529\n",
      "[INFO] Epoch: 46 , batch: 372 , training loss: 4.648892\n",
      "[INFO] Epoch: 46 , batch: 373 , training loss: 4.678662\n",
      "[INFO] Epoch: 46 , batch: 374 , training loss: 4.846461\n",
      "[INFO] Epoch: 46 , batch: 375 , training loss: 4.805854\n",
      "[INFO] Epoch: 46 , batch: 376 , training loss: 4.677471\n",
      "[INFO] Epoch: 46 , batch: 377 , training loss: 4.438046\n",
      "[INFO] Epoch: 46 , batch: 378 , training loss: 4.532077\n",
      "[INFO] Epoch: 46 , batch: 379 , training loss: 4.533257\n",
      "[INFO] Epoch: 46 , batch: 380 , training loss: 4.668085\n",
      "[INFO] Epoch: 46 , batch: 381 , training loss: 4.374010\n",
      "[INFO] Epoch: 46 , batch: 382 , training loss: 4.615157\n",
      "[INFO] Epoch: 46 , batch: 383 , training loss: 4.687243\n",
      "[INFO] Epoch: 46 , batch: 384 , training loss: 4.667360\n",
      "[INFO] Epoch: 46 , batch: 385 , training loss: 4.338456\n",
      "[INFO] Epoch: 46 , batch: 386 , training loss: 4.582318\n",
      "[INFO] Epoch: 46 , batch: 387 , training loss: 4.538032\n",
      "[INFO] Epoch: 46 , batch: 388 , training loss: 4.362236\n",
      "[INFO] Epoch: 46 , batch: 389 , training loss: 4.170668\n",
      "[INFO] Epoch: 46 , batch: 390 , training loss: 4.186855\n",
      "[INFO] Epoch: 46 , batch: 391 , training loss: 4.227410\n",
      "[INFO] Epoch: 46 , batch: 392 , training loss: 4.564353\n",
      "[INFO] Epoch: 46 , batch: 393 , training loss: 4.447761\n",
      "[INFO] Epoch: 46 , batch: 394 , training loss: 4.569600\n",
      "[INFO] Epoch: 46 , batch: 395 , training loss: 4.381627\n",
      "[INFO] Epoch: 46 , batch: 396 , training loss: 4.204864\n",
      "[INFO] Epoch: 46 , batch: 397 , training loss: 4.335929\n",
      "[INFO] Epoch: 46 , batch: 398 , training loss: 4.196700\n",
      "[INFO] Epoch: 46 , batch: 399 , training loss: 4.293077\n",
      "[INFO] Epoch: 46 , batch: 400 , training loss: 4.243699\n",
      "[INFO] Epoch: 46 , batch: 401 , training loss: 4.684411\n",
      "[INFO] Epoch: 46 , batch: 402 , training loss: 4.412279\n",
      "[INFO] Epoch: 46 , batch: 403 , training loss: 4.233501\n",
      "[INFO] Epoch: 46 , batch: 404 , training loss: 4.425490\n",
      "[INFO] Epoch: 46 , batch: 405 , training loss: 4.482155\n",
      "[INFO] Epoch: 46 , batch: 406 , training loss: 4.371612\n",
      "[INFO] Epoch: 46 , batch: 407 , training loss: 4.399280\n",
      "[INFO] Epoch: 46 , batch: 408 , training loss: 4.372899\n",
      "[INFO] Epoch: 46 , batch: 409 , training loss: 4.406989\n",
      "[INFO] Epoch: 46 , batch: 410 , training loss: 4.460411\n",
      "[INFO] Epoch: 46 , batch: 411 , training loss: 4.604653\n",
      "[INFO] Epoch: 46 , batch: 412 , training loss: 4.459491\n",
      "[INFO] Epoch: 46 , batch: 413 , training loss: 4.306959\n",
      "[INFO] Epoch: 46 , batch: 414 , training loss: 4.352014\n",
      "[INFO] Epoch: 46 , batch: 415 , training loss: 4.414664\n",
      "[INFO] Epoch: 46 , batch: 416 , training loss: 4.446471\n",
      "[INFO] Epoch: 46 , batch: 417 , training loss: 4.377903\n",
      "[INFO] Epoch: 46 , batch: 418 , training loss: 4.445332\n",
      "[INFO] Epoch: 46 , batch: 419 , training loss: 4.400640\n",
      "[INFO] Epoch: 46 , batch: 420 , training loss: 4.372411\n",
      "[INFO] Epoch: 46 , batch: 421 , training loss: 4.350842\n",
      "[INFO] Epoch: 46 , batch: 422 , training loss: 4.192741\n",
      "[INFO] Epoch: 46 , batch: 423 , training loss: 4.437617\n",
      "[INFO] Epoch: 46 , batch: 424 , training loss: 4.600766\n",
      "[INFO] Epoch: 46 , batch: 425 , training loss: 4.466883\n",
      "[INFO] Epoch: 46 , batch: 426 , training loss: 4.211865\n",
      "[INFO] Epoch: 46 , batch: 427 , training loss: 4.447488\n",
      "[INFO] Epoch: 46 , batch: 428 , training loss: 4.323362\n",
      "[INFO] Epoch: 46 , batch: 429 , training loss: 4.214155\n",
      "[INFO] Epoch: 46 , batch: 430 , training loss: 4.453829\n",
      "[INFO] Epoch: 46 , batch: 431 , training loss: 4.055856\n",
      "[INFO] Epoch: 46 , batch: 432 , training loss: 4.114077\n",
      "[INFO] Epoch: 46 , batch: 433 , training loss: 4.156063\n",
      "[INFO] Epoch: 46 , batch: 434 , training loss: 4.021585\n",
      "[INFO] Epoch: 46 , batch: 435 , training loss: 4.382257\n",
      "[INFO] Epoch: 46 , batch: 436 , training loss: 4.404729\n",
      "[INFO] Epoch: 46 , batch: 437 , training loss: 4.202007\n",
      "[INFO] Epoch: 46 , batch: 438 , training loss: 4.083472\n",
      "[INFO] Epoch: 46 , batch: 439 , training loss: 4.308484\n",
      "[INFO] Epoch: 46 , batch: 440 , training loss: 4.414958\n",
      "[INFO] Epoch: 46 , batch: 441 , training loss: 4.514963\n",
      "[INFO] Epoch: 46 , batch: 442 , training loss: 4.269958\n",
      "[INFO] Epoch: 46 , batch: 443 , training loss: 4.437795\n",
      "[INFO] Epoch: 46 , batch: 444 , training loss: 4.072453\n",
      "[INFO] Epoch: 46 , batch: 445 , training loss: 3.968426\n",
      "[INFO] Epoch: 46 , batch: 446 , training loss: 3.904122\n",
      "[INFO] Epoch: 46 , batch: 447 , training loss: 4.095928\n",
      "[INFO] Epoch: 46 , batch: 448 , training loss: 4.219313\n",
      "[INFO] Epoch: 46 , batch: 449 , training loss: 4.585323\n",
      "[INFO] Epoch: 46 , batch: 450 , training loss: 4.671097\n",
      "[INFO] Epoch: 46 , batch: 451 , training loss: 4.557280\n",
      "[INFO] Epoch: 46 , batch: 452 , training loss: 4.374018\n",
      "[INFO] Epoch: 46 , batch: 453 , training loss: 4.135448\n",
      "[INFO] Epoch: 46 , batch: 454 , training loss: 4.296193\n",
      "[INFO] Epoch: 46 , batch: 455 , training loss: 4.346891\n",
      "[INFO] Epoch: 46 , batch: 456 , training loss: 4.327522\n",
      "[INFO] Epoch: 46 , batch: 457 , training loss: 4.417601\n",
      "[INFO] Epoch: 46 , batch: 458 , training loss: 4.163212\n",
      "[INFO] Epoch: 46 , batch: 459 , training loss: 4.125619\n",
      "[INFO] Epoch: 46 , batch: 460 , training loss: 4.261214\n",
      "[INFO] Epoch: 46 , batch: 461 , training loss: 4.217954\n",
      "[INFO] Epoch: 46 , batch: 462 , training loss: 4.264552\n",
      "[INFO] Epoch: 46 , batch: 463 , training loss: 4.206175\n",
      "[INFO] Epoch: 46 , batch: 464 , training loss: 4.375924\n",
      "[INFO] Epoch: 46 , batch: 465 , training loss: 4.318016\n",
      "[INFO] Epoch: 46 , batch: 466 , training loss: 4.425629\n",
      "[INFO] Epoch: 46 , batch: 467 , training loss: 4.356674\n",
      "[INFO] Epoch: 46 , batch: 468 , training loss: 4.335471\n",
      "[INFO] Epoch: 46 , batch: 469 , training loss: 4.355563\n",
      "[INFO] Epoch: 46 , batch: 470 , training loss: 4.175011\n",
      "[INFO] Epoch: 46 , batch: 471 , training loss: 4.288780\n",
      "[INFO] Epoch: 46 , batch: 472 , training loss: 4.326598\n",
      "[INFO] Epoch: 46 , batch: 473 , training loss: 4.260178\n",
      "[INFO] Epoch: 46 , batch: 474 , training loss: 4.029547\n",
      "[INFO] Epoch: 46 , batch: 475 , training loss: 3.918488\n",
      "[INFO] Epoch: 46 , batch: 476 , training loss: 4.332303\n",
      "[INFO] Epoch: 46 , batch: 477 , training loss: 4.433399\n",
      "[INFO] Epoch: 46 , batch: 478 , training loss: 4.428435\n",
      "[INFO] Epoch: 46 , batch: 479 , training loss: 4.417666\n",
      "[INFO] Epoch: 46 , batch: 480 , training loss: 4.549276\n",
      "[INFO] Epoch: 46 , batch: 481 , training loss: 4.416963\n",
      "[INFO] Epoch: 46 , batch: 482 , training loss: 4.525253\n",
      "[INFO] Epoch: 46 , batch: 483 , training loss: 4.362635\n",
      "[INFO] Epoch: 46 , batch: 484 , training loss: 4.163890\n",
      "[INFO] Epoch: 46 , batch: 485 , training loss: 4.268864\n",
      "[INFO] Epoch: 46 , batch: 486 , training loss: 4.161452\n",
      "[INFO] Epoch: 46 , batch: 487 , training loss: 4.132193\n",
      "[INFO] Epoch: 46 , batch: 488 , training loss: 4.332673\n",
      "[INFO] Epoch: 46 , batch: 489 , training loss: 4.233123\n",
      "[INFO] Epoch: 46 , batch: 490 , training loss: 4.311609\n",
      "[INFO] Epoch: 46 , batch: 491 , training loss: 4.189125\n",
      "[INFO] Epoch: 46 , batch: 492 , training loss: 4.181736\n",
      "[INFO] Epoch: 46 , batch: 493 , training loss: 4.339456\n",
      "[INFO] Epoch: 46 , batch: 494 , training loss: 4.256849\n",
      "[INFO] Epoch: 46 , batch: 495 , training loss: 4.428176\n",
      "[INFO] Epoch: 46 , batch: 496 , training loss: 4.281616\n",
      "[INFO] Epoch: 46 , batch: 497 , training loss: 4.328162\n",
      "[INFO] Epoch: 46 , batch: 498 , training loss: 4.311850\n",
      "[INFO] Epoch: 46 , batch: 499 , training loss: 4.382203\n",
      "[INFO] Epoch: 46 , batch: 500 , training loss: 4.500342\n",
      "[INFO] Epoch: 46 , batch: 501 , training loss: 4.838477\n",
      "[INFO] Epoch: 46 , batch: 502 , training loss: 4.854311\n",
      "[INFO] Epoch: 46 , batch: 503 , training loss: 4.478585\n",
      "[INFO] Epoch: 46 , batch: 504 , training loss: 4.649504\n",
      "[INFO] Epoch: 46 , batch: 505 , training loss: 4.609511\n",
      "[INFO] Epoch: 46 , batch: 506 , training loss: 4.612788\n",
      "[INFO] Epoch: 46 , batch: 507 , training loss: 4.632999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 46 , batch: 508 , training loss: 4.567935\n",
      "[INFO] Epoch: 46 , batch: 509 , training loss: 4.381943\n",
      "[INFO] Epoch: 46 , batch: 510 , training loss: 4.480635\n",
      "[INFO] Epoch: 46 , batch: 511 , training loss: 4.390144\n",
      "[INFO] Epoch: 46 , batch: 512 , training loss: 4.456573\n",
      "[INFO] Epoch: 46 , batch: 513 , training loss: 4.750403\n",
      "[INFO] Epoch: 46 , batch: 514 , training loss: 4.373122\n",
      "[INFO] Epoch: 46 , batch: 515 , training loss: 4.645488\n",
      "[INFO] Epoch: 46 , batch: 516 , training loss: 4.447607\n",
      "[INFO] Epoch: 46 , batch: 517 , training loss: 4.392859\n",
      "[INFO] Epoch: 46 , batch: 518 , training loss: 4.361473\n",
      "[INFO] Epoch: 46 , batch: 519 , training loss: 4.209286\n",
      "[INFO] Epoch: 46 , batch: 520 , training loss: 4.442626\n",
      "[INFO] Epoch: 46 , batch: 521 , training loss: 4.434275\n",
      "[INFO] Epoch: 46 , batch: 522 , training loss: 4.505460\n",
      "[INFO] Epoch: 46 , batch: 523 , training loss: 4.426692\n",
      "[INFO] Epoch: 46 , batch: 524 , training loss: 4.718602\n",
      "[INFO] Epoch: 46 , batch: 525 , training loss: 4.596883\n",
      "[INFO] Epoch: 46 , batch: 526 , training loss: 4.385646\n",
      "[INFO] Epoch: 46 , batch: 527 , training loss: 4.415302\n",
      "[INFO] Epoch: 46 , batch: 528 , training loss: 4.419528\n",
      "[INFO] Epoch: 46 , batch: 529 , training loss: 4.410061\n",
      "[INFO] Epoch: 46 , batch: 530 , training loss: 4.255660\n",
      "[INFO] Epoch: 46 , batch: 531 , training loss: 4.408285\n",
      "[INFO] Epoch: 46 , batch: 532 , training loss: 4.335338\n",
      "[INFO] Epoch: 46 , batch: 533 , training loss: 4.427681\n",
      "[INFO] Epoch: 46 , batch: 534 , training loss: 4.454933\n",
      "[INFO] Epoch: 46 , batch: 535 , training loss: 4.444804\n",
      "[INFO] Epoch: 46 , batch: 536 , training loss: 4.289140\n",
      "[INFO] Epoch: 46 , batch: 537 , training loss: 4.274559\n",
      "[INFO] Epoch: 46 , batch: 538 , training loss: 4.374818\n",
      "[INFO] Epoch: 46 , batch: 539 , training loss: 4.459575\n",
      "[INFO] Epoch: 46 , batch: 540 , training loss: 5.031765\n",
      "[INFO] Epoch: 46 , batch: 541 , training loss: 4.805901\n",
      "[INFO] Epoch: 46 , batch: 542 , training loss: 4.694925\n",
      "[INFO] Epoch: 47 , batch: 0 , training loss: 3.625553\n",
      "[INFO] Epoch: 47 , batch: 1 , training loss: 3.458722\n",
      "[INFO] Epoch: 47 , batch: 2 , training loss: 3.704110\n",
      "[INFO] Epoch: 47 , batch: 3 , training loss: 3.601713\n",
      "[INFO] Epoch: 47 , batch: 4 , training loss: 3.934530\n",
      "[INFO] Epoch: 47 , batch: 5 , training loss: 3.601810\n",
      "[INFO] Epoch: 47 , batch: 6 , training loss: 3.932154\n",
      "[INFO] Epoch: 47 , batch: 7 , training loss: 3.892616\n",
      "[INFO] Epoch: 47 , batch: 8 , training loss: 3.574710\n",
      "[INFO] Epoch: 47 , batch: 9 , training loss: 3.834048\n",
      "[INFO] Epoch: 47 , batch: 10 , training loss: 3.804788\n",
      "[INFO] Epoch: 47 , batch: 11 , training loss: 3.763510\n",
      "[INFO] Epoch: 47 , batch: 12 , training loss: 3.608490\n",
      "[INFO] Epoch: 47 , batch: 13 , training loss: 3.636696\n",
      "[INFO] Epoch: 47 , batch: 14 , training loss: 3.564327\n",
      "[INFO] Epoch: 47 , batch: 15 , training loss: 3.775062\n",
      "[INFO] Epoch: 47 , batch: 16 , training loss: 3.617224\n",
      "[INFO] Epoch: 47 , batch: 17 , training loss: 3.743355\n",
      "[INFO] Epoch: 47 , batch: 18 , training loss: 3.681216\n",
      "[INFO] Epoch: 47 , batch: 19 , training loss: 3.485646\n",
      "[INFO] Epoch: 47 , batch: 20 , training loss: 3.431824\n",
      "[INFO] Epoch: 47 , batch: 21 , training loss: 3.584651\n",
      "[INFO] Epoch: 47 , batch: 22 , training loss: 3.457281\n",
      "[INFO] Epoch: 47 , batch: 23 , training loss: 3.679190\n",
      "[INFO] Epoch: 47 , batch: 24 , training loss: 3.502758\n",
      "[INFO] Epoch: 47 , batch: 25 , training loss: 3.592911\n",
      "[INFO] Epoch: 47 , batch: 26 , training loss: 3.493805\n",
      "[INFO] Epoch: 47 , batch: 27 , training loss: 3.491371\n",
      "[INFO] Epoch: 47 , batch: 28 , training loss: 3.655908\n",
      "[INFO] Epoch: 47 , batch: 29 , training loss: 3.474687\n",
      "[INFO] Epoch: 47 , batch: 30 , training loss: 3.505985\n",
      "[INFO] Epoch: 47 , batch: 31 , training loss: 3.585935\n",
      "[INFO] Epoch: 47 , batch: 32 , training loss: 3.576242\n",
      "[INFO] Epoch: 47 , batch: 33 , training loss: 3.587920\n",
      "[INFO] Epoch: 47 , batch: 34 , training loss: 3.575020\n",
      "[INFO] Epoch: 47 , batch: 35 , training loss: 3.535670\n",
      "[INFO] Epoch: 47 , batch: 36 , training loss: 3.612688\n",
      "[INFO] Epoch: 47 , batch: 37 , training loss: 3.491332\n",
      "[INFO] Epoch: 47 , batch: 38 , training loss: 3.576220\n",
      "[INFO] Epoch: 47 , batch: 39 , training loss: 3.409148\n",
      "[INFO] Epoch: 47 , batch: 40 , training loss: 3.589197\n",
      "[INFO] Epoch: 47 , batch: 41 , training loss: 3.520672\n",
      "[INFO] Epoch: 47 , batch: 42 , training loss: 3.986037\n",
      "[INFO] Epoch: 47 , batch: 43 , training loss: 3.718689\n",
      "[INFO] Epoch: 47 , batch: 44 , training loss: 4.080863\n",
      "[INFO] Epoch: 47 , batch: 45 , training loss: 4.040135\n",
      "[INFO] Epoch: 47 , batch: 46 , training loss: 3.955696\n",
      "[INFO] Epoch: 47 , batch: 47 , training loss: 3.570278\n",
      "[INFO] Epoch: 47 , batch: 48 , training loss: 3.582202\n",
      "[INFO] Epoch: 47 , batch: 49 , training loss: 3.830141\n",
      "[INFO] Epoch: 47 , batch: 50 , training loss: 3.573165\n",
      "[INFO] Epoch: 47 , batch: 51 , training loss: 3.817874\n",
      "[INFO] Epoch: 47 , batch: 52 , training loss: 3.620452\n",
      "[INFO] Epoch: 47 , batch: 53 , training loss: 3.737470\n",
      "[INFO] Epoch: 47 , batch: 54 , training loss: 3.769585\n",
      "[INFO] Epoch: 47 , batch: 55 , training loss: 3.818180\n",
      "[INFO] Epoch: 47 , batch: 56 , training loss: 3.657906\n",
      "[INFO] Epoch: 47 , batch: 57 , training loss: 3.575843\n",
      "[INFO] Epoch: 47 , batch: 58 , training loss: 3.615688\n",
      "[INFO] Epoch: 47 , batch: 59 , training loss: 3.699659\n",
      "[INFO] Epoch: 47 , batch: 60 , training loss: 3.655717\n",
      "[INFO] Epoch: 47 , batch: 61 , training loss: 3.729886\n",
      "[INFO] Epoch: 47 , batch: 62 , training loss: 3.608099\n",
      "[INFO] Epoch: 47 , batch: 63 , training loss: 3.765209\n",
      "[INFO] Epoch: 47 , batch: 64 , training loss: 4.002071\n",
      "[INFO] Epoch: 47 , batch: 65 , training loss: 3.730603\n",
      "[INFO] Epoch: 47 , batch: 66 , training loss: 3.577941\n",
      "[INFO] Epoch: 47 , batch: 67 , training loss: 3.589102\n",
      "[INFO] Epoch: 47 , batch: 68 , training loss: 3.734934\n",
      "[INFO] Epoch: 47 , batch: 69 , training loss: 3.686362\n",
      "[INFO] Epoch: 47 , batch: 70 , training loss: 3.916957\n",
      "[INFO] Epoch: 47 , batch: 71 , training loss: 3.756142\n",
      "[INFO] Epoch: 47 , batch: 72 , training loss: 3.832918\n",
      "[INFO] Epoch: 47 , batch: 73 , training loss: 3.758348\n",
      "[INFO] Epoch: 47 , batch: 74 , training loss: 3.872577\n",
      "[INFO] Epoch: 47 , batch: 75 , training loss: 3.727733\n",
      "[INFO] Epoch: 47 , batch: 76 , training loss: 3.843229\n",
      "[INFO] Epoch: 47 , batch: 77 , training loss: 3.783849\n",
      "[INFO] Epoch: 47 , batch: 78 , training loss: 3.860928\n",
      "[INFO] Epoch: 47 , batch: 79 , training loss: 3.737146\n",
      "[INFO] Epoch: 47 , batch: 80 , training loss: 3.927581\n",
      "[INFO] Epoch: 47 , batch: 81 , training loss: 3.854721\n",
      "[INFO] Epoch: 47 , batch: 82 , training loss: 3.841082\n",
      "[INFO] Epoch: 47 , batch: 83 , training loss: 3.871329\n",
      "[INFO] Epoch: 47 , batch: 84 , training loss: 3.905378\n",
      "[INFO] Epoch: 47 , batch: 85 , training loss: 3.970251\n",
      "[INFO] Epoch: 47 , batch: 86 , training loss: 3.904603\n",
      "[INFO] Epoch: 47 , batch: 87 , training loss: 3.865703\n",
      "[INFO] Epoch: 47 , batch: 88 , training loss: 3.983768\n",
      "[INFO] Epoch: 47 , batch: 89 , training loss: 3.783397\n",
      "[INFO] Epoch: 47 , batch: 90 , training loss: 3.890564\n",
      "[INFO] Epoch: 47 , batch: 91 , training loss: 3.806520\n",
      "[INFO] Epoch: 47 , batch: 92 , training loss: 3.816010\n",
      "[INFO] Epoch: 47 , batch: 93 , training loss: 3.930183\n",
      "[INFO] Epoch: 47 , batch: 94 , training loss: 4.039623\n",
      "[INFO] Epoch: 47 , batch: 95 , training loss: 3.839882\n",
      "[INFO] Epoch: 47 , batch: 96 , training loss: 3.819235\n",
      "[INFO] Epoch: 47 , batch: 97 , training loss: 3.760435\n",
      "[INFO] Epoch: 47 , batch: 98 , training loss: 3.684893\n",
      "[INFO] Epoch: 47 , batch: 99 , training loss: 3.809407\n",
      "[INFO] Epoch: 47 , batch: 100 , training loss: 3.711590\n",
      "[INFO] Epoch: 47 , batch: 101 , training loss: 3.710504\n",
      "[INFO] Epoch: 47 , batch: 102 , training loss: 3.880784\n",
      "[INFO] Epoch: 47 , batch: 103 , training loss: 3.698150\n",
      "[INFO] Epoch: 47 , batch: 104 , training loss: 3.645913\n",
      "[INFO] Epoch: 47 , batch: 105 , training loss: 3.902091\n",
      "[INFO] Epoch: 47 , batch: 106 , training loss: 3.955495\n",
      "[INFO] Epoch: 47 , batch: 107 , training loss: 3.792626\n",
      "[INFO] Epoch: 47 , batch: 108 , training loss: 3.705208\n",
      "[INFO] Epoch: 47 , batch: 109 , training loss: 3.621783\n",
      "[INFO] Epoch: 47 , batch: 110 , training loss: 3.794619\n",
      "[INFO] Epoch: 47 , batch: 111 , training loss: 3.889108\n",
      "[INFO] Epoch: 47 , batch: 112 , training loss: 3.813415\n",
      "[INFO] Epoch: 47 , batch: 113 , training loss: 3.793977\n",
      "[INFO] Epoch: 47 , batch: 114 , training loss: 3.783535\n",
      "[INFO] Epoch: 47 , batch: 115 , training loss: 3.812240\n",
      "[INFO] Epoch: 47 , batch: 116 , training loss: 3.696918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 47 , batch: 117 , training loss: 3.924545\n",
      "[INFO] Epoch: 47 , batch: 118 , training loss: 3.885054\n",
      "[INFO] Epoch: 47 , batch: 119 , training loss: 4.033671\n",
      "[INFO] Epoch: 47 , batch: 120 , training loss: 4.005968\n",
      "[INFO] Epoch: 47 , batch: 121 , training loss: 3.915914\n",
      "[INFO] Epoch: 47 , batch: 122 , training loss: 3.772105\n",
      "[INFO] Epoch: 47 , batch: 123 , training loss: 3.762393\n",
      "[INFO] Epoch: 47 , batch: 124 , training loss: 3.865697\n",
      "[INFO] Epoch: 47 , batch: 125 , training loss: 3.701411\n",
      "[INFO] Epoch: 47 , batch: 126 , training loss: 3.704883\n",
      "[INFO] Epoch: 47 , batch: 127 , training loss: 3.724151\n",
      "[INFO] Epoch: 47 , batch: 128 , training loss: 3.846895\n",
      "[INFO] Epoch: 47 , batch: 129 , training loss: 3.820771\n",
      "[INFO] Epoch: 47 , batch: 130 , training loss: 3.811050\n",
      "[INFO] Epoch: 47 , batch: 131 , training loss: 3.794037\n",
      "[INFO] Epoch: 47 , batch: 132 , training loss: 3.845552\n",
      "[INFO] Epoch: 47 , batch: 133 , training loss: 3.809584\n",
      "[INFO] Epoch: 47 , batch: 134 , training loss: 3.564701\n",
      "[INFO] Epoch: 47 , batch: 135 , training loss: 3.643798\n",
      "[INFO] Epoch: 47 , batch: 136 , training loss: 3.877145\n",
      "[INFO] Epoch: 47 , batch: 137 , training loss: 3.846235\n",
      "[INFO] Epoch: 47 , batch: 138 , training loss: 3.875198\n",
      "[INFO] Epoch: 47 , batch: 139 , training loss: 4.427668\n",
      "[INFO] Epoch: 47 , batch: 140 , training loss: 4.218647\n",
      "[INFO] Epoch: 47 , batch: 141 , training loss: 4.000906\n",
      "[INFO] Epoch: 47 , batch: 142 , training loss: 3.720974\n",
      "[INFO] Epoch: 47 , batch: 143 , training loss: 3.876236\n",
      "[INFO] Epoch: 47 , batch: 144 , training loss: 3.725663\n",
      "[INFO] Epoch: 47 , batch: 145 , training loss: 3.801737\n",
      "[INFO] Epoch: 47 , batch: 146 , training loss: 3.971248\n",
      "[INFO] Epoch: 47 , batch: 147 , training loss: 3.641789\n",
      "[INFO] Epoch: 47 , batch: 148 , training loss: 3.626865\n",
      "[INFO] Epoch: 47 , batch: 149 , training loss: 3.681406\n",
      "[INFO] Epoch: 47 , batch: 150 , training loss: 3.955045\n",
      "[INFO] Epoch: 47 , batch: 151 , training loss: 3.781371\n",
      "[INFO] Epoch: 47 , batch: 152 , training loss: 3.837914\n",
      "[INFO] Epoch: 47 , batch: 153 , training loss: 3.841556\n",
      "[INFO] Epoch: 47 , batch: 154 , training loss: 3.943140\n",
      "[INFO] Epoch: 47 , batch: 155 , training loss: 4.133256\n",
      "[INFO] Epoch: 47 , batch: 156 , training loss: 3.895599\n",
      "[INFO] Epoch: 47 , batch: 157 , training loss: 3.840407\n",
      "[INFO] Epoch: 47 , batch: 158 , training loss: 3.927576\n",
      "[INFO] Epoch: 47 , batch: 159 , training loss: 3.895580\n",
      "[INFO] Epoch: 47 , batch: 160 , training loss: 4.101093\n",
      "[INFO] Epoch: 47 , batch: 161 , training loss: 4.149875\n",
      "[INFO] Epoch: 47 , batch: 162 , training loss: 4.151095\n",
      "[INFO] Epoch: 47 , batch: 163 , training loss: 4.334428\n",
      "[INFO] Epoch: 47 , batch: 164 , training loss: 4.291572\n",
      "[INFO] Epoch: 47 , batch: 165 , training loss: 4.224905\n",
      "[INFO] Epoch: 47 , batch: 166 , training loss: 4.100585\n",
      "[INFO] Epoch: 47 , batch: 167 , training loss: 4.126204\n",
      "[INFO] Epoch: 47 , batch: 168 , training loss: 3.738388\n",
      "[INFO] Epoch: 47 , batch: 169 , training loss: 3.803317\n",
      "[INFO] Epoch: 47 , batch: 170 , training loss: 3.964256\n",
      "[INFO] Epoch: 47 , batch: 171 , training loss: 3.426711\n",
      "[INFO] Epoch: 47 , batch: 172 , training loss: 3.659372\n",
      "[INFO] Epoch: 47 , batch: 173 , training loss: 3.960471\n",
      "[INFO] Epoch: 47 , batch: 174 , training loss: 4.419712\n",
      "[INFO] Epoch: 47 , batch: 175 , training loss: 4.744812\n",
      "[INFO] Epoch: 47 , batch: 176 , training loss: 4.384484\n",
      "[INFO] Epoch: 47 , batch: 177 , training loss: 3.961177\n",
      "[INFO] Epoch: 47 , batch: 178 , training loss: 3.983423\n",
      "[INFO] Epoch: 47 , batch: 179 , training loss: 4.069583\n",
      "[INFO] Epoch: 47 , batch: 180 , training loss: 4.010552\n",
      "[INFO] Epoch: 47 , batch: 181 , training loss: 4.293080\n",
      "[INFO] Epoch: 47 , batch: 182 , training loss: 4.258415\n",
      "[INFO] Epoch: 47 , batch: 183 , training loss: 4.226831\n",
      "[INFO] Epoch: 47 , batch: 184 , training loss: 4.133906\n",
      "[INFO] Epoch: 47 , batch: 185 , training loss: 4.098446\n",
      "[INFO] Epoch: 47 , batch: 186 , training loss: 4.228061\n",
      "[INFO] Epoch: 47 , batch: 187 , training loss: 4.298496\n",
      "[INFO] Epoch: 47 , batch: 188 , training loss: 4.332252\n",
      "[INFO] Epoch: 47 , batch: 189 , training loss: 4.228584\n",
      "[INFO] Epoch: 47 , batch: 190 , training loss: 4.290545\n",
      "[INFO] Epoch: 47 , batch: 191 , training loss: 4.357531\n",
      "[INFO] Epoch: 47 , batch: 192 , training loss: 4.195548\n",
      "[INFO] Epoch: 47 , batch: 193 , training loss: 4.296789\n",
      "[INFO] Epoch: 47 , batch: 194 , training loss: 4.252098\n",
      "[INFO] Epoch: 47 , batch: 195 , training loss: 4.173950\n",
      "[INFO] Epoch: 47 , batch: 196 , training loss: 4.011333\n",
      "[INFO] Epoch: 47 , batch: 197 , training loss: 4.119091\n",
      "[INFO] Epoch: 47 , batch: 198 , training loss: 4.035382\n",
      "[INFO] Epoch: 47 , batch: 199 , training loss: 4.187670\n",
      "[INFO] Epoch: 47 , batch: 200 , training loss: 4.098035\n",
      "[INFO] Epoch: 47 , batch: 201 , training loss: 3.972141\n",
      "[INFO] Epoch: 47 , batch: 202 , training loss: 3.999564\n",
      "[INFO] Epoch: 47 , batch: 203 , training loss: 4.125123\n",
      "[INFO] Epoch: 47 , batch: 204 , training loss: 4.194458\n",
      "[INFO] Epoch: 47 , batch: 205 , training loss: 3.810853\n",
      "[INFO] Epoch: 47 , batch: 206 , training loss: 3.751071\n",
      "[INFO] Epoch: 47 , batch: 207 , training loss: 3.736472\n",
      "[INFO] Epoch: 47 , batch: 208 , training loss: 4.053273\n",
      "[INFO] Epoch: 47 , batch: 209 , training loss: 4.035772\n",
      "[INFO] Epoch: 47 , batch: 210 , training loss: 4.029936\n",
      "[INFO] Epoch: 47 , batch: 211 , training loss: 4.044741\n",
      "[INFO] Epoch: 47 , batch: 212 , training loss: 4.110039\n",
      "[INFO] Epoch: 47 , batch: 213 , training loss: 4.092760\n",
      "[INFO] Epoch: 47 , batch: 214 , training loss: 4.175912\n",
      "[INFO] Epoch: 47 , batch: 215 , training loss: 4.351184\n",
      "[INFO] Epoch: 47 , batch: 216 , training loss: 4.066052\n",
      "[INFO] Epoch: 47 , batch: 217 , training loss: 4.025017\n",
      "[INFO] Epoch: 47 , batch: 218 , training loss: 4.017581\n",
      "[INFO] Epoch: 47 , batch: 219 , training loss: 4.124281\n",
      "[INFO] Epoch: 47 , batch: 220 , training loss: 3.957504\n",
      "[INFO] Epoch: 47 , batch: 221 , training loss: 3.953094\n",
      "[INFO] Epoch: 47 , batch: 222 , training loss: 4.077408\n",
      "[INFO] Epoch: 47 , batch: 223 , training loss: 4.190004\n",
      "[INFO] Epoch: 47 , batch: 224 , training loss: 4.261257\n",
      "[INFO] Epoch: 47 , batch: 225 , training loss: 4.131066\n",
      "[INFO] Epoch: 47 , batch: 226 , training loss: 4.255861\n",
      "[INFO] Epoch: 47 , batch: 227 , training loss: 4.235656\n",
      "[INFO] Epoch: 47 , batch: 228 , training loss: 4.235365\n",
      "[INFO] Epoch: 47 , batch: 229 , training loss: 4.114851\n",
      "[INFO] Epoch: 47 , batch: 230 , training loss: 3.996317\n",
      "[INFO] Epoch: 47 , batch: 231 , training loss: 3.839931\n",
      "[INFO] Epoch: 47 , batch: 232 , training loss: 3.986255\n",
      "[INFO] Epoch: 47 , batch: 233 , training loss: 4.027872\n",
      "[INFO] Epoch: 47 , batch: 234 , training loss: 3.711330\n",
      "[INFO] Epoch: 47 , batch: 235 , training loss: 3.821692\n",
      "[INFO] Epoch: 47 , batch: 236 , training loss: 3.933093\n",
      "[INFO] Epoch: 47 , batch: 237 , training loss: 4.142944\n",
      "[INFO] Epoch: 47 , batch: 238 , training loss: 3.919744\n",
      "[INFO] Epoch: 47 , batch: 239 , training loss: 3.955629\n",
      "[INFO] Epoch: 47 , batch: 240 , training loss: 3.992654\n",
      "[INFO] Epoch: 47 , batch: 241 , training loss: 3.810557\n",
      "[INFO] Epoch: 47 , batch: 242 , training loss: 3.819419\n",
      "[INFO] Epoch: 47 , batch: 243 , training loss: 4.127084\n",
      "[INFO] Epoch: 47 , batch: 244 , training loss: 4.055045\n",
      "[INFO] Epoch: 47 , batch: 245 , training loss: 3.999887\n",
      "[INFO] Epoch: 47 , batch: 246 , training loss: 3.749189\n",
      "[INFO] Epoch: 47 , batch: 247 , training loss: 3.903499\n",
      "[INFO] Epoch: 47 , batch: 248 , training loss: 3.981284\n",
      "[INFO] Epoch: 47 , batch: 249 , training loss: 3.947348\n",
      "[INFO] Epoch: 47 , batch: 250 , training loss: 3.763835\n",
      "[INFO] Epoch: 47 , batch: 251 , training loss: 4.199700\n",
      "[INFO] Epoch: 47 , batch: 252 , training loss: 3.910687\n",
      "[INFO] Epoch: 47 , batch: 253 , training loss: 3.823593\n",
      "[INFO] Epoch: 47 , batch: 254 , training loss: 4.079163\n",
      "[INFO] Epoch: 47 , batch: 255 , training loss: 4.071825\n",
      "[INFO] Epoch: 47 , batch: 256 , training loss: 4.053911\n",
      "[INFO] Epoch: 47 , batch: 257 , training loss: 4.227726\n",
      "[INFO] Epoch: 47 , batch: 258 , training loss: 4.227607\n",
      "[INFO] Epoch: 47 , batch: 259 , training loss: 4.252488\n",
      "[INFO] Epoch: 47 , batch: 260 , training loss: 4.046053\n",
      "[INFO] Epoch: 47 , batch: 261 , training loss: 4.210643\n",
      "[INFO] Epoch: 47 , batch: 262 , training loss: 4.327712\n",
      "[INFO] Epoch: 47 , batch: 263 , training loss: 4.511854\n",
      "[INFO] Epoch: 47 , batch: 264 , training loss: 3.875798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 47 , batch: 265 , training loss: 3.994462\n",
      "[INFO] Epoch: 47 , batch: 266 , training loss: 4.384192\n",
      "[INFO] Epoch: 47 , batch: 267 , training loss: 4.148249\n",
      "[INFO] Epoch: 47 , batch: 268 , training loss: 4.078931\n",
      "[INFO] Epoch: 47 , batch: 269 , training loss: 4.032008\n",
      "[INFO] Epoch: 47 , batch: 270 , training loss: 4.053492\n",
      "[INFO] Epoch: 47 , batch: 271 , training loss: 4.114981\n",
      "[INFO] Epoch: 47 , batch: 272 , training loss: 4.084622\n",
      "[INFO] Epoch: 47 , batch: 273 , training loss: 4.100100\n",
      "[INFO] Epoch: 47 , batch: 274 , training loss: 4.190498\n",
      "[INFO] Epoch: 47 , batch: 275 , training loss: 4.066297\n",
      "[INFO] Epoch: 47 , batch: 276 , training loss: 4.122035\n",
      "[INFO] Epoch: 47 , batch: 277 , training loss: 4.297259\n",
      "[INFO] Epoch: 47 , batch: 278 , training loss: 3.980254\n",
      "[INFO] Epoch: 47 , batch: 279 , training loss: 3.973003\n",
      "[INFO] Epoch: 47 , batch: 280 , training loss: 3.959159\n",
      "[INFO] Epoch: 47 , batch: 281 , training loss: 4.090239\n",
      "[INFO] Epoch: 47 , batch: 282 , training loss: 3.982201\n",
      "[INFO] Epoch: 47 , batch: 283 , training loss: 3.981110\n",
      "[INFO] Epoch: 47 , batch: 284 , training loss: 4.023764\n",
      "[INFO] Epoch: 47 , batch: 285 , training loss: 3.972149\n",
      "[INFO] Epoch: 47 , batch: 286 , training loss: 3.970582\n",
      "[INFO] Epoch: 47 , batch: 287 , training loss: 3.927040\n",
      "[INFO] Epoch: 47 , batch: 288 , training loss: 3.868594\n",
      "[INFO] Epoch: 47 , batch: 289 , training loss: 3.948043\n",
      "[INFO] Epoch: 47 , batch: 290 , training loss: 3.734534\n",
      "[INFO] Epoch: 47 , batch: 291 , training loss: 3.732896\n",
      "[INFO] Epoch: 47 , batch: 292 , training loss: 3.832812\n",
      "[INFO] Epoch: 47 , batch: 293 , training loss: 3.755718\n",
      "[INFO] Epoch: 47 , batch: 294 , training loss: 4.393069\n",
      "[INFO] Epoch: 47 , batch: 295 , training loss: 4.182661\n",
      "[INFO] Epoch: 47 , batch: 296 , training loss: 4.115520\n",
      "[INFO] Epoch: 47 , batch: 297 , training loss: 4.067048\n",
      "[INFO] Epoch: 47 , batch: 298 , training loss: 3.898809\n",
      "[INFO] Epoch: 47 , batch: 299 , training loss: 3.963072\n",
      "[INFO] Epoch: 47 , batch: 300 , training loss: 3.940995\n",
      "[INFO] Epoch: 47 , batch: 301 , training loss: 3.872559\n",
      "[INFO] Epoch: 47 , batch: 302 , training loss: 4.041893\n",
      "[INFO] Epoch: 47 , batch: 303 , training loss: 4.037875\n",
      "[INFO] Epoch: 47 , batch: 304 , training loss: 4.175469\n",
      "[INFO] Epoch: 47 , batch: 305 , training loss: 4.020233\n",
      "[INFO] Epoch: 47 , batch: 306 , training loss: 4.123384\n",
      "[INFO] Epoch: 47 , batch: 307 , training loss: 4.163317\n",
      "[INFO] Epoch: 47 , batch: 308 , training loss: 3.951201\n",
      "[INFO] Epoch: 47 , batch: 309 , training loss: 3.936951\n",
      "[INFO] Epoch: 47 , batch: 310 , training loss: 3.896972\n",
      "[INFO] Epoch: 47 , batch: 311 , training loss: 3.895028\n",
      "[INFO] Epoch: 47 , batch: 312 , training loss: 3.784474\n",
      "[INFO] Epoch: 47 , batch: 313 , training loss: 3.885149\n",
      "[INFO] Epoch: 47 , batch: 314 , training loss: 3.963331\n",
      "[INFO] Epoch: 47 , batch: 315 , training loss: 4.042896\n",
      "[INFO] Epoch: 47 , batch: 316 , training loss: 4.270996\n",
      "[INFO] Epoch: 47 , batch: 317 , training loss: 4.625256\n",
      "[INFO] Epoch: 47 , batch: 318 , training loss: 4.739843\n",
      "[INFO] Epoch: 47 , batch: 319 , training loss: 4.432854\n",
      "[INFO] Epoch: 47 , batch: 320 , training loss: 3.988317\n",
      "[INFO] Epoch: 47 , batch: 321 , training loss: 3.831221\n",
      "[INFO] Epoch: 47 , batch: 322 , training loss: 3.928469\n",
      "[INFO] Epoch: 47 , batch: 323 , training loss: 3.962770\n",
      "[INFO] Epoch: 47 , batch: 324 , training loss: 3.931869\n",
      "[INFO] Epoch: 47 , batch: 325 , training loss: 4.036896\n",
      "[INFO] Epoch: 47 , batch: 326 , training loss: 4.102411\n",
      "[INFO] Epoch: 47 , batch: 327 , training loss: 4.049255\n",
      "[INFO] Epoch: 47 , batch: 328 , training loss: 4.056557\n",
      "[INFO] Epoch: 47 , batch: 329 , training loss: 3.970464\n",
      "[INFO] Epoch: 47 , batch: 330 , training loss: 3.940397\n",
      "[INFO] Epoch: 47 , batch: 331 , training loss: 4.093276\n",
      "[INFO] Epoch: 47 , batch: 332 , training loss: 3.962060\n",
      "[INFO] Epoch: 47 , batch: 333 , training loss: 3.925377\n",
      "[INFO] Epoch: 47 , batch: 334 , training loss: 3.922557\n",
      "[INFO] Epoch: 47 , batch: 335 , training loss: 4.072134\n",
      "[INFO] Epoch: 47 , batch: 336 , training loss: 4.085488\n",
      "[INFO] Epoch: 47 , batch: 337 , training loss: 4.105628\n",
      "[INFO] Epoch: 47 , batch: 338 , training loss: 4.323388\n",
      "[INFO] Epoch: 47 , batch: 339 , training loss: 4.138962\n",
      "[INFO] Epoch: 47 , batch: 340 , training loss: 4.321435\n",
      "[INFO] Epoch: 47 , batch: 341 , training loss: 4.080495\n",
      "[INFO] Epoch: 47 , batch: 342 , training loss: 3.882702\n",
      "[INFO] Epoch: 47 , batch: 343 , training loss: 3.951282\n",
      "[INFO] Epoch: 47 , batch: 344 , training loss: 3.804576\n",
      "[INFO] Epoch: 47 , batch: 345 , training loss: 3.942366\n",
      "[INFO] Epoch: 47 , batch: 346 , training loss: 3.978097\n",
      "[INFO] Epoch: 47 , batch: 347 , training loss: 3.895221\n",
      "[INFO] Epoch: 47 , batch: 348 , training loss: 3.997148\n",
      "[INFO] Epoch: 47 , batch: 349 , training loss: 4.094230\n",
      "[INFO] Epoch: 47 , batch: 350 , training loss: 3.941160\n",
      "[INFO] Epoch: 47 , batch: 351 , training loss: 4.029203\n",
      "[INFO] Epoch: 47 , batch: 352 , training loss: 4.045049\n",
      "[INFO] Epoch: 47 , batch: 353 , training loss: 4.028726\n",
      "[INFO] Epoch: 47 , batch: 354 , training loss: 4.113367\n",
      "[INFO] Epoch: 47 , batch: 355 , training loss: 4.118018\n",
      "[INFO] Epoch: 47 , batch: 356 , training loss: 3.976348\n",
      "[INFO] Epoch: 47 , batch: 357 , training loss: 4.031214\n",
      "[INFO] Epoch: 47 , batch: 358 , training loss: 3.938465\n",
      "[INFO] Epoch: 47 , batch: 359 , training loss: 3.954495\n",
      "[INFO] Epoch: 47 , batch: 360 , training loss: 4.072549\n",
      "[INFO] Epoch: 47 , batch: 361 , training loss: 4.025314\n",
      "[INFO] Epoch: 47 , batch: 362 , training loss: 4.138583\n",
      "[INFO] Epoch: 47 , batch: 363 , training loss: 4.008357\n",
      "[INFO] Epoch: 47 , batch: 364 , training loss: 4.076211\n",
      "[INFO] Epoch: 47 , batch: 365 , training loss: 3.981710\n",
      "[INFO] Epoch: 47 , batch: 366 , training loss: 4.085843\n",
      "[INFO] Epoch: 47 , batch: 367 , training loss: 4.121694\n",
      "[INFO] Epoch: 47 , batch: 368 , training loss: 4.532164\n",
      "[INFO] Epoch: 47 , batch: 369 , training loss: 4.207717\n",
      "[INFO] Epoch: 47 , batch: 370 , training loss: 3.999673\n",
      "[INFO] Epoch: 47 , batch: 371 , training loss: 4.379333\n",
      "[INFO] Epoch: 47 , batch: 372 , training loss: 4.630927\n",
      "[INFO] Epoch: 47 , batch: 373 , training loss: 4.664607\n",
      "[INFO] Epoch: 47 , batch: 374 , training loss: 4.831959\n",
      "[INFO] Epoch: 47 , batch: 375 , training loss: 4.770100\n",
      "[INFO] Epoch: 47 , batch: 376 , training loss: 4.658160\n",
      "[INFO] Epoch: 47 , batch: 377 , training loss: 4.434417\n",
      "[INFO] Epoch: 47 , batch: 378 , training loss: 4.537820\n",
      "[INFO] Epoch: 47 , batch: 379 , training loss: 4.513932\n",
      "[INFO] Epoch: 47 , batch: 380 , training loss: 4.679028\n",
      "[INFO] Epoch: 47 , batch: 381 , training loss: 4.371799\n",
      "[INFO] Epoch: 47 , batch: 382 , training loss: 4.610070\n",
      "[INFO] Epoch: 47 , batch: 383 , training loss: 4.681958\n",
      "[INFO] Epoch: 47 , batch: 384 , training loss: 4.641095\n",
      "[INFO] Epoch: 47 , batch: 385 , training loss: 4.303246\n",
      "[INFO] Epoch: 47 , batch: 386 , training loss: 4.546722\n",
      "[INFO] Epoch: 47 , batch: 387 , training loss: 4.514435\n",
      "[INFO] Epoch: 47 , batch: 388 , training loss: 4.342590\n",
      "[INFO] Epoch: 47 , batch: 389 , training loss: 4.172455\n",
      "[INFO] Epoch: 47 , batch: 390 , training loss: 4.185441\n",
      "[INFO] Epoch: 47 , batch: 391 , training loss: 4.207870\n",
      "[INFO] Epoch: 47 , batch: 392 , training loss: 4.591698\n",
      "[INFO] Epoch: 47 , batch: 393 , training loss: 4.466855\n",
      "[INFO] Epoch: 47 , batch: 394 , training loss: 4.571890\n",
      "[INFO] Epoch: 47 , batch: 395 , training loss: 4.393255\n",
      "[INFO] Epoch: 47 , batch: 396 , training loss: 4.215579\n",
      "[INFO] Epoch: 47 , batch: 397 , training loss: 4.341267\n",
      "[INFO] Epoch: 47 , batch: 398 , training loss: 4.193231\n",
      "[INFO] Epoch: 47 , batch: 399 , training loss: 4.294281\n",
      "[INFO] Epoch: 47 , batch: 400 , training loss: 4.250027\n",
      "[INFO] Epoch: 47 , batch: 401 , training loss: 4.692780\n",
      "[INFO] Epoch: 47 , batch: 402 , training loss: 4.400409\n",
      "[INFO] Epoch: 47 , batch: 403 , training loss: 4.216621\n",
      "[INFO] Epoch: 47 , batch: 404 , training loss: 4.407358\n",
      "[INFO] Epoch: 47 , batch: 405 , training loss: 4.457828\n",
      "[INFO] Epoch: 47 , batch: 406 , training loss: 4.377133\n",
      "[INFO] Epoch: 47 , batch: 407 , training loss: 4.387899\n",
      "[INFO] Epoch: 47 , batch: 408 , training loss: 4.385907\n",
      "[INFO] Epoch: 47 , batch: 409 , training loss: 4.393554\n",
      "[INFO] Epoch: 47 , batch: 410 , training loss: 4.468198\n",
      "[INFO] Epoch: 47 , batch: 411 , training loss: 4.597137\n",
      "[INFO] Epoch: 47 , batch: 412 , training loss: 4.447025\n",
      "[INFO] Epoch: 47 , batch: 413 , training loss: 4.318307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 47 , batch: 414 , training loss: 4.353868\n",
      "[INFO] Epoch: 47 , batch: 415 , training loss: 4.410403\n",
      "[INFO] Epoch: 47 , batch: 416 , training loss: 4.450389\n",
      "[INFO] Epoch: 47 , batch: 417 , training loss: 4.382257\n",
      "[INFO] Epoch: 47 , batch: 418 , training loss: 4.438700\n",
      "[INFO] Epoch: 47 , batch: 419 , training loss: 4.383143\n",
      "[INFO] Epoch: 47 , batch: 420 , training loss: 4.372610\n",
      "[INFO] Epoch: 47 , batch: 421 , training loss: 4.349707\n",
      "[INFO] Epoch: 47 , batch: 422 , training loss: 4.187109\n",
      "[INFO] Epoch: 47 , batch: 423 , training loss: 4.427421\n",
      "[INFO] Epoch: 47 , batch: 424 , training loss: 4.590421\n",
      "[INFO] Epoch: 47 , batch: 425 , training loss: 4.464659\n",
      "[INFO] Epoch: 47 , batch: 426 , training loss: 4.210494\n",
      "[INFO] Epoch: 47 , batch: 427 , training loss: 4.432581\n",
      "[INFO] Epoch: 47 , batch: 428 , training loss: 4.303010\n",
      "[INFO] Epoch: 47 , batch: 429 , training loss: 4.203489\n",
      "[INFO] Epoch: 47 , batch: 430 , training loss: 4.431836\n",
      "[INFO] Epoch: 47 , batch: 431 , training loss: 4.050576\n",
      "[INFO] Epoch: 47 , batch: 432 , training loss: 4.107208\n",
      "[INFO] Epoch: 47 , batch: 433 , training loss: 4.154472\n",
      "[INFO] Epoch: 47 , batch: 434 , training loss: 4.028385\n",
      "[INFO] Epoch: 47 , batch: 435 , training loss: 4.355381\n",
      "[INFO] Epoch: 47 , batch: 436 , training loss: 4.386757\n",
      "[INFO] Epoch: 47 , batch: 437 , training loss: 4.196390\n",
      "[INFO] Epoch: 47 , batch: 438 , training loss: 4.071683\n",
      "[INFO] Epoch: 47 , batch: 439 , training loss: 4.299302\n",
      "[INFO] Epoch: 47 , batch: 440 , training loss: 4.407136\n",
      "[INFO] Epoch: 47 , batch: 441 , training loss: 4.514503\n",
      "[INFO] Epoch: 47 , batch: 442 , training loss: 4.261039\n",
      "[INFO] Epoch: 47 , batch: 443 , training loss: 4.433749\n",
      "[INFO] Epoch: 47 , batch: 444 , training loss: 4.074688\n",
      "[INFO] Epoch: 47 , batch: 445 , training loss: 3.968666\n",
      "[INFO] Epoch: 47 , batch: 446 , training loss: 3.907466\n",
      "[INFO] Epoch: 47 , batch: 447 , training loss: 4.096455\n",
      "[INFO] Epoch: 47 , batch: 448 , training loss: 4.190027\n",
      "[INFO] Epoch: 47 , batch: 449 , training loss: 4.564125\n",
      "[INFO] Epoch: 47 , batch: 450 , training loss: 4.655760\n",
      "[INFO] Epoch: 47 , batch: 451 , training loss: 4.549984\n",
      "[INFO] Epoch: 47 , batch: 452 , training loss: 4.365160\n",
      "[INFO] Epoch: 47 , batch: 453 , training loss: 4.125531\n",
      "[INFO] Epoch: 47 , batch: 454 , training loss: 4.296991\n",
      "[INFO] Epoch: 47 , batch: 455 , training loss: 4.354901\n",
      "[INFO] Epoch: 47 , batch: 456 , training loss: 4.316241\n",
      "[INFO] Epoch: 47 , batch: 457 , training loss: 4.412617\n",
      "[INFO] Epoch: 47 , batch: 458 , training loss: 4.138891\n",
      "[INFO] Epoch: 47 , batch: 459 , training loss: 4.121121\n",
      "[INFO] Epoch: 47 , batch: 460 , training loss: 4.264320\n",
      "[INFO] Epoch: 47 , batch: 461 , training loss: 4.204094\n",
      "[INFO] Epoch: 47 , batch: 462 , training loss: 4.257088\n",
      "[INFO] Epoch: 47 , batch: 463 , training loss: 4.185382\n",
      "[INFO] Epoch: 47 , batch: 464 , training loss: 4.365032\n",
      "[INFO] Epoch: 47 , batch: 465 , training loss: 4.327622\n",
      "[INFO] Epoch: 47 , batch: 466 , training loss: 4.397011\n",
      "[INFO] Epoch: 47 , batch: 467 , training loss: 4.358290\n",
      "[INFO] Epoch: 47 , batch: 468 , training loss: 4.312614\n",
      "[INFO] Epoch: 47 , batch: 469 , training loss: 4.330181\n",
      "[INFO] Epoch: 47 , batch: 470 , training loss: 4.171429\n",
      "[INFO] Epoch: 47 , batch: 471 , training loss: 4.271311\n",
      "[INFO] Epoch: 47 , batch: 472 , training loss: 4.322707\n",
      "[INFO] Epoch: 47 , batch: 473 , training loss: 4.257382\n",
      "[INFO] Epoch: 47 , batch: 474 , training loss: 4.028225\n",
      "[INFO] Epoch: 47 , batch: 475 , training loss: 3.910901\n",
      "[INFO] Epoch: 47 , batch: 476 , training loss: 4.321584\n",
      "[INFO] Epoch: 47 , batch: 477 , training loss: 4.437002\n",
      "[INFO] Epoch: 47 , batch: 478 , training loss: 4.420905\n",
      "[INFO] Epoch: 47 , batch: 479 , training loss: 4.384719\n",
      "[INFO] Epoch: 47 , batch: 480 , training loss: 4.531903\n",
      "[INFO] Epoch: 47 , batch: 481 , training loss: 4.408292\n",
      "[INFO] Epoch: 47 , batch: 482 , training loss: 4.511993\n",
      "[INFO] Epoch: 47 , batch: 483 , training loss: 4.365559\n",
      "[INFO] Epoch: 47 , batch: 484 , training loss: 4.176886\n",
      "[INFO] Epoch: 47 , batch: 485 , training loss: 4.265386\n",
      "[INFO] Epoch: 47 , batch: 486 , training loss: 4.162987\n",
      "[INFO] Epoch: 47 , batch: 487 , training loss: 4.135686\n",
      "[INFO] Epoch: 47 , batch: 488 , training loss: 4.318966\n",
      "[INFO] Epoch: 47 , batch: 489 , training loss: 4.223972\n",
      "[INFO] Epoch: 47 , batch: 490 , training loss: 4.297760\n",
      "[INFO] Epoch: 47 , batch: 491 , training loss: 4.193129\n",
      "[INFO] Epoch: 47 , batch: 492 , training loss: 4.169326\n",
      "[INFO] Epoch: 47 , batch: 493 , training loss: 4.337961\n",
      "[INFO] Epoch: 47 , batch: 494 , training loss: 4.253887\n",
      "[INFO] Epoch: 47 , batch: 495 , training loss: 4.424206\n",
      "[INFO] Epoch: 47 , batch: 496 , training loss: 4.290942\n",
      "[INFO] Epoch: 47 , batch: 497 , training loss: 4.323370\n",
      "[INFO] Epoch: 47 , batch: 498 , training loss: 4.297256\n",
      "[INFO] Epoch: 47 , batch: 499 , training loss: 4.368534\n",
      "[INFO] Epoch: 47 , batch: 500 , training loss: 4.513780\n",
      "[INFO] Epoch: 47 , batch: 501 , training loss: 4.853035\n",
      "[INFO] Epoch: 47 , batch: 502 , training loss: 4.844410\n",
      "[INFO] Epoch: 47 , batch: 503 , training loss: 4.472470\n",
      "[INFO] Epoch: 47 , batch: 504 , training loss: 4.649383\n",
      "[INFO] Epoch: 47 , batch: 505 , training loss: 4.601967\n",
      "[INFO] Epoch: 47 , batch: 506 , training loss: 4.587540\n",
      "[INFO] Epoch: 47 , batch: 507 , training loss: 4.639126\n",
      "[INFO] Epoch: 47 , batch: 508 , training loss: 4.550432\n",
      "[INFO] Epoch: 47 , batch: 509 , training loss: 4.376684\n",
      "[INFO] Epoch: 47 , batch: 510 , training loss: 4.460938\n",
      "[INFO] Epoch: 47 , batch: 511 , training loss: 4.362089\n",
      "[INFO] Epoch: 47 , batch: 512 , training loss: 4.449344\n",
      "[INFO] Epoch: 47 , batch: 513 , training loss: 4.733446\n",
      "[INFO] Epoch: 47 , batch: 514 , training loss: 4.353242\n",
      "[INFO] Epoch: 47 , batch: 515 , training loss: 4.632972\n",
      "[INFO] Epoch: 47 , batch: 516 , training loss: 4.437926\n",
      "[INFO] Epoch: 47 , batch: 517 , training loss: 4.386980\n",
      "[INFO] Epoch: 47 , batch: 518 , training loss: 4.343324\n",
      "[INFO] Epoch: 47 , batch: 519 , training loss: 4.203527\n",
      "[INFO] Epoch: 47 , batch: 520 , training loss: 4.423689\n",
      "[INFO] Epoch: 47 , batch: 521 , training loss: 4.417075\n",
      "[INFO] Epoch: 47 , batch: 522 , training loss: 4.499408\n",
      "[INFO] Epoch: 47 , batch: 523 , training loss: 4.414517\n",
      "[INFO] Epoch: 47 , batch: 524 , training loss: 4.708386\n",
      "[INFO] Epoch: 47 , batch: 525 , training loss: 4.580144\n",
      "[INFO] Epoch: 47 , batch: 526 , training loss: 4.362916\n",
      "[INFO] Epoch: 47 , batch: 527 , training loss: 4.416316\n",
      "[INFO] Epoch: 47 , batch: 528 , training loss: 4.410449\n",
      "[INFO] Epoch: 47 , batch: 529 , training loss: 4.402132\n",
      "[INFO] Epoch: 47 , batch: 530 , training loss: 4.238618\n",
      "[INFO] Epoch: 47 , batch: 531 , training loss: 4.403661\n",
      "[INFO] Epoch: 47 , batch: 532 , training loss: 4.316689\n",
      "[INFO] Epoch: 47 , batch: 533 , training loss: 4.416606\n",
      "[INFO] Epoch: 47 , batch: 534 , training loss: 4.450029\n",
      "[INFO] Epoch: 47 , batch: 535 , training loss: 4.429927\n",
      "[INFO] Epoch: 47 , batch: 536 , training loss: 4.278854\n",
      "[INFO] Epoch: 47 , batch: 537 , training loss: 4.270594\n",
      "[INFO] Epoch: 47 , batch: 538 , training loss: 4.359544\n",
      "[INFO] Epoch: 47 , batch: 539 , training loss: 4.433038\n",
      "[INFO] Epoch: 47 , batch: 540 , training loss: 4.988186\n",
      "[INFO] Epoch: 47 , batch: 541 , training loss: 4.792336\n",
      "[INFO] Epoch: 47 , batch: 542 , training loss: 4.677186\n",
      "[INFO] Epoch: 48 , batch: 0 , training loss: 3.594410\n",
      "[INFO] Epoch: 48 , batch: 1 , training loss: 3.528500\n",
      "[INFO] Epoch: 48 , batch: 2 , training loss: 3.673726\n",
      "[INFO] Epoch: 48 , batch: 3 , training loss: 3.563304\n",
      "[INFO] Epoch: 48 , batch: 4 , training loss: 3.904167\n",
      "[INFO] Epoch: 48 , batch: 5 , training loss: 3.578916\n",
      "[INFO] Epoch: 48 , batch: 6 , training loss: 3.928807\n",
      "[INFO] Epoch: 48 , batch: 7 , training loss: 3.854650\n",
      "[INFO] Epoch: 48 , batch: 8 , training loss: 3.544036\n",
      "[INFO] Epoch: 48 , batch: 9 , training loss: 3.782933\n",
      "[INFO] Epoch: 48 , batch: 10 , training loss: 3.784137\n",
      "[INFO] Epoch: 48 , batch: 11 , training loss: 3.740737\n",
      "[INFO] Epoch: 48 , batch: 12 , training loss: 3.599515\n",
      "[INFO] Epoch: 48 , batch: 13 , training loss: 3.592753\n",
      "[INFO] Epoch: 48 , batch: 14 , training loss: 3.515816\n",
      "[INFO] Epoch: 48 , batch: 15 , training loss: 3.752576\n",
      "[INFO] Epoch: 48 , batch: 16 , training loss: 3.591718\n",
      "[INFO] Epoch: 48 , batch: 17 , training loss: 3.724411\n",
      "[INFO] Epoch: 48 , batch: 18 , training loss: 3.660542\n",
      "[INFO] Epoch: 48 , batch: 19 , training loss: 3.470267\n",
      "[INFO] Epoch: 48 , batch: 20 , training loss: 3.399997\n",
      "[INFO] Epoch: 48 , batch: 21 , training loss: 3.563765\n",
      "[INFO] Epoch: 48 , batch: 22 , training loss: 3.398442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 48 , batch: 23 , training loss: 3.642562\n",
      "[INFO] Epoch: 48 , batch: 24 , training loss: 3.474717\n",
      "[INFO] Epoch: 48 , batch: 25 , training loss: 3.550514\n",
      "[INFO] Epoch: 48 , batch: 26 , training loss: 3.480280\n",
      "[INFO] Epoch: 48 , batch: 27 , training loss: 3.456706\n",
      "[INFO] Epoch: 48 , batch: 28 , training loss: 3.634218\n",
      "[INFO] Epoch: 48 , batch: 29 , training loss: 3.436448\n",
      "[INFO] Epoch: 48 , batch: 30 , training loss: 3.485947\n",
      "[INFO] Epoch: 48 , batch: 31 , training loss: 3.574774\n",
      "[INFO] Epoch: 48 , batch: 32 , training loss: 3.529854\n",
      "[INFO] Epoch: 48 , batch: 33 , training loss: 3.533601\n",
      "[INFO] Epoch: 48 , batch: 34 , training loss: 3.560288\n",
      "[INFO] Epoch: 48 , batch: 35 , training loss: 3.510040\n",
      "[INFO] Epoch: 48 , batch: 36 , training loss: 3.605576\n",
      "[INFO] Epoch: 48 , batch: 37 , training loss: 3.479010\n",
      "[INFO] Epoch: 48 , batch: 38 , training loss: 3.545219\n",
      "[INFO] Epoch: 48 , batch: 39 , training loss: 3.377616\n",
      "[INFO] Epoch: 48 , batch: 40 , training loss: 3.543277\n",
      "[INFO] Epoch: 48 , batch: 41 , training loss: 3.510185\n",
      "[INFO] Epoch: 48 , batch: 42 , training loss: 3.929833\n",
      "[INFO] Epoch: 48 , batch: 43 , training loss: 3.636326\n",
      "[INFO] Epoch: 48 , batch: 44 , training loss: 4.035573\n",
      "[INFO] Epoch: 48 , batch: 45 , training loss: 3.964858\n",
      "[INFO] Epoch: 48 , batch: 46 , training loss: 3.870863\n",
      "[INFO] Epoch: 48 , batch: 47 , training loss: 3.530408\n",
      "[INFO] Epoch: 48 , batch: 48 , training loss: 3.531127\n",
      "[INFO] Epoch: 48 , batch: 49 , training loss: 3.803438\n",
      "[INFO] Epoch: 48 , batch: 50 , training loss: 3.524725\n",
      "[INFO] Epoch: 48 , batch: 51 , training loss: 3.783503\n",
      "[INFO] Epoch: 48 , batch: 52 , training loss: 3.608492\n",
      "[INFO] Epoch: 48 , batch: 53 , training loss: 3.702093\n",
      "[INFO] Epoch: 48 , batch: 54 , training loss: 3.730114\n",
      "[INFO] Epoch: 48 , batch: 55 , training loss: 3.798841\n",
      "[INFO] Epoch: 48 , batch: 56 , training loss: 3.631191\n",
      "[INFO] Epoch: 48 , batch: 57 , training loss: 3.569553\n",
      "[INFO] Epoch: 48 , batch: 58 , training loss: 3.630453\n",
      "[INFO] Epoch: 48 , batch: 59 , training loss: 3.707670\n",
      "[INFO] Epoch: 48 , batch: 60 , training loss: 3.652965\n",
      "[INFO] Epoch: 48 , batch: 61 , training loss: 3.715872\n",
      "[INFO] Epoch: 48 , batch: 62 , training loss: 3.580605\n",
      "[INFO] Epoch: 48 , batch: 63 , training loss: 3.769467\n",
      "[INFO] Epoch: 48 , batch: 64 , training loss: 4.017216\n",
      "[INFO] Epoch: 48 , batch: 65 , training loss: 3.721191\n",
      "[INFO] Epoch: 48 , batch: 66 , training loss: 3.572140\n",
      "[INFO] Epoch: 48 , batch: 67 , training loss: 3.584511\n",
      "[INFO] Epoch: 48 , batch: 68 , training loss: 3.737175\n",
      "[INFO] Epoch: 48 , batch: 69 , training loss: 3.670556\n",
      "[INFO] Epoch: 48 , batch: 70 , training loss: 3.906027\n",
      "[INFO] Epoch: 48 , batch: 71 , training loss: 3.751371\n",
      "[INFO] Epoch: 48 , batch: 72 , training loss: 3.810314\n",
      "[INFO] Epoch: 48 , batch: 73 , training loss: 3.743500\n",
      "[INFO] Epoch: 48 , batch: 74 , training loss: 3.853974\n",
      "[INFO] Epoch: 48 , batch: 75 , training loss: 3.735578\n",
      "[INFO] Epoch: 48 , batch: 76 , training loss: 3.842492\n",
      "[INFO] Epoch: 48 , batch: 77 , training loss: 3.781252\n",
      "[INFO] Epoch: 48 , batch: 78 , training loss: 3.869138\n",
      "[INFO] Epoch: 48 , batch: 79 , training loss: 3.731250\n",
      "[INFO] Epoch: 48 , batch: 80 , training loss: 3.912911\n",
      "[INFO] Epoch: 48 , batch: 81 , training loss: 3.834275\n",
      "[INFO] Epoch: 48 , batch: 82 , training loss: 3.827113\n",
      "[INFO] Epoch: 48 , batch: 83 , training loss: 3.858557\n",
      "[INFO] Epoch: 48 , batch: 84 , training loss: 3.878742\n",
      "[INFO] Epoch: 48 , batch: 85 , training loss: 3.948584\n",
      "[INFO] Epoch: 48 , batch: 86 , training loss: 3.896151\n",
      "[INFO] Epoch: 48 , batch: 87 , training loss: 3.833853\n",
      "[INFO] Epoch: 48 , batch: 88 , training loss: 3.997932\n",
      "[INFO] Epoch: 48 , batch: 89 , training loss: 3.780654\n",
      "[INFO] Epoch: 48 , batch: 90 , training loss: 3.885664\n",
      "[INFO] Epoch: 48 , batch: 91 , training loss: 3.800477\n",
      "[INFO] Epoch: 48 , batch: 92 , training loss: 3.818494\n",
      "[INFO] Epoch: 48 , batch: 93 , training loss: 3.943493\n",
      "[INFO] Epoch: 48 , batch: 94 , training loss: 4.029625\n",
      "[INFO] Epoch: 48 , batch: 95 , training loss: 3.836027\n",
      "[INFO] Epoch: 48 , batch: 96 , training loss: 3.830074\n",
      "[INFO] Epoch: 48 , batch: 97 , training loss: 3.758990\n",
      "[INFO] Epoch: 48 , batch: 98 , training loss: 3.682540\n",
      "[INFO] Epoch: 48 , batch: 99 , training loss: 3.813048\n",
      "[INFO] Epoch: 48 , batch: 100 , training loss: 3.709600\n",
      "[INFO] Epoch: 48 , batch: 101 , training loss: 3.712636\n",
      "[INFO] Epoch: 48 , batch: 102 , training loss: 3.879987\n",
      "[INFO] Epoch: 48 , batch: 103 , training loss: 3.711185\n",
      "[INFO] Epoch: 48 , batch: 104 , training loss: 3.661989\n",
      "[INFO] Epoch: 48 , batch: 105 , training loss: 3.873504\n",
      "[INFO] Epoch: 48 , batch: 106 , training loss: 3.910591\n",
      "[INFO] Epoch: 48 , batch: 107 , training loss: 3.770834\n",
      "[INFO] Epoch: 48 , batch: 108 , training loss: 3.693125\n",
      "[INFO] Epoch: 48 , batch: 109 , training loss: 3.632632\n",
      "[INFO] Epoch: 48 , batch: 110 , training loss: 3.788361\n",
      "[INFO] Epoch: 48 , batch: 111 , training loss: 3.876291\n",
      "[INFO] Epoch: 48 , batch: 112 , training loss: 3.799229\n",
      "[INFO] Epoch: 48 , batch: 113 , training loss: 3.786889\n",
      "[INFO] Epoch: 48 , batch: 114 , training loss: 3.761811\n",
      "[INFO] Epoch: 48 , batch: 115 , training loss: 3.792082\n",
      "[INFO] Epoch: 48 , batch: 116 , training loss: 3.669369\n",
      "[INFO] Epoch: 48 , batch: 117 , training loss: 3.900884\n",
      "[INFO] Epoch: 48 , batch: 118 , training loss: 3.886682\n",
      "[INFO] Epoch: 48 , batch: 119 , training loss: 4.032207\n",
      "[INFO] Epoch: 48 , batch: 120 , training loss: 4.002765\n",
      "[INFO] Epoch: 48 , batch: 121 , training loss: 3.907862\n",
      "[INFO] Epoch: 48 , batch: 122 , training loss: 3.774255\n",
      "[INFO] Epoch: 48 , batch: 123 , training loss: 3.769496\n",
      "[INFO] Epoch: 48 , batch: 124 , training loss: 3.823337\n",
      "[INFO] Epoch: 48 , batch: 125 , training loss: 3.689733\n",
      "[INFO] Epoch: 48 , batch: 126 , training loss: 3.690692\n",
      "[INFO] Epoch: 48 , batch: 127 , training loss: 3.729322\n",
      "[INFO] Epoch: 48 , batch: 128 , training loss: 3.844833\n",
      "[INFO] Epoch: 48 , batch: 129 , training loss: 3.819823\n",
      "[INFO] Epoch: 48 , batch: 130 , training loss: 3.827305\n",
      "[INFO] Epoch: 48 , batch: 131 , training loss: 3.788672\n",
      "[INFO] Epoch: 48 , batch: 132 , training loss: 3.834181\n",
      "[INFO] Epoch: 48 , batch: 133 , training loss: 3.811512\n",
      "[INFO] Epoch: 48 , batch: 134 , training loss: 3.568944\n",
      "[INFO] Epoch: 48 , batch: 135 , training loss: 3.650412\n",
      "[INFO] Epoch: 48 , batch: 136 , training loss: 3.880178\n",
      "[INFO] Epoch: 48 , batch: 137 , training loss: 3.847726\n",
      "[INFO] Epoch: 48 , batch: 138 , training loss: 3.864499\n",
      "[INFO] Epoch: 48 , batch: 139 , training loss: 4.419928\n",
      "[INFO] Epoch: 48 , batch: 140 , training loss: 4.214153\n",
      "[INFO] Epoch: 48 , batch: 141 , training loss: 3.957345\n",
      "[INFO] Epoch: 48 , batch: 142 , training loss: 3.718618\n",
      "[INFO] Epoch: 48 , batch: 143 , training loss: 3.865263\n",
      "[INFO] Epoch: 48 , batch: 144 , training loss: 3.708099\n",
      "[INFO] Epoch: 48 , batch: 145 , training loss: 3.775163\n",
      "[INFO] Epoch: 48 , batch: 146 , training loss: 3.970650\n",
      "[INFO] Epoch: 48 , batch: 147 , training loss: 3.645563\n",
      "[INFO] Epoch: 48 , batch: 148 , training loss: 3.599912\n",
      "[INFO] Epoch: 48 , batch: 149 , training loss: 3.671815\n",
      "[INFO] Epoch: 48 , batch: 150 , training loss: 3.954747\n",
      "[INFO] Epoch: 48 , batch: 151 , training loss: 3.783634\n",
      "[INFO] Epoch: 48 , batch: 152 , training loss: 3.821983\n",
      "[INFO] Epoch: 48 , batch: 153 , training loss: 3.840667\n",
      "[INFO] Epoch: 48 , batch: 154 , training loss: 3.954381\n",
      "[INFO] Epoch: 48 , batch: 155 , training loss: 4.137994\n",
      "[INFO] Epoch: 48 , batch: 156 , training loss: 3.886827\n",
      "[INFO] Epoch: 48 , batch: 157 , training loss: 3.836543\n",
      "[INFO] Epoch: 48 , batch: 158 , training loss: 3.923765\n",
      "[INFO] Epoch: 48 , batch: 159 , training loss: 3.875499\n",
      "[INFO] Epoch: 48 , batch: 160 , training loss: 4.058440\n",
      "[INFO] Epoch: 48 , batch: 161 , training loss: 4.178263\n",
      "[INFO] Epoch: 48 , batch: 162 , training loss: 4.144723\n",
      "[INFO] Epoch: 48 , batch: 163 , training loss: 4.328172\n",
      "[INFO] Epoch: 48 , batch: 164 , training loss: 4.255309\n",
      "[INFO] Epoch: 48 , batch: 165 , training loss: 4.232405\n",
      "[INFO] Epoch: 48 , batch: 166 , training loss: 4.115798\n",
      "[INFO] Epoch: 48 , batch: 167 , training loss: 4.110215\n",
      "[INFO] Epoch: 48 , batch: 168 , training loss: 3.765339\n",
      "[INFO] Epoch: 48 , batch: 169 , training loss: 3.738570\n",
      "[INFO] Epoch: 48 , batch: 170 , training loss: 3.952317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 48 , batch: 171 , training loss: 3.393822\n",
      "[INFO] Epoch: 48 , batch: 172 , training loss: 3.647697\n",
      "[INFO] Epoch: 48 , batch: 173 , training loss: 3.941448\n",
      "[INFO] Epoch: 48 , batch: 174 , training loss: 4.415146\n",
      "[INFO] Epoch: 48 , batch: 175 , training loss: 4.723542\n",
      "[INFO] Epoch: 48 , batch: 176 , training loss: 4.331188\n",
      "[INFO] Epoch: 48 , batch: 177 , training loss: 3.938386\n",
      "[INFO] Epoch: 48 , batch: 178 , training loss: 3.984232\n",
      "[INFO] Epoch: 48 , batch: 179 , training loss: 4.026006\n",
      "[INFO] Epoch: 48 , batch: 180 , training loss: 4.009607\n",
      "[INFO] Epoch: 48 , batch: 181 , training loss: 4.280703\n",
      "[INFO] Epoch: 48 , batch: 182 , training loss: 4.233521\n",
      "[INFO] Epoch: 48 , batch: 183 , training loss: 4.220863\n",
      "[INFO] Epoch: 48 , batch: 184 , training loss: 4.089161\n",
      "[INFO] Epoch: 48 , batch: 185 , training loss: 4.058758\n",
      "[INFO] Epoch: 48 , batch: 186 , training loss: 4.232439\n",
      "[INFO] Epoch: 48 , batch: 187 , training loss: 4.309390\n",
      "[INFO] Epoch: 48 , batch: 188 , training loss: 4.319325\n",
      "[INFO] Epoch: 48 , batch: 189 , training loss: 4.213478\n",
      "[INFO] Epoch: 48 , batch: 190 , training loss: 4.283504\n",
      "[INFO] Epoch: 48 , batch: 191 , training loss: 4.349898\n",
      "[INFO] Epoch: 48 , batch: 192 , training loss: 4.202727\n",
      "[INFO] Epoch: 48 , batch: 193 , training loss: 4.296621\n",
      "[INFO] Epoch: 48 , batch: 194 , training loss: 4.232845\n",
      "[INFO] Epoch: 48 , batch: 195 , training loss: 4.150003\n",
      "[INFO] Epoch: 48 , batch: 196 , training loss: 4.017969\n",
      "[INFO] Epoch: 48 , batch: 197 , training loss: 4.122163\n",
      "[INFO] Epoch: 48 , batch: 198 , training loss: 4.022038\n",
      "[INFO] Epoch: 48 , batch: 199 , training loss: 4.168865\n",
      "[INFO] Epoch: 48 , batch: 200 , training loss: 4.084347\n",
      "[INFO] Epoch: 48 , batch: 201 , training loss: 3.958050\n",
      "[INFO] Epoch: 48 , batch: 202 , training loss: 3.992170\n",
      "[INFO] Epoch: 48 , batch: 203 , training loss: 4.120693\n",
      "[INFO] Epoch: 48 , batch: 204 , training loss: 4.180643\n",
      "[INFO] Epoch: 48 , batch: 205 , training loss: 3.792244\n",
      "[INFO] Epoch: 48 , batch: 206 , training loss: 3.746804\n",
      "[INFO] Epoch: 48 , batch: 207 , training loss: 3.739020\n",
      "[INFO] Epoch: 48 , batch: 208 , training loss: 4.017663\n",
      "[INFO] Epoch: 48 , batch: 209 , training loss: 4.012758\n",
      "[INFO] Epoch: 48 , batch: 210 , training loss: 4.015486\n",
      "[INFO] Epoch: 48 , batch: 211 , training loss: 4.042141\n",
      "[INFO] Epoch: 48 , batch: 212 , training loss: 4.116000\n",
      "[INFO] Epoch: 48 , batch: 213 , training loss: 4.075010\n",
      "[INFO] Epoch: 48 , batch: 214 , training loss: 4.171599\n",
      "[INFO] Epoch: 48 , batch: 215 , training loss: 4.337172\n",
      "[INFO] Epoch: 48 , batch: 216 , training loss: 4.040633\n",
      "[INFO] Epoch: 48 , batch: 217 , training loss: 4.012538\n",
      "[INFO] Epoch: 48 , batch: 218 , training loss: 4.007803\n",
      "[INFO] Epoch: 48 , batch: 219 , training loss: 4.108944\n",
      "[INFO] Epoch: 48 , batch: 220 , training loss: 3.939973\n",
      "[INFO] Epoch: 48 , batch: 221 , training loss: 3.938384\n",
      "[INFO] Epoch: 48 , batch: 222 , training loss: 4.082905\n",
      "[INFO] Epoch: 48 , batch: 223 , training loss: 4.198798\n",
      "[INFO] Epoch: 48 , batch: 224 , training loss: 4.246090\n",
      "[INFO] Epoch: 48 , batch: 225 , training loss: 4.130509\n",
      "[INFO] Epoch: 48 , batch: 226 , training loss: 4.257097\n",
      "[INFO] Epoch: 48 , batch: 227 , training loss: 4.232882\n",
      "[INFO] Epoch: 48 , batch: 228 , training loss: 4.251147\n",
      "[INFO] Epoch: 48 , batch: 229 , training loss: 4.110799\n",
      "[INFO] Epoch: 48 , batch: 230 , training loss: 3.971012\n",
      "[INFO] Epoch: 48 , batch: 231 , training loss: 3.838197\n",
      "[INFO] Epoch: 48 , batch: 232 , training loss: 3.967720\n",
      "[INFO] Epoch: 48 , batch: 233 , training loss: 4.015777\n",
      "[INFO] Epoch: 48 , batch: 234 , training loss: 3.697294\n",
      "[INFO] Epoch: 48 , batch: 235 , training loss: 3.811764\n",
      "[INFO] Epoch: 48 , batch: 236 , training loss: 3.933900\n",
      "[INFO] Epoch: 48 , batch: 237 , training loss: 4.123467\n",
      "[INFO] Epoch: 48 , batch: 238 , training loss: 3.932048\n",
      "[INFO] Epoch: 48 , batch: 239 , training loss: 3.955382\n",
      "[INFO] Epoch: 48 , batch: 240 , training loss: 3.984210\n",
      "[INFO] Epoch: 48 , batch: 241 , training loss: 3.800417\n",
      "[INFO] Epoch: 48 , batch: 242 , training loss: 3.818963\n",
      "[INFO] Epoch: 48 , batch: 243 , training loss: 4.111786\n",
      "[INFO] Epoch: 48 , batch: 244 , training loss: 4.048195\n",
      "[INFO] Epoch: 48 , batch: 245 , training loss: 3.993974\n",
      "[INFO] Epoch: 48 , batch: 246 , training loss: 3.724072\n",
      "[INFO] Epoch: 48 , batch: 247 , training loss: 3.898412\n",
      "[INFO] Epoch: 48 , batch: 248 , training loss: 3.965985\n",
      "[INFO] Epoch: 48 , batch: 249 , training loss: 3.948206\n",
      "[INFO] Epoch: 48 , batch: 250 , training loss: 3.761951\n",
      "[INFO] Epoch: 48 , batch: 251 , training loss: 4.180129\n",
      "[INFO] Epoch: 48 , batch: 252 , training loss: 3.909463\n",
      "[INFO] Epoch: 48 , batch: 253 , training loss: 3.794264\n",
      "[INFO] Epoch: 48 , batch: 254 , training loss: 4.088259\n",
      "[INFO] Epoch: 48 , batch: 255 , training loss: 4.046089\n",
      "[INFO] Epoch: 48 , batch: 256 , training loss: 4.034083\n",
      "[INFO] Epoch: 48 , batch: 257 , training loss: 4.216981\n",
      "[INFO] Epoch: 48 , batch: 258 , training loss: 4.217150\n",
      "[INFO] Epoch: 48 , batch: 259 , training loss: 4.245103\n",
      "[INFO] Epoch: 48 , batch: 260 , training loss: 4.028618\n",
      "[INFO] Epoch: 48 , batch: 261 , training loss: 4.211948\n",
      "[INFO] Epoch: 48 , batch: 262 , training loss: 4.316000\n",
      "[INFO] Epoch: 48 , batch: 263 , training loss: 4.509069\n",
      "[INFO] Epoch: 48 , batch: 264 , training loss: 3.881536\n",
      "[INFO] Epoch: 48 , batch: 265 , training loss: 3.982855\n",
      "[INFO] Epoch: 48 , batch: 266 , training loss: 4.377043\n",
      "[INFO] Epoch: 48 , batch: 267 , training loss: 4.133874\n",
      "[INFO] Epoch: 48 , batch: 268 , training loss: 4.067270\n",
      "[INFO] Epoch: 48 , batch: 269 , training loss: 4.033420\n",
      "[INFO] Epoch: 48 , batch: 270 , training loss: 4.068986\n",
      "[INFO] Epoch: 48 , batch: 271 , training loss: 4.101486\n",
      "[INFO] Epoch: 48 , batch: 272 , training loss: 4.076197\n",
      "[INFO] Epoch: 48 , batch: 273 , training loss: 4.100019\n",
      "[INFO] Epoch: 48 , batch: 274 , training loss: 4.181201\n",
      "[INFO] Epoch: 48 , batch: 275 , training loss: 4.049111\n",
      "[INFO] Epoch: 48 , batch: 276 , training loss: 4.101520\n",
      "[INFO] Epoch: 48 , batch: 277 , training loss: 4.264178\n",
      "[INFO] Epoch: 48 , batch: 278 , training loss: 3.968759\n",
      "[INFO] Epoch: 48 , batch: 279 , training loss: 3.955756\n",
      "[INFO] Epoch: 48 , batch: 280 , training loss: 3.952833\n",
      "[INFO] Epoch: 48 , batch: 281 , training loss: 4.081625\n",
      "[INFO] Epoch: 48 , batch: 282 , training loss: 3.981721\n",
      "[INFO] Epoch: 48 , batch: 283 , training loss: 3.989501\n",
      "[INFO] Epoch: 48 , batch: 284 , training loss: 4.013576\n",
      "[INFO] Epoch: 48 , batch: 285 , training loss: 3.948139\n",
      "[INFO] Epoch: 48 , batch: 286 , training loss: 3.950126\n",
      "[INFO] Epoch: 48 , batch: 287 , training loss: 3.922268\n",
      "[INFO] Epoch: 48 , batch: 288 , training loss: 3.870518\n",
      "[INFO] Epoch: 48 , batch: 289 , training loss: 3.947497\n",
      "[INFO] Epoch: 48 , batch: 290 , training loss: 3.721742\n",
      "[INFO] Epoch: 48 , batch: 291 , training loss: 3.719297\n",
      "[INFO] Epoch: 48 , batch: 292 , training loss: 3.813087\n",
      "[INFO] Epoch: 48 , batch: 293 , training loss: 3.733352\n",
      "[INFO] Epoch: 48 , batch: 294 , training loss: 4.405092\n",
      "[INFO] Epoch: 48 , batch: 295 , training loss: 4.199855\n",
      "[INFO] Epoch: 48 , batch: 296 , training loss: 4.125645\n",
      "[INFO] Epoch: 48 , batch: 297 , training loss: 4.065089\n",
      "[INFO] Epoch: 48 , batch: 298 , training loss: 3.896725\n",
      "[INFO] Epoch: 48 , batch: 299 , training loss: 3.964759\n",
      "[INFO] Epoch: 48 , batch: 300 , training loss: 3.939152\n",
      "[INFO] Epoch: 48 , batch: 301 , training loss: 3.875746\n",
      "[INFO] Epoch: 48 , batch: 302 , training loss: 4.035930\n",
      "[INFO] Epoch: 48 , batch: 303 , training loss: 4.031343\n",
      "[INFO] Epoch: 48 , batch: 304 , training loss: 4.160785\n",
      "[INFO] Epoch: 48 , batch: 305 , training loss: 4.006754\n",
      "[INFO] Epoch: 48 , batch: 306 , training loss: 4.128831\n",
      "[INFO] Epoch: 48 , batch: 307 , training loss: 4.142706\n",
      "[INFO] Epoch: 48 , batch: 308 , training loss: 3.953429\n",
      "[INFO] Epoch: 48 , batch: 309 , training loss: 3.932966\n",
      "[INFO] Epoch: 48 , batch: 310 , training loss: 3.889477\n",
      "[INFO] Epoch: 48 , batch: 311 , training loss: 3.882225\n",
      "[INFO] Epoch: 48 , batch: 312 , training loss: 3.780077\n",
      "[INFO] Epoch: 48 , batch: 313 , training loss: 3.889404\n",
      "[INFO] Epoch: 48 , batch: 314 , training loss: 3.951357\n",
      "[INFO] Epoch: 48 , batch: 315 , training loss: 4.024302\n",
      "[INFO] Epoch: 48 , batch: 316 , training loss: 4.241720\n",
      "[INFO] Epoch: 48 , batch: 317 , training loss: 4.586584\n",
      "[INFO] Epoch: 48 , batch: 318 , training loss: 4.713507\n",
      "[INFO] Epoch: 48 , batch: 319 , training loss: 4.421670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 48 , batch: 320 , training loss: 3.999094\n",
      "[INFO] Epoch: 48 , batch: 321 , training loss: 3.814270\n",
      "[INFO] Epoch: 48 , batch: 322 , training loss: 3.924277\n",
      "[INFO] Epoch: 48 , batch: 323 , training loss: 3.961211\n",
      "[INFO] Epoch: 48 , batch: 324 , training loss: 3.916640\n",
      "[INFO] Epoch: 48 , batch: 325 , training loss: 4.010545\n",
      "[INFO] Epoch: 48 , batch: 326 , training loss: 4.097042\n",
      "[INFO] Epoch: 48 , batch: 327 , training loss: 4.029241\n",
      "[INFO] Epoch: 48 , batch: 328 , training loss: 4.050029\n",
      "[INFO] Epoch: 48 , batch: 329 , training loss: 3.944658\n",
      "[INFO] Epoch: 48 , batch: 330 , training loss: 3.937478\n",
      "[INFO] Epoch: 48 , batch: 331 , training loss: 4.086831\n",
      "[INFO] Epoch: 48 , batch: 332 , training loss: 3.940468\n",
      "[INFO] Epoch: 48 , batch: 333 , training loss: 3.919544\n",
      "[INFO] Epoch: 48 , batch: 334 , training loss: 3.923876\n",
      "[INFO] Epoch: 48 , batch: 335 , training loss: 4.065576\n",
      "[INFO] Epoch: 48 , batch: 336 , training loss: 4.065668\n",
      "[INFO] Epoch: 48 , batch: 337 , training loss: 4.099550\n",
      "[INFO] Epoch: 48 , batch: 338 , training loss: 4.301678\n",
      "[INFO] Epoch: 48 , batch: 339 , training loss: 4.118291\n",
      "[INFO] Epoch: 48 , batch: 340 , training loss: 4.322861\n",
      "[INFO] Epoch: 48 , batch: 341 , training loss: 4.066217\n",
      "[INFO] Epoch: 48 , batch: 342 , training loss: 3.874628\n",
      "[INFO] Epoch: 48 , batch: 343 , training loss: 3.932302\n",
      "[INFO] Epoch: 48 , batch: 344 , training loss: 3.797045\n",
      "[INFO] Epoch: 48 , batch: 345 , training loss: 3.914044\n",
      "[INFO] Epoch: 48 , batch: 346 , training loss: 3.977756\n",
      "[INFO] Epoch: 48 , batch: 347 , training loss: 3.895198\n",
      "[INFO] Epoch: 48 , batch: 348 , training loss: 3.978491\n",
      "[INFO] Epoch: 48 , batch: 349 , training loss: 4.068894\n",
      "[INFO] Epoch: 48 , batch: 350 , training loss: 3.920760\n",
      "[INFO] Epoch: 48 , batch: 351 , training loss: 4.016058\n",
      "[INFO] Epoch: 48 , batch: 352 , training loss: 4.018538\n",
      "[INFO] Epoch: 48 , batch: 353 , training loss: 4.011192\n",
      "[INFO] Epoch: 48 , batch: 354 , training loss: 4.096148\n",
      "[INFO] Epoch: 48 , batch: 355 , training loss: 4.094059\n",
      "[INFO] Epoch: 48 , batch: 356 , training loss: 3.964882\n",
      "[INFO] Epoch: 48 , batch: 357 , training loss: 4.029955\n",
      "[INFO] Epoch: 48 , batch: 358 , training loss: 3.933309\n",
      "[INFO] Epoch: 48 , batch: 359 , training loss: 3.955546\n",
      "[INFO] Epoch: 48 , batch: 360 , training loss: 4.054471\n",
      "[INFO] Epoch: 48 , batch: 361 , training loss: 4.024797\n",
      "[INFO] Epoch: 48 , batch: 362 , training loss: 4.124387\n",
      "[INFO] Epoch: 48 , batch: 363 , training loss: 3.993694\n",
      "[INFO] Epoch: 48 , batch: 364 , training loss: 4.057533\n",
      "[INFO] Epoch: 48 , batch: 365 , training loss: 3.990927\n",
      "[INFO] Epoch: 48 , batch: 366 , training loss: 4.076198\n",
      "[INFO] Epoch: 48 , batch: 367 , training loss: 4.114599\n",
      "[INFO] Epoch: 48 , batch: 368 , training loss: 4.506911\n",
      "[INFO] Epoch: 48 , batch: 369 , training loss: 4.205062\n",
      "[INFO] Epoch: 48 , batch: 370 , training loss: 3.995527\n",
      "[INFO] Epoch: 48 , batch: 371 , training loss: 4.359138\n",
      "[INFO] Epoch: 48 , batch: 372 , training loss: 4.613475\n",
      "[INFO] Epoch: 48 , batch: 373 , training loss: 4.630915\n",
      "[INFO] Epoch: 48 , batch: 374 , training loss: 4.780756\n",
      "[INFO] Epoch: 48 , batch: 375 , training loss: 4.766753\n",
      "[INFO] Epoch: 48 , batch: 376 , training loss: 4.644519\n",
      "[INFO] Epoch: 48 , batch: 377 , training loss: 4.414985\n",
      "[INFO] Epoch: 48 , batch: 378 , training loss: 4.515219\n",
      "[INFO] Epoch: 48 , batch: 379 , training loss: 4.484342\n",
      "[INFO] Epoch: 48 , batch: 380 , training loss: 4.682269\n",
      "[INFO] Epoch: 48 , batch: 381 , training loss: 4.368255\n",
      "[INFO] Epoch: 48 , batch: 382 , training loss: 4.595361\n",
      "[INFO] Epoch: 48 , batch: 383 , training loss: 4.647463\n",
      "[INFO] Epoch: 48 , batch: 384 , training loss: 4.628206\n",
      "[INFO] Epoch: 48 , batch: 385 , training loss: 4.275725\n",
      "[INFO] Epoch: 48 , batch: 386 , training loss: 4.561747\n",
      "[INFO] Epoch: 48 , batch: 387 , training loss: 4.512713\n",
      "[INFO] Epoch: 48 , batch: 388 , training loss: 4.324980\n",
      "[INFO] Epoch: 48 , batch: 389 , training loss: 4.139238\n",
      "[INFO] Epoch: 48 , batch: 390 , training loss: 4.171088\n",
      "[INFO] Epoch: 48 , batch: 391 , training loss: 4.197850\n",
      "[INFO] Epoch: 48 , batch: 392 , training loss: 4.554329\n",
      "[INFO] Epoch: 48 , batch: 393 , training loss: 4.445508\n",
      "[INFO] Epoch: 48 , batch: 394 , training loss: 4.558815\n",
      "[INFO] Epoch: 48 , batch: 395 , training loss: 4.375245\n",
      "[INFO] Epoch: 48 , batch: 396 , training loss: 4.203941\n",
      "[INFO] Epoch: 48 , batch: 397 , training loss: 4.319705\n",
      "[INFO] Epoch: 48 , batch: 398 , training loss: 4.185277\n",
      "[INFO] Epoch: 48 , batch: 399 , training loss: 4.269224\n",
      "[INFO] Epoch: 48 , batch: 400 , training loss: 4.247710\n",
      "[INFO] Epoch: 48 , batch: 401 , training loss: 4.678998\n",
      "[INFO] Epoch: 48 , batch: 402 , training loss: 4.397924\n",
      "[INFO] Epoch: 48 , batch: 403 , training loss: 4.226414\n",
      "[INFO] Epoch: 48 , batch: 404 , training loss: 4.395417\n",
      "[INFO] Epoch: 48 , batch: 405 , training loss: 4.452873\n",
      "[INFO] Epoch: 48 , batch: 406 , training loss: 4.369202\n",
      "[INFO] Epoch: 48 , batch: 407 , training loss: 4.382376\n",
      "[INFO] Epoch: 48 , batch: 408 , training loss: 4.365003\n",
      "[INFO] Epoch: 48 , batch: 409 , training loss: 4.382426\n",
      "[INFO] Epoch: 48 , batch: 410 , training loss: 4.462009\n",
      "[INFO] Epoch: 48 , batch: 411 , training loss: 4.607351\n",
      "[INFO] Epoch: 48 , batch: 412 , training loss: 4.441011\n",
      "[INFO] Epoch: 48 , batch: 413 , training loss: 4.299911\n",
      "[INFO] Epoch: 48 , batch: 414 , training loss: 4.353002\n",
      "[INFO] Epoch: 48 , batch: 415 , training loss: 4.402856\n",
      "[INFO] Epoch: 48 , batch: 416 , training loss: 4.471345\n",
      "[INFO] Epoch: 48 , batch: 417 , training loss: 4.392148\n",
      "[INFO] Epoch: 48 , batch: 418 , training loss: 4.443986\n",
      "[INFO] Epoch: 48 , batch: 419 , training loss: 4.403817\n",
      "[INFO] Epoch: 48 , batch: 420 , training loss: 4.390244\n",
      "[INFO] Epoch: 48 , batch: 421 , training loss: 4.370056\n",
      "[INFO] Epoch: 48 , batch: 422 , training loss: 4.214336\n",
      "[INFO] Epoch: 48 , batch: 423 , training loss: 4.462608\n",
      "[INFO] Epoch: 48 , batch: 424 , training loss: 4.620670\n",
      "[INFO] Epoch: 48 , batch: 425 , training loss: 4.485043\n",
      "[INFO] Epoch: 48 , batch: 426 , training loss: 4.230114\n",
      "[INFO] Epoch: 48 , batch: 427 , training loss: 4.465711\n",
      "[INFO] Epoch: 48 , batch: 428 , training loss: 4.322745\n",
      "[INFO] Epoch: 48 , batch: 429 , training loss: 4.225797\n",
      "[INFO] Epoch: 48 , batch: 430 , training loss: 4.449820\n",
      "[INFO] Epoch: 48 , batch: 431 , training loss: 4.076507\n",
      "[INFO] Epoch: 48 , batch: 432 , training loss: 4.143837\n",
      "[INFO] Epoch: 48 , batch: 433 , training loss: 4.186254\n",
      "[INFO] Epoch: 48 , batch: 434 , training loss: 4.051044\n",
      "[INFO] Epoch: 48 , batch: 435 , training loss: 4.384214\n",
      "[INFO] Epoch: 48 , batch: 436 , training loss: 4.420356\n",
      "[INFO] Epoch: 48 , batch: 437 , training loss: 4.224643\n",
      "[INFO] Epoch: 48 , batch: 438 , training loss: 4.098322\n",
      "[INFO] Epoch: 48 , batch: 439 , training loss: 4.319340\n",
      "[INFO] Epoch: 48 , batch: 440 , training loss: 4.435512\n",
      "[INFO] Epoch: 48 , batch: 441 , training loss: 4.533442\n",
      "[INFO] Epoch: 48 , batch: 442 , training loss: 4.291664\n",
      "[INFO] Epoch: 48 , batch: 443 , training loss: 4.455354\n",
      "[INFO] Epoch: 48 , batch: 444 , training loss: 4.093512\n",
      "[INFO] Epoch: 48 , batch: 445 , training loss: 3.977227\n",
      "[INFO] Epoch: 48 , batch: 446 , training loss: 3.925272\n",
      "[INFO] Epoch: 48 , batch: 447 , training loss: 4.121937\n",
      "[INFO] Epoch: 48 , batch: 448 , training loss: 4.233013\n",
      "[INFO] Epoch: 48 , batch: 449 , training loss: 4.582933\n",
      "[INFO] Epoch: 48 , batch: 450 , training loss: 4.666182\n",
      "[INFO] Epoch: 48 , batch: 451 , training loss: 4.570551\n",
      "[INFO] Epoch: 48 , batch: 452 , training loss: 4.382749\n",
      "[INFO] Epoch: 48 , batch: 453 , training loss: 4.152163\n",
      "[INFO] Epoch: 48 , batch: 454 , training loss: 4.305319\n",
      "[INFO] Epoch: 48 , batch: 455 , training loss: 4.364534\n",
      "[INFO] Epoch: 48 , batch: 456 , training loss: 4.329724\n",
      "[INFO] Epoch: 48 , batch: 457 , training loss: 4.422825\n",
      "[INFO] Epoch: 48 , batch: 458 , training loss: 4.159823\n",
      "[INFO] Epoch: 48 , batch: 459 , training loss: 4.132496\n",
      "[INFO] Epoch: 48 , batch: 460 , training loss: 4.276649\n",
      "[INFO] Epoch: 48 , batch: 461 , training loss: 4.222816\n",
      "[INFO] Epoch: 48 , batch: 462 , training loss: 4.276542\n",
      "[INFO] Epoch: 48 , batch: 463 , training loss: 4.202460\n",
      "[INFO] Epoch: 48 , batch: 464 , training loss: 4.379006\n",
      "[INFO] Epoch: 48 , batch: 465 , training loss: 4.328416\n",
      "[INFO] Epoch: 48 , batch: 466 , training loss: 4.417559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 48 , batch: 467 , training loss: 4.375100\n",
      "[INFO] Epoch: 48 , batch: 468 , training loss: 4.334562\n",
      "[INFO] Epoch: 48 , batch: 469 , training loss: 4.350138\n",
      "[INFO] Epoch: 48 , batch: 470 , training loss: 4.186121\n",
      "[INFO] Epoch: 48 , batch: 471 , training loss: 4.283218\n",
      "[INFO] Epoch: 48 , batch: 472 , training loss: 4.348625\n",
      "[INFO] Epoch: 48 , batch: 473 , training loss: 4.273444\n",
      "[INFO] Epoch: 48 , batch: 474 , training loss: 4.051102\n",
      "[INFO] Epoch: 48 , batch: 475 , training loss: 3.923388\n",
      "[INFO] Epoch: 48 , batch: 476 , training loss: 4.336970\n",
      "[INFO] Epoch: 48 , batch: 477 , training loss: 4.456655\n",
      "[INFO] Epoch: 48 , batch: 478 , training loss: 4.434731\n",
      "[INFO] Epoch: 48 , batch: 479 , training loss: 4.418443\n",
      "[INFO] Epoch: 48 , batch: 480 , training loss: 4.561304\n",
      "[INFO] Epoch: 48 , batch: 481 , training loss: 4.422282\n",
      "[INFO] Epoch: 48 , batch: 482 , training loss: 4.538654\n",
      "[INFO] Epoch: 48 , batch: 483 , training loss: 4.371149\n",
      "[INFO] Epoch: 48 , batch: 484 , training loss: 4.183054\n",
      "[INFO] Epoch: 48 , batch: 485 , training loss: 4.266393\n",
      "[INFO] Epoch: 48 , batch: 486 , training loss: 4.152963\n",
      "[INFO] Epoch: 48 , batch: 487 , training loss: 4.145622\n",
      "[INFO] Epoch: 48 , batch: 488 , training loss: 4.318378\n",
      "[INFO] Epoch: 48 , batch: 489 , training loss: 4.247920\n",
      "[INFO] Epoch: 48 , batch: 490 , training loss: 4.304754\n",
      "[INFO] Epoch: 48 , batch: 491 , training loss: 4.202116\n",
      "[INFO] Epoch: 48 , batch: 492 , training loss: 4.194288\n",
      "[INFO] Epoch: 48 , batch: 493 , training loss: 4.359437\n",
      "[INFO] Epoch: 48 , batch: 494 , training loss: 4.261865\n",
      "[INFO] Epoch: 48 , batch: 495 , training loss: 4.441505\n",
      "[INFO] Epoch: 48 , batch: 496 , training loss: 4.301222\n",
      "[INFO] Epoch: 48 , batch: 497 , training loss: 4.343819\n",
      "[INFO] Epoch: 48 , batch: 498 , training loss: 4.310722\n",
      "[INFO] Epoch: 48 , batch: 499 , training loss: 4.364890\n",
      "[INFO] Epoch: 48 , batch: 500 , training loss: 4.529214\n",
      "[INFO] Epoch: 48 , batch: 501 , training loss: 4.850091\n",
      "[INFO] Epoch: 48 , batch: 502 , training loss: 4.853701\n",
      "[INFO] Epoch: 48 , batch: 503 , training loss: 4.486054\n",
      "[INFO] Epoch: 48 , batch: 504 , training loss: 4.665057\n",
      "[INFO] Epoch: 48 , batch: 505 , training loss: 4.620820\n",
      "[INFO] Epoch: 48 , batch: 506 , training loss: 4.600058\n",
      "[INFO] Epoch: 48 , batch: 507 , training loss: 4.659444\n",
      "[INFO] Epoch: 48 , batch: 508 , training loss: 4.567908\n",
      "[INFO] Epoch: 48 , batch: 509 , training loss: 4.379241\n",
      "[INFO] Epoch: 48 , batch: 510 , training loss: 4.490559\n",
      "[INFO] Epoch: 48 , batch: 511 , training loss: 4.419032\n",
      "[INFO] Epoch: 48 , batch: 512 , training loss: 4.501058\n",
      "[INFO] Epoch: 48 , batch: 513 , training loss: 4.753333\n",
      "[INFO] Epoch: 48 , batch: 514 , training loss: 4.386695\n",
      "[INFO] Epoch: 48 , batch: 515 , training loss: 4.645809\n",
      "[INFO] Epoch: 48 , batch: 516 , training loss: 4.446975\n",
      "[INFO] Epoch: 48 , batch: 517 , training loss: 4.408468\n",
      "[INFO] Epoch: 48 , batch: 518 , training loss: 4.374517\n",
      "[INFO] Epoch: 48 , batch: 519 , training loss: 4.236170\n",
      "[INFO] Epoch: 48 , batch: 520 , training loss: 4.446586\n",
      "[INFO] Epoch: 48 , batch: 521 , training loss: 4.431535\n",
      "[INFO] Epoch: 48 , batch: 522 , training loss: 4.510230\n",
      "[INFO] Epoch: 48 , batch: 523 , training loss: 4.424700\n",
      "[INFO] Epoch: 48 , batch: 524 , training loss: 4.696996\n",
      "[INFO] Epoch: 48 , batch: 525 , training loss: 4.592041\n",
      "[INFO] Epoch: 48 , batch: 526 , training loss: 4.376686\n",
      "[INFO] Epoch: 48 , batch: 527 , training loss: 4.420518\n",
      "[INFO] Epoch: 48 , batch: 528 , training loss: 4.428918\n",
      "[INFO] Epoch: 48 , batch: 529 , training loss: 4.405062\n",
      "[INFO] Epoch: 48 , batch: 530 , training loss: 4.273776\n",
      "[INFO] Epoch: 48 , batch: 531 , training loss: 4.405045\n",
      "[INFO] Epoch: 48 , batch: 532 , training loss: 4.329976\n",
      "[INFO] Epoch: 48 , batch: 533 , training loss: 4.429014\n",
      "[INFO] Epoch: 48 , batch: 534 , training loss: 4.450466\n",
      "[INFO] Epoch: 48 , batch: 535 , training loss: 4.436535\n",
      "[INFO] Epoch: 48 , batch: 536 , training loss: 4.274864\n",
      "[INFO] Epoch: 48 , batch: 537 , training loss: 4.277902\n",
      "[INFO] Epoch: 48 , batch: 538 , training loss: 4.378408\n",
      "[INFO] Epoch: 48 , batch: 539 , training loss: 4.450643\n",
      "[INFO] Epoch: 48 , batch: 540 , training loss: 4.978481\n",
      "[INFO] Epoch: 48 , batch: 541 , training loss: 4.806527\n",
      "[INFO] Epoch: 48 , batch: 542 , training loss: 4.680551\n",
      "[INFO] Epoch: 49 , batch: 0 , training loss: 3.585875\n",
      "[INFO] Epoch: 49 , batch: 1 , training loss: 3.480714\n",
      "[INFO] Epoch: 49 , batch: 2 , training loss: 3.697716\n",
      "[INFO] Epoch: 49 , batch: 3 , training loss: 3.598449\n",
      "[INFO] Epoch: 49 , batch: 4 , training loss: 3.919429\n",
      "[INFO] Epoch: 49 , batch: 5 , training loss: 3.627262\n",
      "[INFO] Epoch: 49 , batch: 6 , training loss: 3.955238\n",
      "[INFO] Epoch: 49 , batch: 7 , training loss: 3.893619\n",
      "[INFO] Epoch: 49 , batch: 8 , training loss: 3.578444\n",
      "[INFO] Epoch: 49 , batch: 9 , training loss: 3.820890\n",
      "[INFO] Epoch: 49 , batch: 10 , training loss: 3.795040\n",
      "[INFO] Epoch: 49 , batch: 11 , training loss: 3.742333\n",
      "[INFO] Epoch: 49 , batch: 12 , training loss: 3.622293\n",
      "[INFO] Epoch: 49 , batch: 13 , training loss: 3.616713\n",
      "[INFO] Epoch: 49 , batch: 14 , training loss: 3.546216\n",
      "[INFO] Epoch: 49 , batch: 15 , training loss: 3.760309\n",
      "[INFO] Epoch: 49 , batch: 16 , training loss: 3.569610\n",
      "[INFO] Epoch: 49 , batch: 17 , training loss: 3.751845\n",
      "[INFO] Epoch: 49 , batch: 18 , training loss: 3.677837\n",
      "[INFO] Epoch: 49 , batch: 19 , training loss: 3.465683\n",
      "[INFO] Epoch: 49 , batch: 20 , training loss: 3.420445\n",
      "[INFO] Epoch: 49 , batch: 21 , training loss: 3.575593\n",
      "[INFO] Epoch: 49 , batch: 22 , training loss: 3.446407\n",
      "[INFO] Epoch: 49 , batch: 23 , training loss: 3.665744\n",
      "[INFO] Epoch: 49 , batch: 24 , training loss: 3.494455\n",
      "[INFO] Epoch: 49 , batch: 25 , training loss: 3.577741\n",
      "[INFO] Epoch: 49 , batch: 26 , training loss: 3.494953\n",
      "[INFO] Epoch: 49 , batch: 27 , training loss: 3.488452\n",
      "[INFO] Epoch: 49 , batch: 28 , training loss: 3.668221\n",
      "[INFO] Epoch: 49 , batch: 29 , training loss: 3.463014\n",
      "[INFO] Epoch: 49 , batch: 30 , training loss: 3.484309\n",
      "[INFO] Epoch: 49 , batch: 31 , training loss: 3.605919\n",
      "[INFO] Epoch: 49 , batch: 32 , training loss: 3.539145\n",
      "[INFO] Epoch: 49 , batch: 33 , training loss: 3.571150\n",
      "[INFO] Epoch: 49 , batch: 34 , training loss: 3.562114\n",
      "[INFO] Epoch: 49 , batch: 35 , training loss: 3.550714\n",
      "[INFO] Epoch: 49 , batch: 36 , training loss: 3.622931\n",
      "[INFO] Epoch: 49 , batch: 37 , training loss: 3.481621\n",
      "[INFO] Epoch: 49 , batch: 38 , training loss: 3.570764\n",
      "[INFO] Epoch: 49 , batch: 39 , training loss: 3.403095\n",
      "[INFO] Epoch: 49 , batch: 40 , training loss: 3.593273\n",
      "[INFO] Epoch: 49 , batch: 41 , training loss: 3.556570\n",
      "[INFO] Epoch: 49 , batch: 42 , training loss: 3.931865\n",
      "[INFO] Epoch: 49 , batch: 43 , training loss: 3.704968\n",
      "[INFO] Epoch: 49 , batch: 44 , training loss: 4.046272\n",
      "[INFO] Epoch: 49 , batch: 45 , training loss: 4.018753\n",
      "[INFO] Epoch: 49 , batch: 46 , training loss: 3.977991\n",
      "[INFO] Epoch: 49 , batch: 47 , training loss: 3.556891\n",
      "[INFO] Epoch: 49 , batch: 48 , training loss: 3.545652\n",
      "[INFO] Epoch: 49 , batch: 49 , training loss: 3.805588\n",
      "[INFO] Epoch: 49 , batch: 50 , training loss: 3.558331\n",
      "[INFO] Epoch: 49 , batch: 51 , training loss: 3.812762\n",
      "[INFO] Epoch: 49 , batch: 52 , training loss: 3.601025\n",
      "[INFO] Epoch: 49 , batch: 53 , training loss: 3.720668\n",
      "[INFO] Epoch: 49 , batch: 54 , training loss: 3.757141\n",
      "[INFO] Epoch: 49 , batch: 55 , training loss: 3.839302\n",
      "[INFO] Epoch: 49 , batch: 56 , training loss: 3.652064\n",
      "[INFO] Epoch: 49 , batch: 57 , training loss: 3.578738\n",
      "[INFO] Epoch: 49 , batch: 58 , training loss: 3.650628\n",
      "[INFO] Epoch: 49 , batch: 59 , training loss: 3.695609\n",
      "[INFO] Epoch: 49 , batch: 60 , training loss: 3.676225\n",
      "[INFO] Epoch: 49 , batch: 61 , training loss: 3.736206\n",
      "[INFO] Epoch: 49 , batch: 62 , training loss: 3.615336\n",
      "[INFO] Epoch: 49 , batch: 63 , training loss: 3.797304\n",
      "[INFO] Epoch: 49 , batch: 64 , training loss: 4.032246\n",
      "[INFO] Epoch: 49 , batch: 65 , training loss: 3.725077\n",
      "[INFO] Epoch: 49 , batch: 66 , training loss: 3.599153\n",
      "[INFO] Epoch: 49 , batch: 67 , training loss: 3.612282\n",
      "[INFO] Epoch: 49 , batch: 68 , training loss: 3.748030\n",
      "[INFO] Epoch: 49 , batch: 69 , training loss: 3.709143\n",
      "[INFO] Epoch: 49 , batch: 70 , training loss: 3.915589\n",
      "[INFO] Epoch: 49 , batch: 71 , training loss: 3.759036\n",
      "[INFO] Epoch: 49 , batch: 72 , training loss: 3.838864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 49 , batch: 73 , training loss: 3.764492\n",
      "[INFO] Epoch: 49 , batch: 74 , training loss: 3.883721\n",
      "[INFO] Epoch: 49 , batch: 75 , training loss: 3.732493\n",
      "[INFO] Epoch: 49 , batch: 76 , training loss: 3.851645\n",
      "[INFO] Epoch: 49 , batch: 77 , training loss: 3.808739\n",
      "[INFO] Epoch: 49 , batch: 78 , training loss: 3.871434\n",
      "[INFO] Epoch: 49 , batch: 79 , training loss: 3.737136\n",
      "[INFO] Epoch: 49 , batch: 80 , training loss: 3.941859\n",
      "[INFO] Epoch: 49 , batch: 81 , training loss: 3.853488\n",
      "[INFO] Epoch: 49 , batch: 82 , training loss: 3.818770\n",
      "[INFO] Epoch: 49 , batch: 83 , training loss: 3.897649\n",
      "[INFO] Epoch: 49 , batch: 84 , training loss: 3.905342\n",
      "[INFO] Epoch: 49 , batch: 85 , training loss: 3.970809\n",
      "[INFO] Epoch: 49 , batch: 86 , training loss: 3.898114\n",
      "[INFO] Epoch: 49 , batch: 87 , training loss: 3.859388\n",
      "[INFO] Epoch: 49 , batch: 88 , training loss: 3.977537\n",
      "[INFO] Epoch: 49 , batch: 89 , training loss: 3.783857\n",
      "[INFO] Epoch: 49 , batch: 90 , training loss: 3.896421\n",
      "[INFO] Epoch: 49 , batch: 91 , training loss: 3.823145\n",
      "[INFO] Epoch: 49 , batch: 92 , training loss: 3.821352\n",
      "[INFO] Epoch: 49 , batch: 93 , training loss: 3.947987\n",
      "[INFO] Epoch: 49 , batch: 94 , training loss: 4.074631\n",
      "[INFO] Epoch: 49 , batch: 95 , training loss: 3.853915\n",
      "[INFO] Epoch: 49 , batch: 96 , training loss: 3.838780\n",
      "[INFO] Epoch: 49 , batch: 97 , training loss: 3.761153\n",
      "[INFO] Epoch: 49 , batch: 98 , training loss: 3.712929\n",
      "[INFO] Epoch: 49 , batch: 99 , training loss: 3.838115\n",
      "[INFO] Epoch: 49 , batch: 100 , training loss: 3.722523\n",
      "[INFO] Epoch: 49 , batch: 101 , training loss: 3.735050\n",
      "[INFO] Epoch: 49 , batch: 102 , training loss: 3.915377\n",
      "[INFO] Epoch: 49 , batch: 103 , training loss: 3.719042\n",
      "[INFO] Epoch: 49 , batch: 104 , training loss: 3.654555\n",
      "[INFO] Epoch: 49 , batch: 105 , training loss: 3.904152\n",
      "[INFO] Epoch: 49 , batch: 106 , training loss: 3.922850\n",
      "[INFO] Epoch: 49 , batch: 107 , training loss: 3.760979\n",
      "[INFO] Epoch: 49 , batch: 108 , training loss: 3.704415\n",
      "[INFO] Epoch: 49 , batch: 109 , training loss: 3.655584\n",
      "[INFO] Epoch: 49 , batch: 110 , training loss: 3.798874\n",
      "[INFO] Epoch: 49 , batch: 111 , training loss: 3.877452\n",
      "[INFO] Epoch: 49 , batch: 112 , training loss: 3.807582\n",
      "[INFO] Epoch: 49 , batch: 113 , training loss: 3.803606\n",
      "[INFO] Epoch: 49 , batch: 114 , training loss: 3.782232\n",
      "[INFO] Epoch: 49 , batch: 115 , training loss: 3.822944\n",
      "[INFO] Epoch: 49 , batch: 116 , training loss: 3.700874\n",
      "[INFO] Epoch: 49 , batch: 117 , training loss: 3.908686\n",
      "[INFO] Epoch: 49 , batch: 118 , training loss: 3.889021\n",
      "[INFO] Epoch: 49 , batch: 119 , training loss: 4.033587\n",
      "[INFO] Epoch: 49 , batch: 120 , training loss: 3.989522\n",
      "[INFO] Epoch: 49 , batch: 121 , training loss: 3.892019\n",
      "[INFO] Epoch: 49 , batch: 122 , training loss: 3.771386\n",
      "[INFO] Epoch: 49 , batch: 123 , training loss: 3.772267\n",
      "[INFO] Epoch: 49 , batch: 124 , training loss: 3.857205\n",
      "[INFO] Epoch: 49 , batch: 125 , training loss: 3.698014\n",
      "[INFO] Epoch: 49 , batch: 126 , training loss: 3.718657\n",
      "[INFO] Epoch: 49 , batch: 127 , training loss: 3.742758\n",
      "[INFO] Epoch: 49 , batch: 128 , training loss: 3.847206\n",
      "[INFO] Epoch: 49 , batch: 129 , training loss: 3.809570\n",
      "[INFO] Epoch: 49 , batch: 130 , training loss: 3.830390\n",
      "[INFO] Epoch: 49 , batch: 131 , training loss: 3.807951\n",
      "[INFO] Epoch: 49 , batch: 132 , training loss: 3.826980\n",
      "[INFO] Epoch: 49 , batch: 133 , training loss: 3.817070\n",
      "[INFO] Epoch: 49 , batch: 134 , training loss: 3.583990\n",
      "[INFO] Epoch: 49 , batch: 135 , training loss: 3.637008\n",
      "[INFO] Epoch: 49 , batch: 136 , training loss: 3.902778\n",
      "[INFO] Epoch: 49 , batch: 137 , training loss: 3.874357\n",
      "[INFO] Epoch: 49 , batch: 138 , training loss: 3.909361\n",
      "[INFO] Epoch: 49 , batch: 139 , training loss: 4.458939\n",
      "[INFO] Epoch: 49 , batch: 140 , training loss: 4.244075\n",
      "[INFO] Epoch: 49 , batch: 141 , training loss: 3.993900\n",
      "[INFO] Epoch: 49 , batch: 142 , training loss: 3.724115\n",
      "[INFO] Epoch: 49 , batch: 143 , training loss: 3.881336\n",
      "[INFO] Epoch: 49 , batch: 144 , training loss: 3.708342\n",
      "[INFO] Epoch: 49 , batch: 145 , training loss: 3.794619\n",
      "[INFO] Epoch: 49 , batch: 146 , training loss: 3.981685\n",
      "[INFO] Epoch: 49 , batch: 147 , training loss: 3.644123\n",
      "[INFO] Epoch: 49 , batch: 148 , training loss: 3.609460\n",
      "[INFO] Epoch: 49 , batch: 149 , training loss: 3.707442\n",
      "[INFO] Epoch: 49 , batch: 150 , training loss: 3.979861\n",
      "[INFO] Epoch: 49 , batch: 151 , training loss: 3.829116\n",
      "[INFO] Epoch: 49 , batch: 152 , training loss: 3.840775\n",
      "[INFO] Epoch: 49 , batch: 153 , training loss: 3.859678\n",
      "[INFO] Epoch: 49 , batch: 154 , training loss: 3.942593\n",
      "[INFO] Epoch: 49 , batch: 155 , training loss: 4.178780\n",
      "[INFO] Epoch: 49 , batch: 156 , training loss: 3.920975\n",
      "[INFO] Epoch: 49 , batch: 157 , training loss: 3.869992\n",
      "[INFO] Epoch: 49 , batch: 158 , training loss: 3.979218\n",
      "[INFO] Epoch: 49 , batch: 159 , training loss: 3.935665\n",
      "[INFO] Epoch: 49 , batch: 160 , training loss: 4.099747\n",
      "[INFO] Epoch: 49 , batch: 161 , training loss: 4.190367\n",
      "[INFO] Epoch: 49 , batch: 162 , training loss: 4.182290\n",
      "[INFO] Epoch: 49 , batch: 163 , training loss: 4.359241\n",
      "[INFO] Epoch: 49 , batch: 164 , training loss: 4.286190\n",
      "[INFO] Epoch: 49 , batch: 165 , training loss: 4.231986\n",
      "[INFO] Epoch: 49 , batch: 166 , training loss: 4.145068\n",
      "[INFO] Epoch: 49 , batch: 167 , training loss: 4.198793\n",
      "[INFO] Epoch: 49 , batch: 168 , training loss: 3.837042\n",
      "[INFO] Epoch: 49 , batch: 169 , training loss: 3.822092\n",
      "[INFO] Epoch: 49 , batch: 170 , training loss: 4.030578\n",
      "[INFO] Epoch: 49 , batch: 171 , training loss: 3.424169\n",
      "[INFO] Epoch: 49 , batch: 172 , training loss: 3.699477\n",
      "[INFO] Epoch: 49 , batch: 173 , training loss: 3.997555\n",
      "[INFO] Epoch: 49 , batch: 174 , training loss: 4.445096\n",
      "[INFO] Epoch: 49 , batch: 175 , training loss: 4.712026\n",
      "[INFO] Epoch: 49 , batch: 176 , training loss: 4.382618\n",
      "[INFO] Epoch: 49 , batch: 177 , training loss: 3.983225\n",
      "[INFO] Epoch: 49 , batch: 178 , training loss: 4.000267\n",
      "[INFO] Epoch: 49 , batch: 179 , training loss: 4.067481\n",
      "[INFO] Epoch: 49 , batch: 180 , training loss: 4.033821\n",
      "[INFO] Epoch: 49 , batch: 181 , training loss: 4.317584\n",
      "[INFO] Epoch: 49 , batch: 182 , training loss: 4.245097\n",
      "[INFO] Epoch: 49 , batch: 183 , training loss: 4.239643\n",
      "[INFO] Epoch: 49 , batch: 184 , training loss: 4.124256\n",
      "[INFO] Epoch: 49 , batch: 185 , training loss: 4.089688\n",
      "[INFO] Epoch: 49 , batch: 186 , training loss: 4.220854\n",
      "[INFO] Epoch: 49 , batch: 187 , training loss: 4.331979\n",
      "[INFO] Epoch: 49 , batch: 188 , training loss: 4.335066\n",
      "[INFO] Epoch: 49 , batch: 189 , training loss: 4.236228\n",
      "[INFO] Epoch: 49 , batch: 190 , training loss: 4.288578\n",
      "[INFO] Epoch: 49 , batch: 191 , training loss: 4.407115\n",
      "[INFO] Epoch: 49 , batch: 192 , training loss: 4.215331\n",
      "[INFO] Epoch: 49 , batch: 193 , training loss: 4.311826\n",
      "[INFO] Epoch: 49 , batch: 194 , training loss: 4.237312\n",
      "[INFO] Epoch: 49 , batch: 195 , training loss: 4.198638\n",
      "[INFO] Epoch: 49 , batch: 196 , training loss: 4.046210\n",
      "[INFO] Epoch: 49 , batch: 197 , training loss: 4.148152\n",
      "[INFO] Epoch: 49 , batch: 198 , training loss: 4.052068\n",
      "[INFO] Epoch: 49 , batch: 199 , training loss: 4.199324\n",
      "[INFO] Epoch: 49 , batch: 200 , training loss: 4.105048\n",
      "[INFO] Epoch: 49 , batch: 201 , training loss: 3.981506\n",
      "[INFO] Epoch: 49 , batch: 202 , training loss: 4.000462\n",
      "[INFO] Epoch: 49 , batch: 203 , training loss: 4.121760\n",
      "[INFO] Epoch: 49 , batch: 204 , training loss: 4.210572\n",
      "[INFO] Epoch: 49 , batch: 205 , training loss: 3.823653\n",
      "[INFO] Epoch: 49 , batch: 206 , training loss: 3.746169\n",
      "[INFO] Epoch: 49 , batch: 207 , training loss: 3.750831\n",
      "[INFO] Epoch: 49 , batch: 208 , training loss: 4.040756\n",
      "[INFO] Epoch: 49 , batch: 209 , training loss: 4.032354\n",
      "[INFO] Epoch: 49 , batch: 210 , training loss: 4.055798\n",
      "[INFO] Epoch: 49 , batch: 211 , training loss: 4.062998\n",
      "[INFO] Epoch: 49 , batch: 212 , training loss: 4.129389\n",
      "[INFO] Epoch: 49 , batch: 213 , training loss: 4.092129\n",
      "[INFO] Epoch: 49 , batch: 214 , training loss: 4.181203\n",
      "[INFO] Epoch: 49 , batch: 215 , training loss: 4.353571\n",
      "[INFO] Epoch: 49 , batch: 216 , training loss: 4.073246\n",
      "[INFO] Epoch: 49 , batch: 217 , training loss: 4.029600\n",
      "[INFO] Epoch: 49 , batch: 218 , training loss: 4.010832\n",
      "[INFO] Epoch: 49 , batch: 219 , training loss: 4.115719\n",
      "[INFO] Epoch: 49 , batch: 220 , training loss: 3.955785\n",
      "[INFO] Epoch: 49 , batch: 221 , training loss: 3.963393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 49 , batch: 222 , training loss: 4.092528\n",
      "[INFO] Epoch: 49 , batch: 223 , training loss: 4.217715\n",
      "[INFO] Epoch: 49 , batch: 224 , training loss: 4.244508\n",
      "[INFO] Epoch: 49 , batch: 225 , training loss: 4.145590\n",
      "[INFO] Epoch: 49 , batch: 226 , training loss: 4.279922\n",
      "[INFO] Epoch: 49 , batch: 227 , training loss: 4.237809\n",
      "[INFO] Epoch: 49 , batch: 228 , training loss: 4.243947\n",
      "[INFO] Epoch: 49 , batch: 229 , training loss: 4.115871\n",
      "[INFO] Epoch: 49 , batch: 230 , training loss: 3.977183\n",
      "[INFO] Epoch: 49 , batch: 231 , training loss: 3.831680\n",
      "[INFO] Epoch: 49 , batch: 232 , training loss: 3.976171\n",
      "[INFO] Epoch: 49 , batch: 233 , training loss: 4.033042\n",
      "[INFO] Epoch: 49 , batch: 234 , training loss: 3.701003\n",
      "[INFO] Epoch: 49 , batch: 235 , training loss: 3.822409\n",
      "[INFO] Epoch: 49 , batch: 236 , training loss: 3.921830\n",
      "[INFO] Epoch: 49 , batch: 237 , training loss: 4.128709\n",
      "[INFO] Epoch: 49 , batch: 238 , training loss: 3.933766\n",
      "[INFO] Epoch: 49 , batch: 239 , training loss: 3.963824\n",
      "[INFO] Epoch: 49 , batch: 240 , training loss: 3.986775\n",
      "[INFO] Epoch: 49 , batch: 241 , training loss: 3.808826\n",
      "[INFO] Epoch: 49 , batch: 242 , training loss: 3.840797\n",
      "[INFO] Epoch: 49 , batch: 243 , training loss: 4.117178\n",
      "[INFO] Epoch: 49 , batch: 244 , training loss: 4.068347\n",
      "[INFO] Epoch: 49 , batch: 245 , training loss: 4.013249\n",
      "[INFO] Epoch: 49 , batch: 246 , training loss: 3.727468\n",
      "[INFO] Epoch: 49 , batch: 247 , training loss: 3.900619\n",
      "[INFO] Epoch: 49 , batch: 248 , training loss: 3.979140\n",
      "[INFO] Epoch: 49 , batch: 249 , training loss: 3.964262\n",
      "[INFO] Epoch: 49 , batch: 250 , training loss: 3.766595\n",
      "[INFO] Epoch: 49 , batch: 251 , training loss: 4.204618\n",
      "[INFO] Epoch: 49 , batch: 252 , training loss: 3.920544\n",
      "[INFO] Epoch: 49 , batch: 253 , training loss: 3.822734\n",
      "[INFO] Epoch: 49 , batch: 254 , training loss: 4.095053\n",
      "[INFO] Epoch: 49 , batch: 255 , training loss: 4.076770\n",
      "[INFO] Epoch: 49 , batch: 256 , training loss: 4.059818\n",
      "[INFO] Epoch: 49 , batch: 257 , training loss: 4.249428\n",
      "[INFO] Epoch: 49 , batch: 258 , training loss: 4.234065\n",
      "[INFO] Epoch: 49 , batch: 259 , training loss: 4.277444\n",
      "[INFO] Epoch: 49 , batch: 260 , training loss: 4.046779\n",
      "[INFO] Epoch: 49 , batch: 261 , training loss: 4.224561\n",
      "[INFO] Epoch: 49 , batch: 262 , training loss: 4.342880\n",
      "[INFO] Epoch: 49 , batch: 263 , training loss: 4.513474\n",
      "[INFO] Epoch: 49 , batch: 264 , training loss: 3.880884\n",
      "[INFO] Epoch: 49 , batch: 265 , training loss: 4.009943\n",
      "[INFO] Epoch: 49 , batch: 266 , training loss: 4.399332\n",
      "[INFO] Epoch: 49 , batch: 267 , training loss: 4.136688\n",
      "[INFO] Epoch: 49 , batch: 268 , training loss: 4.099266\n",
      "[INFO] Epoch: 49 , batch: 269 , training loss: 4.051054\n",
      "[INFO] Epoch: 49 , batch: 270 , training loss: 4.079366\n",
      "[INFO] Epoch: 49 , batch: 271 , training loss: 4.108049\n",
      "[INFO] Epoch: 49 , batch: 272 , training loss: 4.103673\n",
      "[INFO] Epoch: 49 , batch: 273 , training loss: 4.099311\n",
      "[INFO] Epoch: 49 , batch: 274 , training loss: 4.194653\n",
      "[INFO] Epoch: 49 , batch: 275 , training loss: 4.063485\n",
      "[INFO] Epoch: 49 , batch: 276 , training loss: 4.111239\n",
      "[INFO] Epoch: 49 , batch: 277 , training loss: 4.277637\n",
      "[INFO] Epoch: 49 , batch: 278 , training loss: 3.997163\n",
      "[INFO] Epoch: 49 , batch: 279 , training loss: 3.985398\n",
      "[INFO] Epoch: 49 , batch: 280 , training loss: 3.961199\n",
      "[INFO] Epoch: 49 , batch: 281 , training loss: 4.090430\n",
      "[INFO] Epoch: 49 , batch: 282 , training loss: 3.985852\n",
      "[INFO] Epoch: 49 , batch: 283 , training loss: 4.005391\n",
      "[INFO] Epoch: 49 , batch: 284 , training loss: 4.025952\n",
      "[INFO] Epoch: 49 , batch: 285 , training loss: 3.980153\n",
      "[INFO] Epoch: 49 , batch: 286 , training loss: 3.969251\n",
      "[INFO] Epoch: 49 , batch: 287 , training loss: 3.937390\n",
      "[INFO] Epoch: 49 , batch: 288 , training loss: 3.877943\n",
      "[INFO] Epoch: 49 , batch: 289 , training loss: 3.966643\n",
      "[INFO] Epoch: 49 , batch: 290 , training loss: 3.739625\n",
      "[INFO] Epoch: 49 , batch: 291 , training loss: 3.746211\n",
      "[INFO] Epoch: 49 , batch: 292 , training loss: 3.823386\n",
      "[INFO] Epoch: 49 , batch: 293 , training loss: 3.755866\n",
      "[INFO] Epoch: 49 , batch: 294 , training loss: 4.420266\n",
      "[INFO] Epoch: 49 , batch: 295 , training loss: 4.197258\n",
      "[INFO] Epoch: 49 , batch: 296 , training loss: 4.137227\n",
      "[INFO] Epoch: 49 , batch: 297 , training loss: 4.078224\n",
      "[INFO] Epoch: 49 , batch: 298 , training loss: 3.909711\n",
      "[INFO] Epoch: 49 , batch: 299 , training loss: 3.977849\n",
      "[INFO] Epoch: 49 , batch: 300 , training loss: 3.940889\n",
      "[INFO] Epoch: 49 , batch: 301 , training loss: 3.876637\n",
      "[INFO] Epoch: 49 , batch: 302 , training loss: 4.041366\n",
      "[INFO] Epoch: 49 , batch: 303 , training loss: 4.055405\n",
      "[INFO] Epoch: 49 , batch: 304 , training loss: 4.192780\n",
      "[INFO] Epoch: 49 , batch: 305 , training loss: 4.043370\n",
      "[INFO] Epoch: 49 , batch: 306 , training loss: 4.154068\n",
      "[INFO] Epoch: 49 , batch: 307 , training loss: 4.165153\n",
      "[INFO] Epoch: 49 , batch: 308 , training loss: 3.956437\n",
      "[INFO] Epoch: 49 , batch: 309 , training loss: 3.951981\n",
      "[INFO] Epoch: 49 , batch: 310 , training loss: 3.901808\n",
      "[INFO] Epoch: 49 , batch: 311 , training loss: 3.895509\n",
      "[INFO] Epoch: 49 , batch: 312 , training loss: 3.796286\n",
      "[INFO] Epoch: 49 , batch: 313 , training loss: 3.900732\n",
      "[INFO] Epoch: 49 , batch: 314 , training loss: 3.969920\n",
      "[INFO] Epoch: 49 , batch: 315 , training loss: 4.054075\n",
      "[INFO] Epoch: 49 , batch: 316 , training loss: 4.271656\n",
      "[INFO] Epoch: 49 , batch: 317 , training loss: 4.649728\n",
      "[INFO] Epoch: 49 , batch: 318 , training loss: 4.742997\n",
      "[INFO] Epoch: 49 , batch: 319 , training loss: 4.440630\n",
      "[INFO] Epoch: 49 , batch: 320 , training loss: 3.993582\n",
      "[INFO] Epoch: 49 , batch: 321 , training loss: 3.826545\n",
      "[INFO] Epoch: 49 , batch: 322 , training loss: 3.928572\n",
      "[INFO] Epoch: 49 , batch: 323 , training loss: 3.956507\n",
      "[INFO] Epoch: 49 , batch: 324 , training loss: 3.930486\n",
      "[INFO] Epoch: 49 , batch: 325 , training loss: 4.036986\n",
      "[INFO] Epoch: 49 , batch: 326 , training loss: 4.107133\n",
      "[INFO] Epoch: 49 , batch: 327 , training loss: 4.053925\n",
      "[INFO] Epoch: 49 , batch: 328 , training loss: 4.066760\n",
      "[INFO] Epoch: 49 , batch: 329 , training loss: 3.965183\n",
      "[INFO] Epoch: 49 , batch: 330 , training loss: 3.952506\n",
      "[INFO] Epoch: 49 , batch: 331 , training loss: 4.091159\n",
      "[INFO] Epoch: 49 , batch: 332 , training loss: 3.936698\n",
      "[INFO] Epoch: 49 , batch: 333 , training loss: 3.917632\n",
      "[INFO] Epoch: 49 , batch: 334 , training loss: 3.933842\n",
      "[INFO] Epoch: 49 , batch: 335 , training loss: 4.060480\n",
      "[INFO] Epoch: 49 , batch: 336 , training loss: 4.090465\n",
      "[INFO] Epoch: 49 , batch: 337 , training loss: 4.110156\n",
      "[INFO] Epoch: 49 , batch: 338 , training loss: 4.313944\n",
      "[INFO] Epoch: 49 , batch: 339 , training loss: 4.132016\n",
      "[INFO] Epoch: 49 , batch: 340 , training loss: 4.314606\n",
      "[INFO] Epoch: 49 , batch: 341 , training loss: 4.086365\n",
      "[INFO] Epoch: 49 , batch: 342 , training loss: 3.880957\n",
      "[INFO] Epoch: 49 , batch: 343 , training loss: 3.946065\n",
      "[INFO] Epoch: 49 , batch: 344 , training loss: 3.811780\n",
      "[INFO] Epoch: 49 , batch: 345 , training loss: 3.938555\n",
      "[INFO] Epoch: 49 , batch: 346 , training loss: 3.979956\n",
      "[INFO] Epoch: 49 , batch: 347 , training loss: 3.900747\n",
      "[INFO] Epoch: 49 , batch: 348 , training loss: 3.987963\n",
      "[INFO] Epoch: 49 , batch: 349 , training loss: 4.072391\n",
      "[INFO] Epoch: 49 , batch: 350 , training loss: 3.938565\n",
      "[INFO] Epoch: 49 , batch: 351 , training loss: 4.039039\n",
      "[INFO] Epoch: 49 , batch: 352 , training loss: 4.037430\n",
      "[INFO] Epoch: 49 , batch: 353 , training loss: 4.025936\n",
      "[INFO] Epoch: 49 , batch: 354 , training loss: 4.119144\n",
      "[INFO] Epoch: 49 , batch: 355 , training loss: 4.106274\n",
      "[INFO] Epoch: 49 , batch: 356 , training loss: 3.978812\n",
      "[INFO] Epoch: 49 , batch: 357 , training loss: 4.033318\n",
      "[INFO] Epoch: 49 , batch: 358 , training loss: 3.963202\n",
      "[INFO] Epoch: 49 , batch: 359 , training loss: 3.958024\n",
      "[INFO] Epoch: 49 , batch: 360 , training loss: 4.080678\n",
      "[INFO] Epoch: 49 , batch: 361 , training loss: 4.041796\n",
      "[INFO] Epoch: 49 , batch: 362 , training loss: 4.122347\n",
      "[INFO] Epoch: 49 , batch: 363 , training loss: 4.021749\n",
      "[INFO] Epoch: 49 , batch: 364 , training loss: 4.066130\n",
      "[INFO] Epoch: 49 , batch: 365 , training loss: 3.993383\n",
      "[INFO] Epoch: 49 , batch: 366 , training loss: 4.083247\n",
      "[INFO] Epoch: 49 , batch: 367 , training loss: 4.119536\n",
      "[INFO] Epoch: 49 , batch: 368 , training loss: 4.519652\n",
      "[INFO] Epoch: 49 , batch: 369 , training loss: 4.206128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 49 , batch: 370 , training loss: 4.006313\n",
      "[INFO] Epoch: 49 , batch: 371 , training loss: 4.386966\n",
      "[INFO] Epoch: 49 , batch: 372 , training loss: 4.643270\n",
      "[INFO] Epoch: 49 , batch: 373 , training loss: 4.689167\n",
      "[INFO] Epoch: 49 , batch: 374 , training loss: 4.844664\n",
      "[INFO] Epoch: 49 , batch: 375 , training loss: 4.794846\n",
      "[INFO] Epoch: 49 , batch: 376 , training loss: 4.651857\n",
      "[INFO] Epoch: 49 , batch: 377 , training loss: 4.443308\n",
      "[INFO] Epoch: 49 , batch: 378 , training loss: 4.557363\n",
      "[INFO] Epoch: 49 , batch: 379 , training loss: 4.500107\n",
      "[INFO] Epoch: 49 , batch: 380 , training loss: 4.699841\n",
      "[INFO] Epoch: 49 , batch: 381 , training loss: 4.376997\n",
      "[INFO] Epoch: 49 , batch: 382 , training loss: 4.619482\n",
      "[INFO] Epoch: 49 , batch: 383 , training loss: 4.668070\n",
      "[INFO] Epoch: 49 , batch: 384 , training loss: 4.646447\n",
      "[INFO] Epoch: 49 , batch: 385 , training loss: 4.322615\n",
      "[INFO] Epoch: 49 , batch: 386 , training loss: 4.594927\n",
      "[INFO] Epoch: 49 , batch: 387 , training loss: 4.526247\n",
      "[INFO] Epoch: 49 , batch: 388 , training loss: 4.365055\n",
      "[INFO] Epoch: 49 , batch: 389 , training loss: 4.178711\n",
      "[INFO] Epoch: 49 , batch: 390 , training loss: 4.210716\n",
      "[INFO] Epoch: 49 , batch: 391 , training loss: 4.205827\n",
      "[INFO] Epoch: 49 , batch: 392 , training loss: 4.586633\n",
      "[INFO] Epoch: 49 , batch: 393 , training loss: 4.465417\n",
      "[INFO] Epoch: 49 , batch: 394 , training loss: 4.601484\n",
      "[INFO] Epoch: 49 , batch: 395 , training loss: 4.414983\n",
      "[INFO] Epoch: 49 , batch: 396 , training loss: 4.215347\n",
      "[INFO] Epoch: 49 , batch: 397 , training loss: 4.354381\n",
      "[INFO] Epoch: 49 , batch: 398 , training loss: 4.204233\n",
      "[INFO] Epoch: 49 , batch: 399 , training loss: 4.288109\n",
      "[INFO] Epoch: 49 , batch: 400 , training loss: 4.269326\n",
      "[INFO] Epoch: 49 , batch: 401 , training loss: 4.692650\n",
      "[INFO] Epoch: 49 , batch: 402 , training loss: 4.417492\n",
      "[INFO] Epoch: 49 , batch: 403 , training loss: 4.259626\n",
      "[INFO] Epoch: 49 , batch: 404 , training loss: 4.409250\n",
      "[INFO] Epoch: 49 , batch: 405 , training loss: 4.480222\n",
      "[INFO] Epoch: 49 , batch: 406 , training loss: 4.386838\n",
      "[INFO] Epoch: 49 , batch: 407 , training loss: 4.411617\n",
      "[INFO] Epoch: 49 , batch: 408 , training loss: 4.385706\n",
      "[INFO] Epoch: 49 , batch: 409 , training loss: 4.416644\n",
      "[INFO] Epoch: 49 , batch: 410 , training loss: 4.458594\n",
      "[INFO] Epoch: 49 , batch: 411 , training loss: 4.625935\n",
      "[INFO] Epoch: 49 , batch: 412 , training loss: 4.457023\n",
      "[INFO] Epoch: 49 , batch: 413 , training loss: 4.314947\n",
      "[INFO] Epoch: 49 , batch: 414 , training loss: 4.366245\n",
      "[INFO] Epoch: 49 , batch: 415 , training loss: 4.415442\n",
      "[INFO] Epoch: 49 , batch: 416 , training loss: 4.477033\n",
      "[INFO] Epoch: 49 , batch: 417 , training loss: 4.392013\n",
      "[INFO] Epoch: 49 , batch: 418 , training loss: 4.453487\n",
      "[INFO] Epoch: 49 , batch: 419 , training loss: 4.405336\n",
      "[INFO] Epoch: 49 , batch: 420 , training loss: 4.381450\n",
      "[INFO] Epoch: 49 , batch: 421 , training loss: 4.359229\n",
      "[INFO] Epoch: 49 , batch: 422 , training loss: 4.202685\n",
      "[INFO] Epoch: 49 , batch: 423 , training loss: 4.448933\n",
      "[INFO] Epoch: 49 , batch: 424 , training loss: 4.601903\n",
      "[INFO] Epoch: 49 , batch: 425 , training loss: 4.470942\n",
      "[INFO] Epoch: 49 , batch: 426 , training loss: 4.225431\n",
      "[INFO] Epoch: 49 , batch: 427 , training loss: 4.448050\n",
      "[INFO] Epoch: 49 , batch: 428 , training loss: 4.336299\n",
      "[INFO] Epoch: 49 , batch: 429 , training loss: 4.227670\n",
      "[INFO] Epoch: 49 , batch: 430 , training loss: 4.442122\n",
      "[INFO] Epoch: 49 , batch: 431 , training loss: 4.065129\n",
      "[INFO] Epoch: 49 , batch: 432 , training loss: 4.122431\n",
      "[INFO] Epoch: 49 , batch: 433 , training loss: 4.166089\n",
      "[INFO] Epoch: 49 , batch: 434 , training loss: 4.032911\n",
      "[INFO] Epoch: 49 , batch: 435 , training loss: 4.378226\n",
      "[INFO] Epoch: 49 , batch: 436 , training loss: 4.412333\n",
      "[INFO] Epoch: 49 , batch: 437 , training loss: 4.221273\n",
      "[INFO] Epoch: 49 , batch: 438 , training loss: 4.074933\n",
      "[INFO] Epoch: 49 , batch: 439 , training loss: 4.317364\n",
      "[INFO] Epoch: 49 , batch: 440 , training loss: 4.415681\n",
      "[INFO] Epoch: 49 , batch: 441 , training loss: 4.538612\n",
      "[INFO] Epoch: 49 , batch: 442 , training loss: 4.288176\n",
      "[INFO] Epoch: 49 , batch: 443 , training loss: 4.467017\n",
      "[INFO] Epoch: 49 , batch: 444 , training loss: 4.078001\n",
      "[INFO] Epoch: 49 , batch: 445 , training loss: 3.970792\n",
      "[INFO] Epoch: 49 , batch: 446 , training loss: 3.903054\n",
      "[INFO] Epoch: 49 , batch: 447 , training loss: 4.112645\n",
      "[INFO] Epoch: 49 , batch: 448 , training loss: 4.212807\n",
      "[INFO] Epoch: 49 , batch: 449 , training loss: 4.582955\n",
      "[INFO] Epoch: 49 , batch: 450 , training loss: 4.677867\n",
      "[INFO] Epoch: 49 , batch: 451 , training loss: 4.556847\n",
      "[INFO] Epoch: 49 , batch: 452 , training loss: 4.377818\n",
      "[INFO] Epoch: 49 , batch: 453 , training loss: 4.148460\n",
      "[INFO] Epoch: 49 , batch: 454 , training loss: 4.300963\n",
      "[INFO] Epoch: 49 , batch: 455 , training loss: 4.349465\n",
      "[INFO] Epoch: 49 , batch: 456 , training loss: 4.340288\n",
      "[INFO] Epoch: 49 , batch: 457 , training loss: 4.417334\n",
      "[INFO] Epoch: 49 , batch: 458 , training loss: 4.154427\n",
      "[INFO] Epoch: 49 , batch: 459 , training loss: 4.134097\n",
      "[INFO] Epoch: 49 , batch: 460 , training loss: 4.260902\n",
      "[INFO] Epoch: 49 , batch: 461 , training loss: 4.224276\n",
      "[INFO] Epoch: 49 , batch: 462 , training loss: 4.272941\n",
      "[INFO] Epoch: 49 , batch: 463 , training loss: 4.196590\n",
      "[INFO] Epoch: 49 , batch: 464 , training loss: 4.364447\n",
      "[INFO] Epoch: 49 , batch: 465 , training loss: 4.318002\n",
      "[INFO] Epoch: 49 , batch: 466 , training loss: 4.417343\n",
      "[INFO] Epoch: 49 , batch: 467 , training loss: 4.362348\n",
      "[INFO] Epoch: 49 , batch: 468 , training loss: 4.336455\n",
      "[INFO] Epoch: 49 , batch: 469 , training loss: 4.343814\n",
      "[INFO] Epoch: 49 , batch: 470 , training loss: 4.175448\n",
      "[INFO] Epoch: 49 , batch: 471 , training loss: 4.269678\n",
      "[INFO] Epoch: 49 , batch: 472 , training loss: 4.326242\n",
      "[INFO] Epoch: 49 , batch: 473 , training loss: 4.254809\n",
      "[INFO] Epoch: 49 , batch: 474 , training loss: 4.035241\n",
      "[INFO] Epoch: 49 , batch: 475 , training loss: 3.924541\n",
      "[INFO] Epoch: 49 , batch: 476 , training loss: 4.315400\n",
      "[INFO] Epoch: 49 , batch: 477 , training loss: 4.434126\n",
      "[INFO] Epoch: 49 , batch: 478 , training loss: 4.433069\n",
      "[INFO] Epoch: 49 , batch: 479 , training loss: 4.420550\n",
      "[INFO] Epoch: 49 , batch: 480 , training loss: 4.543723\n",
      "[INFO] Epoch: 49 , batch: 481 , training loss: 4.415344\n",
      "[INFO] Epoch: 49 , batch: 482 , training loss: 4.530398\n",
      "[INFO] Epoch: 49 , batch: 483 , training loss: 4.365965\n",
      "[INFO] Epoch: 49 , batch: 484 , training loss: 4.179417\n",
      "[INFO] Epoch: 49 , batch: 485 , training loss: 4.261132\n",
      "[INFO] Epoch: 49 , batch: 486 , training loss: 4.159614\n",
      "[INFO] Epoch: 49 , batch: 487 , training loss: 4.131879\n",
      "[INFO] Epoch: 49 , batch: 488 , training loss: 4.318023\n",
      "[INFO] Epoch: 49 , batch: 489 , training loss: 4.229075\n",
      "[INFO] Epoch: 49 , batch: 490 , training loss: 4.298396\n",
      "[INFO] Epoch: 49 , batch: 491 , training loss: 4.194136\n",
      "[INFO] Epoch: 49 , batch: 492 , training loss: 4.189202\n",
      "[INFO] Epoch: 49 , batch: 493 , training loss: 4.350715\n",
      "[INFO] Epoch: 49 , batch: 494 , training loss: 4.261954\n",
      "[INFO] Epoch: 49 , batch: 495 , training loss: 4.428760\n",
      "[INFO] Epoch: 49 , batch: 496 , training loss: 4.298076\n",
      "[INFO] Epoch: 49 , batch: 497 , training loss: 4.327684\n",
      "[INFO] Epoch: 49 , batch: 498 , training loss: 4.308774\n",
      "[INFO] Epoch: 49 , batch: 499 , training loss: 4.366857\n",
      "[INFO] Epoch: 49 , batch: 500 , training loss: 4.527293\n",
      "[INFO] Epoch: 49 , batch: 501 , training loss: 4.843615\n",
      "[INFO] Epoch: 49 , batch: 502 , training loss: 4.849226\n",
      "[INFO] Epoch: 49 , batch: 503 , training loss: 4.488498\n",
      "[INFO] Epoch: 49 , batch: 504 , training loss: 4.663286\n",
      "[INFO] Epoch: 49 , batch: 505 , training loss: 4.612417\n",
      "[INFO] Epoch: 49 , batch: 506 , training loss: 4.609193\n",
      "[INFO] Epoch: 49 , batch: 507 , training loss: 4.646411\n",
      "[INFO] Epoch: 49 , batch: 508 , training loss: 4.564174\n",
      "[INFO] Epoch: 49 , batch: 509 , training loss: 4.386867\n",
      "[INFO] Epoch: 49 , batch: 510 , training loss: 4.476696\n",
      "[INFO] Epoch: 49 , batch: 511 , training loss: 4.385088\n",
      "[INFO] Epoch: 49 , batch: 512 , training loss: 4.468349\n",
      "[INFO] Epoch: 49 , batch: 513 , training loss: 4.731878\n",
      "[INFO] Epoch: 49 , batch: 514 , training loss: 4.370035\n",
      "[INFO] Epoch: 49 , batch: 515 , training loss: 4.650467\n",
      "[INFO] Epoch: 49 , batch: 516 , training loss: 4.450963\n",
      "[INFO] Epoch: 49 , batch: 517 , training loss: 4.407757\n",
      "[INFO] Epoch: 49 , batch: 518 , training loss: 4.373965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 49 , batch: 519 , training loss: 4.216776\n",
      "[INFO] Epoch: 49 , batch: 520 , training loss: 4.441014\n",
      "[INFO] Epoch: 49 , batch: 521 , training loss: 4.415326\n",
      "[INFO] Epoch: 49 , batch: 522 , training loss: 4.503145\n",
      "[INFO] Epoch: 49 , batch: 523 , training loss: 4.429726\n",
      "[INFO] Epoch: 49 , batch: 524 , training loss: 4.708620\n",
      "[INFO] Epoch: 49 , batch: 525 , training loss: 4.599171\n",
      "[INFO] Epoch: 49 , batch: 526 , training loss: 4.376487\n",
      "[INFO] Epoch: 49 , batch: 527 , training loss: 4.421784\n",
      "[INFO] Epoch: 49 , batch: 528 , training loss: 4.422353\n",
      "[INFO] Epoch: 49 , batch: 529 , training loss: 4.407742\n",
      "[INFO] Epoch: 49 , batch: 530 , training loss: 4.253235\n",
      "[INFO] Epoch: 49 , batch: 531 , training loss: 4.412956\n",
      "[INFO] Epoch: 49 , batch: 532 , training loss: 4.314877\n",
      "[INFO] Epoch: 49 , batch: 533 , training loss: 4.431568\n",
      "[INFO] Epoch: 49 , batch: 534 , training loss: 4.446666\n",
      "[INFO] Epoch: 49 , batch: 535 , training loss: 4.439694\n",
      "[INFO] Epoch: 49 , batch: 536 , training loss: 4.276809\n",
      "[INFO] Epoch: 49 , batch: 537 , training loss: 4.270920\n",
      "[INFO] Epoch: 49 , batch: 538 , training loss: 4.370962\n",
      "[INFO] Epoch: 49 , batch: 539 , training loss: 4.443933\n",
      "[INFO] Epoch: 49 , batch: 540 , training loss: 5.004686\n",
      "[INFO] Epoch: 49 , batch: 541 , training loss: 4.809188\n",
      "[INFO] Epoch: 49 , batch: 542 , training loss: 4.699964\n",
      "[INFO] Epoch: 50 , batch: 0 , training loss: 3.624814\n",
      "[INFO] Epoch: 50 , batch: 1 , training loss: 3.484136\n",
      "[INFO] Epoch: 50 , batch: 2 , training loss: 3.698670\n",
      "[INFO] Epoch: 50 , batch: 3 , training loss: 3.578904\n",
      "[INFO] Epoch: 50 , batch: 4 , training loss: 3.942629\n",
      "[INFO] Epoch: 50 , batch: 5 , training loss: 3.628541\n",
      "[INFO] Epoch: 50 , batch: 6 , training loss: 3.928416\n",
      "[INFO] Epoch: 50 , batch: 7 , training loss: 3.872313\n",
      "[INFO] Epoch: 50 , batch: 8 , training loss: 3.568736\n",
      "[INFO] Epoch: 50 , batch: 9 , training loss: 3.813235\n",
      "[INFO] Epoch: 50 , batch: 10 , training loss: 3.806898\n",
      "[INFO] Epoch: 50 , batch: 11 , training loss: 3.761869\n",
      "[INFO] Epoch: 50 , batch: 12 , training loss: 3.630976\n",
      "[INFO] Epoch: 50 , batch: 13 , training loss: 3.628238\n",
      "[INFO] Epoch: 50 , batch: 14 , training loss: 3.551409\n",
      "[INFO] Epoch: 50 , batch: 15 , training loss: 3.784447\n",
      "[INFO] Epoch: 50 , batch: 16 , training loss: 3.592396\n",
      "[INFO] Epoch: 50 , batch: 17 , training loss: 3.726113\n",
      "[INFO] Epoch: 50 , batch: 18 , training loss: 3.681436\n",
      "[INFO] Epoch: 50 , batch: 19 , training loss: 3.451576\n",
      "[INFO] Epoch: 50 , batch: 20 , training loss: 3.411397\n",
      "[INFO] Epoch: 50 , batch: 21 , training loss: 3.565795\n",
      "[INFO] Epoch: 50 , batch: 22 , training loss: 3.440881\n",
      "[INFO] Epoch: 50 , batch: 23 , training loss: 3.655028\n",
      "[INFO] Epoch: 50 , batch: 24 , training loss: 3.511941\n",
      "[INFO] Epoch: 50 , batch: 25 , training loss: 3.563160\n",
      "[INFO] Epoch: 50 , batch: 26 , training loss: 3.472038\n",
      "[INFO] Epoch: 50 , batch: 27 , training loss: 3.470464\n",
      "[INFO] Epoch: 50 , batch: 28 , training loss: 3.668101\n",
      "[INFO] Epoch: 50 , batch: 29 , training loss: 3.458932\n",
      "[INFO] Epoch: 50 , batch: 30 , training loss: 3.493235\n",
      "[INFO] Epoch: 50 , batch: 31 , training loss: 3.585174\n",
      "[INFO] Epoch: 50 , batch: 32 , training loss: 3.539989\n",
      "[INFO] Epoch: 50 , batch: 33 , training loss: 3.583114\n",
      "[INFO] Epoch: 50 , batch: 34 , training loss: 3.577068\n",
      "[INFO] Epoch: 50 , batch: 35 , training loss: 3.534361\n",
      "[INFO] Epoch: 50 , batch: 36 , training loss: 3.592631\n",
      "[INFO] Epoch: 50 , batch: 37 , training loss: 3.462336\n",
      "[INFO] Epoch: 50 , batch: 38 , training loss: 3.560030\n",
      "[INFO] Epoch: 50 , batch: 39 , training loss: 3.388179\n",
      "[INFO] Epoch: 50 , batch: 40 , training loss: 3.587279\n",
      "[INFO] Epoch: 50 , batch: 41 , training loss: 3.516224\n",
      "[INFO] Epoch: 50 , batch: 42 , training loss: 3.949315\n",
      "[INFO] Epoch: 50 , batch: 43 , training loss: 3.693049\n",
      "[INFO] Epoch: 50 , batch: 44 , training loss: 4.102441\n",
      "[INFO] Epoch: 50 , batch: 45 , training loss: 3.988935\n",
      "[INFO] Epoch: 50 , batch: 46 , training loss: 3.994798\n",
      "[INFO] Epoch: 50 , batch: 47 , training loss: 3.546244\n",
      "[INFO] Epoch: 50 , batch: 48 , training loss: 3.565917\n",
      "[INFO] Epoch: 50 , batch: 49 , training loss: 3.811498\n",
      "[INFO] Epoch: 50 , batch: 50 , training loss: 3.534363\n",
      "[INFO] Epoch: 50 , batch: 51 , training loss: 3.831618\n",
      "[INFO] Epoch: 50 , batch: 52 , training loss: 3.595226\n",
      "[INFO] Epoch: 50 , batch: 53 , training loss: 3.716714\n",
      "[INFO] Epoch: 50 , batch: 54 , training loss: 3.736182\n",
      "[INFO] Epoch: 50 , batch: 55 , training loss: 3.834119\n",
      "[INFO] Epoch: 50 , batch: 56 , training loss: 3.623662\n",
      "[INFO] Epoch: 50 , batch: 57 , training loss: 3.600283\n",
      "[INFO] Epoch: 50 , batch: 58 , training loss: 3.622331\n",
      "[INFO] Epoch: 50 , batch: 59 , training loss: 3.702447\n",
      "[INFO] Epoch: 50 , batch: 60 , training loss: 3.659264\n",
      "[INFO] Epoch: 50 , batch: 61 , training loss: 3.724524\n",
      "[INFO] Epoch: 50 , batch: 62 , training loss: 3.599016\n",
      "[INFO] Epoch: 50 , batch: 63 , training loss: 3.790988\n",
      "[INFO] Epoch: 50 , batch: 64 , training loss: 4.026377\n",
      "[INFO] Epoch: 50 , batch: 65 , training loss: 3.741813\n",
      "[INFO] Epoch: 50 , batch: 66 , training loss: 3.563294\n",
      "[INFO] Epoch: 50 , batch: 67 , training loss: 3.608181\n",
      "[INFO] Epoch: 50 , batch: 68 , training loss: 3.718172\n",
      "[INFO] Epoch: 50 , batch: 69 , training loss: 3.707798\n",
      "[INFO] Epoch: 50 , batch: 70 , training loss: 3.924485\n",
      "[INFO] Epoch: 50 , batch: 71 , training loss: 3.762191\n",
      "[INFO] Epoch: 50 , batch: 72 , training loss: 3.817886\n",
      "[INFO] Epoch: 50 , batch: 73 , training loss: 3.755617\n",
      "[INFO] Epoch: 50 , batch: 74 , training loss: 3.850281\n",
      "[INFO] Epoch: 50 , batch: 75 , training loss: 3.724346\n",
      "[INFO] Epoch: 50 , batch: 76 , training loss: 3.830992\n",
      "[INFO] Epoch: 50 , batch: 77 , training loss: 3.797454\n",
      "[INFO] Epoch: 50 , batch: 78 , training loss: 3.854812\n",
      "[INFO] Epoch: 50 , batch: 79 , training loss: 3.735857\n",
      "[INFO] Epoch: 50 , batch: 80 , training loss: 3.937348\n",
      "[INFO] Epoch: 50 , batch: 81 , training loss: 3.860176\n",
      "[INFO] Epoch: 50 , batch: 82 , training loss: 3.819803\n",
      "[INFO] Epoch: 50 , batch: 83 , training loss: 3.889187\n",
      "[INFO] Epoch: 50 , batch: 84 , training loss: 3.892375\n",
      "[INFO] Epoch: 50 , batch: 85 , training loss: 3.955773\n",
      "[INFO] Epoch: 50 , batch: 86 , training loss: 3.873975\n",
      "[INFO] Epoch: 50 , batch: 87 , training loss: 3.842556\n",
      "[INFO] Epoch: 50 , batch: 88 , training loss: 3.980658\n",
      "[INFO] Epoch: 50 , batch: 89 , training loss: 3.773427\n",
      "[INFO] Epoch: 50 , batch: 90 , training loss: 3.867610\n",
      "[INFO] Epoch: 50 , batch: 91 , training loss: 3.802914\n",
      "[INFO] Epoch: 50 , batch: 92 , training loss: 3.807533\n",
      "[INFO] Epoch: 50 , batch: 93 , training loss: 3.923710\n",
      "[INFO] Epoch: 50 , batch: 94 , training loss: 4.050451\n",
      "[INFO] Epoch: 50 , batch: 95 , training loss: 3.836103\n",
      "[INFO] Epoch: 50 , batch: 96 , training loss: 3.843740\n",
      "[INFO] Epoch: 50 , batch: 97 , training loss: 3.767523\n",
      "[INFO] Epoch: 50 , batch: 98 , training loss: 3.717111\n",
      "[INFO] Epoch: 50 , batch: 99 , training loss: 3.828363\n",
      "[INFO] Epoch: 50 , batch: 100 , training loss: 3.717071\n",
      "[INFO] Epoch: 50 , batch: 101 , training loss: 3.707637\n",
      "[INFO] Epoch: 50 , batch: 102 , training loss: 3.904330\n",
      "[INFO] Epoch: 50 , batch: 103 , training loss: 3.697035\n",
      "[INFO] Epoch: 50 , batch: 104 , training loss: 3.666457\n",
      "[INFO] Epoch: 50 , batch: 105 , training loss: 3.896855\n",
      "[INFO] Epoch: 50 , batch: 106 , training loss: 3.925762\n",
      "[INFO] Epoch: 50 , batch: 107 , training loss: 3.763163\n",
      "[INFO] Epoch: 50 , batch: 108 , training loss: 3.695326\n",
      "[INFO] Epoch: 50 , batch: 109 , training loss: 3.634775\n",
      "[INFO] Epoch: 50 , batch: 110 , training loss: 3.775115\n",
      "[INFO] Epoch: 50 , batch: 111 , training loss: 3.870210\n",
      "[INFO] Epoch: 50 , batch: 112 , training loss: 3.793585\n",
      "[INFO] Epoch: 50 , batch: 113 , training loss: 3.766084\n",
      "[INFO] Epoch: 50 , batch: 114 , training loss: 3.777475\n",
      "[INFO] Epoch: 50 , batch: 115 , training loss: 3.788766\n",
      "[INFO] Epoch: 50 , batch: 116 , training loss: 3.685797\n",
      "[INFO] Epoch: 50 , batch: 117 , training loss: 3.887999\n",
      "[INFO] Epoch: 50 , batch: 118 , training loss: 3.865330\n",
      "[INFO] Epoch: 50 , batch: 119 , training loss: 4.041569\n",
      "[INFO] Epoch: 50 , batch: 120 , training loss: 3.985989\n",
      "[INFO] Epoch: 50 , batch: 121 , training loss: 3.892510\n",
      "[INFO] Epoch: 50 , batch: 122 , training loss: 3.800857\n",
      "[INFO] Epoch: 50 , batch: 123 , training loss: 3.755576\n",
      "[INFO] Epoch: 50 , batch: 124 , training loss: 3.864883\n",
      "[INFO] Epoch: 50 , batch: 125 , training loss: 3.690359\n",
      "[INFO] Epoch: 50 , batch: 126 , training loss: 3.699887\n",
      "[INFO] Epoch: 50 , batch: 127 , training loss: 3.712679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 50 , batch: 128 , training loss: 3.847430\n",
      "[INFO] Epoch: 50 , batch: 129 , training loss: 3.796490\n",
      "[INFO] Epoch: 50 , batch: 130 , training loss: 3.813517\n",
      "[INFO] Epoch: 50 , batch: 131 , training loss: 3.802573\n",
      "[INFO] Epoch: 50 , batch: 132 , training loss: 3.821883\n",
      "[INFO] Epoch: 50 , batch: 133 , training loss: 3.797600\n",
      "[INFO] Epoch: 50 , batch: 134 , training loss: 3.585126\n",
      "[INFO] Epoch: 50 , batch: 135 , training loss: 3.637657\n",
      "[INFO] Epoch: 50 , batch: 136 , training loss: 3.893887\n",
      "[INFO] Epoch: 50 , batch: 137 , training loss: 3.844485\n",
      "[INFO] Epoch: 50 , batch: 138 , training loss: 3.895700\n",
      "[INFO] Epoch: 50 , batch: 139 , training loss: 4.418809\n",
      "[INFO] Epoch: 50 , batch: 140 , training loss: 4.216943\n",
      "[INFO] Epoch: 50 , batch: 141 , training loss: 3.981909\n",
      "[INFO] Epoch: 50 , batch: 142 , training loss: 3.719034\n",
      "[INFO] Epoch: 50 , batch: 143 , training loss: 3.892154\n",
      "[INFO] Epoch: 50 , batch: 144 , training loss: 3.695122\n",
      "[INFO] Epoch: 50 , batch: 145 , training loss: 3.794044\n",
      "[INFO] Epoch: 50 , batch: 146 , training loss: 3.949669\n",
      "[INFO] Epoch: 50 , batch: 147 , training loss: 3.634669\n",
      "[INFO] Epoch: 50 , batch: 148 , training loss: 3.597996\n",
      "[INFO] Epoch: 50 , batch: 149 , training loss: 3.674613\n",
      "[INFO] Epoch: 50 , batch: 150 , training loss: 3.939857\n",
      "[INFO] Epoch: 50 , batch: 151 , training loss: 3.806181\n",
      "[INFO] Epoch: 50 , batch: 152 , training loss: 3.829268\n",
      "[INFO] Epoch: 50 , batch: 153 , training loss: 3.865791\n",
      "[INFO] Epoch: 50 , batch: 154 , training loss: 3.935327\n",
      "[INFO] Epoch: 50 , batch: 155 , training loss: 4.140844\n",
      "[INFO] Epoch: 50 , batch: 156 , training loss: 3.896034\n",
      "[INFO] Epoch: 50 , batch: 157 , training loss: 3.858605\n",
      "[INFO] Epoch: 50 , batch: 158 , training loss: 3.974892\n",
      "[INFO] Epoch: 50 , batch: 159 , training loss: 3.901668\n",
      "[INFO] Epoch: 50 , batch: 160 , training loss: 4.115224\n",
      "[INFO] Epoch: 50 , batch: 161 , training loss: 4.171227\n",
      "[INFO] Epoch: 50 , batch: 162 , training loss: 4.163768\n",
      "[INFO] Epoch: 50 , batch: 163 , training loss: 4.329298\n",
      "[INFO] Epoch: 50 , batch: 164 , training loss: 4.300065\n",
      "[INFO] Epoch: 50 , batch: 165 , training loss: 4.216254\n",
      "[INFO] Epoch: 50 , batch: 166 , training loss: 4.102455\n",
      "[INFO] Epoch: 50 , batch: 167 , training loss: 4.141393\n",
      "[INFO] Epoch: 50 , batch: 168 , training loss: 3.831581\n",
      "[INFO] Epoch: 50 , batch: 169 , training loss: 3.798289\n",
      "[INFO] Epoch: 50 , batch: 170 , training loss: 3.993272\n",
      "[INFO] Epoch: 50 , batch: 171 , training loss: 3.464812\n",
      "[INFO] Epoch: 50 , batch: 172 , training loss: 3.664323\n",
      "[INFO] Epoch: 50 , batch: 173 , training loss: 3.982089\n",
      "[INFO] Epoch: 50 , batch: 174 , training loss: 4.455251\n",
      "[INFO] Epoch: 50 , batch: 175 , training loss: 4.710054\n",
      "[INFO] Epoch: 50 , batch: 176 , training loss: 4.360762\n",
      "[INFO] Epoch: 50 , batch: 177 , training loss: 3.973534\n",
      "[INFO] Epoch: 50 , batch: 178 , training loss: 3.993379\n",
      "[INFO] Epoch: 50 , batch: 179 , training loss: 4.036088\n",
      "[INFO] Epoch: 50 , batch: 180 , training loss: 4.018675\n",
      "[INFO] Epoch: 50 , batch: 181 , training loss: 4.299956\n",
      "[INFO] Epoch: 50 , batch: 182 , training loss: 4.247563\n",
      "[INFO] Epoch: 50 , batch: 183 , training loss: 4.227063\n",
      "[INFO] Epoch: 50 , batch: 184 , training loss: 4.104490\n",
      "[INFO] Epoch: 50 , batch: 185 , training loss: 4.076932\n",
      "[INFO] Epoch: 50 , batch: 186 , training loss: 4.209085\n",
      "[INFO] Epoch: 50 , batch: 187 , training loss: 4.313208\n",
      "[INFO] Epoch: 50 , batch: 188 , training loss: 4.311806\n",
      "[INFO] Epoch: 50 , batch: 189 , training loss: 4.228733\n",
      "[INFO] Epoch: 50 , batch: 190 , training loss: 4.281092\n",
      "[INFO] Epoch: 50 , batch: 191 , training loss: 4.381312\n",
      "[INFO] Epoch: 50 , batch: 192 , training loss: 4.197154\n",
      "[INFO] Epoch: 50 , batch: 193 , training loss: 4.321524\n",
      "[INFO] Epoch: 50 , batch: 194 , training loss: 4.240164\n",
      "[INFO] Epoch: 50 , batch: 195 , training loss: 4.176617\n",
      "[INFO] Epoch: 50 , batch: 196 , training loss: 4.035974\n",
      "[INFO] Epoch: 50 , batch: 197 , training loss: 4.122361\n",
      "[INFO] Epoch: 50 , batch: 198 , training loss: 4.034648\n",
      "[INFO] Epoch: 50 , batch: 199 , training loss: 4.175475\n",
      "[INFO] Epoch: 50 , batch: 200 , training loss: 4.090762\n",
      "[INFO] Epoch: 50 , batch: 201 , training loss: 3.986508\n",
      "[INFO] Epoch: 50 , batch: 202 , training loss: 3.986506\n",
      "[INFO] Epoch: 50 , batch: 203 , training loss: 4.125710\n",
      "[INFO] Epoch: 50 , batch: 204 , training loss: 4.193974\n",
      "[INFO] Epoch: 50 , batch: 205 , training loss: 3.811103\n",
      "[INFO] Epoch: 50 , batch: 206 , training loss: 3.756158\n",
      "[INFO] Epoch: 50 , batch: 207 , training loss: 3.735322\n",
      "[INFO] Epoch: 50 , batch: 208 , training loss: 4.029589\n",
      "[INFO] Epoch: 50 , batch: 209 , training loss: 4.018553\n",
      "[INFO] Epoch: 50 , batch: 210 , training loss: 4.038213\n",
      "[INFO] Epoch: 50 , batch: 211 , training loss: 4.035295\n",
      "[INFO] Epoch: 50 , batch: 212 , training loss: 4.118982\n",
      "[INFO] Epoch: 50 , batch: 213 , training loss: 4.093717\n",
      "[INFO] Epoch: 50 , batch: 214 , training loss: 4.173312\n",
      "[INFO] Epoch: 50 , batch: 215 , training loss: 4.355482\n",
      "[INFO] Epoch: 50 , batch: 216 , training loss: 4.047685\n",
      "[INFO] Epoch: 50 , batch: 217 , training loss: 4.018432\n",
      "[INFO] Epoch: 50 , batch: 218 , training loss: 4.009660\n",
      "[INFO] Epoch: 50 , batch: 219 , training loss: 4.123384\n",
      "[INFO] Epoch: 50 , batch: 220 , training loss: 3.953014\n",
      "[INFO] Epoch: 50 , batch: 221 , training loss: 3.955472\n",
      "[INFO] Epoch: 50 , batch: 222 , training loss: 4.073332\n",
      "[INFO] Epoch: 50 , batch: 223 , training loss: 4.211171\n",
      "[INFO] Epoch: 50 , batch: 224 , training loss: 4.234593\n",
      "[INFO] Epoch: 50 , batch: 225 , training loss: 4.143353\n",
      "[INFO] Epoch: 50 , batch: 226 , training loss: 4.241405\n",
      "[INFO] Epoch: 50 , batch: 227 , training loss: 4.240654\n",
      "[INFO] Epoch: 50 , batch: 228 , training loss: 4.236683\n",
      "[INFO] Epoch: 50 , batch: 229 , training loss: 4.105340\n",
      "[INFO] Epoch: 50 , batch: 230 , training loss: 3.978055\n",
      "[INFO] Epoch: 50 , batch: 231 , training loss: 3.831480\n",
      "[INFO] Epoch: 50 , batch: 232 , training loss: 3.970855\n",
      "[INFO] Epoch: 50 , batch: 233 , training loss: 4.015164\n",
      "[INFO] Epoch: 50 , batch: 234 , training loss: 3.693300\n",
      "[INFO] Epoch: 50 , batch: 235 , training loss: 3.810730\n",
      "[INFO] Epoch: 50 , batch: 236 , training loss: 3.916565\n",
      "[INFO] Epoch: 50 , batch: 237 , training loss: 4.111752\n",
      "[INFO] Epoch: 50 , batch: 238 , training loss: 3.913897\n",
      "[INFO] Epoch: 50 , batch: 239 , training loss: 3.952084\n",
      "[INFO] Epoch: 50 , batch: 240 , training loss: 3.973807\n",
      "[INFO] Epoch: 50 , batch: 241 , training loss: 3.793653\n",
      "[INFO] Epoch: 50 , batch: 242 , training loss: 3.809381\n",
      "[INFO] Epoch: 50 , batch: 243 , training loss: 4.109713\n",
      "[INFO] Epoch: 50 , batch: 244 , training loss: 4.057133\n",
      "[INFO] Epoch: 50 , batch: 245 , training loss: 3.988589\n",
      "[INFO] Epoch: 50 , batch: 246 , training loss: 3.730469\n",
      "[INFO] Epoch: 50 , batch: 247 , training loss: 3.896242\n",
      "[INFO] Epoch: 50 , batch: 248 , training loss: 3.966045\n",
      "[INFO] Epoch: 50 , batch: 249 , training loss: 3.936819\n",
      "[INFO] Epoch: 50 , batch: 250 , training loss: 3.766426\n",
      "[INFO] Epoch: 50 , batch: 251 , training loss: 4.180589\n",
      "[INFO] Epoch: 50 , batch: 252 , training loss: 3.911658\n",
      "[INFO] Epoch: 50 , batch: 253 , training loss: 3.803323\n",
      "[INFO] Epoch: 50 , batch: 254 , training loss: 4.079388\n",
      "[INFO] Epoch: 50 , batch: 255 , training loss: 4.042356\n",
      "[INFO] Epoch: 50 , batch: 256 , training loss: 4.035913\n",
      "[INFO] Epoch: 50 , batch: 257 , training loss: 4.224379\n",
      "[INFO] Epoch: 50 , batch: 258 , training loss: 4.208585\n",
      "[INFO] Epoch: 50 , batch: 259 , training loss: 4.256188\n",
      "[INFO] Epoch: 50 , batch: 260 , training loss: 4.032680\n",
      "[INFO] Epoch: 50 , batch: 261 , training loss: 4.211908\n",
      "[INFO] Epoch: 50 , batch: 262 , training loss: 4.330564\n",
      "[INFO] Epoch: 50 , batch: 263 , training loss: 4.502911\n",
      "[INFO] Epoch: 50 , batch: 264 , training loss: 3.872941\n",
      "[INFO] Epoch: 50 , batch: 265 , training loss: 3.990073\n",
      "[INFO] Epoch: 50 , batch: 266 , training loss: 4.383479\n",
      "[INFO] Epoch: 50 , batch: 267 , training loss: 4.135292\n",
      "[INFO] Epoch: 50 , batch: 268 , training loss: 4.068408\n",
      "[INFO] Epoch: 50 , batch: 269 , training loss: 4.020957\n",
      "[INFO] Epoch: 50 , batch: 270 , training loss: 4.077734\n",
      "[INFO] Epoch: 50 , batch: 271 , training loss: 4.106831\n",
      "[INFO] Epoch: 50 , batch: 272 , training loss: 4.083542\n",
      "[INFO] Epoch: 50 , batch: 273 , training loss: 4.095465\n",
      "[INFO] Epoch: 50 , batch: 274 , training loss: 4.187243\n",
      "[INFO] Epoch: 50 , batch: 275 , training loss: 4.060868\n",
      "[INFO] Epoch: 50 , batch: 276 , training loss: 4.111776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 50 , batch: 277 , training loss: 4.268016\n",
      "[INFO] Epoch: 50 , batch: 278 , training loss: 3.977961\n",
      "[INFO] Epoch: 50 , batch: 279 , training loss: 3.970287\n",
      "[INFO] Epoch: 50 , batch: 280 , training loss: 3.952059\n",
      "[INFO] Epoch: 50 , batch: 281 , training loss: 4.076497\n",
      "[INFO] Epoch: 50 , batch: 282 , training loss: 3.984058\n",
      "[INFO] Epoch: 50 , batch: 283 , training loss: 3.983764\n",
      "[INFO] Epoch: 50 , batch: 284 , training loss: 4.028734\n",
      "[INFO] Epoch: 50 , batch: 285 , training loss: 3.959434\n",
      "[INFO] Epoch: 50 , batch: 286 , training loss: 3.976697\n",
      "[INFO] Epoch: 50 , batch: 287 , training loss: 3.914229\n",
      "[INFO] Epoch: 50 , batch: 288 , training loss: 3.844163\n",
      "[INFO] Epoch: 50 , batch: 289 , training loss: 3.952339\n",
      "[INFO] Epoch: 50 , batch: 290 , training loss: 3.727323\n",
      "[INFO] Epoch: 50 , batch: 291 , training loss: 3.728145\n",
      "[INFO] Epoch: 50 , batch: 292 , training loss: 3.820476\n",
      "[INFO] Epoch: 50 , batch: 293 , training loss: 3.754133\n",
      "[INFO] Epoch: 50 , batch: 294 , training loss: 4.407879\n",
      "[INFO] Epoch: 50 , batch: 295 , training loss: 4.202375\n",
      "[INFO] Epoch: 50 , batch: 296 , training loss: 4.114678\n",
      "[INFO] Epoch: 50 , batch: 297 , training loss: 4.062958\n",
      "[INFO] Epoch: 50 , batch: 298 , training loss: 3.891556\n",
      "[INFO] Epoch: 50 , batch: 299 , training loss: 3.966191\n",
      "[INFO] Epoch: 50 , batch: 300 , training loss: 3.932926\n",
      "[INFO] Epoch: 50 , batch: 301 , training loss: 3.850671\n",
      "[INFO] Epoch: 50 , batch: 302 , training loss: 4.042644\n",
      "[INFO] Epoch: 50 , batch: 303 , training loss: 4.035191\n",
      "[INFO] Epoch: 50 , batch: 304 , training loss: 4.165900\n",
      "[INFO] Epoch: 50 , batch: 305 , training loss: 4.009569\n",
      "[INFO] Epoch: 50 , batch: 306 , training loss: 4.123192\n",
      "[INFO] Epoch: 50 , batch: 307 , training loss: 4.163042\n",
      "[INFO] Epoch: 50 , batch: 308 , training loss: 3.952122\n",
      "[INFO] Epoch: 50 , batch: 309 , training loss: 3.945294\n",
      "[INFO] Epoch: 50 , batch: 310 , training loss: 3.894927\n",
      "[INFO] Epoch: 50 , batch: 311 , training loss: 3.880544\n",
      "[INFO] Epoch: 50 , batch: 312 , training loss: 3.779647\n",
      "[INFO] Epoch: 50 , batch: 313 , training loss: 3.873573\n",
      "[INFO] Epoch: 50 , batch: 314 , training loss: 3.947528\n",
      "[INFO] Epoch: 50 , batch: 315 , training loss: 4.034249\n",
      "[INFO] Epoch: 50 , batch: 316 , training loss: 4.257177\n",
      "[INFO] Epoch: 50 , batch: 317 , training loss: 4.608255\n",
      "[INFO] Epoch: 50 , batch: 318 , training loss: 4.712776\n",
      "[INFO] Epoch: 50 , batch: 319 , training loss: 4.437353\n",
      "[INFO] Epoch: 50 , batch: 320 , training loss: 3.970353\n",
      "[INFO] Epoch: 50 , batch: 321 , training loss: 3.818143\n",
      "[INFO] Epoch: 50 , batch: 322 , training loss: 3.903742\n",
      "[INFO] Epoch: 50 , batch: 323 , training loss: 3.941659\n",
      "[INFO] Epoch: 50 , batch: 324 , training loss: 3.917245\n",
      "[INFO] Epoch: 50 , batch: 325 , training loss: 4.006591\n",
      "[INFO] Epoch: 50 , batch: 326 , training loss: 4.094046\n",
      "[INFO] Epoch: 50 , batch: 327 , training loss: 4.029366\n",
      "[INFO] Epoch: 50 , batch: 328 , training loss: 4.056965\n",
      "[INFO] Epoch: 50 , batch: 329 , training loss: 3.952507\n",
      "[INFO] Epoch: 50 , batch: 330 , training loss: 3.927263\n",
      "[INFO] Epoch: 50 , batch: 331 , training loss: 4.083499\n",
      "[INFO] Epoch: 50 , batch: 332 , training loss: 3.928655\n",
      "[INFO] Epoch: 50 , batch: 333 , training loss: 3.907489\n",
      "[INFO] Epoch: 50 , batch: 334 , training loss: 3.919213\n",
      "[INFO] Epoch: 50 , batch: 335 , training loss: 4.064581\n",
      "[INFO] Epoch: 50 , batch: 336 , training loss: 4.066340\n",
      "[INFO] Epoch: 50 , batch: 337 , training loss: 4.104204\n",
      "[INFO] Epoch: 50 , batch: 338 , training loss: 4.308350\n",
      "[INFO] Epoch: 50 , batch: 339 , training loss: 4.119629\n",
      "[INFO] Epoch: 50 , batch: 340 , training loss: 4.307626\n",
      "[INFO] Epoch: 50 , batch: 341 , training loss: 4.067077\n",
      "[INFO] Epoch: 50 , batch: 342 , training loss: 3.855223\n",
      "[INFO] Epoch: 50 , batch: 343 , training loss: 3.946532\n",
      "[INFO] Epoch: 50 , batch: 344 , training loss: 3.811138\n",
      "[INFO] Epoch: 50 , batch: 345 , training loss: 3.925714\n",
      "[INFO] Epoch: 50 , batch: 346 , training loss: 3.961011\n",
      "[INFO] Epoch: 50 , batch: 347 , training loss: 3.901618\n",
      "[INFO] Epoch: 50 , batch: 348 , training loss: 3.961121\n",
      "[INFO] Epoch: 50 , batch: 349 , training loss: 4.067461\n",
      "[INFO] Epoch: 50 , batch: 350 , training loss: 3.928986\n",
      "[INFO] Epoch: 50 , batch: 351 , training loss: 4.019139\n",
      "[INFO] Epoch: 50 , batch: 352 , training loss: 4.024716\n",
      "[INFO] Epoch: 50 , batch: 353 , training loss: 4.010606\n",
      "[INFO] Epoch: 50 , batch: 354 , training loss: 4.095770\n",
      "[INFO] Epoch: 50 , batch: 355 , training loss: 4.090144\n",
      "[INFO] Epoch: 50 , batch: 356 , training loss: 3.964077\n",
      "[INFO] Epoch: 50 , batch: 357 , training loss: 4.004775\n",
      "[INFO] Epoch: 50 , batch: 358 , training loss: 3.935928\n",
      "[INFO] Epoch: 50 , batch: 359 , training loss: 3.943612\n",
      "[INFO] Epoch: 50 , batch: 360 , training loss: 4.056593\n",
      "[INFO] Epoch: 50 , batch: 361 , training loss: 4.026257\n",
      "[INFO] Epoch: 50 , batch: 362 , training loss: 4.106911\n",
      "[INFO] Epoch: 50 , batch: 363 , training loss: 3.998591\n",
      "[INFO] Epoch: 50 , batch: 364 , training loss: 4.060434\n",
      "[INFO] Epoch: 50 , batch: 365 , training loss: 3.979226\n",
      "[INFO] Epoch: 50 , batch: 366 , training loss: 4.068039\n",
      "[INFO] Epoch: 50 , batch: 367 , training loss: 4.116053\n",
      "[INFO] Epoch: 50 , batch: 368 , training loss: 4.500695\n",
      "[INFO] Epoch: 50 , batch: 369 , training loss: 4.200004\n",
      "[INFO] Epoch: 50 , batch: 370 , training loss: 3.979756\n",
      "[INFO] Epoch: 50 , batch: 371 , training loss: 4.349119\n",
      "[INFO] Epoch: 50 , batch: 372 , training loss: 4.608623\n",
      "[INFO] Epoch: 50 , batch: 373 , training loss: 4.658212\n",
      "[INFO] Epoch: 50 , batch: 374 , training loss: 4.795766\n",
      "[INFO] Epoch: 50 , batch: 375 , training loss: 4.797503\n",
      "[INFO] Epoch: 50 , batch: 376 , training loss: 4.642274\n",
      "[INFO] Epoch: 50 , batch: 377 , training loss: 4.425960\n",
      "[INFO] Epoch: 50 , batch: 378 , training loss: 4.522800\n",
      "[INFO] Epoch: 50 , batch: 379 , training loss: 4.477690\n",
      "[INFO] Epoch: 50 , batch: 380 , training loss: 4.679427\n",
      "[INFO] Epoch: 50 , batch: 381 , training loss: 4.359855\n",
      "[INFO] Epoch: 50 , batch: 382 , training loss: 4.616841\n",
      "[INFO] Epoch: 50 , batch: 383 , training loss: 4.669738\n",
      "[INFO] Epoch: 50 , batch: 384 , training loss: 4.626501\n",
      "[INFO] Epoch: 50 , batch: 385 , training loss: 4.287408\n",
      "[INFO] Epoch: 50 , batch: 386 , training loss: 4.539487\n",
      "[INFO] Epoch: 50 , batch: 387 , training loss: 4.487891\n",
      "[INFO] Epoch: 50 , batch: 388 , training loss: 4.321880\n",
      "[INFO] Epoch: 50 , batch: 389 , training loss: 4.137918\n",
      "[INFO] Epoch: 50 , batch: 390 , training loss: 4.168902\n",
      "[INFO] Epoch: 50 , batch: 391 , training loss: 4.191417\n",
      "[INFO] Epoch: 50 , batch: 392 , training loss: 4.557213\n",
      "[INFO] Epoch: 50 , batch: 393 , training loss: 4.432434\n",
      "[INFO] Epoch: 50 , batch: 394 , training loss: 4.557193\n",
      "[INFO] Epoch: 50 , batch: 395 , training loss: 4.367609\n",
      "[INFO] Epoch: 50 , batch: 396 , training loss: 4.173589\n",
      "[INFO] Epoch: 50 , batch: 397 , training loss: 4.325427\n",
      "[INFO] Epoch: 50 , batch: 398 , training loss: 4.170631\n",
      "[INFO] Epoch: 50 , batch: 399 , training loss: 4.266661\n",
      "[INFO] Epoch: 50 , batch: 400 , training loss: 4.240932\n",
      "[INFO] Epoch: 50 , batch: 401 , training loss: 4.660049\n",
      "[INFO] Epoch: 50 , batch: 402 , training loss: 4.384870\n",
      "[INFO] Epoch: 50 , batch: 403 , training loss: 4.222366\n",
      "[INFO] Epoch: 50 , batch: 404 , training loss: 4.395743\n",
      "[INFO] Epoch: 50 , batch: 405 , training loss: 4.430458\n",
      "[INFO] Epoch: 50 , batch: 406 , training loss: 4.352409\n",
      "[INFO] Epoch: 50 , batch: 407 , training loss: 4.382105\n",
      "[INFO] Epoch: 50 , batch: 408 , training loss: 4.363643\n",
      "[INFO] Epoch: 50 , batch: 409 , training loss: 4.390338\n",
      "[INFO] Epoch: 50 , batch: 410 , training loss: 4.442645\n",
      "[INFO] Epoch: 50 , batch: 411 , training loss: 4.593134\n",
      "[INFO] Epoch: 50 , batch: 412 , training loss: 4.426297\n",
      "[INFO] Epoch: 50 , batch: 413 , training loss: 4.287544\n",
      "[INFO] Epoch: 50 , batch: 414 , training loss: 4.340394\n",
      "[INFO] Epoch: 50 , batch: 415 , training loss: 4.385283\n",
      "[INFO] Epoch: 50 , batch: 416 , training loss: 4.439819\n",
      "[INFO] Epoch: 50 , batch: 417 , training loss: 4.369069\n",
      "[INFO] Epoch: 50 , batch: 418 , training loss: 4.420180\n",
      "[INFO] Epoch: 50 , batch: 419 , training loss: 4.367537\n",
      "[INFO] Epoch: 50 , batch: 420 , training loss: 4.357831\n",
      "[INFO] Epoch: 50 , batch: 421 , training loss: 4.340951\n",
      "[INFO] Epoch: 50 , batch: 422 , training loss: 4.176514\n",
      "[INFO] Epoch: 50 , batch: 423 , training loss: 4.415304\n",
      "[INFO] Epoch: 50 , batch: 424 , training loss: 4.579652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 50 , batch: 425 , training loss: 4.462111\n",
      "[INFO] Epoch: 50 , batch: 426 , training loss: 4.201645\n",
      "[INFO] Epoch: 50 , batch: 427 , training loss: 4.412850\n",
      "[INFO] Epoch: 50 , batch: 428 , training loss: 4.286721\n",
      "[INFO] Epoch: 50 , batch: 429 , training loss: 4.194027\n",
      "[INFO] Epoch: 50 , batch: 430 , training loss: 4.428517\n",
      "[INFO] Epoch: 50 , batch: 431 , training loss: 4.051391\n",
      "[INFO] Epoch: 50 , batch: 432 , training loss: 4.095698\n",
      "[INFO] Epoch: 50 , batch: 433 , training loss: 4.146473\n",
      "[INFO] Epoch: 50 , batch: 434 , training loss: 4.018380\n",
      "[INFO] Epoch: 50 , batch: 435 , training loss: 4.368337\n",
      "[INFO] Epoch: 50 , batch: 436 , training loss: 4.389323\n",
      "[INFO] Epoch: 50 , batch: 437 , training loss: 4.184348\n",
      "[INFO] Epoch: 50 , batch: 438 , training loss: 4.055107\n",
      "[INFO] Epoch: 50 , batch: 439 , training loss: 4.296894\n",
      "[INFO] Epoch: 50 , batch: 440 , training loss: 4.388348\n",
      "[INFO] Epoch: 50 , batch: 441 , training loss: 4.505739\n",
      "[INFO] Epoch: 50 , batch: 442 , training loss: 4.259990\n",
      "[INFO] Epoch: 50 , batch: 443 , training loss: 4.430782\n",
      "[INFO] Epoch: 50 , batch: 444 , training loss: 4.049350\n",
      "[INFO] Epoch: 50 , batch: 445 , training loss: 3.957941\n",
      "[INFO] Epoch: 50 , batch: 446 , training loss: 3.884068\n",
      "[INFO] Epoch: 50 , batch: 447 , training loss: 4.080944\n",
      "[INFO] Epoch: 50 , batch: 448 , training loss: 4.186729\n",
      "[INFO] Epoch: 50 , batch: 449 , training loss: 4.551239\n",
      "[INFO] Epoch: 50 , batch: 450 , training loss: 4.642872\n",
      "[INFO] Epoch: 50 , batch: 451 , training loss: 4.531402\n",
      "[INFO] Epoch: 50 , batch: 452 , training loss: 4.357213\n",
      "[INFO] Epoch: 50 , batch: 453 , training loss: 4.123374\n",
      "[INFO] Epoch: 50 , batch: 454 , training loss: 4.264361\n",
      "[INFO] Epoch: 50 , batch: 455 , training loss: 4.329118\n",
      "[INFO] Epoch: 50 , batch: 456 , training loss: 4.313308\n",
      "[INFO] Epoch: 50 , batch: 457 , training loss: 4.416756\n",
      "[INFO] Epoch: 50 , batch: 458 , training loss: 4.136408\n",
      "[INFO] Epoch: 50 , batch: 459 , training loss: 4.113476\n",
      "[INFO] Epoch: 50 , batch: 460 , training loss: 4.250252\n",
      "[INFO] Epoch: 50 , batch: 461 , training loss: 4.192046\n",
      "[INFO] Epoch: 50 , batch: 462 , training loss: 4.256282\n",
      "[INFO] Epoch: 50 , batch: 463 , training loss: 4.181512\n",
      "[INFO] Epoch: 50 , batch: 464 , training loss: 4.338207\n",
      "[INFO] Epoch: 50 , batch: 465 , training loss: 4.302588\n",
      "[INFO] Epoch: 50 , batch: 466 , training loss: 4.380559\n",
      "[INFO] Epoch: 50 , batch: 467 , training loss: 4.334462\n",
      "[INFO] Epoch: 50 , batch: 468 , training loss: 4.311105\n",
      "[INFO] Epoch: 50 , batch: 469 , training loss: 4.342894\n",
      "[INFO] Epoch: 50 , batch: 470 , training loss: 4.166089\n",
      "[INFO] Epoch: 50 , batch: 471 , training loss: 4.255302\n",
      "[INFO] Epoch: 50 , batch: 472 , training loss: 4.309145\n",
      "[INFO] Epoch: 50 , batch: 473 , training loss: 4.238764\n",
      "[INFO] Epoch: 50 , batch: 474 , training loss: 4.004356\n",
      "[INFO] Epoch: 50 , batch: 475 , training loss: 3.900495\n",
      "[INFO] Epoch: 50 , batch: 476 , training loss: 4.305075\n",
      "[INFO] Epoch: 50 , batch: 477 , training loss: 4.426869\n",
      "[INFO] Epoch: 50 , batch: 478 , training loss: 4.408308\n",
      "[INFO] Epoch: 50 , batch: 479 , training loss: 4.402483\n",
      "[INFO] Epoch: 50 , batch: 480 , training loss: 4.531922\n",
      "[INFO] Epoch: 50 , batch: 481 , training loss: 4.404436\n",
      "[INFO] Epoch: 50 , batch: 482 , training loss: 4.505549\n",
      "[INFO] Epoch: 50 , batch: 483 , training loss: 4.341254\n",
      "[INFO] Epoch: 50 , batch: 484 , training loss: 4.155788\n",
      "[INFO] Epoch: 50 , batch: 485 , training loss: 4.236537\n",
      "[INFO] Epoch: 50 , batch: 486 , training loss: 4.143870\n",
      "[INFO] Epoch: 50 , batch: 487 , training loss: 4.121907\n",
      "[INFO] Epoch: 50 , batch: 488 , training loss: 4.302822\n",
      "[INFO] Epoch: 50 , batch: 489 , training loss: 4.219215\n",
      "[INFO] Epoch: 50 , batch: 490 , training loss: 4.281532\n",
      "[INFO] Epoch: 50 , batch: 491 , training loss: 4.180203\n",
      "[INFO] Epoch: 50 , batch: 492 , training loss: 4.177016\n",
      "[INFO] Epoch: 50 , batch: 493 , training loss: 4.342522\n",
      "[INFO] Epoch: 50 , batch: 494 , training loss: 4.233467\n",
      "[INFO] Epoch: 50 , batch: 495 , training loss: 4.425466\n",
      "[INFO] Epoch: 50 , batch: 496 , training loss: 4.287813\n",
      "[INFO] Epoch: 50 , batch: 497 , training loss: 4.308041\n",
      "[INFO] Epoch: 50 , batch: 498 , training loss: 4.281664\n",
      "[INFO] Epoch: 50 , batch: 499 , training loss: 4.341595\n",
      "[INFO] Epoch: 50 , batch: 500 , training loss: 4.491431\n",
      "[INFO] Epoch: 50 , batch: 501 , training loss: 4.812289\n",
      "[INFO] Epoch: 50 , batch: 502 , training loss: 4.822613\n",
      "[INFO] Epoch: 50 , batch: 503 , training loss: 4.464163\n",
      "[INFO] Epoch: 50 , batch: 504 , training loss: 4.626946\n",
      "[INFO] Epoch: 50 , batch: 505 , training loss: 4.600718\n",
      "[INFO] Epoch: 50 , batch: 506 , training loss: 4.604924\n",
      "[INFO] Epoch: 50 , batch: 507 , training loss: 4.637291\n",
      "[INFO] Epoch: 50 , batch: 508 , training loss: 4.549647\n",
      "[INFO] Epoch: 50 , batch: 509 , training loss: 4.361331\n",
      "[INFO] Epoch: 50 , batch: 510 , training loss: 4.448641\n",
      "[INFO] Epoch: 50 , batch: 511 , training loss: 4.367795\n",
      "[INFO] Epoch: 50 , batch: 512 , training loss: 4.462701\n",
      "[INFO] Epoch: 50 , batch: 513 , training loss: 4.741535\n",
      "[INFO] Epoch: 50 , batch: 514 , training loss: 4.369154\n",
      "[INFO] Epoch: 50 , batch: 515 , training loss: 4.614572\n",
      "[INFO] Epoch: 50 , batch: 516 , training loss: 4.429214\n",
      "[INFO] Epoch: 50 , batch: 517 , training loss: 4.373115\n",
      "[INFO] Epoch: 50 , batch: 518 , training loss: 4.356966\n",
      "[INFO] Epoch: 50 , batch: 519 , training loss: 4.193161\n",
      "[INFO] Epoch: 50 , batch: 520 , training loss: 4.422536\n",
      "[INFO] Epoch: 50 , batch: 521 , training loss: 4.387263\n",
      "[INFO] Epoch: 50 , batch: 522 , training loss: 4.478336\n",
      "[INFO] Epoch: 50 , batch: 523 , training loss: 4.405693\n",
      "[INFO] Epoch: 50 , batch: 524 , training loss: 4.691598\n",
      "[INFO] Epoch: 50 , batch: 525 , training loss: 4.568989\n",
      "[INFO] Epoch: 50 , batch: 526 , training loss: 4.375486\n",
      "[INFO] Epoch: 50 , batch: 527 , training loss: 4.411127\n",
      "[INFO] Epoch: 50 , batch: 528 , training loss: 4.415934\n",
      "[INFO] Epoch: 50 , batch: 529 , training loss: 4.394866\n",
      "[INFO] Epoch: 50 , batch: 530 , training loss: 4.236274\n",
      "[INFO] Epoch: 50 , batch: 531 , training loss: 4.392825\n",
      "[INFO] Epoch: 50 , batch: 532 , training loss: 4.306118\n",
      "[INFO] Epoch: 50 , batch: 533 , training loss: 4.429713\n",
      "[INFO] Epoch: 50 , batch: 534 , training loss: 4.430871\n",
      "[INFO] Epoch: 50 , batch: 535 , training loss: 4.428276\n",
      "[INFO] Epoch: 50 , batch: 536 , training loss: 4.262714\n",
      "[INFO] Epoch: 50 , batch: 537 , training loss: 4.276000\n",
      "[INFO] Epoch: 50 , batch: 538 , training loss: 4.368062\n",
      "[INFO] Epoch: 50 , batch: 539 , training loss: 4.416420\n",
      "[INFO] Epoch: 50 , batch: 540 , training loss: 4.961866\n",
      "[INFO] Epoch: 50 , batch: 541 , training loss: 4.742195\n",
      "[INFO] Epoch: 50 , batch: 542 , training loss: 4.660549\n",
      "[INFO] Epoch: 51 , batch: 0 , training loss: 3.499954\n",
      "[INFO] Epoch: 51 , batch: 1 , training loss: 3.439120\n",
      "[INFO] Epoch: 51 , batch: 2 , training loss: 3.639847\n",
      "[INFO] Epoch: 51 , batch: 3 , training loss: 3.524956\n",
      "[INFO] Epoch: 51 , batch: 4 , training loss: 3.894202\n",
      "[INFO] Epoch: 51 , batch: 5 , training loss: 3.554219\n",
      "[INFO] Epoch: 51 , batch: 6 , training loss: 3.872944\n",
      "[INFO] Epoch: 51 , batch: 7 , training loss: 3.856975\n",
      "[INFO] Epoch: 51 , batch: 8 , training loss: 3.519109\n",
      "[INFO] Epoch: 51 , batch: 9 , training loss: 3.779893\n",
      "[INFO] Epoch: 51 , batch: 10 , training loss: 3.741242\n",
      "[INFO] Epoch: 51 , batch: 11 , training loss: 3.706238\n",
      "[INFO] Epoch: 51 , batch: 12 , training loss: 3.576036\n",
      "[INFO] Epoch: 51 , batch: 13 , training loss: 3.586166\n",
      "[INFO] Epoch: 51 , batch: 14 , training loss: 3.515070\n",
      "[INFO] Epoch: 51 , batch: 15 , training loss: 3.722735\n",
      "[INFO] Epoch: 51 , batch: 16 , training loss: 3.543030\n",
      "[INFO] Epoch: 51 , batch: 17 , training loss: 3.690199\n",
      "[INFO] Epoch: 51 , batch: 18 , training loss: 3.628801\n",
      "[INFO] Epoch: 51 , batch: 19 , training loss: 3.403442\n",
      "[INFO] Epoch: 51 , batch: 20 , training loss: 3.369242\n",
      "[INFO] Epoch: 51 , batch: 21 , training loss: 3.507806\n",
      "[INFO] Epoch: 51 , batch: 22 , training loss: 3.362124\n",
      "[INFO] Epoch: 51 , batch: 23 , training loss: 3.594667\n",
      "[INFO] Epoch: 51 , batch: 24 , training loss: 3.455594\n",
      "[INFO] Epoch: 51 , batch: 25 , training loss: 3.540882\n",
      "[INFO] Epoch: 51 , batch: 26 , training loss: 3.450212\n",
      "[INFO] Epoch: 51 , batch: 27 , training loss: 3.439205\n",
      "[INFO] Epoch: 51 , batch: 28 , training loss: 3.613351\n",
      "[INFO] Epoch: 51 , batch: 29 , training loss: 3.426090\n",
      "[INFO] Epoch: 51 , batch: 30 , training loss: 3.457808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 51 , batch: 31 , training loss: 3.544180\n",
      "[INFO] Epoch: 51 , batch: 32 , training loss: 3.478261\n",
      "[INFO] Epoch: 51 , batch: 33 , training loss: 3.544538\n",
      "[INFO] Epoch: 51 , batch: 34 , training loss: 3.537792\n",
      "[INFO] Epoch: 51 , batch: 35 , training loss: 3.492089\n",
      "[INFO] Epoch: 51 , batch: 36 , training loss: 3.580146\n",
      "[INFO] Epoch: 51 , batch: 37 , training loss: 3.470461\n",
      "[INFO] Epoch: 51 , batch: 38 , training loss: 3.519591\n",
      "[INFO] Epoch: 51 , batch: 39 , training loss: 3.355613\n",
      "[INFO] Epoch: 51 , batch: 40 , training loss: 3.541048\n",
      "[INFO] Epoch: 51 , batch: 41 , training loss: 3.488879\n",
      "[INFO] Epoch: 51 , batch: 42 , training loss: 3.900094\n",
      "[INFO] Epoch: 51 , batch: 43 , training loss: 3.664885\n",
      "[INFO] Epoch: 51 , batch: 44 , training loss: 4.054389\n",
      "[INFO] Epoch: 51 , batch: 45 , training loss: 3.955236\n",
      "[INFO] Epoch: 51 , batch: 46 , training loss: 3.836242\n",
      "[INFO] Epoch: 51 , batch: 47 , training loss: 3.490731\n",
      "[INFO] Epoch: 51 , batch: 48 , training loss: 3.513989\n",
      "[INFO] Epoch: 51 , batch: 49 , training loss: 3.752182\n",
      "[INFO] Epoch: 51 , batch: 50 , training loss: 3.491918\n",
      "[INFO] Epoch: 51 , batch: 51 , training loss: 3.814304\n",
      "[INFO] Epoch: 51 , batch: 52 , training loss: 3.576132\n",
      "[INFO] Epoch: 51 , batch: 53 , training loss: 3.708639\n",
      "[INFO] Epoch: 51 , batch: 54 , training loss: 3.702118\n",
      "[INFO] Epoch: 51 , batch: 55 , training loss: 3.780933\n",
      "[INFO] Epoch: 51 , batch: 56 , training loss: 3.601923\n",
      "[INFO] Epoch: 51 , batch: 57 , training loss: 3.568729\n",
      "[INFO] Epoch: 51 , batch: 58 , training loss: 3.616421\n",
      "[INFO] Epoch: 51 , batch: 59 , training loss: 3.702353\n",
      "[INFO] Epoch: 51 , batch: 60 , training loss: 3.624789\n",
      "[INFO] Epoch: 51 , batch: 61 , training loss: 3.703439\n",
      "[INFO] Epoch: 51 , batch: 62 , training loss: 3.585009\n",
      "[INFO] Epoch: 51 , batch: 63 , training loss: 3.766401\n",
      "[INFO] Epoch: 51 , batch: 64 , training loss: 4.009796\n",
      "[INFO] Epoch: 51 , batch: 65 , training loss: 3.713087\n",
      "[INFO] Epoch: 51 , batch: 66 , training loss: 3.557257\n",
      "[INFO] Epoch: 51 , batch: 67 , training loss: 3.575739\n",
      "[INFO] Epoch: 51 , batch: 68 , training loss: 3.714484\n",
      "[INFO] Epoch: 51 , batch: 69 , training loss: 3.682397\n",
      "[INFO] Epoch: 51 , batch: 70 , training loss: 3.909523\n",
      "[INFO] Epoch: 51 , batch: 71 , training loss: 3.739458\n",
      "[INFO] Epoch: 51 , batch: 72 , training loss: 3.795832\n",
      "[INFO] Epoch: 51 , batch: 73 , training loss: 3.752648\n",
      "[INFO] Epoch: 51 , batch: 74 , training loss: 3.839022\n",
      "[INFO] Epoch: 51 , batch: 75 , training loss: 3.725739\n",
      "[INFO] Epoch: 51 , batch: 76 , training loss: 3.806613\n",
      "[INFO] Epoch: 51 , batch: 77 , training loss: 3.769792\n",
      "[INFO] Epoch: 51 , batch: 78 , training loss: 3.869826\n",
      "[INFO] Epoch: 51 , batch: 79 , training loss: 3.727285\n",
      "[INFO] Epoch: 51 , batch: 80 , training loss: 3.887281\n",
      "[INFO] Epoch: 51 , batch: 81 , training loss: 3.813600\n",
      "[INFO] Epoch: 51 , batch: 82 , training loss: 3.829259\n",
      "[INFO] Epoch: 51 , batch: 83 , training loss: 3.871776\n",
      "[INFO] Epoch: 51 , batch: 84 , training loss: 3.885967\n",
      "[INFO] Epoch: 51 , batch: 85 , training loss: 3.936458\n",
      "[INFO] Epoch: 51 , batch: 86 , training loss: 3.859005\n",
      "[INFO] Epoch: 51 , batch: 87 , training loss: 3.823742\n",
      "[INFO] Epoch: 51 , batch: 88 , training loss: 3.962512\n",
      "[INFO] Epoch: 51 , batch: 89 , training loss: 3.773745\n",
      "[INFO] Epoch: 51 , batch: 90 , training loss: 3.849949\n",
      "[INFO] Epoch: 51 , batch: 91 , training loss: 3.798763\n",
      "[INFO] Epoch: 51 , batch: 92 , training loss: 3.796753\n",
      "[INFO] Epoch: 51 , batch: 93 , training loss: 3.905033\n",
      "[INFO] Epoch: 51 , batch: 94 , training loss: 4.033421\n",
      "[INFO] Epoch: 51 , batch: 95 , training loss: 3.807957\n",
      "[INFO] Epoch: 51 , batch: 96 , training loss: 3.822349\n",
      "[INFO] Epoch: 51 , batch: 97 , training loss: 3.749904\n",
      "[INFO] Epoch: 51 , batch: 98 , training loss: 3.687323\n",
      "[INFO] Epoch: 51 , batch: 99 , training loss: 3.815702\n",
      "[INFO] Epoch: 51 , batch: 100 , training loss: 3.717528\n",
      "[INFO] Epoch: 51 , batch: 101 , training loss: 3.715558\n",
      "[INFO] Epoch: 51 , batch: 102 , training loss: 3.889367\n",
      "[INFO] Epoch: 51 , batch: 103 , training loss: 3.690395\n",
      "[INFO] Epoch: 51 , batch: 104 , training loss: 3.635182\n",
      "[INFO] Epoch: 51 , batch: 105 , training loss: 3.865643\n",
      "[INFO] Epoch: 51 , batch: 106 , training loss: 3.918095\n",
      "[INFO] Epoch: 51 , batch: 107 , training loss: 3.753857\n",
      "[INFO] Epoch: 51 , batch: 108 , training loss: 3.683645\n",
      "[INFO] Epoch: 51 , batch: 109 , training loss: 3.610018\n",
      "[INFO] Epoch: 51 , batch: 110 , training loss: 3.779458\n",
      "[INFO] Epoch: 51 , batch: 111 , training loss: 3.853588\n",
      "[INFO] Epoch: 51 , batch: 112 , training loss: 3.768101\n",
      "[INFO] Epoch: 51 , batch: 113 , training loss: 3.784534\n",
      "[INFO] Epoch: 51 , batch: 114 , training loss: 3.750772\n",
      "[INFO] Epoch: 51 , batch: 115 , training loss: 3.793629\n",
      "[INFO] Epoch: 51 , batch: 116 , training loss: 3.659254\n",
      "[INFO] Epoch: 51 , batch: 117 , training loss: 3.889957\n",
      "[INFO] Epoch: 51 , batch: 118 , training loss: 3.863986\n",
      "[INFO] Epoch: 51 , batch: 119 , training loss: 4.016357\n",
      "[INFO] Epoch: 51 , batch: 120 , training loss: 3.979706\n",
      "[INFO] Epoch: 51 , batch: 121 , training loss: 3.859648\n",
      "[INFO] Epoch: 51 , batch: 122 , training loss: 3.768723\n",
      "[INFO] Epoch: 51 , batch: 123 , training loss: 3.738158\n",
      "[INFO] Epoch: 51 , batch: 124 , training loss: 3.802498\n",
      "[INFO] Epoch: 51 , batch: 125 , training loss: 3.702879\n",
      "[INFO] Epoch: 51 , batch: 126 , training loss: 3.717396\n",
      "[INFO] Epoch: 51 , batch: 127 , training loss: 3.692899\n",
      "[INFO] Epoch: 51 , batch: 128 , training loss: 3.828840\n",
      "[INFO] Epoch: 51 , batch: 129 , training loss: 3.774201\n",
      "[INFO] Epoch: 51 , batch: 130 , training loss: 3.803691\n",
      "[INFO] Epoch: 51 , batch: 131 , training loss: 3.801737\n",
      "[INFO] Epoch: 51 , batch: 132 , training loss: 3.794132\n",
      "[INFO] Epoch: 51 , batch: 133 , training loss: 3.778668\n",
      "[INFO] Epoch: 51 , batch: 134 , training loss: 3.567916\n",
      "[INFO] Epoch: 51 , batch: 135 , training loss: 3.618431\n",
      "[INFO] Epoch: 51 , batch: 136 , training loss: 3.883419\n",
      "[INFO] Epoch: 51 , batch: 137 , training loss: 3.832812\n",
      "[INFO] Epoch: 51 , batch: 138 , training loss: 3.862815\n",
      "[INFO] Epoch: 51 , batch: 139 , training loss: 4.384497\n",
      "[INFO] Epoch: 51 , batch: 140 , training loss: 4.156424\n",
      "[INFO] Epoch: 51 , batch: 141 , training loss: 3.949374\n",
      "[INFO] Epoch: 51 , batch: 142 , training loss: 3.698272\n",
      "[INFO] Epoch: 51 , batch: 143 , training loss: 3.860393\n",
      "[INFO] Epoch: 51 , batch: 144 , training loss: 3.687566\n",
      "[INFO] Epoch: 51 , batch: 145 , training loss: 3.742945\n",
      "[INFO] Epoch: 51 , batch: 146 , training loss: 3.937089\n",
      "[INFO] Epoch: 51 , batch: 147 , training loss: 3.637517\n",
      "[INFO] Epoch: 51 , batch: 148 , training loss: 3.571307\n",
      "[INFO] Epoch: 51 , batch: 149 , training loss: 3.660379\n",
      "[INFO] Epoch: 51 , batch: 150 , training loss: 3.937002\n",
      "[INFO] Epoch: 51 , batch: 151 , training loss: 3.797490\n",
      "[INFO] Epoch: 51 , batch: 152 , training loss: 3.833045\n",
      "[INFO] Epoch: 51 , batch: 153 , training loss: 3.842195\n",
      "[INFO] Epoch: 51 , batch: 154 , training loss: 3.918732\n",
      "[INFO] Epoch: 51 , batch: 155 , training loss: 4.107813\n",
      "[INFO] Epoch: 51 , batch: 156 , training loss: 3.877177\n",
      "[INFO] Epoch: 51 , batch: 157 , training loss: 3.822546\n",
      "[INFO] Epoch: 51 , batch: 158 , training loss: 3.907949\n",
      "[INFO] Epoch: 51 , batch: 159 , training loss: 3.844495\n",
      "[INFO] Epoch: 51 , batch: 160 , training loss: 4.059889\n",
      "[INFO] Epoch: 51 , batch: 161 , training loss: 4.141244\n",
      "[INFO] Epoch: 51 , batch: 162 , training loss: 4.135810\n",
      "[INFO] Epoch: 51 , batch: 163 , training loss: 4.299076\n",
      "[INFO] Epoch: 51 , batch: 164 , training loss: 4.241391\n",
      "[INFO] Epoch: 51 , batch: 165 , training loss: 4.185738\n",
      "[INFO] Epoch: 51 , batch: 166 , training loss: 4.046640\n",
      "[INFO] Epoch: 51 , batch: 167 , training loss: 4.038503\n",
      "[INFO] Epoch: 51 , batch: 168 , training loss: 3.697334\n",
      "[INFO] Epoch: 51 , batch: 169 , training loss: 3.714031\n",
      "[INFO] Epoch: 51 , batch: 170 , training loss: 3.891413\n",
      "[INFO] Epoch: 51 , batch: 171 , training loss: 3.336732\n",
      "[INFO] Epoch: 51 , batch: 172 , training loss: 3.605941\n",
      "[INFO] Epoch: 51 , batch: 173 , training loss: 3.906957\n",
      "[INFO] Epoch: 51 , batch: 174 , training loss: 4.394568\n",
      "[INFO] Epoch: 51 , batch: 175 , training loss: 4.705327\n",
      "[INFO] Epoch: 51 , batch: 176 , training loss: 4.302574\n",
      "[INFO] Epoch: 51 , batch: 177 , training loss: 3.923456\n",
      "[INFO] Epoch: 51 , batch: 178 , training loss: 3.956722\n",
      "[INFO] Epoch: 51 , batch: 179 , training loss: 4.010015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 51 , batch: 180 , training loss: 3.976142\n",
      "[INFO] Epoch: 51 , batch: 181 , training loss: 4.268240\n",
      "[INFO] Epoch: 51 , batch: 182 , training loss: 4.209341\n",
      "[INFO] Epoch: 51 , batch: 183 , training loss: 4.214491\n",
      "[INFO] Epoch: 51 , batch: 184 , training loss: 4.096470\n",
      "[INFO] Epoch: 51 , batch: 185 , training loss: 4.060768\n",
      "[INFO] Epoch: 51 , batch: 186 , training loss: 4.192819\n",
      "[INFO] Epoch: 51 , batch: 187 , training loss: 4.291013\n",
      "[INFO] Epoch: 51 , batch: 188 , training loss: 4.282972\n",
      "[INFO] Epoch: 51 , batch: 189 , training loss: 4.186077\n",
      "[INFO] Epoch: 51 , batch: 190 , training loss: 4.243710\n",
      "[INFO] Epoch: 51 , batch: 191 , training loss: 4.345315\n",
      "[INFO] Epoch: 51 , batch: 192 , training loss: 4.191671\n",
      "[INFO] Epoch: 51 , batch: 193 , training loss: 4.286297\n",
      "[INFO] Epoch: 51 , batch: 194 , training loss: 4.212187\n",
      "[INFO] Epoch: 51 , batch: 195 , training loss: 4.135977\n",
      "[INFO] Epoch: 51 , batch: 196 , training loss: 4.002203\n",
      "[INFO] Epoch: 51 , batch: 197 , training loss: 4.085144\n",
      "[INFO] Epoch: 51 , batch: 198 , training loss: 4.006031\n",
      "[INFO] Epoch: 51 , batch: 199 , training loss: 4.157331\n",
      "[INFO] Epoch: 51 , batch: 200 , training loss: 4.067630\n",
      "[INFO] Epoch: 51 , batch: 201 , training loss: 3.955770\n",
      "[INFO] Epoch: 51 , batch: 202 , training loss: 3.965632\n",
      "[INFO] Epoch: 51 , batch: 203 , training loss: 4.115785\n",
      "[INFO] Epoch: 51 , batch: 204 , training loss: 4.184317\n",
      "[INFO] Epoch: 51 , batch: 205 , training loss: 3.796616\n",
      "[INFO] Epoch: 51 , batch: 206 , training loss: 3.735785\n",
      "[INFO] Epoch: 51 , batch: 207 , training loss: 3.715019\n",
      "[INFO] Epoch: 51 , batch: 208 , training loss: 4.018488\n",
      "[INFO] Epoch: 51 , batch: 209 , training loss: 4.012303\n",
      "[INFO] Epoch: 51 , batch: 210 , training loss: 3.999409\n",
      "[INFO] Epoch: 51 , batch: 211 , training loss: 4.017187\n",
      "[INFO] Epoch: 51 , batch: 212 , training loss: 4.106843\n",
      "[INFO] Epoch: 51 , batch: 213 , training loss: 4.067739\n",
      "[INFO] Epoch: 51 , batch: 214 , training loss: 4.161339\n",
      "[INFO] Epoch: 51 , batch: 215 , training loss: 4.335964\n",
      "[INFO] Epoch: 51 , batch: 216 , training loss: 4.038101\n",
      "[INFO] Epoch: 51 , batch: 217 , training loss: 4.008378\n",
      "[INFO] Epoch: 51 , batch: 218 , training loss: 3.996276\n",
      "[INFO] Epoch: 51 , batch: 219 , training loss: 4.106604\n",
      "[INFO] Epoch: 51 , batch: 220 , training loss: 3.926352\n",
      "[INFO] Epoch: 51 , batch: 221 , training loss: 3.936919\n",
      "[INFO] Epoch: 51 , batch: 222 , training loss: 4.056813\n",
      "[INFO] Epoch: 51 , batch: 223 , training loss: 4.204356\n",
      "[INFO] Epoch: 51 , batch: 224 , training loss: 4.237166\n",
      "[INFO] Epoch: 51 , batch: 225 , training loss: 4.124614\n",
      "[INFO] Epoch: 51 , batch: 226 , training loss: 4.222143\n",
      "[INFO] Epoch: 51 , batch: 227 , training loss: 4.235804\n",
      "[INFO] Epoch: 51 , batch: 228 , training loss: 4.229638\n",
      "[INFO] Epoch: 51 , batch: 229 , training loss: 4.097308\n",
      "[INFO] Epoch: 51 , batch: 230 , training loss: 3.961841\n",
      "[INFO] Epoch: 51 , batch: 231 , training loss: 3.828298\n",
      "[INFO] Epoch: 51 , batch: 232 , training loss: 3.959726\n",
      "[INFO] Epoch: 51 , batch: 233 , training loss: 3.994821\n",
      "[INFO] Epoch: 51 , batch: 234 , training loss: 3.683113\n",
      "[INFO] Epoch: 51 , batch: 235 , training loss: 3.801461\n",
      "[INFO] Epoch: 51 , batch: 236 , training loss: 3.908996\n",
      "[INFO] Epoch: 51 , batch: 237 , training loss: 4.098645\n",
      "[INFO] Epoch: 51 , batch: 238 , training loss: 3.907706\n",
      "[INFO] Epoch: 51 , batch: 239 , training loss: 3.933434\n",
      "[INFO] Epoch: 51 , batch: 240 , training loss: 3.971535\n",
      "[INFO] Epoch: 51 , batch: 241 , training loss: 3.790110\n",
      "[INFO] Epoch: 51 , batch: 242 , training loss: 3.815790\n",
      "[INFO] Epoch: 51 , batch: 243 , training loss: 4.090823\n",
      "[INFO] Epoch: 51 , batch: 244 , training loss: 4.049096\n",
      "[INFO] Epoch: 51 , batch: 245 , training loss: 4.005964\n",
      "[INFO] Epoch: 51 , batch: 246 , training loss: 3.719626\n",
      "[INFO] Epoch: 51 , batch: 247 , training loss: 3.885162\n",
      "[INFO] Epoch: 51 , batch: 248 , training loss: 3.964630\n",
      "[INFO] Epoch: 51 , batch: 249 , training loss: 3.929558\n",
      "[INFO] Epoch: 51 , batch: 250 , training loss: 3.751237\n",
      "[INFO] Epoch: 51 , batch: 251 , training loss: 4.159307\n",
      "[INFO] Epoch: 51 , batch: 252 , training loss: 3.895339\n",
      "[INFO] Epoch: 51 , batch: 253 , training loss: 3.797649\n",
      "[INFO] Epoch: 51 , batch: 254 , training loss: 4.075869\n",
      "[INFO] Epoch: 51 , batch: 255 , training loss: 4.038681\n",
      "[INFO] Epoch: 51 , batch: 256 , training loss: 4.026815\n",
      "[INFO] Epoch: 51 , batch: 257 , training loss: 4.211135\n",
      "[INFO] Epoch: 51 , batch: 258 , training loss: 4.198622\n",
      "[INFO] Epoch: 51 , batch: 259 , training loss: 4.253129\n",
      "[INFO] Epoch: 51 , batch: 260 , training loss: 4.011636\n",
      "[INFO] Epoch: 51 , batch: 261 , training loss: 4.203970\n",
      "[INFO] Epoch: 51 , batch: 262 , training loss: 4.323781\n",
      "[INFO] Epoch: 51 , batch: 263 , training loss: 4.504787\n",
      "[INFO] Epoch: 51 , batch: 264 , training loss: 3.865706\n",
      "[INFO] Epoch: 51 , batch: 265 , training loss: 3.970371\n",
      "[INFO] Epoch: 51 , batch: 266 , training loss: 4.356972\n",
      "[INFO] Epoch: 51 , batch: 267 , training loss: 4.134752\n",
      "[INFO] Epoch: 51 , batch: 268 , training loss: 4.057895\n",
      "[INFO] Epoch: 51 , batch: 269 , training loss: 4.028454\n",
      "[INFO] Epoch: 51 , batch: 270 , training loss: 4.072327\n",
      "[INFO] Epoch: 51 , batch: 271 , training loss: 4.111439\n",
      "[INFO] Epoch: 51 , batch: 272 , training loss: 4.079221\n",
      "[INFO] Epoch: 51 , batch: 273 , training loss: 4.106625\n",
      "[INFO] Epoch: 51 , batch: 274 , training loss: 4.191690\n",
      "[INFO] Epoch: 51 , batch: 275 , training loss: 4.046524\n",
      "[INFO] Epoch: 51 , batch: 276 , training loss: 4.127796\n",
      "[INFO] Epoch: 51 , batch: 277 , training loss: 4.284586\n",
      "[INFO] Epoch: 51 , batch: 278 , training loss: 3.978832\n",
      "[INFO] Epoch: 51 , batch: 279 , training loss: 3.980403\n",
      "[INFO] Epoch: 51 , batch: 280 , training loss: 3.957147\n",
      "[INFO] Epoch: 51 , batch: 281 , training loss: 4.085757\n",
      "[INFO] Epoch: 51 , batch: 282 , training loss: 3.973201\n",
      "[INFO] Epoch: 51 , batch: 283 , training loss: 3.994276\n",
      "[INFO] Epoch: 51 , batch: 284 , training loss: 4.030432\n",
      "[INFO] Epoch: 51 , batch: 285 , training loss: 3.965696\n",
      "[INFO] Epoch: 51 , batch: 286 , training loss: 3.973526\n",
      "[INFO] Epoch: 51 , batch: 287 , training loss: 3.913491\n",
      "[INFO] Epoch: 51 , batch: 288 , training loss: 3.866896\n",
      "[INFO] Epoch: 51 , batch: 289 , training loss: 3.969589\n",
      "[INFO] Epoch: 51 , batch: 290 , training loss: 3.743142\n",
      "[INFO] Epoch: 51 , batch: 291 , training loss: 3.748511\n",
      "[INFO] Epoch: 51 , batch: 292 , training loss: 3.834524\n",
      "[INFO] Epoch: 51 , batch: 293 , training loss: 3.763787\n",
      "[INFO] Epoch: 51 , batch: 294 , training loss: 4.409103\n",
      "[INFO] Epoch: 51 , batch: 295 , training loss: 4.218141\n",
      "[INFO] Epoch: 51 , batch: 296 , training loss: 4.114826\n",
      "[INFO] Epoch: 51 , batch: 297 , training loss: 4.082521\n",
      "[INFO] Epoch: 51 , batch: 298 , training loss: 3.915534\n",
      "[INFO] Epoch: 51 , batch: 299 , training loss: 3.974865\n",
      "[INFO] Epoch: 51 , batch: 300 , training loss: 3.953661\n",
      "[INFO] Epoch: 51 , batch: 301 , training loss: 3.862324\n",
      "[INFO] Epoch: 51 , batch: 302 , training loss: 4.037330\n",
      "[INFO] Epoch: 51 , batch: 303 , training loss: 4.054857\n",
      "[INFO] Epoch: 51 , batch: 304 , training loss: 4.166575\n",
      "[INFO] Epoch: 51 , batch: 305 , training loss: 4.023255\n",
      "[INFO] Epoch: 51 , batch: 306 , training loss: 4.144063\n",
      "[INFO] Epoch: 51 , batch: 307 , training loss: 4.163174\n",
      "[INFO] Epoch: 51 , batch: 308 , training loss: 3.958591\n",
      "[INFO] Epoch: 51 , batch: 309 , training loss: 3.932636\n",
      "[INFO] Epoch: 51 , batch: 310 , training loss: 3.892615\n",
      "[INFO] Epoch: 51 , batch: 311 , training loss: 3.885185\n",
      "[INFO] Epoch: 51 , batch: 312 , training loss: 3.781811\n",
      "[INFO] Epoch: 51 , batch: 313 , training loss: 3.883460\n",
      "[INFO] Epoch: 51 , batch: 314 , training loss: 3.961757\n",
      "[INFO] Epoch: 51 , batch: 315 , training loss: 4.038630\n",
      "[INFO] Epoch: 51 , batch: 316 , training loss: 4.262979\n",
      "[INFO] Epoch: 51 , batch: 317 , training loss: 4.620049\n",
      "[INFO] Epoch: 51 , batch: 318 , training loss: 4.733222\n",
      "[INFO] Epoch: 51 , batch: 319 , training loss: 4.441348\n",
      "[INFO] Epoch: 51 , batch: 320 , training loss: 3.977455\n",
      "[INFO] Epoch: 51 , batch: 321 , training loss: 3.828740\n",
      "[INFO] Epoch: 51 , batch: 322 , training loss: 3.922096\n",
      "[INFO] Epoch: 51 , batch: 323 , training loss: 3.951013\n",
      "[INFO] Epoch: 51 , batch: 324 , training loss: 3.923761\n",
      "[INFO] Epoch: 51 , batch: 325 , training loss: 4.034608\n",
      "[INFO] Epoch: 51 , batch: 326 , training loss: 4.104293\n",
      "[INFO] Epoch: 51 , batch: 327 , training loss: 4.047590\n",
      "[INFO] Epoch: 51 , batch: 328 , training loss: 4.059216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 51 , batch: 329 , training loss: 3.968366\n",
      "[INFO] Epoch: 51 , batch: 330 , training loss: 3.939046\n",
      "[INFO] Epoch: 51 , batch: 331 , training loss: 4.091322\n",
      "[INFO] Epoch: 51 , batch: 332 , training loss: 3.951290\n",
      "[INFO] Epoch: 51 , batch: 333 , training loss: 3.912993\n",
      "[INFO] Epoch: 51 , batch: 334 , training loss: 3.924311\n",
      "[INFO] Epoch: 51 , batch: 335 , training loss: 4.054318\n",
      "[INFO] Epoch: 51 , batch: 336 , training loss: 4.062251\n",
      "[INFO] Epoch: 51 , batch: 337 , training loss: 4.094329\n",
      "[INFO] Epoch: 51 , batch: 338 , training loss: 4.298530\n",
      "[INFO] Epoch: 51 , batch: 339 , training loss: 4.130732\n",
      "[INFO] Epoch: 51 , batch: 340 , training loss: 4.326462\n",
      "[INFO] Epoch: 51 , batch: 341 , training loss: 4.090541\n",
      "[INFO] Epoch: 51 , batch: 342 , training loss: 3.878514\n",
      "[INFO] Epoch: 51 , batch: 343 , training loss: 3.955138\n",
      "[INFO] Epoch: 51 , batch: 344 , training loss: 3.809918\n",
      "[INFO] Epoch: 51 , batch: 345 , training loss: 3.932371\n",
      "[INFO] Epoch: 51 , batch: 346 , training loss: 3.991843\n",
      "[INFO] Epoch: 51 , batch: 347 , training loss: 3.897775\n",
      "[INFO] Epoch: 51 , batch: 348 , training loss: 3.981664\n",
      "[INFO] Epoch: 51 , batch: 349 , training loss: 4.067755\n",
      "[INFO] Epoch: 51 , batch: 350 , training loss: 3.939940\n",
      "[INFO] Epoch: 51 , batch: 351 , training loss: 4.031526\n",
      "[INFO] Epoch: 51 , batch: 352 , training loss: 4.023891\n",
      "[INFO] Epoch: 51 , batch: 353 , training loss: 4.028035\n",
      "[INFO] Epoch: 51 , batch: 354 , training loss: 4.114241\n",
      "[INFO] Epoch: 51 , batch: 355 , training loss: 4.094778\n",
      "[INFO] Epoch: 51 , batch: 356 , training loss: 3.962248\n",
      "[INFO] Epoch: 51 , batch: 357 , training loss: 4.033828\n",
      "[INFO] Epoch: 51 , batch: 358 , training loss: 3.944398\n",
      "[INFO] Epoch: 51 , batch: 359 , training loss: 3.946694\n",
      "[INFO] Epoch: 51 , batch: 360 , training loss: 4.054495\n",
      "[INFO] Epoch: 51 , batch: 361 , training loss: 4.024329\n",
      "[INFO] Epoch: 51 , batch: 362 , training loss: 4.126459\n",
      "[INFO] Epoch: 51 , batch: 363 , training loss: 3.998214\n",
      "[INFO] Epoch: 51 , batch: 364 , training loss: 4.055255\n",
      "[INFO] Epoch: 51 , batch: 365 , training loss: 3.997827\n",
      "[INFO] Epoch: 51 , batch: 366 , training loss: 4.071568\n",
      "[INFO] Epoch: 51 , batch: 367 , training loss: 4.132468\n",
      "[INFO] Epoch: 51 , batch: 368 , training loss: 4.519852\n",
      "[INFO] Epoch: 51 , batch: 369 , training loss: 4.228869\n",
      "[INFO] Epoch: 51 , batch: 370 , training loss: 3.990734\n",
      "[INFO] Epoch: 51 , batch: 371 , training loss: 4.365623\n",
      "[INFO] Epoch: 51 , batch: 372 , training loss: 4.625223\n",
      "[INFO] Epoch: 51 , batch: 373 , training loss: 4.668596\n",
      "[INFO] Epoch: 51 , batch: 374 , training loss: 4.786702\n",
      "[INFO] Epoch: 51 , batch: 375 , training loss: 4.760072\n",
      "[INFO] Epoch: 51 , batch: 376 , training loss: 4.634840\n",
      "[INFO] Epoch: 51 , batch: 377 , training loss: 4.437251\n",
      "[INFO] Epoch: 51 , batch: 378 , training loss: 4.537443\n",
      "[INFO] Epoch: 51 , batch: 379 , training loss: 4.489600\n",
      "[INFO] Epoch: 51 , batch: 380 , training loss: 4.687811\n",
      "[INFO] Epoch: 51 , batch: 381 , training loss: 4.354462\n",
      "[INFO] Epoch: 51 , batch: 382 , training loss: 4.596928\n",
      "[INFO] Epoch: 51 , batch: 383 , training loss: 4.681225\n",
      "[INFO] Epoch: 51 , batch: 384 , training loss: 4.606302\n",
      "[INFO] Epoch: 51 , batch: 385 , training loss: 4.284450\n",
      "[INFO] Epoch: 51 , batch: 386 , training loss: 4.542068\n",
      "[INFO] Epoch: 51 , batch: 387 , training loss: 4.507909\n",
      "[INFO] Epoch: 51 , batch: 388 , training loss: 4.344470\n",
      "[INFO] Epoch: 51 , batch: 389 , training loss: 4.150934\n",
      "[INFO] Epoch: 51 , batch: 390 , training loss: 4.183458\n",
      "[INFO] Epoch: 51 , batch: 391 , training loss: 4.210482\n",
      "[INFO] Epoch: 51 , batch: 392 , training loss: 4.566407\n",
      "[INFO] Epoch: 51 , batch: 393 , training loss: 4.447208\n",
      "[INFO] Epoch: 51 , batch: 394 , training loss: 4.574939\n",
      "[INFO] Epoch: 51 , batch: 395 , training loss: 4.374972\n",
      "[INFO] Epoch: 51 , batch: 396 , training loss: 4.183112\n",
      "[INFO] Epoch: 51 , batch: 397 , training loss: 4.325158\n",
      "[INFO] Epoch: 51 , batch: 398 , training loss: 4.195431\n",
      "[INFO] Epoch: 51 , batch: 399 , training loss: 4.288434\n",
      "[INFO] Epoch: 51 , batch: 400 , training loss: 4.259630\n",
      "[INFO] Epoch: 51 , batch: 401 , training loss: 4.667183\n",
      "[INFO] Epoch: 51 , batch: 402 , training loss: 4.393265\n",
      "[INFO] Epoch: 51 , batch: 403 , training loss: 4.237750\n",
      "[INFO] Epoch: 51 , batch: 404 , training loss: 4.411790\n",
      "[INFO] Epoch: 51 , batch: 405 , training loss: 4.463257\n",
      "[INFO] Epoch: 51 , batch: 406 , training loss: 4.357409\n",
      "[INFO] Epoch: 51 , batch: 407 , training loss: 4.370641\n",
      "[INFO] Epoch: 51 , batch: 408 , training loss: 4.357378\n",
      "[INFO] Epoch: 51 , batch: 409 , training loss: 4.392653\n",
      "[INFO] Epoch: 51 , batch: 410 , training loss: 4.445752\n",
      "[INFO] Epoch: 51 , batch: 411 , training loss: 4.597490\n",
      "[INFO] Epoch: 51 , batch: 412 , training loss: 4.424033\n",
      "[INFO] Epoch: 51 , batch: 413 , training loss: 4.290197\n",
      "[INFO] Epoch: 51 , batch: 414 , training loss: 4.340015\n",
      "[INFO] Epoch: 51 , batch: 415 , training loss: 4.408441\n",
      "[INFO] Epoch: 51 , batch: 416 , training loss: 4.439115\n",
      "[INFO] Epoch: 51 , batch: 417 , training loss: 4.363008\n",
      "[INFO] Epoch: 51 , batch: 418 , training loss: 4.429447\n",
      "[INFO] Epoch: 51 , batch: 419 , training loss: 4.389229\n",
      "[INFO] Epoch: 51 , batch: 420 , training loss: 4.358381\n",
      "[INFO] Epoch: 51 , batch: 421 , training loss: 4.342089\n",
      "[INFO] Epoch: 51 , batch: 422 , training loss: 4.191058\n",
      "[INFO] Epoch: 51 , batch: 423 , training loss: 4.421008\n",
      "[INFO] Epoch: 51 , batch: 424 , training loss: 4.595993\n",
      "[INFO] Epoch: 51 , batch: 425 , training loss: 4.450615\n",
      "[INFO] Epoch: 51 , batch: 426 , training loss: 4.191989\n",
      "[INFO] Epoch: 51 , batch: 427 , training loss: 4.419480\n",
      "[INFO] Epoch: 51 , batch: 428 , training loss: 4.285289\n",
      "[INFO] Epoch: 51 , batch: 429 , training loss: 4.196608\n",
      "[INFO] Epoch: 51 , batch: 430 , training loss: 4.429436\n",
      "[INFO] Epoch: 51 , batch: 431 , training loss: 4.066279\n",
      "[INFO] Epoch: 51 , batch: 432 , training loss: 4.098374\n",
      "[INFO] Epoch: 51 , batch: 433 , training loss: 4.155443\n",
      "[INFO] Epoch: 51 , batch: 434 , training loss: 4.016634\n",
      "[INFO] Epoch: 51 , batch: 435 , training loss: 4.364203\n",
      "[INFO] Epoch: 51 , batch: 436 , training loss: 4.400728\n",
      "[INFO] Epoch: 51 , batch: 437 , training loss: 4.196491\n",
      "[INFO] Epoch: 51 , batch: 438 , training loss: 4.062074\n",
      "[INFO] Epoch: 51 , batch: 439 , training loss: 4.306013\n",
      "[INFO] Epoch: 51 , batch: 440 , training loss: 4.403862\n",
      "[INFO] Epoch: 51 , batch: 441 , training loss: 4.508473\n",
      "[INFO] Epoch: 51 , batch: 442 , training loss: 4.273096\n",
      "[INFO] Epoch: 51 , batch: 443 , training loss: 4.429302\n",
      "[INFO] Epoch: 51 , batch: 444 , training loss: 4.071341\n",
      "[INFO] Epoch: 51 , batch: 445 , training loss: 3.964612\n",
      "[INFO] Epoch: 51 , batch: 446 , training loss: 3.890928\n",
      "[INFO] Epoch: 51 , batch: 447 , training loss: 4.085188\n",
      "[INFO] Epoch: 51 , batch: 448 , training loss: 4.196033\n",
      "[INFO] Epoch: 51 , batch: 449 , training loss: 4.574874\n",
      "[INFO] Epoch: 51 , batch: 450 , training loss: 4.657837\n",
      "[INFO] Epoch: 51 , batch: 451 , training loss: 4.549352\n",
      "[INFO] Epoch: 51 , batch: 452 , training loss: 4.371723\n",
      "[INFO] Epoch: 51 , batch: 453 , training loss: 4.118194\n",
      "[INFO] Epoch: 51 , batch: 454 , training loss: 4.287015\n",
      "[INFO] Epoch: 51 , batch: 455 , training loss: 4.334107\n",
      "[INFO] Epoch: 51 , batch: 456 , training loss: 4.323083\n",
      "[INFO] Epoch: 51 , batch: 457 , training loss: 4.413353\n",
      "[INFO] Epoch: 51 , batch: 458 , training loss: 4.147612\n",
      "[INFO] Epoch: 51 , batch: 459 , training loss: 4.135993\n",
      "[INFO] Epoch: 51 , batch: 460 , training loss: 4.251442\n",
      "[INFO] Epoch: 51 , batch: 461 , training loss: 4.205338\n",
      "[INFO] Epoch: 51 , batch: 462 , training loss: 4.259326\n",
      "[INFO] Epoch: 51 , batch: 463 , training loss: 4.192713\n",
      "[INFO] Epoch: 51 , batch: 464 , training loss: 4.354454\n",
      "[INFO] Epoch: 51 , batch: 465 , training loss: 4.300823\n",
      "[INFO] Epoch: 51 , batch: 466 , training loss: 4.399880\n",
      "[INFO] Epoch: 51 , batch: 467 , training loss: 4.345859\n",
      "[INFO] Epoch: 51 , batch: 468 , training loss: 4.311298\n",
      "[INFO] Epoch: 51 , batch: 469 , training loss: 4.339424\n",
      "[INFO] Epoch: 51 , batch: 470 , training loss: 4.155912\n",
      "[INFO] Epoch: 51 , batch: 471 , training loss: 4.267743\n",
      "[INFO] Epoch: 51 , batch: 472 , training loss: 4.322303\n",
      "[INFO] Epoch: 51 , batch: 473 , training loss: 4.251989\n",
      "[INFO] Epoch: 51 , batch: 474 , training loss: 4.022681\n",
      "[INFO] Epoch: 51 , batch: 475 , training loss: 3.908637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 51 , batch: 476 , training loss: 4.298717\n",
      "[INFO] Epoch: 51 , batch: 477 , training loss: 4.423945\n",
      "[INFO] Epoch: 51 , batch: 478 , training loss: 4.422401\n",
      "[INFO] Epoch: 51 , batch: 479 , training loss: 4.398718\n",
      "[INFO] Epoch: 51 , batch: 480 , training loss: 4.547872\n",
      "[INFO] Epoch: 51 , batch: 481 , training loss: 4.408816\n",
      "[INFO] Epoch: 51 , batch: 482 , training loss: 4.515450\n",
      "[INFO] Epoch: 51 , batch: 483 , training loss: 4.352664\n",
      "[INFO] Epoch: 51 , batch: 484 , training loss: 4.165459\n",
      "[INFO] Epoch: 51 , batch: 485 , training loss: 4.240338\n",
      "[INFO] Epoch: 51 , batch: 486 , training loss: 4.142952\n",
      "[INFO] Epoch: 51 , batch: 487 , training loss: 4.138291\n",
      "[INFO] Epoch: 51 , batch: 488 , training loss: 4.313232\n",
      "[INFO] Epoch: 51 , batch: 489 , training loss: 4.225598\n",
      "[INFO] Epoch: 51 , batch: 490 , training loss: 4.289835\n",
      "[INFO] Epoch: 51 , batch: 491 , training loss: 4.181530\n",
      "[INFO] Epoch: 51 , batch: 492 , training loss: 4.174510\n",
      "[INFO] Epoch: 51 , batch: 493 , training loss: 4.340377\n",
      "[INFO] Epoch: 51 , batch: 494 , training loss: 4.246730\n",
      "[INFO] Epoch: 51 , batch: 495 , training loss: 4.412108\n",
      "[INFO] Epoch: 51 , batch: 496 , training loss: 4.290720\n",
      "[INFO] Epoch: 51 , batch: 497 , training loss: 4.315944\n",
      "[INFO] Epoch: 51 , batch: 498 , training loss: 4.281501\n",
      "[INFO] Epoch: 51 , batch: 499 , training loss: 4.357703\n",
      "[INFO] Epoch: 51 , batch: 500 , training loss: 4.508195\n",
      "[INFO] Epoch: 51 , batch: 501 , training loss: 4.828411\n",
      "[INFO] Epoch: 51 , batch: 502 , training loss: 4.838575\n",
      "[INFO] Epoch: 51 , batch: 503 , training loss: 4.453543\n",
      "[INFO] Epoch: 51 , batch: 504 , training loss: 4.619543\n",
      "[INFO] Epoch: 51 , batch: 505 , training loss: 4.602615\n",
      "[INFO] Epoch: 51 , batch: 506 , training loss: 4.583472\n",
      "[INFO] Epoch: 51 , batch: 507 , training loss: 4.611117\n",
      "[INFO] Epoch: 51 , batch: 508 , training loss: 4.531363\n",
      "[INFO] Epoch: 51 , batch: 509 , training loss: 4.372684\n",
      "[INFO] Epoch: 51 , batch: 510 , training loss: 4.461992\n",
      "[INFO] Epoch: 51 , batch: 511 , training loss: 4.364215\n",
      "[INFO] Epoch: 51 , batch: 512 , training loss: 4.444364\n",
      "[INFO] Epoch: 51 , batch: 513 , training loss: 4.731745\n",
      "[INFO] Epoch: 51 , batch: 514 , training loss: 4.372507\n",
      "[INFO] Epoch: 51 , batch: 515 , training loss: 4.626731\n",
      "[INFO] Epoch: 51 , batch: 516 , training loss: 4.432469\n",
      "[INFO] Epoch: 51 , batch: 517 , training loss: 4.388492\n",
      "[INFO] Epoch: 51 , batch: 518 , training loss: 4.358274\n",
      "[INFO] Epoch: 51 , batch: 519 , training loss: 4.193116\n",
      "[INFO] Epoch: 51 , batch: 520 , training loss: 4.426986\n",
      "[INFO] Epoch: 51 , batch: 521 , training loss: 4.406756\n",
      "[INFO] Epoch: 51 , batch: 522 , training loss: 4.477501\n",
      "[INFO] Epoch: 51 , batch: 523 , training loss: 4.421525\n",
      "[INFO] Epoch: 51 , batch: 524 , training loss: 4.697726\n",
      "[INFO] Epoch: 51 , batch: 525 , training loss: 4.578310\n",
      "[INFO] Epoch: 51 , batch: 526 , training loss: 4.357814\n",
      "[INFO] Epoch: 51 , batch: 527 , training loss: 4.400997\n",
      "[INFO] Epoch: 51 , batch: 528 , training loss: 4.398658\n",
      "[INFO] Epoch: 51 , batch: 529 , training loss: 4.392812\n",
      "[INFO] Epoch: 51 , batch: 530 , training loss: 4.241976\n",
      "[INFO] Epoch: 51 , batch: 531 , training loss: 4.381660\n",
      "[INFO] Epoch: 51 , batch: 532 , training loss: 4.302800\n",
      "[INFO] Epoch: 51 , batch: 533 , training loss: 4.432686\n",
      "[INFO] Epoch: 51 , batch: 534 , training loss: 4.439570\n",
      "[INFO] Epoch: 51 , batch: 535 , training loss: 4.419115\n",
      "[INFO] Epoch: 51 , batch: 536 , training loss: 4.267802\n",
      "[INFO] Epoch: 51 , batch: 537 , training loss: 4.259147\n",
      "[INFO] Epoch: 51 , batch: 538 , training loss: 4.362054\n",
      "[INFO] Epoch: 51 , batch: 539 , training loss: 4.430125\n",
      "[INFO] Epoch: 51 , batch: 540 , training loss: 4.967310\n",
      "[INFO] Epoch: 51 , batch: 541 , training loss: 4.765229\n",
      "[INFO] Epoch: 51 , batch: 542 , training loss: 4.685570\n",
      "[INFO] Epoch: 52 , batch: 0 , training loss: 3.583744\n",
      "[INFO] Epoch: 52 , batch: 1 , training loss: 3.499244\n",
      "[INFO] Epoch: 52 , batch: 2 , training loss: 3.695667\n",
      "[INFO] Epoch: 52 , batch: 3 , training loss: 3.592701\n",
      "[INFO] Epoch: 52 , batch: 4 , training loss: 3.968917\n",
      "[INFO] Epoch: 52 , batch: 5 , training loss: 3.593113\n",
      "[INFO] Epoch: 52 , batch: 6 , training loss: 4.005553\n",
      "[INFO] Epoch: 52 , batch: 7 , training loss: 3.921654\n",
      "[INFO] Epoch: 52 , batch: 8 , training loss: 3.599296\n",
      "[INFO] Epoch: 52 , batch: 9 , training loss: 3.849509\n",
      "[INFO] Epoch: 52 , batch: 10 , training loss: 3.839432\n",
      "[INFO] Epoch: 52 , batch: 11 , training loss: 3.754811\n",
      "[INFO] Epoch: 52 , batch: 12 , training loss: 3.671088\n",
      "[INFO] Epoch: 52 , batch: 13 , training loss: 3.679261\n",
      "[INFO] Epoch: 52 , batch: 14 , training loss: 3.591653\n",
      "[INFO] Epoch: 52 , batch: 15 , training loss: 3.826042\n",
      "[INFO] Epoch: 52 , batch: 16 , training loss: 3.658073\n",
      "[INFO] Epoch: 52 , batch: 17 , training loss: 3.804483\n",
      "[INFO] Epoch: 52 , batch: 18 , training loss: 3.755788\n",
      "[INFO] Epoch: 52 , batch: 19 , training loss: 3.528849\n",
      "[INFO] Epoch: 52 , batch: 20 , training loss: 3.491848\n",
      "[INFO] Epoch: 52 , batch: 21 , training loss: 3.603477\n",
      "[INFO] Epoch: 52 , batch: 22 , training loss: 3.498780\n",
      "[INFO] Epoch: 52 , batch: 23 , training loss: 3.714696\n",
      "[INFO] Epoch: 52 , batch: 24 , training loss: 3.557508\n",
      "[INFO] Epoch: 52 , batch: 25 , training loss: 3.653600\n",
      "[INFO] Epoch: 52 , batch: 26 , training loss: 3.519567\n",
      "[INFO] Epoch: 52 , batch: 27 , training loss: 3.564495\n",
      "[INFO] Epoch: 52 , batch: 28 , training loss: 3.743458\n",
      "[INFO] Epoch: 52 , batch: 29 , training loss: 3.529671\n",
      "[INFO] Epoch: 52 , batch: 30 , training loss: 3.542938\n",
      "[INFO] Epoch: 52 , batch: 31 , training loss: 3.656048\n",
      "[INFO] Epoch: 52 , batch: 32 , training loss: 3.599978\n",
      "[INFO] Epoch: 52 , batch: 33 , training loss: 3.658686\n",
      "[INFO] Epoch: 52 , batch: 34 , training loss: 3.670590\n",
      "[INFO] Epoch: 52 , batch: 35 , training loss: 3.601210\n",
      "[INFO] Epoch: 52 , batch: 36 , training loss: 3.673817\n",
      "[INFO] Epoch: 52 , batch: 37 , training loss: 3.528882\n",
      "[INFO] Epoch: 52 , batch: 38 , training loss: 3.634356\n",
      "[INFO] Epoch: 52 , batch: 39 , training loss: 3.458201\n",
      "[INFO] Epoch: 52 , batch: 40 , training loss: 3.644263\n",
      "[INFO] Epoch: 52 , batch: 41 , training loss: 3.619850\n",
      "[INFO] Epoch: 52 , batch: 42 , training loss: 4.105980\n",
      "[INFO] Epoch: 52 , batch: 43 , training loss: 3.865420\n",
      "[INFO] Epoch: 52 , batch: 44 , training loss: 4.203269\n",
      "[INFO] Epoch: 52 , batch: 45 , training loss: 4.141336\n",
      "[INFO] Epoch: 52 , batch: 46 , training loss: 4.170083\n",
      "[INFO] Epoch: 52 , batch: 47 , training loss: 3.657571\n",
      "[INFO] Epoch: 52 , batch: 48 , training loss: 3.665505\n",
      "[INFO] Epoch: 52 , batch: 49 , training loss: 3.872569\n",
      "[INFO] Epoch: 52 , batch: 50 , training loss: 3.646959\n",
      "[INFO] Epoch: 52 , batch: 51 , training loss: 3.873062\n",
      "[INFO] Epoch: 52 , batch: 52 , training loss: 3.675094\n",
      "[INFO] Epoch: 52 , batch: 53 , training loss: 3.793420\n",
      "[INFO] Epoch: 52 , batch: 54 , training loss: 3.787318\n",
      "[INFO] Epoch: 52 , batch: 55 , training loss: 3.895074\n",
      "[INFO] Epoch: 52 , batch: 56 , training loss: 3.700548\n",
      "[INFO] Epoch: 52 , batch: 57 , training loss: 3.669828\n",
      "[INFO] Epoch: 52 , batch: 58 , training loss: 3.694956\n",
      "[INFO] Epoch: 52 , batch: 59 , training loss: 3.796701\n",
      "[INFO] Epoch: 52 , batch: 60 , training loss: 3.697493\n",
      "[INFO] Epoch: 52 , batch: 61 , training loss: 3.799213\n",
      "[INFO] Epoch: 52 , batch: 62 , training loss: 3.665239\n",
      "[INFO] Epoch: 52 , batch: 63 , training loss: 3.868439\n",
      "[INFO] Epoch: 52 , batch: 64 , training loss: 4.084444\n",
      "[INFO] Epoch: 52 , batch: 65 , training loss: 3.780507\n",
      "[INFO] Epoch: 52 , batch: 66 , training loss: 3.641148\n",
      "[INFO] Epoch: 52 , batch: 67 , training loss: 3.640604\n",
      "[INFO] Epoch: 52 , batch: 68 , training loss: 3.871185\n",
      "[INFO] Epoch: 52 , batch: 69 , training loss: 3.768280\n",
      "[INFO] Epoch: 52 , batch: 70 , training loss: 3.958813\n",
      "[INFO] Epoch: 52 , batch: 71 , training loss: 3.801805\n",
      "[INFO] Epoch: 52 , batch: 72 , training loss: 3.889181\n",
      "[INFO] Epoch: 52 , batch: 73 , training loss: 3.832057\n",
      "[INFO] Epoch: 52 , batch: 74 , training loss: 3.918889\n",
      "[INFO] Epoch: 52 , batch: 75 , training loss: 3.784759\n",
      "[INFO] Epoch: 52 , batch: 76 , training loss: 3.906356\n",
      "[INFO] Epoch: 52 , batch: 77 , training loss: 3.874285\n",
      "[INFO] Epoch: 52 , batch: 78 , training loss: 3.947329\n",
      "[INFO] Epoch: 52 , batch: 79 , training loss: 3.782606\n",
      "[INFO] Epoch: 52 , batch: 80 , training loss: 3.958087\n",
      "[INFO] Epoch: 52 , batch: 81 , training loss: 3.916922\n",
      "[INFO] Epoch: 52 , batch: 82 , training loss: 3.882100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 52 , batch: 83 , training loss: 3.964156\n",
      "[INFO] Epoch: 52 , batch: 84 , training loss: 3.966559\n",
      "[INFO] Epoch: 52 , batch: 85 , training loss: 4.033404\n",
      "[INFO] Epoch: 52 , batch: 86 , training loss: 3.953297\n",
      "[INFO] Epoch: 52 , batch: 87 , training loss: 3.937216\n",
      "[INFO] Epoch: 52 , batch: 88 , training loss: 4.054630\n",
      "[INFO] Epoch: 52 , batch: 89 , training loss: 3.864565\n",
      "[INFO] Epoch: 52 , batch: 90 , training loss: 3.918260\n",
      "[INFO] Epoch: 52 , batch: 91 , training loss: 3.870911\n",
      "[INFO] Epoch: 52 , batch: 92 , training loss: 3.883951\n",
      "[INFO] Epoch: 52 , batch: 93 , training loss: 3.980191\n",
      "[INFO] Epoch: 52 , batch: 94 , training loss: 4.125740\n",
      "[INFO] Epoch: 52 , batch: 95 , training loss: 3.909537\n",
      "[INFO] Epoch: 52 , batch: 96 , training loss: 3.885929\n",
      "[INFO] Epoch: 52 , batch: 97 , training loss: 3.805914\n",
      "[INFO] Epoch: 52 , batch: 98 , training loss: 3.786350\n",
      "[INFO] Epoch: 52 , batch: 99 , training loss: 3.884684\n",
      "[INFO] Epoch: 52 , batch: 100 , training loss: 3.783066\n",
      "[INFO] Epoch: 52 , batch: 101 , training loss: 3.803319\n",
      "[INFO] Epoch: 52 , batch: 102 , training loss: 3.990482\n",
      "[INFO] Epoch: 52 , batch: 103 , training loss: 3.774692\n",
      "[INFO] Epoch: 52 , batch: 104 , training loss: 3.729347\n",
      "[INFO] Epoch: 52 , batch: 105 , training loss: 3.983941\n",
      "[INFO] Epoch: 52 , batch: 106 , training loss: 4.009472\n",
      "[INFO] Epoch: 52 , batch: 107 , training loss: 3.856272\n",
      "[INFO] Epoch: 52 , batch: 108 , training loss: 3.790354\n",
      "[INFO] Epoch: 52 , batch: 109 , training loss: 3.700952\n",
      "[INFO] Epoch: 52 , batch: 110 , training loss: 3.863831\n",
      "[INFO] Epoch: 52 , batch: 111 , training loss: 3.934760\n",
      "[INFO] Epoch: 52 , batch: 112 , training loss: 3.873855\n",
      "[INFO] Epoch: 52 , batch: 113 , training loss: 3.850519\n",
      "[INFO] Epoch: 52 , batch: 114 , training loss: 3.863427\n",
      "[INFO] Epoch: 52 , batch: 115 , training loss: 3.886233\n",
      "[INFO] Epoch: 52 , batch: 116 , training loss: 3.770678\n",
      "[INFO] Epoch: 52 , batch: 117 , training loss: 4.005527\n",
      "[INFO] Epoch: 52 , batch: 118 , training loss: 3.966497\n",
      "[INFO] Epoch: 52 , batch: 119 , training loss: 4.110829\n",
      "[INFO] Epoch: 52 , batch: 120 , training loss: 4.077997\n",
      "[INFO] Epoch: 52 , batch: 121 , training loss: 3.952179\n",
      "[INFO] Epoch: 52 , batch: 122 , training loss: 3.869059\n",
      "[INFO] Epoch: 52 , batch: 123 , training loss: 3.819613\n",
      "[INFO] Epoch: 52 , batch: 124 , training loss: 3.937236\n",
      "[INFO] Epoch: 52 , batch: 125 , training loss: 3.774581\n",
      "[INFO] Epoch: 52 , batch: 126 , training loss: 3.789441\n",
      "[INFO] Epoch: 52 , batch: 127 , training loss: 3.785258\n",
      "[INFO] Epoch: 52 , batch: 128 , training loss: 3.945427\n",
      "[INFO] Epoch: 52 , batch: 129 , training loss: 3.851271\n",
      "[INFO] Epoch: 52 , batch: 130 , training loss: 3.912065\n",
      "[INFO] Epoch: 52 , batch: 131 , training loss: 3.877609\n",
      "[INFO] Epoch: 52 , batch: 132 , training loss: 3.913900\n",
      "[INFO] Epoch: 52 , batch: 133 , training loss: 3.864874\n",
      "[INFO] Epoch: 52 , batch: 134 , training loss: 3.634972\n",
      "[INFO] Epoch: 52 , batch: 135 , training loss: 3.700367\n",
      "[INFO] Epoch: 52 , batch: 136 , training loss: 3.930652\n",
      "[INFO] Epoch: 52 , batch: 137 , training loss: 3.907132\n",
      "[INFO] Epoch: 52 , batch: 138 , training loss: 3.965955\n",
      "[INFO] Epoch: 52 , batch: 139 , training loss: 4.556955\n",
      "[INFO] Epoch: 52 , batch: 140 , training loss: 4.353719\n",
      "[INFO] Epoch: 52 , batch: 141 , training loss: 4.110192\n",
      "[INFO] Epoch: 52 , batch: 142 , training loss: 3.819614\n",
      "[INFO] Epoch: 52 , batch: 143 , training loss: 3.935819\n",
      "[INFO] Epoch: 52 , batch: 144 , training loss: 3.766059\n",
      "[INFO] Epoch: 52 , batch: 145 , training loss: 3.860185\n",
      "[INFO] Epoch: 52 , batch: 146 , training loss: 4.049691\n",
      "[INFO] Epoch: 52 , batch: 147 , training loss: 3.714545\n",
      "[INFO] Epoch: 52 , batch: 148 , training loss: 3.659242\n",
      "[INFO] Epoch: 52 , batch: 149 , training loss: 3.743416\n",
      "[INFO] Epoch: 52 , batch: 150 , training loss: 4.003425\n",
      "[INFO] Epoch: 52 , batch: 151 , training loss: 3.879471\n",
      "[INFO] Epoch: 52 , batch: 152 , training loss: 3.881450\n",
      "[INFO] Epoch: 52 , batch: 153 , training loss: 3.924312\n",
      "[INFO] Epoch: 52 , batch: 154 , training loss: 3.989927\n",
      "[INFO] Epoch: 52 , batch: 155 , training loss: 4.228044\n",
      "[INFO] Epoch: 52 , batch: 156 , training loss: 3.941824\n",
      "[INFO] Epoch: 52 , batch: 157 , training loss: 3.913849\n",
      "[INFO] Epoch: 52 , batch: 158 , training loss: 4.038953\n",
      "[INFO] Epoch: 52 , batch: 159 , training loss: 3.960269\n",
      "[INFO] Epoch: 52 , batch: 160 , training loss: 4.205749\n",
      "[INFO] Epoch: 52 , batch: 161 , training loss: 4.228880\n",
      "[INFO] Epoch: 52 , batch: 162 , training loss: 4.219010\n",
      "[INFO] Epoch: 52 , batch: 163 , training loss: 4.416487\n",
      "[INFO] Epoch: 52 , batch: 164 , training loss: 4.338542\n",
      "[INFO] Epoch: 52 , batch: 165 , training loss: 4.262742\n",
      "[INFO] Epoch: 52 , batch: 166 , training loss: 4.157088\n",
      "[INFO] Epoch: 52 , batch: 167 , training loss: 4.198004\n",
      "[INFO] Epoch: 52 , batch: 168 , training loss: 3.907518\n",
      "[INFO] Epoch: 52 , batch: 169 , training loss: 3.898744\n",
      "[INFO] Epoch: 52 , batch: 170 , training loss: 4.040405\n",
      "[INFO] Epoch: 52 , batch: 171 , training loss: 3.544577\n",
      "[INFO] Epoch: 52 , batch: 172 , training loss: 3.764204\n",
      "[INFO] Epoch: 52 , batch: 173 , training loss: 4.046346\n",
      "[INFO] Epoch: 52 , batch: 174 , training loss: 4.542807\n",
      "[INFO] Epoch: 52 , batch: 175 , training loss: 4.789415\n",
      "[INFO] Epoch: 52 , batch: 176 , training loss: 4.467714\n",
      "[INFO] Epoch: 52 , batch: 177 , training loss: 4.050116\n",
      "[INFO] Epoch: 52 , batch: 178 , training loss: 4.040404\n",
      "[INFO] Epoch: 52 , batch: 179 , training loss: 4.076871\n",
      "[INFO] Epoch: 52 , batch: 180 , training loss: 4.094459\n",
      "[INFO] Epoch: 52 , batch: 181 , training loss: 4.340492\n",
      "[INFO] Epoch: 52 , batch: 182 , training loss: 4.296591\n",
      "[INFO] Epoch: 52 , batch: 183 , training loss: 4.295062\n",
      "[INFO] Epoch: 52 , batch: 184 , training loss: 4.156707\n",
      "[INFO] Epoch: 52 , batch: 185 , training loss: 4.127327\n",
      "[INFO] Epoch: 52 , batch: 186 , training loss: 4.293974\n",
      "[INFO] Epoch: 52 , batch: 187 , training loss: 4.350131\n",
      "[INFO] Epoch: 52 , batch: 188 , training loss: 4.370803\n",
      "[INFO] Epoch: 52 , batch: 189 , training loss: 4.258779\n",
      "[INFO] Epoch: 52 , batch: 190 , training loss: 4.315389\n",
      "[INFO] Epoch: 52 , batch: 191 , training loss: 4.417979\n",
      "[INFO] Epoch: 52 , batch: 192 , training loss: 4.271379\n",
      "[INFO] Epoch: 52 , batch: 193 , training loss: 4.369032\n",
      "[INFO] Epoch: 52 , batch: 194 , training loss: 4.275827\n",
      "[INFO] Epoch: 52 , batch: 195 , training loss: 4.219560\n",
      "[INFO] Epoch: 52 , batch: 196 , training loss: 4.081789\n",
      "[INFO] Epoch: 52 , batch: 197 , training loss: 4.159728\n",
      "[INFO] Epoch: 52 , batch: 198 , training loss: 4.085174\n",
      "[INFO] Epoch: 52 , batch: 199 , training loss: 4.232464\n",
      "[INFO] Epoch: 52 , batch: 200 , training loss: 4.138270\n",
      "[INFO] Epoch: 52 , batch: 201 , training loss: 4.037606\n",
      "[INFO] Epoch: 52 , batch: 202 , training loss: 4.023760\n",
      "[INFO] Epoch: 52 , batch: 203 , training loss: 4.161641\n",
      "[INFO] Epoch: 52 , batch: 204 , training loss: 4.217163\n",
      "[INFO] Epoch: 52 , batch: 205 , training loss: 3.846543\n",
      "[INFO] Epoch: 52 , batch: 206 , training loss: 3.789291\n",
      "[INFO] Epoch: 52 , batch: 207 , training loss: 3.765355\n",
      "[INFO] Epoch: 52 , batch: 208 , training loss: 4.088110\n",
      "[INFO] Epoch: 52 , batch: 209 , training loss: 4.095342\n",
      "[INFO] Epoch: 52 , batch: 210 , training loss: 4.086025\n",
      "[INFO] Epoch: 52 , batch: 211 , training loss: 4.087845\n",
      "[INFO] Epoch: 52 , batch: 212 , training loss: 4.153989\n",
      "[INFO] Epoch: 52 , batch: 213 , training loss: 4.125414\n",
      "[INFO] Epoch: 52 , batch: 214 , training loss: 4.190458\n",
      "[INFO] Epoch: 52 , batch: 215 , training loss: 4.401921\n",
      "[INFO] Epoch: 52 , batch: 216 , training loss: 4.093988\n",
      "[INFO] Epoch: 52 , batch: 217 , training loss: 4.047334\n",
      "[INFO] Epoch: 52 , batch: 218 , training loss: 4.053843\n",
      "[INFO] Epoch: 52 , batch: 219 , training loss: 4.166216\n",
      "[INFO] Epoch: 52 , batch: 220 , training loss: 3.979664\n",
      "[INFO] Epoch: 52 , batch: 221 , training loss: 3.996821\n",
      "[INFO] Epoch: 52 , batch: 222 , training loss: 4.126482\n",
      "[INFO] Epoch: 52 , batch: 223 , training loss: 4.277238\n",
      "[INFO] Epoch: 52 , batch: 224 , training loss: 4.291987\n",
      "[INFO] Epoch: 52 , batch: 225 , training loss: 4.175282\n",
      "[INFO] Epoch: 52 , batch: 226 , training loss: 4.282747\n",
      "[INFO] Epoch: 52 , batch: 227 , training loss: 4.283052\n",
      "[INFO] Epoch: 52 , batch: 228 , training loss: 4.281796\n",
      "[INFO] Epoch: 52 , batch: 229 , training loss: 4.159540\n",
      "[INFO] Epoch: 52 , batch: 230 , training loss: 4.029692\n",
      "[INFO] Epoch: 52 , batch: 231 , training loss: 3.868194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 52 , batch: 232 , training loss: 4.027950\n",
      "[INFO] Epoch: 52 , batch: 233 , training loss: 4.075695\n",
      "[INFO] Epoch: 52 , batch: 234 , training loss: 3.739593\n",
      "[INFO] Epoch: 52 , batch: 235 , training loss: 3.872302\n",
      "[INFO] Epoch: 52 , batch: 236 , training loss: 3.970618\n",
      "[INFO] Epoch: 52 , batch: 237 , training loss: 4.188049\n",
      "[INFO] Epoch: 52 , batch: 238 , training loss: 3.974128\n",
      "[INFO] Epoch: 52 , batch: 239 , training loss: 4.007437\n",
      "[INFO] Epoch: 52 , batch: 240 , training loss: 4.046713\n",
      "[INFO] Epoch: 52 , batch: 241 , training loss: 3.854146\n",
      "[INFO] Epoch: 52 , batch: 242 , training loss: 3.873308\n",
      "[INFO] Epoch: 52 , batch: 243 , training loss: 4.162013\n",
      "[INFO] Epoch: 52 , batch: 244 , training loss: 4.109415\n",
      "[INFO] Epoch: 52 , batch: 245 , training loss: 4.048677\n",
      "[INFO] Epoch: 52 , batch: 246 , training loss: 3.781679\n",
      "[INFO] Epoch: 52 , batch: 247 , training loss: 3.951435\n",
      "[INFO] Epoch: 52 , batch: 248 , training loss: 4.038149\n",
      "[INFO] Epoch: 52 , batch: 249 , training loss: 4.015441\n",
      "[INFO] Epoch: 52 , batch: 250 , training loss: 3.806509\n",
      "[INFO] Epoch: 52 , batch: 251 , training loss: 4.222150\n",
      "[INFO] Epoch: 52 , batch: 252 , training loss: 3.955709\n",
      "[INFO] Epoch: 52 , batch: 253 , training loss: 3.864321\n",
      "[INFO] Epoch: 52 , batch: 254 , training loss: 4.178153\n",
      "[INFO] Epoch: 52 , batch: 255 , training loss: 4.134596\n",
      "[INFO] Epoch: 52 , batch: 256 , training loss: 4.086802\n",
      "[INFO] Epoch: 52 , batch: 257 , training loss: 4.276010\n",
      "[INFO] Epoch: 52 , batch: 258 , training loss: 4.259524\n",
      "[INFO] Epoch: 52 , batch: 259 , training loss: 4.306156\n",
      "[INFO] Epoch: 52 , batch: 260 , training loss: 4.073906\n",
      "[INFO] Epoch: 52 , batch: 261 , training loss: 4.271126\n",
      "[INFO] Epoch: 52 , batch: 262 , training loss: 4.379714\n",
      "[INFO] Epoch: 52 , batch: 263 , training loss: 4.558289\n",
      "[INFO] Epoch: 52 , batch: 264 , training loss: 3.918460\n",
      "[INFO] Epoch: 52 , batch: 265 , training loss: 4.047207\n",
      "[INFO] Epoch: 52 , batch: 266 , training loss: 4.423604\n",
      "[INFO] Epoch: 52 , batch: 267 , training loss: 4.176733\n",
      "[INFO] Epoch: 52 , batch: 268 , training loss: 4.100002\n",
      "[INFO] Epoch: 52 , batch: 269 , training loss: 4.076781\n",
      "[INFO] Epoch: 52 , batch: 270 , training loss: 4.106263\n",
      "[INFO] Epoch: 52 , batch: 271 , training loss: 4.138422\n",
      "[INFO] Epoch: 52 , batch: 272 , training loss: 4.117570\n",
      "[INFO] Epoch: 52 , batch: 273 , training loss: 4.131413\n",
      "[INFO] Epoch: 52 , batch: 274 , training loss: 4.235345\n",
      "[INFO] Epoch: 52 , batch: 275 , training loss: 4.096909\n",
      "[INFO] Epoch: 52 , batch: 276 , training loss: 4.173197\n",
      "[INFO] Epoch: 52 , batch: 277 , training loss: 4.319423\n",
      "[INFO] Epoch: 52 , batch: 278 , training loss: 4.008102\n",
      "[INFO] Epoch: 52 , batch: 279 , training loss: 4.015706\n",
      "[INFO] Epoch: 52 , batch: 280 , training loss: 3.987945\n",
      "[INFO] Epoch: 52 , batch: 281 , training loss: 4.113531\n",
      "[INFO] Epoch: 52 , batch: 282 , training loss: 4.023819\n",
      "[INFO] Epoch: 52 , batch: 283 , training loss: 4.033965\n",
      "[INFO] Epoch: 52 , batch: 284 , training loss: 4.060095\n",
      "[INFO] Epoch: 52 , batch: 285 , training loss: 4.007581\n",
      "[INFO] Epoch: 52 , batch: 286 , training loss: 4.006531\n",
      "[INFO] Epoch: 52 , batch: 287 , training loss: 3.951823\n",
      "[INFO] Epoch: 52 , batch: 288 , training loss: 3.906942\n",
      "[INFO] Epoch: 52 , batch: 289 , training loss: 3.984630\n",
      "[INFO] Epoch: 52 , batch: 290 , training loss: 3.767920\n",
      "[INFO] Epoch: 52 , batch: 291 , training loss: 3.769634\n",
      "[INFO] Epoch: 52 , batch: 292 , training loss: 3.857813\n",
      "[INFO] Epoch: 52 , batch: 293 , training loss: 3.795976\n",
      "[INFO] Epoch: 52 , batch: 294 , training loss: 4.418107\n",
      "[INFO] Epoch: 52 , batch: 295 , training loss: 4.214751\n",
      "[INFO] Epoch: 52 , batch: 296 , training loss: 4.164516\n",
      "[INFO] Epoch: 52 , batch: 297 , training loss: 4.108710\n",
      "[INFO] Epoch: 52 , batch: 298 , training loss: 3.939574\n",
      "[INFO] Epoch: 52 , batch: 299 , training loss: 3.978534\n",
      "[INFO] Epoch: 52 , batch: 300 , training loss: 3.969066\n",
      "[INFO] Epoch: 52 , batch: 301 , training loss: 3.887720\n",
      "[INFO] Epoch: 52 , batch: 302 , training loss: 4.077848\n",
      "[INFO] Epoch: 52 , batch: 303 , training loss: 4.066020\n",
      "[INFO] Epoch: 52 , batch: 304 , training loss: 4.180393\n",
      "[INFO] Epoch: 52 , batch: 305 , training loss: 4.034140\n",
      "[INFO] Epoch: 52 , batch: 306 , training loss: 4.162405\n",
      "[INFO] Epoch: 52 , batch: 307 , training loss: 4.192277\n",
      "[INFO] Epoch: 52 , batch: 308 , training loss: 3.976393\n",
      "[INFO] Epoch: 52 , batch: 309 , training loss: 3.963710\n",
      "[INFO] Epoch: 52 , batch: 310 , training loss: 3.911641\n",
      "[INFO] Epoch: 52 , batch: 311 , training loss: 3.914258\n",
      "[INFO] Epoch: 52 , batch: 312 , training loss: 3.798770\n",
      "[INFO] Epoch: 52 , batch: 313 , training loss: 3.911395\n",
      "[INFO] Epoch: 52 , batch: 314 , training loss: 3.971550\n",
      "[INFO] Epoch: 52 , batch: 315 , training loss: 4.061921\n",
      "[INFO] Epoch: 52 , batch: 316 , training loss: 4.284578\n",
      "[INFO] Epoch: 52 , batch: 317 , training loss: 4.647936\n",
      "[INFO] Epoch: 52 , batch: 318 , training loss: 4.773689\n",
      "[INFO] Epoch: 52 , batch: 319 , training loss: 4.460751\n",
      "[INFO] Epoch: 52 , batch: 320 , training loss: 3.991408\n",
      "[INFO] Epoch: 52 , batch: 321 , training loss: 3.834816\n",
      "[INFO] Epoch: 52 , batch: 322 , training loss: 3.935170\n",
      "[INFO] Epoch: 52 , batch: 323 , training loss: 3.979119\n",
      "[INFO] Epoch: 52 , batch: 324 , training loss: 3.937278\n",
      "[INFO] Epoch: 52 , batch: 325 , training loss: 4.059699\n",
      "[INFO] Epoch: 52 , batch: 326 , training loss: 4.122323\n",
      "[INFO] Epoch: 52 , batch: 327 , training loss: 4.075853\n",
      "[INFO] Epoch: 52 , batch: 328 , training loss: 4.068514\n",
      "[INFO] Epoch: 52 , batch: 329 , training loss: 3.973810\n",
      "[INFO] Epoch: 52 , batch: 330 , training loss: 3.949847\n",
      "[INFO] Epoch: 52 , batch: 331 , training loss: 4.099507\n",
      "[INFO] Epoch: 52 , batch: 332 , training loss: 3.955207\n",
      "[INFO] Epoch: 52 , batch: 333 , training loss: 3.930219\n",
      "[INFO] Epoch: 52 , batch: 334 , training loss: 3.946352\n",
      "[INFO] Epoch: 52 , batch: 335 , training loss: 4.068754\n",
      "[INFO] Epoch: 52 , batch: 336 , training loss: 4.083676\n",
      "[INFO] Epoch: 52 , batch: 337 , training loss: 4.111974\n",
      "[INFO] Epoch: 52 , batch: 338 , training loss: 4.328849\n",
      "[INFO] Epoch: 52 , batch: 339 , training loss: 4.145133\n",
      "[INFO] Epoch: 52 , batch: 340 , training loss: 4.348444\n",
      "[INFO] Epoch: 52 , batch: 341 , training loss: 4.122889\n",
      "[INFO] Epoch: 52 , batch: 342 , training loss: 3.876070\n",
      "[INFO] Epoch: 52 , batch: 343 , training loss: 3.967368\n",
      "[INFO] Epoch: 52 , batch: 344 , training loss: 3.833514\n",
      "[INFO] Epoch: 52 , batch: 345 , training loss: 3.948735\n",
      "[INFO] Epoch: 52 , batch: 346 , training loss: 3.996120\n",
      "[INFO] Epoch: 52 , batch: 347 , training loss: 3.917197\n",
      "[INFO] Epoch: 52 , batch: 348 , training loss: 3.991910\n",
      "[INFO] Epoch: 52 , batch: 349 , training loss: 4.090857\n",
      "[INFO] Epoch: 52 , batch: 350 , training loss: 3.947942\n",
      "[INFO] Epoch: 52 , batch: 351 , training loss: 4.041831\n",
      "[INFO] Epoch: 52 , batch: 352 , training loss: 4.046161\n",
      "[INFO] Epoch: 52 , batch: 353 , training loss: 4.037021\n",
      "[INFO] Epoch: 52 , batch: 354 , training loss: 4.130136\n",
      "[INFO] Epoch: 52 , batch: 355 , training loss: 4.121982\n",
      "[INFO] Epoch: 52 , batch: 356 , training loss: 3.970945\n",
      "[INFO] Epoch: 52 , batch: 357 , training loss: 4.049911\n",
      "[INFO] Epoch: 52 , batch: 358 , training loss: 3.971719\n",
      "[INFO] Epoch: 52 , batch: 359 , training loss: 3.964469\n",
      "[INFO] Epoch: 52 , batch: 360 , training loss: 4.076473\n",
      "[INFO] Epoch: 52 , batch: 361 , training loss: 4.035414\n",
      "[INFO] Epoch: 52 , batch: 362 , training loss: 4.136620\n",
      "[INFO] Epoch: 52 , batch: 363 , training loss: 4.014527\n",
      "[INFO] Epoch: 52 , batch: 364 , training loss: 4.081432\n",
      "[INFO] Epoch: 52 , batch: 365 , training loss: 3.996045\n",
      "[INFO] Epoch: 52 , batch: 366 , training loss: 4.098954\n",
      "[INFO] Epoch: 52 , batch: 367 , training loss: 4.152106\n",
      "[INFO] Epoch: 52 , batch: 368 , training loss: 4.553135\n",
      "[INFO] Epoch: 52 , batch: 369 , training loss: 4.241276\n",
      "[INFO] Epoch: 52 , batch: 370 , training loss: 4.009246\n",
      "[INFO] Epoch: 52 , batch: 371 , training loss: 4.398458\n",
      "[INFO] Epoch: 52 , batch: 372 , training loss: 4.676961\n",
      "[INFO] Epoch: 52 , batch: 373 , training loss: 4.673634\n",
      "[INFO] Epoch: 52 , batch: 374 , training loss: 4.872642\n",
      "[INFO] Epoch: 52 , batch: 375 , training loss: 4.804401\n",
      "[INFO] Epoch: 52 , batch: 376 , training loss: 4.708663\n",
      "[INFO] Epoch: 52 , batch: 377 , training loss: 4.467700\n",
      "[INFO] Epoch: 52 , batch: 378 , training loss: 4.547878\n",
      "[INFO] Epoch: 52 , batch: 379 , training loss: 4.516935\n",
      "[INFO] Epoch: 52 , batch: 380 , training loss: 4.703480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 52 , batch: 381 , training loss: 4.366475\n",
      "[INFO] Epoch: 52 , batch: 382 , training loss: 4.630723\n",
      "[INFO] Epoch: 52 , batch: 383 , training loss: 4.711809\n",
      "[INFO] Epoch: 52 , batch: 384 , training loss: 4.655458\n",
      "[INFO] Epoch: 52 , batch: 385 , training loss: 4.335677\n",
      "[INFO] Epoch: 52 , batch: 386 , training loss: 4.562765\n",
      "[INFO] Epoch: 52 , batch: 387 , training loss: 4.529477\n",
      "[INFO] Epoch: 52 , batch: 388 , training loss: 4.352334\n",
      "[INFO] Epoch: 52 , batch: 389 , training loss: 4.155809\n",
      "[INFO] Epoch: 52 , batch: 390 , training loss: 4.182433\n",
      "[INFO] Epoch: 52 , batch: 391 , training loss: 4.228729\n",
      "[INFO] Epoch: 52 , batch: 392 , training loss: 4.597958\n",
      "[INFO] Epoch: 52 , batch: 393 , training loss: 4.463673\n",
      "[INFO] Epoch: 52 , batch: 394 , training loss: 4.581342\n",
      "[INFO] Epoch: 52 , batch: 395 , training loss: 4.384931\n",
      "[INFO] Epoch: 52 , batch: 396 , training loss: 4.195731\n",
      "[INFO] Epoch: 52 , batch: 397 , training loss: 4.337384\n",
      "[INFO] Epoch: 52 , batch: 398 , training loss: 4.213518\n",
      "[INFO] Epoch: 52 , batch: 399 , training loss: 4.298711\n",
      "[INFO] Epoch: 52 , batch: 400 , training loss: 4.266426\n",
      "[INFO] Epoch: 52 , batch: 401 , training loss: 4.679604\n",
      "[INFO] Epoch: 52 , batch: 402 , training loss: 4.411030\n",
      "[INFO] Epoch: 52 , batch: 403 , training loss: 4.245548\n",
      "[INFO] Epoch: 52 , batch: 404 , training loss: 4.420449\n",
      "[INFO] Epoch: 52 , batch: 405 , training loss: 4.474744\n",
      "[INFO] Epoch: 52 , batch: 406 , training loss: 4.379772\n",
      "[INFO] Epoch: 52 , batch: 407 , training loss: 4.401764\n",
      "[INFO] Epoch: 52 , batch: 408 , training loss: 4.377947\n",
      "[INFO] Epoch: 52 , batch: 409 , training loss: 4.409130\n",
      "[INFO] Epoch: 52 , batch: 410 , training loss: 4.459015\n",
      "[INFO] Epoch: 52 , batch: 411 , training loss: 4.618097\n",
      "[INFO] Epoch: 52 , batch: 412 , training loss: 4.448860\n",
      "[INFO] Epoch: 52 , batch: 413 , training loss: 4.310924\n",
      "[INFO] Epoch: 52 , batch: 414 , training loss: 4.357809\n",
      "[INFO] Epoch: 52 , batch: 415 , training loss: 4.425036\n",
      "[INFO] Epoch: 52 , batch: 416 , training loss: 4.449974\n",
      "[INFO] Epoch: 52 , batch: 417 , training loss: 4.392029\n",
      "[INFO] Epoch: 52 , batch: 418 , training loss: 4.434752\n",
      "[INFO] Epoch: 52 , batch: 419 , training loss: 4.397446\n",
      "[INFO] Epoch: 52 , batch: 420 , training loss: 4.388622\n",
      "[INFO] Epoch: 52 , batch: 421 , training loss: 4.366188\n",
      "[INFO] Epoch: 52 , batch: 422 , training loss: 4.200094\n",
      "[INFO] Epoch: 52 , batch: 423 , training loss: 4.441513\n",
      "[INFO] Epoch: 52 , batch: 424 , training loss: 4.600074\n",
      "[INFO] Epoch: 52 , batch: 425 , training loss: 4.494480\n",
      "[INFO] Epoch: 52 , batch: 426 , training loss: 4.226583\n",
      "[INFO] Epoch: 52 , batch: 427 , training loss: 4.455629\n",
      "[INFO] Epoch: 52 , batch: 428 , training loss: 4.329301\n",
      "[INFO] Epoch: 52 , batch: 429 , training loss: 4.230191\n",
      "[INFO] Epoch: 52 , batch: 430 , training loss: 4.462836\n",
      "[INFO] Epoch: 52 , batch: 431 , training loss: 4.088384\n",
      "[INFO] Epoch: 52 , batch: 432 , training loss: 4.115436\n",
      "[INFO] Epoch: 52 , batch: 433 , training loss: 4.172654\n",
      "[INFO] Epoch: 52 , batch: 434 , training loss: 4.037209\n",
      "[INFO] Epoch: 52 , batch: 435 , training loss: 4.396869\n",
      "[INFO] Epoch: 52 , batch: 436 , training loss: 4.430201\n",
      "[INFO] Epoch: 52 , batch: 437 , training loss: 4.210622\n",
      "[INFO] Epoch: 52 , batch: 438 , training loss: 4.105717\n",
      "[INFO] Epoch: 52 , batch: 439 , training loss: 4.314853\n",
      "[INFO] Epoch: 52 , batch: 440 , training loss: 4.432285\n",
      "[INFO] Epoch: 52 , batch: 441 , training loss: 4.536295\n",
      "[INFO] Epoch: 52 , batch: 442 , training loss: 4.293670\n",
      "[INFO] Epoch: 52 , batch: 443 , training loss: 4.456585\n",
      "[INFO] Epoch: 52 , batch: 444 , training loss: 4.083462\n",
      "[INFO] Epoch: 52 , batch: 445 , training loss: 3.977358\n",
      "[INFO] Epoch: 52 , batch: 446 , training loss: 3.919459\n",
      "[INFO] Epoch: 52 , batch: 447 , training loss: 4.110979\n",
      "[INFO] Epoch: 52 , batch: 448 , training loss: 4.221380\n",
      "[INFO] Epoch: 52 , batch: 449 , training loss: 4.588469\n",
      "[INFO] Epoch: 52 , batch: 450 , training loss: 4.682190\n",
      "[INFO] Epoch: 52 , batch: 451 , training loss: 4.589823\n",
      "[INFO] Epoch: 52 , batch: 452 , training loss: 4.386004\n",
      "[INFO] Epoch: 52 , batch: 453 , training loss: 4.161098\n",
      "[INFO] Epoch: 52 , batch: 454 , training loss: 4.316941\n",
      "[INFO] Epoch: 52 , batch: 455 , training loss: 4.360744\n",
      "[INFO] Epoch: 52 , batch: 456 , training loss: 4.357036\n",
      "[INFO] Epoch: 52 , batch: 457 , training loss: 4.448194\n",
      "[INFO] Epoch: 52 , batch: 458 , training loss: 4.162751\n",
      "[INFO] Epoch: 52 , batch: 459 , training loss: 4.145183\n",
      "[INFO] Epoch: 52 , batch: 460 , training loss: 4.291761\n",
      "[INFO] Epoch: 52 , batch: 461 , training loss: 4.233401\n",
      "[INFO] Epoch: 52 , batch: 462 , training loss: 4.295951\n",
      "[INFO] Epoch: 52 , batch: 463 , training loss: 4.212582\n",
      "[INFO] Epoch: 52 , batch: 464 , training loss: 4.367610\n",
      "[INFO] Epoch: 52 , batch: 465 , training loss: 4.323661\n",
      "[INFO] Epoch: 52 , batch: 466 , training loss: 4.414400\n",
      "[INFO] Epoch: 52 , batch: 467 , training loss: 4.373814\n",
      "[INFO] Epoch: 52 , batch: 468 , training loss: 4.334447\n",
      "[INFO] Epoch: 52 , batch: 469 , training loss: 4.373051\n",
      "[INFO] Epoch: 52 , batch: 470 , training loss: 4.185582\n",
      "[INFO] Epoch: 52 , batch: 471 , training loss: 4.293860\n",
      "[INFO] Epoch: 52 , batch: 472 , training loss: 4.336736\n",
      "[INFO] Epoch: 52 , batch: 473 , training loss: 4.273948\n",
      "[INFO] Epoch: 52 , batch: 474 , training loss: 4.045070\n",
      "[INFO] Epoch: 52 , batch: 475 , training loss: 3.917751\n",
      "[INFO] Epoch: 52 , batch: 476 , training loss: 4.340001\n",
      "[INFO] Epoch: 52 , batch: 477 , training loss: 4.453301\n",
      "[INFO] Epoch: 52 , batch: 478 , training loss: 4.452476\n",
      "[INFO] Epoch: 52 , batch: 479 , training loss: 4.430852\n",
      "[INFO] Epoch: 52 , batch: 480 , training loss: 4.564714\n",
      "[INFO] Epoch: 52 , batch: 481 , training loss: 4.419689\n",
      "[INFO] Epoch: 52 , batch: 482 , training loss: 4.532386\n",
      "[INFO] Epoch: 52 , batch: 483 , training loss: 4.371117\n",
      "[INFO] Epoch: 52 , batch: 484 , training loss: 4.185782\n",
      "[INFO] Epoch: 52 , batch: 485 , training loss: 4.280762\n",
      "[INFO] Epoch: 52 , batch: 486 , training loss: 4.176806\n",
      "[INFO] Epoch: 52 , batch: 487 , training loss: 4.151412\n",
      "[INFO] Epoch: 52 , batch: 488 , training loss: 4.333232\n",
      "[INFO] Epoch: 52 , batch: 489 , training loss: 4.238577\n",
      "[INFO] Epoch: 52 , batch: 490 , training loss: 4.301688\n",
      "[INFO] Epoch: 52 , batch: 491 , training loss: 4.200901\n",
      "[INFO] Epoch: 52 , batch: 492 , training loss: 4.198997\n",
      "[INFO] Epoch: 52 , batch: 493 , training loss: 4.362897\n",
      "[INFO] Epoch: 52 , batch: 494 , training loss: 4.265068\n",
      "[INFO] Epoch: 52 , batch: 495 , training loss: 4.436893\n",
      "[INFO] Epoch: 52 , batch: 496 , training loss: 4.306584\n",
      "[INFO] Epoch: 52 , batch: 497 , training loss: 4.355779\n",
      "[INFO] Epoch: 52 , batch: 498 , training loss: 4.315219\n",
      "[INFO] Epoch: 52 , batch: 499 , training loss: 4.385775\n",
      "[INFO] Epoch: 52 , batch: 500 , training loss: 4.537385\n",
      "[INFO] Epoch: 52 , batch: 501 , training loss: 4.878620\n",
      "[INFO] Epoch: 52 , batch: 502 , training loss: 4.885005\n",
      "[INFO] Epoch: 52 , batch: 503 , training loss: 4.512202\n",
      "[INFO] Epoch: 52 , batch: 504 , training loss: 4.660855\n",
      "[INFO] Epoch: 52 , batch: 505 , training loss: 4.643145\n",
      "[INFO] Epoch: 52 , batch: 506 , training loss: 4.623970\n",
      "[INFO] Epoch: 52 , batch: 507 , training loss: 4.659904\n",
      "[INFO] Epoch: 52 , batch: 508 , training loss: 4.581839\n",
      "[INFO] Epoch: 52 , batch: 509 , training loss: 4.393615\n",
      "[INFO] Epoch: 52 , batch: 510 , training loss: 4.499443\n",
      "[INFO] Epoch: 52 , batch: 511 , training loss: 4.414514\n",
      "[INFO] Epoch: 52 , batch: 512 , training loss: 4.504157\n",
      "[INFO] Epoch: 52 , batch: 513 , training loss: 4.785348\n",
      "[INFO] Epoch: 52 , batch: 514 , training loss: 4.391876\n",
      "[INFO] Epoch: 52 , batch: 515 , training loss: 4.664410\n",
      "[INFO] Epoch: 52 , batch: 516 , training loss: 4.462252\n",
      "[INFO] Epoch: 52 , batch: 517 , training loss: 4.425756\n",
      "[INFO] Epoch: 52 , batch: 518 , training loss: 4.376166\n",
      "[INFO] Epoch: 52 , batch: 519 , training loss: 4.230055\n",
      "[INFO] Epoch: 52 , batch: 520 , training loss: 4.471056\n",
      "[INFO] Epoch: 52 , batch: 521 , training loss: 4.424109\n",
      "[INFO] Epoch: 52 , batch: 522 , training loss: 4.506880\n",
      "[INFO] Epoch: 52 , batch: 523 , training loss: 4.431558\n",
      "[INFO] Epoch: 52 , batch: 524 , training loss: 4.724778\n",
      "[INFO] Epoch: 52 , batch: 525 , training loss: 4.606605\n",
      "[INFO] Epoch: 52 , batch: 526 , training loss: 4.382246\n",
      "[INFO] Epoch: 52 , batch: 527 , training loss: 4.432175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 52 , batch: 528 , training loss: 4.439819\n",
      "[INFO] Epoch: 52 , batch: 529 , training loss: 4.419346\n",
      "[INFO] Epoch: 52 , batch: 530 , training loss: 4.261981\n",
      "[INFO] Epoch: 52 , batch: 531 , training loss: 4.426772\n",
      "[INFO] Epoch: 52 , batch: 532 , training loss: 4.337116\n",
      "[INFO] Epoch: 52 , batch: 533 , training loss: 4.438332\n",
      "[INFO] Epoch: 52 , batch: 534 , training loss: 4.446557\n",
      "[INFO] Epoch: 52 , batch: 535 , training loss: 4.456114\n",
      "[INFO] Epoch: 52 , batch: 536 , training loss: 4.302561\n",
      "[INFO] Epoch: 52 , batch: 537 , training loss: 4.298216\n",
      "[INFO] Epoch: 52 , batch: 538 , training loss: 4.376416\n",
      "[INFO] Epoch: 52 , batch: 539 , training loss: 4.468099\n",
      "[INFO] Epoch: 52 , batch: 540 , training loss: 4.984800\n",
      "[INFO] Epoch: 52 , batch: 541 , training loss: 4.847991\n",
      "[INFO] Epoch: 52 , batch: 542 , training loss: 4.702118\n",
      "[INFO] Epoch: 53 , batch: 0 , training loss: 3.610790\n",
      "[INFO] Epoch: 53 , batch: 1 , training loss: 3.547527\n",
      "[INFO] Epoch: 53 , batch: 2 , training loss: 3.697503\n",
      "[INFO] Epoch: 53 , batch: 3 , training loss: 3.644338\n",
      "[INFO] Epoch: 53 , batch: 4 , training loss: 3.992645\n",
      "[INFO] Epoch: 53 , batch: 5 , training loss: 3.637486\n",
      "[INFO] Epoch: 53 , batch: 6 , training loss: 4.016850\n",
      "[INFO] Epoch: 53 , batch: 7 , training loss: 3.915276\n",
      "[INFO] Epoch: 53 , batch: 8 , training loss: 3.596559\n",
      "[INFO] Epoch: 53 , batch: 9 , training loss: 3.854425\n",
      "[INFO] Epoch: 53 , batch: 10 , training loss: 3.815563\n",
      "[INFO] Epoch: 53 , batch: 11 , training loss: 3.770271\n",
      "[INFO] Epoch: 53 , batch: 12 , training loss: 3.649448\n",
      "[INFO] Epoch: 53 , batch: 13 , training loss: 3.676336\n",
      "[INFO] Epoch: 53 , batch: 14 , training loss: 3.591735\n",
      "[INFO] Epoch: 53 , batch: 15 , training loss: 3.791142\n",
      "[INFO] Epoch: 53 , batch: 16 , training loss: 3.613741\n",
      "[INFO] Epoch: 53 , batch: 17 , training loss: 3.749490\n",
      "[INFO] Epoch: 53 , batch: 18 , training loss: 3.711027\n",
      "[INFO] Epoch: 53 , batch: 19 , training loss: 3.477384\n",
      "[INFO] Epoch: 53 , batch: 20 , training loss: 3.445821\n",
      "[INFO] Epoch: 53 , batch: 21 , training loss: 3.574467\n",
      "[INFO] Epoch: 53 , batch: 22 , training loss: 3.442872\n",
      "[INFO] Epoch: 53 , batch: 23 , training loss: 3.694689\n",
      "[INFO] Epoch: 53 , batch: 24 , training loss: 3.554113\n",
      "[INFO] Epoch: 53 , batch: 25 , training loss: 3.595553\n",
      "[INFO] Epoch: 53 , batch: 26 , training loss: 3.498390\n",
      "[INFO] Epoch: 53 , batch: 27 , training loss: 3.503016\n",
      "[INFO] Epoch: 53 , batch: 28 , training loss: 3.690999\n",
      "[INFO] Epoch: 53 , batch: 29 , training loss: 3.492431\n",
      "[INFO] Epoch: 53 , batch: 30 , training loss: 3.529672\n",
      "[INFO] Epoch: 53 , batch: 31 , training loss: 3.605110\n",
      "[INFO] Epoch: 53 , batch: 32 , training loss: 3.562762\n",
      "[INFO] Epoch: 53 , batch: 33 , training loss: 3.596438\n",
      "[INFO] Epoch: 53 , batch: 34 , training loss: 3.594615\n",
      "[INFO] Epoch: 53 , batch: 35 , training loss: 3.527753\n",
      "[INFO] Epoch: 53 , batch: 36 , training loss: 3.650984\n",
      "[INFO] Epoch: 53 , batch: 37 , training loss: 3.516284\n",
      "[INFO] Epoch: 53 , batch: 38 , training loss: 3.592948\n",
      "[INFO] Epoch: 53 , batch: 39 , training loss: 3.384416\n",
      "[INFO] Epoch: 53 , batch: 40 , training loss: 3.601310\n",
      "[INFO] Epoch: 53 , batch: 41 , training loss: 3.564630\n",
      "[INFO] Epoch: 53 , batch: 42 , training loss: 4.033792\n",
      "[INFO] Epoch: 53 , batch: 43 , training loss: 3.786188\n",
      "[INFO] Epoch: 53 , batch: 44 , training loss: 4.097523\n",
      "[INFO] Epoch: 53 , batch: 45 , training loss: 4.045293\n",
      "[INFO] Epoch: 53 , batch: 46 , training loss: 4.075449\n",
      "[INFO] Epoch: 53 , batch: 47 , training loss: 3.583845\n",
      "[INFO] Epoch: 53 , batch: 48 , training loss: 3.619877\n",
      "[INFO] Epoch: 53 , batch: 49 , training loss: 3.833487\n",
      "[INFO] Epoch: 53 , batch: 50 , training loss: 3.558160\n",
      "[INFO] Epoch: 53 , batch: 51 , training loss: 3.816309\n",
      "[INFO] Epoch: 53 , batch: 52 , training loss: 3.623045\n",
      "[INFO] Epoch: 53 , batch: 53 , training loss: 3.750226\n",
      "[INFO] Epoch: 53 , batch: 54 , training loss: 3.773057\n",
      "[INFO] Epoch: 53 , batch: 55 , training loss: 3.841827\n",
      "[INFO] Epoch: 53 , batch: 56 , training loss: 3.648776\n",
      "[INFO] Epoch: 53 , batch: 57 , training loss: 3.585230\n",
      "[INFO] Epoch: 53 , batch: 58 , training loss: 3.639489\n",
      "[INFO] Epoch: 53 , batch: 59 , training loss: 3.718933\n",
      "[INFO] Epoch: 53 , batch: 60 , training loss: 3.654394\n",
      "[INFO] Epoch: 53 , batch: 61 , training loss: 3.703687\n",
      "[INFO] Epoch: 53 , batch: 62 , training loss: 3.622156\n",
      "[INFO] Epoch: 53 , batch: 63 , training loss: 3.807824\n",
      "[INFO] Epoch: 53 , batch: 64 , training loss: 4.011059\n",
      "[INFO] Epoch: 53 , batch: 65 , training loss: 3.724677\n",
      "[INFO] Epoch: 53 , batch: 66 , training loss: 3.581873\n",
      "[INFO] Epoch: 53 , batch: 67 , training loss: 3.598684\n",
      "[INFO] Epoch: 53 , batch: 68 , training loss: 3.753084\n",
      "[INFO] Epoch: 53 , batch: 69 , training loss: 3.692295\n",
      "[INFO] Epoch: 53 , batch: 70 , training loss: 3.938912\n",
      "[INFO] Epoch: 53 , batch: 71 , training loss: 3.762295\n",
      "[INFO] Epoch: 53 , batch: 72 , training loss: 3.831547\n",
      "[INFO] Epoch: 53 , batch: 73 , training loss: 3.792264\n",
      "[INFO] Epoch: 53 , batch: 74 , training loss: 3.852122\n",
      "[INFO] Epoch: 53 , batch: 75 , training loss: 3.760774\n",
      "[INFO] Epoch: 53 , batch: 76 , training loss: 3.816329\n",
      "[INFO] Epoch: 53 , batch: 77 , training loss: 3.791716\n",
      "[INFO] Epoch: 53 , batch: 78 , training loss: 3.906530\n",
      "[INFO] Epoch: 53 , batch: 79 , training loss: 3.740137\n",
      "[INFO] Epoch: 53 , batch: 80 , training loss: 3.933057\n",
      "[INFO] Epoch: 53 , batch: 81 , training loss: 3.863904\n",
      "[INFO] Epoch: 53 , batch: 82 , training loss: 3.844874\n",
      "[INFO] Epoch: 53 , batch: 83 , training loss: 3.913237\n",
      "[INFO] Epoch: 53 , batch: 84 , training loss: 3.927492\n",
      "[INFO] Epoch: 53 , batch: 85 , training loss: 3.977389\n",
      "[INFO] Epoch: 53 , batch: 86 , training loss: 3.916426\n",
      "[INFO] Epoch: 53 , batch: 87 , training loss: 3.877913\n",
      "[INFO] Epoch: 53 , batch: 88 , training loss: 4.007295\n",
      "[INFO] Epoch: 53 , batch: 89 , training loss: 3.794137\n",
      "[INFO] Epoch: 53 , batch: 90 , training loss: 3.864790\n",
      "[INFO] Epoch: 53 , batch: 91 , training loss: 3.815635\n",
      "[INFO] Epoch: 53 , batch: 92 , training loss: 3.844658\n",
      "[INFO] Epoch: 53 , batch: 93 , training loss: 3.949245\n",
      "[INFO] Epoch: 53 , batch: 94 , training loss: 4.057048\n",
      "[INFO] Epoch: 53 , batch: 95 , training loss: 3.861364\n",
      "[INFO] Epoch: 53 , batch: 96 , training loss: 3.842327\n",
      "[INFO] Epoch: 53 , batch: 97 , training loss: 3.752610\n",
      "[INFO] Epoch: 53 , batch: 98 , training loss: 3.719831\n",
      "[INFO] Epoch: 53 , batch: 99 , training loss: 3.855889\n",
      "[INFO] Epoch: 53 , batch: 100 , training loss: 3.722862\n",
      "[INFO] Epoch: 53 , batch: 101 , training loss: 3.764409\n",
      "[INFO] Epoch: 53 , batch: 102 , training loss: 3.895222\n",
      "[INFO] Epoch: 53 , batch: 103 , training loss: 3.730566\n",
      "[INFO] Epoch: 53 , batch: 104 , training loss: 3.653250\n",
      "[INFO] Epoch: 53 , batch: 105 , training loss: 3.897710\n",
      "[INFO] Epoch: 53 , batch: 106 , training loss: 3.937809\n",
      "[INFO] Epoch: 53 , batch: 107 , training loss: 3.794835\n",
      "[INFO] Epoch: 53 , batch: 108 , training loss: 3.716801\n",
      "[INFO] Epoch: 53 , batch: 109 , training loss: 3.646263\n",
      "[INFO] Epoch: 53 , batch: 110 , training loss: 3.828392\n",
      "[INFO] Epoch: 53 , batch: 111 , training loss: 3.894216\n",
      "[INFO] Epoch: 53 , batch: 112 , training loss: 3.804697\n",
      "[INFO] Epoch: 53 , batch: 113 , training loss: 3.801167\n",
      "[INFO] Epoch: 53 , batch: 114 , training loss: 3.790989\n",
      "[INFO] Epoch: 53 , batch: 115 , training loss: 3.815161\n",
      "[INFO] Epoch: 53 , batch: 116 , training loss: 3.718535\n",
      "[INFO] Epoch: 53 , batch: 117 , training loss: 3.911414\n",
      "[INFO] Epoch: 53 , batch: 118 , training loss: 3.911138\n",
      "[INFO] Epoch: 53 , batch: 119 , training loss: 4.034208\n",
      "[INFO] Epoch: 53 , batch: 120 , training loss: 4.031477\n",
      "[INFO] Epoch: 53 , batch: 121 , training loss: 3.888975\n",
      "[INFO] Epoch: 53 , batch: 122 , training loss: 3.819556\n",
      "[INFO] Epoch: 53 , batch: 123 , training loss: 3.778111\n",
      "[INFO] Epoch: 53 , batch: 124 , training loss: 3.845826\n",
      "[INFO] Epoch: 53 , batch: 125 , training loss: 3.741337\n",
      "[INFO] Epoch: 53 , batch: 126 , training loss: 3.715896\n",
      "[INFO] Epoch: 53 , batch: 127 , training loss: 3.739495\n",
      "[INFO] Epoch: 53 , batch: 128 , training loss: 3.842748\n",
      "[INFO] Epoch: 53 , batch: 129 , training loss: 3.797440\n",
      "[INFO] Epoch: 53 , batch: 130 , training loss: 3.833150\n",
      "[INFO] Epoch: 53 , batch: 131 , training loss: 3.815370\n",
      "[INFO] Epoch: 53 , batch: 132 , training loss: 3.821745\n",
      "[INFO] Epoch: 53 , batch: 133 , training loss: 3.797857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 53 , batch: 134 , training loss: 3.580096\n",
      "[INFO] Epoch: 53 , batch: 135 , training loss: 3.643549\n",
      "[INFO] Epoch: 53 , batch: 136 , training loss: 3.909375\n",
      "[INFO] Epoch: 53 , batch: 137 , training loss: 3.861624\n",
      "[INFO] Epoch: 53 , batch: 138 , training loss: 3.902566\n",
      "[INFO] Epoch: 53 , batch: 139 , training loss: 4.450416\n",
      "[INFO] Epoch: 53 , batch: 140 , training loss: 4.240007\n",
      "[INFO] Epoch: 53 , batch: 141 , training loss: 4.018833\n",
      "[INFO] Epoch: 53 , batch: 142 , training loss: 3.743179\n",
      "[INFO] Epoch: 53 , batch: 143 , training loss: 3.895237\n",
      "[INFO] Epoch: 53 , batch: 144 , training loss: 3.721239\n",
      "[INFO] Epoch: 53 , batch: 145 , training loss: 3.807203\n",
      "[INFO] Epoch: 53 , batch: 146 , training loss: 3.977875\n",
      "[INFO] Epoch: 53 , batch: 147 , training loss: 3.641577\n",
      "[INFO] Epoch: 53 , batch: 148 , training loss: 3.608057\n",
      "[INFO] Epoch: 53 , batch: 149 , training loss: 3.708474\n",
      "[INFO] Epoch: 53 , batch: 150 , training loss: 3.967648\n",
      "[INFO] Epoch: 53 , batch: 151 , training loss: 3.815088\n",
      "[INFO] Epoch: 53 , batch: 152 , training loss: 3.831747\n",
      "[INFO] Epoch: 53 , batch: 153 , training loss: 3.875219\n",
      "[INFO] Epoch: 53 , batch: 154 , training loss: 3.940909\n",
      "[INFO] Epoch: 53 , batch: 155 , training loss: 4.193693\n",
      "[INFO] Epoch: 53 , batch: 156 , training loss: 3.899818\n",
      "[INFO] Epoch: 53 , batch: 157 , training loss: 3.856582\n",
      "[INFO] Epoch: 53 , batch: 158 , training loss: 3.965174\n",
      "[INFO] Epoch: 53 , batch: 159 , training loss: 3.893643\n",
      "[INFO] Epoch: 53 , batch: 160 , training loss: 4.100325\n",
      "[INFO] Epoch: 53 , batch: 161 , training loss: 4.180931\n",
      "[INFO] Epoch: 53 , batch: 162 , training loss: 4.152950\n",
      "[INFO] Epoch: 53 , batch: 163 , training loss: 4.349606\n",
      "[INFO] Epoch: 53 , batch: 164 , training loss: 4.291713\n",
      "[INFO] Epoch: 53 , batch: 165 , training loss: 4.209307\n",
      "[INFO] Epoch: 53 , batch: 166 , training loss: 4.113948\n",
      "[INFO] Epoch: 53 , batch: 167 , training loss: 4.136383\n",
      "[INFO] Epoch: 53 , batch: 168 , training loss: 3.793273\n",
      "[INFO] Epoch: 53 , batch: 169 , training loss: 3.792967\n",
      "[INFO] Epoch: 53 , batch: 170 , training loss: 3.939788\n",
      "[INFO] Epoch: 53 , batch: 171 , training loss: 3.429884\n",
      "[INFO] Epoch: 53 , batch: 172 , training loss: 3.664009\n",
      "[INFO] Epoch: 53 , batch: 173 , training loss: 3.991902\n",
      "[INFO] Epoch: 53 , batch: 174 , training loss: 4.496088\n",
      "[INFO] Epoch: 53 , batch: 175 , training loss: 4.727573\n",
      "[INFO] Epoch: 53 , batch: 176 , training loss: 4.366747\n",
      "[INFO] Epoch: 53 , batch: 177 , training loss: 3.959459\n",
      "[INFO] Epoch: 53 , batch: 178 , training loss: 3.991518\n",
      "[INFO] Epoch: 53 , batch: 179 , training loss: 4.033414\n",
      "[INFO] Epoch: 53 , batch: 180 , training loss: 4.041327\n",
      "[INFO] Epoch: 53 , batch: 181 , training loss: 4.300578\n",
      "[INFO] Epoch: 53 , batch: 182 , training loss: 4.252450\n",
      "[INFO] Epoch: 53 , batch: 183 , training loss: 4.251242\n",
      "[INFO] Epoch: 53 , batch: 184 , training loss: 4.139141\n",
      "[INFO] Epoch: 53 , batch: 185 , training loss: 4.084848\n",
      "[INFO] Epoch: 53 , batch: 186 , training loss: 4.245821\n",
      "[INFO] Epoch: 53 , batch: 187 , training loss: 4.305064\n",
      "[INFO] Epoch: 53 , batch: 188 , training loss: 4.323185\n",
      "[INFO] Epoch: 53 , batch: 189 , training loss: 4.251956\n",
      "[INFO] Epoch: 53 , batch: 190 , training loss: 4.277666\n",
      "[INFO] Epoch: 53 , batch: 191 , training loss: 4.399979\n",
      "[INFO] Epoch: 53 , batch: 192 , training loss: 4.220083\n",
      "[INFO] Epoch: 53 , batch: 193 , training loss: 4.309346\n",
      "[INFO] Epoch: 53 , batch: 194 , training loss: 4.233250\n",
      "[INFO] Epoch: 53 , batch: 195 , training loss: 4.187093\n",
      "[INFO] Epoch: 53 , batch: 196 , training loss: 4.011515\n",
      "[INFO] Epoch: 53 , batch: 197 , training loss: 4.133063\n",
      "[INFO] Epoch: 53 , batch: 198 , training loss: 4.061380\n",
      "[INFO] Epoch: 53 , batch: 199 , training loss: 4.170251\n",
      "[INFO] Epoch: 53 , batch: 200 , training loss: 4.102873\n",
      "[INFO] Epoch: 53 , batch: 201 , training loss: 4.002705\n",
      "[INFO] Epoch: 53 , batch: 202 , training loss: 3.988058\n",
      "[INFO] Epoch: 53 , batch: 203 , training loss: 4.136232\n",
      "[INFO] Epoch: 53 , batch: 204 , training loss: 4.204078\n",
      "[INFO] Epoch: 53 , batch: 205 , training loss: 3.819610\n",
      "[INFO] Epoch: 53 , batch: 206 , training loss: 3.735184\n",
      "[INFO] Epoch: 53 , batch: 207 , training loss: 3.730047\n",
      "[INFO] Epoch: 53 , batch: 208 , training loss: 4.042171\n",
      "[INFO] Epoch: 53 , batch: 209 , training loss: 4.041040\n",
      "[INFO] Epoch: 53 , batch: 210 , training loss: 4.033068\n",
      "[INFO] Epoch: 53 , batch: 211 , training loss: 4.037595\n",
      "[INFO] Epoch: 53 , batch: 212 , training loss: 4.145660\n",
      "[INFO] Epoch: 53 , batch: 213 , training loss: 4.110474\n",
      "[INFO] Epoch: 53 , batch: 214 , training loss: 4.162925\n",
      "[INFO] Epoch: 53 , batch: 215 , training loss: 4.340649\n",
      "[INFO] Epoch: 53 , batch: 216 , training loss: 4.057188\n",
      "[INFO] Epoch: 53 , batch: 217 , training loss: 4.008893\n",
      "[INFO] Epoch: 53 , batch: 218 , training loss: 4.010624\n",
      "[INFO] Epoch: 53 , batch: 219 , training loss: 4.134389\n",
      "[INFO] Epoch: 53 , batch: 220 , training loss: 3.955924\n",
      "[INFO] Epoch: 53 , batch: 221 , training loss: 3.961866\n",
      "[INFO] Epoch: 53 , batch: 222 , training loss: 4.077399\n",
      "[INFO] Epoch: 53 , batch: 223 , training loss: 4.213645\n",
      "[INFO] Epoch: 53 , batch: 224 , training loss: 4.253725\n",
      "[INFO] Epoch: 53 , batch: 225 , training loss: 4.142366\n",
      "[INFO] Epoch: 53 , batch: 226 , training loss: 4.253879\n",
      "[INFO] Epoch: 53 , batch: 227 , training loss: 4.250139\n",
      "[INFO] Epoch: 53 , batch: 228 , training loss: 4.257732\n",
      "[INFO] Epoch: 53 , batch: 229 , training loss: 4.115197\n",
      "[INFO] Epoch: 53 , batch: 230 , training loss: 3.978109\n",
      "[INFO] Epoch: 53 , batch: 231 , training loss: 3.832336\n",
      "[INFO] Epoch: 53 , batch: 232 , training loss: 3.980246\n",
      "[INFO] Epoch: 53 , batch: 233 , training loss: 4.017846\n",
      "[INFO] Epoch: 53 , batch: 234 , training loss: 3.696859\n",
      "[INFO] Epoch: 53 , batch: 235 , training loss: 3.825759\n",
      "[INFO] Epoch: 53 , batch: 236 , training loss: 3.928670\n",
      "[INFO] Epoch: 53 , batch: 237 , training loss: 4.136568\n",
      "[INFO] Epoch: 53 , batch: 238 , training loss: 3.949392\n",
      "[INFO] Epoch: 53 , batch: 239 , training loss: 3.964368\n",
      "[INFO] Epoch: 53 , batch: 240 , training loss: 3.997374\n",
      "[INFO] Epoch: 53 , batch: 241 , training loss: 3.796206\n",
      "[INFO] Epoch: 53 , batch: 242 , training loss: 3.829563\n",
      "[INFO] Epoch: 53 , batch: 243 , training loss: 4.119497\n",
      "[INFO] Epoch: 53 , batch: 244 , training loss: 4.064473\n",
      "[INFO] Epoch: 53 , batch: 245 , training loss: 4.008788\n",
      "[INFO] Epoch: 53 , batch: 246 , training loss: 3.740922\n",
      "[INFO] Epoch: 53 , batch: 247 , training loss: 3.909553\n",
      "[INFO] Epoch: 53 , batch: 248 , training loss: 3.980521\n",
      "[INFO] Epoch: 53 , batch: 249 , training loss: 3.952780\n",
      "[INFO] Epoch: 53 , batch: 250 , training loss: 3.787842\n",
      "[INFO] Epoch: 53 , batch: 251 , training loss: 4.209161\n",
      "[INFO] Epoch: 53 , batch: 252 , training loss: 3.909259\n",
      "[INFO] Epoch: 53 , batch: 253 , training loss: 3.839427\n",
      "[INFO] Epoch: 53 , batch: 254 , training loss: 4.107831\n",
      "[INFO] Epoch: 53 , batch: 255 , training loss: 4.100133\n",
      "[INFO] Epoch: 53 , batch: 256 , training loss: 4.062485\n",
      "[INFO] Epoch: 53 , batch: 257 , training loss: 4.247169\n",
      "[INFO] Epoch: 53 , batch: 258 , training loss: 4.233600\n",
      "[INFO] Epoch: 53 , batch: 259 , training loss: 4.263377\n",
      "[INFO] Epoch: 53 , batch: 260 , training loss: 4.045806\n",
      "[INFO] Epoch: 53 , batch: 261 , training loss: 4.223376\n",
      "[INFO] Epoch: 53 , batch: 262 , training loss: 4.361416\n",
      "[INFO] Epoch: 53 , batch: 263 , training loss: 4.521449\n",
      "[INFO] Epoch: 53 , batch: 264 , training loss: 3.881591\n",
      "[INFO] Epoch: 53 , batch: 265 , training loss: 3.996336\n",
      "[INFO] Epoch: 53 , batch: 266 , training loss: 4.391533\n",
      "[INFO] Epoch: 53 , batch: 267 , training loss: 4.144788\n",
      "[INFO] Epoch: 53 , batch: 268 , training loss: 4.088027\n",
      "[INFO] Epoch: 53 , batch: 269 , training loss: 4.043934\n",
      "[INFO] Epoch: 53 , batch: 270 , training loss: 4.064743\n",
      "[INFO] Epoch: 53 , batch: 271 , training loss: 4.112537\n",
      "[INFO] Epoch: 53 , batch: 272 , training loss: 4.078659\n",
      "[INFO] Epoch: 53 , batch: 273 , training loss: 4.102811\n",
      "[INFO] Epoch: 53 , batch: 274 , training loss: 4.215752\n",
      "[INFO] Epoch: 53 , batch: 275 , training loss: 4.068674\n",
      "[INFO] Epoch: 53 , batch: 276 , training loss: 4.127839\n",
      "[INFO] Epoch: 53 , batch: 277 , training loss: 4.271450\n",
      "[INFO] Epoch: 53 , batch: 278 , training loss: 3.987498\n",
      "[INFO] Epoch: 53 , batch: 279 , training loss: 3.975640\n",
      "[INFO] Epoch: 53 , batch: 280 , training loss: 3.966672\n",
      "[INFO] Epoch: 53 , batch: 281 , training loss: 4.077341\n",
      "[INFO] Epoch: 53 , batch: 282 , training loss: 3.976368\n",
      "[INFO] Epoch: 53 , batch: 283 , training loss: 4.004725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 53 , batch: 284 , training loss: 4.030886\n",
      "[INFO] Epoch: 53 , batch: 285 , training loss: 3.970023\n",
      "[INFO] Epoch: 53 , batch: 286 , training loss: 3.978029\n",
      "[INFO] Epoch: 53 , batch: 287 , training loss: 3.907774\n",
      "[INFO] Epoch: 53 , batch: 288 , training loss: 3.882473\n",
      "[INFO] Epoch: 53 , batch: 289 , training loss: 3.980663\n",
      "[INFO] Epoch: 53 , batch: 290 , training loss: 3.749370\n",
      "[INFO] Epoch: 53 , batch: 291 , training loss: 3.738267\n",
      "[INFO] Epoch: 53 , batch: 292 , training loss: 3.846375\n",
      "[INFO] Epoch: 53 , batch: 293 , training loss: 3.764888\n",
      "[INFO] Epoch: 53 , batch: 294 , training loss: 4.422499\n",
      "[INFO] Epoch: 53 , batch: 295 , training loss: 4.188761\n",
      "[INFO] Epoch: 53 , batch: 296 , training loss: 4.130471\n",
      "[INFO] Epoch: 53 , batch: 297 , training loss: 4.089756\n",
      "[INFO] Epoch: 53 , batch: 298 , training loss: 3.928859\n",
      "[INFO] Epoch: 53 , batch: 299 , training loss: 3.952861\n",
      "[INFO] Epoch: 53 , batch: 300 , training loss: 3.966249\n",
      "[INFO] Epoch: 53 , batch: 301 , training loss: 3.857106\n",
      "[INFO] Epoch: 53 , batch: 302 , training loss: 4.044233\n",
      "[INFO] Epoch: 53 , batch: 303 , training loss: 4.052296\n",
      "[INFO] Epoch: 53 , batch: 304 , training loss: 4.175492\n",
      "[INFO] Epoch: 53 , batch: 305 , training loss: 4.032960\n",
      "[INFO] Epoch: 53 , batch: 306 , training loss: 4.136060\n",
      "[INFO] Epoch: 53 , batch: 307 , training loss: 4.162918\n",
      "[INFO] Epoch: 53 , batch: 308 , training loss: 3.964494\n",
      "[INFO] Epoch: 53 , batch: 309 , training loss: 3.952208\n",
      "[INFO] Epoch: 53 , batch: 310 , training loss: 3.890971\n",
      "[INFO] Epoch: 53 , batch: 311 , training loss: 3.893028\n",
      "[INFO] Epoch: 53 , batch: 312 , training loss: 3.793202\n",
      "[INFO] Epoch: 53 , batch: 313 , training loss: 3.898517\n",
      "[INFO] Epoch: 53 , batch: 314 , training loss: 3.961231\n",
      "[INFO] Epoch: 53 , batch: 315 , training loss: 4.042402\n",
      "[INFO] Epoch: 53 , batch: 316 , training loss: 4.283651\n",
      "[INFO] Epoch: 53 , batch: 317 , training loss: 4.628635\n",
      "[INFO] Epoch: 53 , batch: 318 , training loss: 4.729862\n",
      "[INFO] Epoch: 53 , batch: 319 , training loss: 4.444497\n",
      "[INFO] Epoch: 53 , batch: 320 , training loss: 3.981421\n",
      "[INFO] Epoch: 53 , batch: 321 , training loss: 3.822210\n",
      "[INFO] Epoch: 53 , batch: 322 , training loss: 3.926661\n",
      "[INFO] Epoch: 53 , batch: 323 , training loss: 3.970359\n",
      "[INFO] Epoch: 53 , batch: 324 , training loss: 3.934439\n",
      "[INFO] Epoch: 53 , batch: 325 , training loss: 4.037643\n",
      "[INFO] Epoch: 53 , batch: 326 , training loss: 4.110017\n",
      "[INFO] Epoch: 53 , batch: 327 , training loss: 4.042255\n",
      "[INFO] Epoch: 53 , batch: 328 , training loss: 4.051391\n",
      "[INFO] Epoch: 53 , batch: 329 , training loss: 3.960048\n",
      "[INFO] Epoch: 53 , batch: 330 , training loss: 3.945429\n",
      "[INFO] Epoch: 53 , batch: 331 , training loss: 4.095435\n",
      "[INFO] Epoch: 53 , batch: 332 , training loss: 3.954908\n",
      "[INFO] Epoch: 53 , batch: 333 , training loss: 3.910481\n",
      "[INFO] Epoch: 53 , batch: 334 , training loss: 3.931199\n",
      "[INFO] Epoch: 53 , batch: 335 , training loss: 4.054030\n",
      "[INFO] Epoch: 53 , batch: 336 , training loss: 4.070025\n",
      "[INFO] Epoch: 53 , batch: 337 , training loss: 4.118044\n",
      "[INFO] Epoch: 53 , batch: 338 , training loss: 4.311090\n",
      "[INFO] Epoch: 53 , batch: 339 , training loss: 4.128591\n",
      "[INFO] Epoch: 53 , batch: 340 , training loss: 4.331370\n",
      "[INFO] Epoch: 53 , batch: 341 , training loss: 4.076263\n",
      "[INFO] Epoch: 53 , batch: 342 , training loss: 3.877791\n",
      "[INFO] Epoch: 53 , batch: 343 , training loss: 3.947376\n",
      "[INFO] Epoch: 53 , batch: 344 , training loss: 3.814613\n",
      "[INFO] Epoch: 53 , batch: 345 , training loss: 3.932469\n",
      "[INFO] Epoch: 53 , batch: 346 , training loss: 3.985331\n",
      "[INFO] Epoch: 53 , batch: 347 , training loss: 3.898072\n",
      "[INFO] Epoch: 53 , batch: 348 , training loss: 3.977097\n",
      "[INFO] Epoch: 53 , batch: 349 , training loss: 4.070075\n",
      "[INFO] Epoch: 53 , batch: 350 , training loss: 3.938888\n",
      "[INFO] Epoch: 53 , batch: 351 , training loss: 4.034606\n",
      "[INFO] Epoch: 53 , batch: 352 , training loss: 4.024084\n",
      "[INFO] Epoch: 53 , batch: 353 , training loss: 4.033562\n",
      "[INFO] Epoch: 53 , batch: 354 , training loss: 4.100170\n",
      "[INFO] Epoch: 53 , batch: 355 , training loss: 4.100518\n",
      "[INFO] Epoch: 53 , batch: 356 , training loss: 3.969024\n",
      "[INFO] Epoch: 53 , batch: 357 , training loss: 4.035551\n",
      "[INFO] Epoch: 53 , batch: 358 , training loss: 3.948930\n",
      "[INFO] Epoch: 53 , batch: 359 , training loss: 3.948831\n",
      "[INFO] Epoch: 53 , batch: 360 , training loss: 4.053312\n",
      "[INFO] Epoch: 53 , batch: 361 , training loss: 4.025538\n",
      "[INFO] Epoch: 53 , batch: 362 , training loss: 4.136883\n",
      "[INFO] Epoch: 53 , batch: 363 , training loss: 4.021364\n",
      "[INFO] Epoch: 53 , batch: 364 , training loss: 4.057475\n",
      "[INFO] Epoch: 53 , batch: 365 , training loss: 3.977055\n",
      "[INFO] Epoch: 53 , batch: 366 , training loss: 4.074361\n",
      "[INFO] Epoch: 53 , batch: 367 , training loss: 4.133780\n",
      "[INFO] Epoch: 53 , batch: 368 , training loss: 4.554904\n",
      "[INFO] Epoch: 53 , batch: 369 , training loss: 4.209365\n",
      "[INFO] Epoch: 53 , batch: 370 , training loss: 3.982538\n",
      "[INFO] Epoch: 53 , batch: 371 , training loss: 4.366588\n",
      "[INFO] Epoch: 53 , batch: 372 , training loss: 4.641234\n",
      "[INFO] Epoch: 53 , batch: 373 , training loss: 4.648638\n",
      "[INFO] Epoch: 53 , batch: 374 , training loss: 4.837060\n",
      "[INFO] Epoch: 53 , batch: 375 , training loss: 4.791171\n",
      "[INFO] Epoch: 53 , batch: 376 , training loss: 4.689502\n",
      "[INFO] Epoch: 53 , batch: 377 , training loss: 4.451147\n",
      "[INFO] Epoch: 53 , batch: 378 , training loss: 4.546390\n",
      "[INFO] Epoch: 53 , batch: 379 , training loss: 4.511989\n",
      "[INFO] Epoch: 53 , batch: 380 , training loss: 4.706395\n",
      "[INFO] Epoch: 53 , batch: 381 , training loss: 4.387414\n",
      "[INFO] Epoch: 53 , batch: 382 , training loss: 4.634012\n",
      "[INFO] Epoch: 53 , batch: 383 , training loss: 4.697909\n",
      "[INFO] Epoch: 53 , batch: 384 , training loss: 4.633329\n",
      "[INFO] Epoch: 53 , batch: 385 , training loss: 4.331520\n",
      "[INFO] Epoch: 53 , batch: 386 , training loss: 4.579130\n",
      "[INFO] Epoch: 53 , batch: 387 , training loss: 4.531589\n",
      "[INFO] Epoch: 53 , batch: 388 , training loss: 4.373553\n",
      "[INFO] Epoch: 53 , batch: 389 , training loss: 4.173150\n",
      "[INFO] Epoch: 53 , batch: 390 , training loss: 4.188194\n",
      "[INFO] Epoch: 53 , batch: 391 , training loss: 4.212659\n",
      "[INFO] Epoch: 53 , batch: 392 , training loss: 4.595940\n",
      "[INFO] Epoch: 53 , batch: 393 , training loss: 4.467135\n",
      "[INFO] Epoch: 53 , batch: 394 , training loss: 4.570182\n",
      "[INFO] Epoch: 53 , batch: 395 , training loss: 4.379188\n",
      "[INFO] Epoch: 53 , batch: 396 , training loss: 4.191571\n",
      "[INFO] Epoch: 53 , batch: 397 , training loss: 4.332116\n",
      "[INFO] Epoch: 53 , batch: 398 , training loss: 4.215965\n",
      "[INFO] Epoch: 53 , batch: 399 , training loss: 4.290894\n",
      "[INFO] Epoch: 53 , batch: 400 , training loss: 4.255726\n",
      "[INFO] Epoch: 53 , batch: 401 , training loss: 4.703946\n",
      "[INFO] Epoch: 53 , batch: 402 , training loss: 4.408927\n",
      "[INFO] Epoch: 53 , batch: 403 , training loss: 4.244950\n",
      "[INFO] Epoch: 53 , batch: 404 , training loss: 4.451443\n",
      "[INFO] Epoch: 53 , batch: 405 , training loss: 4.480439\n",
      "[INFO] Epoch: 53 , batch: 406 , training loss: 4.379272\n",
      "[INFO] Epoch: 53 , batch: 407 , training loss: 4.406387\n",
      "[INFO] Epoch: 53 , batch: 408 , training loss: 4.378104\n",
      "[INFO] Epoch: 53 , batch: 409 , training loss: 4.417388\n",
      "[INFO] Epoch: 53 , batch: 410 , training loss: 4.447905\n",
      "[INFO] Epoch: 53 , batch: 411 , training loss: 4.611352\n",
      "[INFO] Epoch: 53 , batch: 412 , training loss: 4.449250\n",
      "[INFO] Epoch: 53 , batch: 413 , training loss: 4.334836\n",
      "[INFO] Epoch: 53 , batch: 414 , training loss: 4.376549\n",
      "[INFO] Epoch: 53 , batch: 415 , training loss: 4.424484\n",
      "[INFO] Epoch: 53 , batch: 416 , training loss: 4.462047\n",
      "[INFO] Epoch: 53 , batch: 417 , training loss: 4.387826\n",
      "[INFO] Epoch: 53 , batch: 418 , training loss: 4.453302\n",
      "[INFO] Epoch: 53 , batch: 419 , training loss: 4.403356\n",
      "[INFO] Epoch: 53 , batch: 420 , training loss: 4.379669\n",
      "[INFO] Epoch: 53 , batch: 421 , training loss: 4.365981\n",
      "[INFO] Epoch: 53 , batch: 422 , training loss: 4.215693\n",
      "[INFO] Epoch: 53 , batch: 423 , training loss: 4.452507\n",
      "[INFO] Epoch: 53 , batch: 424 , training loss: 4.614662\n",
      "[INFO] Epoch: 53 , batch: 425 , training loss: 4.487234\n",
      "[INFO] Epoch: 53 , batch: 426 , training loss: 4.229196\n",
      "[INFO] Epoch: 53 , batch: 427 , training loss: 4.463172\n",
      "[INFO] Epoch: 53 , batch: 428 , training loss: 4.315095\n",
      "[INFO] Epoch: 53 , batch: 429 , training loss: 4.226128\n",
      "[INFO] Epoch: 53 , batch: 430 , training loss: 4.464455\n",
      "[INFO] Epoch: 53 , batch: 431 , training loss: 4.076020\n",
      "[INFO] Epoch: 53 , batch: 432 , training loss: 4.111359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 53 , batch: 433 , training loss: 4.173895\n",
      "[INFO] Epoch: 53 , batch: 434 , training loss: 4.029911\n",
      "[INFO] Epoch: 53 , batch: 435 , training loss: 4.388618\n",
      "[INFO] Epoch: 53 , batch: 436 , training loss: 4.419536\n",
      "[INFO] Epoch: 53 , batch: 437 , training loss: 4.217496\n",
      "[INFO] Epoch: 53 , batch: 438 , training loss: 4.089121\n",
      "[INFO] Epoch: 53 , batch: 439 , training loss: 4.327029\n",
      "[INFO] Epoch: 53 , batch: 440 , training loss: 4.431402\n",
      "[INFO] Epoch: 53 , batch: 441 , training loss: 4.534055\n",
      "[INFO] Epoch: 53 , batch: 442 , training loss: 4.289551\n",
      "[INFO] Epoch: 53 , batch: 443 , training loss: 4.451623\n",
      "[INFO] Epoch: 53 , batch: 444 , training loss: 4.084174\n",
      "[INFO] Epoch: 53 , batch: 445 , training loss: 3.957250\n",
      "[INFO] Epoch: 53 , batch: 446 , training loss: 3.914588\n",
      "[INFO] Epoch: 53 , batch: 447 , training loss: 4.112214\n",
      "[INFO] Epoch: 53 , batch: 448 , training loss: 4.226576\n",
      "[INFO] Epoch: 53 , batch: 449 , training loss: 4.613267\n",
      "[INFO] Epoch: 53 , batch: 450 , training loss: 4.685223\n",
      "[INFO] Epoch: 53 , batch: 451 , training loss: 4.566951\n",
      "[INFO] Epoch: 53 , batch: 452 , training loss: 4.393766\n",
      "[INFO] Epoch: 53 , batch: 453 , training loss: 4.154714\n",
      "[INFO] Epoch: 53 , batch: 454 , training loss: 4.318355\n",
      "[INFO] Epoch: 53 , batch: 455 , training loss: 4.353813\n",
      "[INFO] Epoch: 53 , batch: 456 , training loss: 4.356146\n",
      "[INFO] Epoch: 53 , batch: 457 , training loss: 4.440300\n",
      "[INFO] Epoch: 53 , batch: 458 , training loss: 4.170181\n",
      "[INFO] Epoch: 53 , batch: 459 , training loss: 4.146002\n",
      "[INFO] Epoch: 53 , batch: 460 , training loss: 4.276148\n",
      "[INFO] Epoch: 53 , batch: 461 , training loss: 4.212355\n",
      "[INFO] Epoch: 53 , batch: 462 , training loss: 4.303231\n",
      "[INFO] Epoch: 53 , batch: 463 , training loss: 4.205556\n",
      "[INFO] Epoch: 53 , batch: 464 , training loss: 4.371920\n",
      "[INFO] Epoch: 53 , batch: 465 , training loss: 4.317078\n",
      "[INFO] Epoch: 53 , batch: 466 , training loss: 4.399589\n",
      "[INFO] Epoch: 53 , batch: 467 , training loss: 4.356019\n",
      "[INFO] Epoch: 53 , batch: 468 , training loss: 4.333957\n",
      "[INFO] Epoch: 53 , batch: 469 , training loss: 4.359518\n",
      "[INFO] Epoch: 53 , batch: 470 , training loss: 4.184083\n",
      "[INFO] Epoch: 53 , batch: 471 , training loss: 4.290779\n",
      "[INFO] Epoch: 53 , batch: 472 , training loss: 4.340528\n",
      "[INFO] Epoch: 53 , batch: 473 , training loss: 4.256949\n",
      "[INFO] Epoch: 53 , batch: 474 , training loss: 4.032276\n",
      "[INFO] Epoch: 53 , batch: 475 , training loss: 3.923946\n",
      "[INFO] Epoch: 53 , batch: 476 , training loss: 4.324664\n",
      "[INFO] Epoch: 53 , batch: 477 , training loss: 4.451374\n",
      "[INFO] Epoch: 53 , batch: 478 , training loss: 4.435162\n",
      "[INFO] Epoch: 53 , batch: 479 , training loss: 4.430746\n",
      "[INFO] Epoch: 53 , batch: 480 , training loss: 4.571445\n",
      "[INFO] Epoch: 53 , batch: 481 , training loss: 4.431183\n",
      "[INFO] Epoch: 53 , batch: 482 , training loss: 4.537564\n",
      "[INFO] Epoch: 53 , batch: 483 , training loss: 4.353784\n",
      "[INFO] Epoch: 53 , batch: 484 , training loss: 4.164879\n",
      "[INFO] Epoch: 53 , batch: 485 , training loss: 4.277870\n",
      "[INFO] Epoch: 53 , batch: 486 , training loss: 4.168483\n",
      "[INFO] Epoch: 53 , batch: 487 , training loss: 4.145683\n",
      "[INFO] Epoch: 53 , batch: 488 , training loss: 4.326730\n",
      "[INFO] Epoch: 53 , batch: 489 , training loss: 4.239754\n",
      "[INFO] Epoch: 53 , batch: 490 , training loss: 4.292466\n",
      "[INFO] Epoch: 53 , batch: 491 , training loss: 4.224313\n",
      "[INFO] Epoch: 53 , batch: 492 , training loss: 4.202234\n",
      "[INFO] Epoch: 53 , batch: 493 , training loss: 4.346244\n",
      "[INFO] Epoch: 53 , batch: 494 , training loss: 4.272549\n",
      "[INFO] Epoch: 53 , batch: 495 , training loss: 4.436599\n",
      "[INFO] Epoch: 53 , batch: 496 , training loss: 4.302784\n",
      "[INFO] Epoch: 53 , batch: 497 , training loss: 4.332485\n",
      "[INFO] Epoch: 53 , batch: 498 , training loss: 4.318025\n",
      "[INFO] Epoch: 53 , batch: 499 , training loss: 4.384461\n",
      "[INFO] Epoch: 53 , batch: 500 , training loss: 4.524934\n",
      "[INFO] Epoch: 53 , batch: 501 , training loss: 4.845683\n",
      "[INFO] Epoch: 53 , batch: 502 , training loss: 4.873052\n",
      "[INFO] Epoch: 53 , batch: 503 , training loss: 4.493531\n",
      "[INFO] Epoch: 53 , batch: 504 , training loss: 4.645359\n",
      "[INFO] Epoch: 53 , batch: 505 , training loss: 4.615430\n",
      "[INFO] Epoch: 53 , batch: 506 , training loss: 4.603248\n",
      "[INFO] Epoch: 53 , batch: 507 , training loss: 4.655679\n",
      "[INFO] Epoch: 53 , batch: 508 , training loss: 4.555624\n",
      "[INFO] Epoch: 53 , batch: 509 , training loss: 4.377698\n",
      "[INFO] Epoch: 53 , batch: 510 , training loss: 4.481872\n",
      "[INFO] Epoch: 53 , batch: 511 , training loss: 4.389324\n",
      "[INFO] Epoch: 53 , batch: 512 , training loss: 4.478496\n",
      "[INFO] Epoch: 53 , batch: 513 , training loss: 4.746182\n",
      "[INFO] Epoch: 53 , batch: 514 , training loss: 4.386478\n",
      "[INFO] Epoch: 53 , batch: 515 , training loss: 4.645095\n",
      "[INFO] Epoch: 53 , batch: 516 , training loss: 4.449337\n",
      "[INFO] Epoch: 53 , batch: 517 , training loss: 4.413587\n",
      "[INFO] Epoch: 53 , batch: 518 , training loss: 4.366846\n",
      "[INFO] Epoch: 53 , batch: 519 , training loss: 4.198434\n",
      "[INFO] Epoch: 53 , batch: 520 , training loss: 4.447997\n",
      "[INFO] Epoch: 53 , batch: 521 , training loss: 4.411056\n",
      "[INFO] Epoch: 53 , batch: 522 , training loss: 4.499370\n",
      "[INFO] Epoch: 53 , batch: 523 , training loss: 4.415982\n",
      "[INFO] Epoch: 53 , batch: 524 , training loss: 4.729313\n",
      "[INFO] Epoch: 53 , batch: 525 , training loss: 4.601522\n",
      "[INFO] Epoch: 53 , batch: 526 , training loss: 4.371794\n",
      "[INFO] Epoch: 53 , batch: 527 , training loss: 4.409471\n",
      "[INFO] Epoch: 53 , batch: 528 , training loss: 4.439037\n",
      "[INFO] Epoch: 53 , batch: 529 , training loss: 4.412967\n",
      "[INFO] Epoch: 53 , batch: 530 , training loss: 4.255966\n",
      "[INFO] Epoch: 53 , batch: 531 , training loss: 4.409935\n",
      "[INFO] Epoch: 53 , batch: 532 , training loss: 4.325139\n",
      "[INFO] Epoch: 53 , batch: 533 , training loss: 4.424867\n",
      "[INFO] Epoch: 53 , batch: 534 , training loss: 4.445750\n",
      "[INFO] Epoch: 53 , batch: 535 , training loss: 4.449288\n",
      "[INFO] Epoch: 53 , batch: 536 , training loss: 4.287862\n",
      "[INFO] Epoch: 53 , batch: 537 , training loss: 4.290639\n",
      "[INFO] Epoch: 53 , batch: 538 , training loss: 4.370754\n",
      "[INFO] Epoch: 53 , batch: 539 , training loss: 4.451159\n",
      "[INFO] Epoch: 53 , batch: 540 , training loss: 5.022072\n",
      "[INFO] Epoch: 53 , batch: 541 , training loss: 4.823294\n",
      "[INFO] Epoch: 53 , batch: 542 , training loss: 4.716306\n",
      "[INFO] Epoch: 54 , batch: 0 , training loss: 3.627724\n",
      "[INFO] Epoch: 54 , batch: 1 , training loss: 3.585781\n",
      "[INFO] Epoch: 54 , batch: 2 , training loss: 3.680386\n",
      "[INFO] Epoch: 54 , batch: 3 , training loss: 3.617019\n",
      "[INFO] Epoch: 54 , batch: 4 , training loss: 3.941345\n",
      "[INFO] Epoch: 54 , batch: 5 , training loss: 3.597151\n",
      "[INFO] Epoch: 54 , batch: 6 , training loss: 4.063400\n",
      "[INFO] Epoch: 54 , batch: 7 , training loss: 3.907288\n",
      "[INFO] Epoch: 54 , batch: 8 , training loss: 3.591492\n",
      "[INFO] Epoch: 54 , batch: 9 , training loss: 3.830477\n",
      "[INFO] Epoch: 54 , batch: 10 , training loss: 3.809355\n",
      "[INFO] Epoch: 54 , batch: 11 , training loss: 3.741203\n",
      "[INFO] Epoch: 54 , batch: 12 , training loss: 3.635466\n",
      "[INFO] Epoch: 54 , batch: 13 , training loss: 3.660395\n",
      "[INFO] Epoch: 54 , batch: 14 , training loss: 3.566192\n",
      "[INFO] Epoch: 54 , batch: 15 , training loss: 3.788416\n",
      "[INFO] Epoch: 54 , batch: 16 , training loss: 3.613736\n",
      "[INFO] Epoch: 54 , batch: 17 , training loss: 3.735077\n",
      "[INFO] Epoch: 54 , batch: 18 , training loss: 3.703570\n",
      "[INFO] Epoch: 54 , batch: 19 , training loss: 3.461142\n",
      "[INFO] Epoch: 54 , batch: 20 , training loss: 3.424408\n",
      "[INFO] Epoch: 54 , batch: 21 , training loss: 3.574291\n",
      "[INFO] Epoch: 54 , batch: 22 , training loss: 3.436189\n",
      "[INFO] Epoch: 54 , batch: 23 , training loss: 3.664937\n",
      "[INFO] Epoch: 54 , batch: 24 , training loss: 3.522596\n",
      "[INFO] Epoch: 54 , batch: 25 , training loss: 3.612638\n",
      "[INFO] Epoch: 54 , batch: 26 , training loss: 3.487020\n",
      "[INFO] Epoch: 54 , batch: 27 , training loss: 3.498422\n",
      "[INFO] Epoch: 54 , batch: 28 , training loss: 3.708005\n",
      "[INFO] Epoch: 54 , batch: 29 , training loss: 3.500280\n",
      "[INFO] Epoch: 54 , batch: 30 , training loss: 3.528282\n",
      "[INFO] Epoch: 54 , batch: 31 , training loss: 3.606224\n",
      "[INFO] Epoch: 54 , batch: 32 , training loss: 3.571739\n",
      "[INFO] Epoch: 54 , batch: 33 , training loss: 3.589501\n",
      "[INFO] Epoch: 54 , batch: 34 , training loss: 3.596977\n",
      "[INFO] Epoch: 54 , batch: 35 , training loss: 3.518957\n",
      "[INFO] Epoch: 54 , batch: 36 , training loss: 3.651583\n",
      "[INFO] Epoch: 54 , batch: 37 , training loss: 3.507122\n",
      "[INFO] Epoch: 54 , batch: 38 , training loss: 3.600933\n",
      "[INFO] Epoch: 54 , batch: 39 , training loss: 3.403449\n",
      "[INFO] Epoch: 54 , batch: 40 , training loss: 3.587039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 54 , batch: 41 , training loss: 3.554371\n",
      "[INFO] Epoch: 54 , batch: 42 , training loss: 3.983018\n",
      "[INFO] Epoch: 54 , batch: 43 , training loss: 3.738821\n",
      "[INFO] Epoch: 54 , batch: 44 , training loss: 4.098156\n",
      "[INFO] Epoch: 54 , batch: 45 , training loss: 4.078734\n",
      "[INFO] Epoch: 54 , batch: 46 , training loss: 4.009336\n",
      "[INFO] Epoch: 54 , batch: 47 , training loss: 3.552441\n",
      "[INFO] Epoch: 54 , batch: 48 , training loss: 3.582609\n",
      "[INFO] Epoch: 54 , batch: 49 , training loss: 3.813240\n",
      "[INFO] Epoch: 54 , batch: 50 , training loss: 3.545354\n",
      "[INFO] Epoch: 54 , batch: 51 , training loss: 3.805148\n",
      "[INFO] Epoch: 54 , batch: 52 , training loss: 3.618866\n",
      "[INFO] Epoch: 54 , batch: 53 , training loss: 3.748023\n",
      "[INFO] Epoch: 54 , batch: 54 , training loss: 3.756549\n",
      "[INFO] Epoch: 54 , batch: 55 , training loss: 3.856560\n",
      "[INFO] Epoch: 54 , batch: 56 , training loss: 3.678534\n",
      "[INFO] Epoch: 54 , batch: 57 , training loss: 3.632417\n",
      "[INFO] Epoch: 54 , batch: 58 , training loss: 3.648534\n",
      "[INFO] Epoch: 54 , batch: 59 , training loss: 3.736330\n",
      "[INFO] Epoch: 54 , batch: 60 , training loss: 3.696969\n",
      "[INFO] Epoch: 54 , batch: 61 , training loss: 3.780192\n",
      "[INFO] Epoch: 54 , batch: 62 , training loss: 3.638216\n",
      "[INFO] Epoch: 54 , batch: 63 , training loss: 3.866989\n",
      "[INFO] Epoch: 54 , batch: 64 , training loss: 4.064397\n",
      "[INFO] Epoch: 54 , batch: 65 , training loss: 3.759448\n",
      "[INFO] Epoch: 54 , batch: 66 , training loss: 3.590499\n",
      "[INFO] Epoch: 54 , batch: 67 , training loss: 3.657351\n",
      "[INFO] Epoch: 54 , batch: 68 , training loss: 3.823180\n",
      "[INFO] Epoch: 54 , batch: 69 , training loss: 3.751954\n",
      "[INFO] Epoch: 54 , batch: 70 , training loss: 3.983046\n",
      "[INFO] Epoch: 54 , batch: 71 , training loss: 3.819884\n",
      "[INFO] Epoch: 54 , batch: 72 , training loss: 3.871583\n",
      "[INFO] Epoch: 54 , batch: 73 , training loss: 3.811496\n",
      "[INFO] Epoch: 54 , batch: 74 , training loss: 3.909447\n",
      "[INFO] Epoch: 54 , batch: 75 , training loss: 3.804917\n",
      "[INFO] Epoch: 54 , batch: 76 , training loss: 3.882775\n",
      "[INFO] Epoch: 54 , batch: 77 , training loss: 3.823747\n",
      "[INFO] Epoch: 54 , batch: 78 , training loss: 3.908531\n",
      "[INFO] Epoch: 54 , batch: 79 , training loss: 3.782616\n",
      "[INFO] Epoch: 54 , batch: 80 , training loss: 3.975851\n",
      "[INFO] Epoch: 54 , batch: 81 , training loss: 3.905534\n",
      "[INFO] Epoch: 54 , batch: 82 , training loss: 3.858754\n",
      "[INFO] Epoch: 54 , batch: 83 , training loss: 3.932781\n",
      "[INFO] Epoch: 54 , batch: 84 , training loss: 3.932795\n",
      "[INFO] Epoch: 54 , batch: 85 , training loss: 4.011094\n",
      "[INFO] Epoch: 54 , batch: 86 , training loss: 3.950527\n",
      "[INFO] Epoch: 54 , batch: 87 , training loss: 3.910589\n",
      "[INFO] Epoch: 54 , batch: 88 , training loss: 4.059988\n",
      "[INFO] Epoch: 54 , batch: 89 , training loss: 3.822735\n",
      "[INFO] Epoch: 54 , batch: 90 , training loss: 3.907366\n",
      "[INFO] Epoch: 54 , batch: 91 , training loss: 3.834559\n",
      "[INFO] Epoch: 54 , batch: 92 , training loss: 3.863600\n",
      "[INFO] Epoch: 54 , batch: 93 , training loss: 3.977009\n",
      "[INFO] Epoch: 54 , batch: 94 , training loss: 4.090546\n",
      "[INFO] Epoch: 54 , batch: 95 , training loss: 3.897530\n",
      "[INFO] Epoch: 54 , batch: 96 , training loss: 3.885318\n",
      "[INFO] Epoch: 54 , batch: 97 , training loss: 3.802386\n",
      "[INFO] Epoch: 54 , batch: 98 , training loss: 3.745630\n",
      "[INFO] Epoch: 54 , batch: 99 , training loss: 3.869498\n",
      "[INFO] Epoch: 54 , batch: 100 , training loss: 3.757693\n",
      "[INFO] Epoch: 54 , batch: 101 , training loss: 3.785202\n",
      "[INFO] Epoch: 54 , batch: 102 , training loss: 3.925761\n",
      "[INFO] Epoch: 54 , batch: 103 , training loss: 3.754126\n",
      "[INFO] Epoch: 54 , batch: 104 , training loss: 3.696614\n",
      "[INFO] Epoch: 54 , batch: 105 , training loss: 3.930711\n",
      "[INFO] Epoch: 54 , batch: 106 , training loss: 3.997458\n",
      "[INFO] Epoch: 54 , batch: 107 , training loss: 3.827884\n",
      "[INFO] Epoch: 54 , batch: 108 , training loss: 3.762689\n",
      "[INFO] Epoch: 54 , batch: 109 , training loss: 3.668588\n",
      "[INFO] Epoch: 54 , batch: 110 , training loss: 3.846284\n",
      "[INFO] Epoch: 54 , batch: 111 , training loss: 3.915982\n",
      "[INFO] Epoch: 54 , batch: 112 , training loss: 3.849304\n",
      "[INFO] Epoch: 54 , batch: 113 , training loss: 3.862129\n",
      "[INFO] Epoch: 54 , batch: 114 , training loss: 3.853467\n",
      "[INFO] Epoch: 54 , batch: 115 , training loss: 3.838488\n",
      "[INFO] Epoch: 54 , batch: 116 , training loss: 3.746590\n",
      "[INFO] Epoch: 54 , batch: 117 , training loss: 3.963331\n",
      "[INFO] Epoch: 54 , batch: 118 , training loss: 3.923349\n",
      "[INFO] Epoch: 54 , batch: 119 , training loss: 4.050948\n",
      "[INFO] Epoch: 54 , batch: 120 , training loss: 4.052362\n",
      "[INFO] Epoch: 54 , batch: 121 , training loss: 3.937007\n",
      "[INFO] Epoch: 54 , batch: 122 , training loss: 3.834057\n",
      "[INFO] Epoch: 54 , batch: 123 , training loss: 3.809660\n",
      "[INFO] Epoch: 54 , batch: 124 , training loss: 3.915060\n",
      "[INFO] Epoch: 54 , batch: 125 , training loss: 3.745043\n",
      "[INFO] Epoch: 54 , batch: 126 , training loss: 3.768220\n",
      "[INFO] Epoch: 54 , batch: 127 , training loss: 3.787372\n",
      "[INFO] Epoch: 54 , batch: 128 , training loss: 3.883529\n",
      "[INFO] Epoch: 54 , batch: 129 , training loss: 3.825847\n",
      "[INFO] Epoch: 54 , batch: 130 , training loss: 3.845668\n",
      "[INFO] Epoch: 54 , batch: 131 , training loss: 3.871482\n",
      "[INFO] Epoch: 54 , batch: 132 , training loss: 3.852391\n",
      "[INFO] Epoch: 54 , batch: 133 , training loss: 3.834340\n",
      "[INFO] Epoch: 54 , batch: 134 , training loss: 3.602985\n",
      "[INFO] Epoch: 54 , batch: 135 , training loss: 3.679007\n",
      "[INFO] Epoch: 54 , batch: 136 , training loss: 3.922476\n",
      "[INFO] Epoch: 54 , batch: 137 , training loss: 3.888347\n",
      "[INFO] Epoch: 54 , batch: 138 , training loss: 3.946221\n",
      "[INFO] Epoch: 54 , batch: 139 , training loss: 4.480034\n",
      "[INFO] Epoch: 54 , batch: 140 , training loss: 4.281157\n",
      "[INFO] Epoch: 54 , batch: 141 , training loss: 4.026208\n",
      "[INFO] Epoch: 54 , batch: 142 , training loss: 3.765614\n",
      "[INFO] Epoch: 54 , batch: 143 , training loss: 3.920780\n",
      "[INFO] Epoch: 54 , batch: 144 , training loss: 3.739994\n",
      "[INFO] Epoch: 54 , batch: 145 , training loss: 3.812426\n",
      "[INFO] Epoch: 54 , batch: 146 , training loss: 4.028511\n",
      "[INFO] Epoch: 54 , batch: 147 , training loss: 3.669660\n",
      "[INFO] Epoch: 54 , batch: 148 , training loss: 3.636016\n",
      "[INFO] Epoch: 54 , batch: 149 , training loss: 3.693719\n",
      "[INFO] Epoch: 54 , batch: 150 , training loss: 4.008661\n",
      "[INFO] Epoch: 54 , batch: 151 , training loss: 3.829984\n",
      "[INFO] Epoch: 54 , batch: 152 , training loss: 3.837973\n",
      "[INFO] Epoch: 54 , batch: 153 , training loss: 3.870989\n",
      "[INFO] Epoch: 54 , batch: 154 , training loss: 3.991565\n",
      "[INFO] Epoch: 54 , batch: 155 , training loss: 4.225947\n",
      "[INFO] Epoch: 54 , batch: 156 , training loss: 3.955503\n",
      "[INFO] Epoch: 54 , batch: 157 , training loss: 3.861857\n",
      "[INFO] Epoch: 54 , batch: 158 , training loss: 4.038182\n",
      "[INFO] Epoch: 54 , batch: 159 , training loss: 3.929815\n",
      "[INFO] Epoch: 54 , batch: 160 , training loss: 4.128678\n",
      "[INFO] Epoch: 54 , batch: 161 , training loss: 4.201663\n",
      "[INFO] Epoch: 54 , batch: 162 , training loss: 4.207085\n",
      "[INFO] Epoch: 54 , batch: 163 , training loss: 4.409310\n",
      "[INFO] Epoch: 54 , batch: 164 , training loss: 4.326521\n",
      "[INFO] Epoch: 54 , batch: 165 , training loss: 4.245118\n",
      "[INFO] Epoch: 54 , batch: 166 , training loss: 4.178898\n",
      "[INFO] Epoch: 54 , batch: 167 , training loss: 4.220838\n",
      "[INFO] Epoch: 54 , batch: 168 , training loss: 3.893853\n",
      "[INFO] Epoch: 54 , batch: 169 , training loss: 3.878073\n",
      "[INFO] Epoch: 54 , batch: 170 , training loss: 4.019763\n",
      "[INFO] Epoch: 54 , batch: 171 , training loss: 3.525404\n",
      "[INFO] Epoch: 54 , batch: 172 , training loss: 3.772474\n",
      "[INFO] Epoch: 54 , batch: 173 , training loss: 4.053619\n",
      "[INFO] Epoch: 54 , batch: 174 , training loss: 4.521026\n",
      "[INFO] Epoch: 54 , batch: 175 , training loss: 4.775834\n",
      "[INFO] Epoch: 54 , batch: 176 , training loss: 4.445278\n",
      "[INFO] Epoch: 54 , batch: 177 , training loss: 4.025301\n",
      "[INFO] Epoch: 54 , batch: 178 , training loss: 4.037180\n",
      "[INFO] Epoch: 54 , batch: 179 , training loss: 4.103514\n",
      "[INFO] Epoch: 54 , batch: 180 , training loss: 4.077693\n",
      "[INFO] Epoch: 54 , batch: 181 , training loss: 4.360380\n",
      "[INFO] Epoch: 54 , batch: 182 , training loss: 4.292315\n",
      "[INFO] Epoch: 54 , batch: 183 , training loss: 4.262786\n",
      "[INFO] Epoch: 54 , batch: 184 , training loss: 4.162743\n",
      "[INFO] Epoch: 54 , batch: 185 , training loss: 4.121483\n",
      "[INFO] Epoch: 54 , batch: 186 , training loss: 4.266562\n",
      "[INFO] Epoch: 54 , batch: 187 , training loss: 4.336071\n",
      "[INFO] Epoch: 54 , batch: 188 , training loss: 4.355540\n",
      "[INFO] Epoch: 54 , batch: 189 , training loss: 4.282311\n",
      "[INFO] Epoch: 54 , batch: 190 , training loss: 4.341208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 54 , batch: 191 , training loss: 4.423223\n",
      "[INFO] Epoch: 54 , batch: 192 , training loss: 4.262409\n",
      "[INFO] Epoch: 54 , batch: 193 , training loss: 4.330890\n",
      "[INFO] Epoch: 54 , batch: 194 , training loss: 4.292007\n",
      "[INFO] Epoch: 54 , batch: 195 , training loss: 4.253356\n",
      "[INFO] Epoch: 54 , batch: 196 , training loss: 4.072853\n",
      "[INFO] Epoch: 54 , batch: 197 , training loss: 4.186442\n",
      "[INFO] Epoch: 54 , batch: 198 , training loss: 4.088473\n",
      "[INFO] Epoch: 54 , batch: 199 , training loss: 4.234177\n",
      "[INFO] Epoch: 54 , batch: 200 , training loss: 4.164030\n",
      "[INFO] Epoch: 54 , batch: 201 , training loss: 4.042979\n",
      "[INFO] Epoch: 54 , batch: 202 , training loss: 4.021925\n",
      "[INFO] Epoch: 54 , batch: 203 , training loss: 4.158066\n",
      "[INFO] Epoch: 54 , batch: 204 , training loss: 4.249160\n",
      "[INFO] Epoch: 54 , batch: 205 , training loss: 3.855875\n",
      "[INFO] Epoch: 54 , batch: 206 , training loss: 3.787740\n",
      "[INFO] Epoch: 54 , batch: 207 , training loss: 3.778571\n",
      "[INFO] Epoch: 54 , batch: 208 , training loss: 4.114011\n",
      "[INFO] Epoch: 54 , batch: 209 , training loss: 4.100430\n",
      "[INFO] Epoch: 54 , batch: 210 , training loss: 4.054558\n",
      "[INFO] Epoch: 54 , batch: 211 , training loss: 4.088964\n",
      "[INFO] Epoch: 54 , batch: 212 , training loss: 4.186050\n",
      "[INFO] Epoch: 54 , batch: 213 , training loss: 4.136856\n",
      "[INFO] Epoch: 54 , batch: 214 , training loss: 4.219932\n",
      "[INFO] Epoch: 54 , batch: 215 , training loss: 4.382521\n",
      "[INFO] Epoch: 54 , batch: 216 , training loss: 4.107169\n",
      "[INFO] Epoch: 54 , batch: 217 , training loss: 4.051543\n",
      "[INFO] Epoch: 54 , batch: 218 , training loss: 4.049873\n",
      "[INFO] Epoch: 54 , batch: 219 , training loss: 4.169789\n",
      "[INFO] Epoch: 54 , batch: 220 , training loss: 3.971033\n",
      "[INFO] Epoch: 54 , batch: 221 , training loss: 3.975523\n",
      "[INFO] Epoch: 54 , batch: 222 , training loss: 4.113902\n",
      "[INFO] Epoch: 54 , batch: 223 , training loss: 4.278400\n",
      "[INFO] Epoch: 54 , batch: 224 , training loss: 4.267491\n",
      "[INFO] Epoch: 54 , batch: 225 , training loss: 4.185882\n",
      "[INFO] Epoch: 54 , batch: 226 , training loss: 4.285629\n",
      "[INFO] Epoch: 54 , batch: 227 , training loss: 4.290678\n",
      "[INFO] Epoch: 54 , batch: 228 , training loss: 4.264916\n",
      "[INFO] Epoch: 54 , batch: 229 , training loss: 4.167685\n",
      "[INFO] Epoch: 54 , batch: 230 , training loss: 4.002615\n",
      "[INFO] Epoch: 54 , batch: 231 , training loss: 3.858282\n",
      "[INFO] Epoch: 54 , batch: 232 , training loss: 3.990205\n",
      "[INFO] Epoch: 54 , batch: 233 , training loss: 4.054780\n",
      "[INFO] Epoch: 54 , batch: 234 , training loss: 3.738557\n",
      "[INFO] Epoch: 54 , batch: 235 , training loss: 3.828224\n",
      "[INFO] Epoch: 54 , batch: 236 , training loss: 3.943590\n",
      "[INFO] Epoch: 54 , batch: 237 , training loss: 4.160598\n",
      "[INFO] Epoch: 54 , batch: 238 , training loss: 3.962481\n",
      "[INFO] Epoch: 54 , batch: 239 , training loss: 3.980659\n",
      "[INFO] Epoch: 54 , batch: 240 , training loss: 4.028182\n",
      "[INFO] Epoch: 54 , batch: 241 , training loss: 3.825762\n",
      "[INFO] Epoch: 54 , batch: 242 , training loss: 3.851196\n",
      "[INFO] Epoch: 54 , batch: 243 , training loss: 4.137978\n",
      "[INFO] Epoch: 54 , batch: 244 , training loss: 4.102017\n",
      "[INFO] Epoch: 54 , batch: 245 , training loss: 4.017462\n",
      "[INFO] Epoch: 54 , batch: 246 , training loss: 3.761775\n",
      "[INFO] Epoch: 54 , batch: 247 , training loss: 3.922625\n",
      "[INFO] Epoch: 54 , batch: 248 , training loss: 4.034013\n",
      "[INFO] Epoch: 54 , batch: 249 , training loss: 3.984413\n",
      "[INFO] Epoch: 54 , batch: 250 , training loss: 3.803977\n",
      "[INFO] Epoch: 54 , batch: 251 , training loss: 4.238676\n",
      "[INFO] Epoch: 54 , batch: 252 , training loss: 3.925595\n",
      "[INFO] Epoch: 54 , batch: 253 , training loss: 3.864160\n",
      "[INFO] Epoch: 54 , batch: 254 , training loss: 4.114430\n",
      "[INFO] Epoch: 54 , batch: 255 , training loss: 4.108104\n",
      "[INFO] Epoch: 54 , batch: 256 , training loss: 4.071332\n",
      "[INFO] Epoch: 54 , batch: 257 , training loss: 4.276928\n",
      "[INFO] Epoch: 54 , batch: 258 , training loss: 4.240850\n",
      "[INFO] Epoch: 54 , batch: 259 , training loss: 4.307680\n",
      "[INFO] Epoch: 54 , batch: 260 , training loss: 4.064227\n",
      "[INFO] Epoch: 54 , batch: 261 , training loss: 4.267975\n",
      "[INFO] Epoch: 54 , batch: 262 , training loss: 4.372834\n",
      "[INFO] Epoch: 54 , batch: 263 , training loss: 4.567441\n",
      "[INFO] Epoch: 54 , batch: 264 , training loss: 3.902492\n",
      "[INFO] Epoch: 54 , batch: 265 , training loss: 4.025713\n",
      "[INFO] Epoch: 54 , batch: 266 , training loss: 4.429980\n",
      "[INFO] Epoch: 54 , batch: 267 , training loss: 4.161599\n",
      "[INFO] Epoch: 54 , batch: 268 , training loss: 4.103962\n",
      "[INFO] Epoch: 54 , batch: 269 , training loss: 4.075612\n",
      "[INFO] Epoch: 54 , batch: 270 , training loss: 4.089302\n",
      "[INFO] Epoch: 54 , batch: 271 , training loss: 4.132706\n",
      "[INFO] Epoch: 54 , batch: 272 , training loss: 4.125652\n",
      "[INFO] Epoch: 54 , batch: 273 , training loss: 4.141249\n",
      "[INFO] Epoch: 54 , batch: 274 , training loss: 4.243667\n",
      "[INFO] Epoch: 54 , batch: 275 , training loss: 4.117670\n",
      "[INFO] Epoch: 54 , batch: 276 , training loss: 4.156165\n",
      "[INFO] Epoch: 54 , batch: 277 , training loss: 4.323154\n",
      "[INFO] Epoch: 54 , batch: 278 , training loss: 4.010333\n",
      "[INFO] Epoch: 54 , batch: 279 , training loss: 4.011132\n",
      "[INFO] Epoch: 54 , batch: 280 , training loss: 3.991794\n",
      "[INFO] Epoch: 54 , batch: 281 , training loss: 4.103430\n",
      "[INFO] Epoch: 54 , batch: 282 , training loss: 4.026404\n",
      "[INFO] Epoch: 54 , batch: 283 , training loss: 4.021727\n",
      "[INFO] Epoch: 54 , batch: 284 , training loss: 4.054534\n",
      "[INFO] Epoch: 54 , batch: 285 , training loss: 4.002735\n",
      "[INFO] Epoch: 54 , batch: 286 , training loss: 3.989449\n",
      "[INFO] Epoch: 54 , batch: 287 , training loss: 3.946981\n",
      "[INFO] Epoch: 54 , batch: 288 , training loss: 3.916663\n",
      "[INFO] Epoch: 54 , batch: 289 , training loss: 3.991826\n",
      "[INFO] Epoch: 54 , batch: 290 , training loss: 3.781880\n",
      "[INFO] Epoch: 54 , batch: 291 , training loss: 3.779057\n",
      "[INFO] Epoch: 54 , batch: 292 , training loss: 3.865833\n",
      "[INFO] Epoch: 54 , batch: 293 , training loss: 3.784271\n",
      "[INFO] Epoch: 54 , batch: 294 , training loss: 4.454310\n",
      "[INFO] Epoch: 54 , batch: 295 , training loss: 4.227139\n",
      "[INFO] Epoch: 54 , batch: 296 , training loss: 4.171932\n",
      "[INFO] Epoch: 54 , batch: 297 , training loss: 4.125543\n",
      "[INFO] Epoch: 54 , batch: 298 , training loss: 3.957129\n",
      "[INFO] Epoch: 54 , batch: 299 , training loss: 3.989813\n",
      "[INFO] Epoch: 54 , batch: 300 , training loss: 3.990821\n",
      "[INFO] Epoch: 54 , batch: 301 , training loss: 3.890814\n",
      "[INFO] Epoch: 54 , batch: 302 , training loss: 4.072153\n",
      "[INFO] Epoch: 54 , batch: 303 , training loss: 4.064542\n",
      "[INFO] Epoch: 54 , batch: 304 , training loss: 4.199434\n",
      "[INFO] Epoch: 54 , batch: 305 , training loss: 4.046637\n",
      "[INFO] Epoch: 54 , batch: 306 , training loss: 4.160788\n",
      "[INFO] Epoch: 54 , batch: 307 , training loss: 4.171650\n",
      "[INFO] Epoch: 54 , batch: 308 , training loss: 3.991328\n",
      "[INFO] Epoch: 54 , batch: 309 , training loss: 3.983215\n",
      "[INFO] Epoch: 54 , batch: 310 , training loss: 3.904673\n",
      "[INFO] Epoch: 54 , batch: 311 , training loss: 3.913809\n",
      "[INFO] Epoch: 54 , batch: 312 , training loss: 3.817568\n",
      "[INFO] Epoch: 54 , batch: 313 , training loss: 3.934865\n",
      "[INFO] Epoch: 54 , batch: 314 , training loss: 3.974287\n",
      "[INFO] Epoch: 54 , batch: 315 , training loss: 4.065869\n",
      "[INFO] Epoch: 54 , batch: 316 , training loss: 4.305939\n",
      "[INFO] Epoch: 54 , batch: 317 , training loss: 4.662361\n",
      "[INFO] Epoch: 54 , batch: 318 , training loss: 4.784872\n",
      "[INFO] Epoch: 54 , batch: 319 , training loss: 4.481514\n",
      "[INFO] Epoch: 54 , batch: 320 , training loss: 4.005439\n",
      "[INFO] Epoch: 54 , batch: 321 , training loss: 3.847108\n",
      "[INFO] Epoch: 54 , batch: 322 , training loss: 3.938902\n",
      "[INFO] Epoch: 54 , batch: 323 , training loss: 3.997219\n",
      "[INFO] Epoch: 54 , batch: 324 , training loss: 3.931782\n",
      "[INFO] Epoch: 54 , batch: 325 , training loss: 4.077963\n",
      "[INFO] Epoch: 54 , batch: 326 , training loss: 4.136333\n",
      "[INFO] Epoch: 54 , batch: 327 , training loss: 4.068767\n",
      "[INFO] Epoch: 54 , batch: 328 , training loss: 4.066553\n",
      "[INFO] Epoch: 54 , batch: 329 , training loss: 3.984139\n",
      "[INFO] Epoch: 54 , batch: 330 , training loss: 3.971700\n",
      "[INFO] Epoch: 54 , batch: 331 , training loss: 4.118833\n",
      "[INFO] Epoch: 54 , batch: 332 , training loss: 3.958815\n",
      "[INFO] Epoch: 54 , batch: 333 , training loss: 3.926656\n",
      "[INFO] Epoch: 54 , batch: 334 , training loss: 3.964203\n",
      "[INFO] Epoch: 54 , batch: 335 , training loss: 4.074661\n",
      "[INFO] Epoch: 54 , batch: 336 , training loss: 4.082410\n",
      "[INFO] Epoch: 54 , batch: 337 , training loss: 4.125868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 54 , batch: 338 , training loss: 4.350954\n",
      "[INFO] Epoch: 54 , batch: 339 , training loss: 4.146971\n",
      "[INFO] Epoch: 54 , batch: 340 , training loss: 4.330907\n",
      "[INFO] Epoch: 54 , batch: 341 , training loss: 4.100265\n",
      "[INFO] Epoch: 54 , batch: 342 , training loss: 3.901505\n",
      "[INFO] Epoch: 54 , batch: 343 , training loss: 3.979049\n",
      "[INFO] Epoch: 54 , batch: 344 , training loss: 3.838895\n",
      "[INFO] Epoch: 54 , batch: 345 , training loss: 3.952304\n",
      "[INFO] Epoch: 54 , batch: 346 , training loss: 4.018098\n",
      "[INFO] Epoch: 54 , batch: 347 , training loss: 3.925636\n",
      "[INFO] Epoch: 54 , batch: 348 , training loss: 4.008394\n",
      "[INFO] Epoch: 54 , batch: 349 , training loss: 4.103591\n",
      "[INFO] Epoch: 54 , batch: 350 , training loss: 3.966219\n",
      "[INFO] Epoch: 54 , batch: 351 , training loss: 4.054429\n",
      "[INFO] Epoch: 54 , batch: 352 , training loss: 4.053273\n",
      "[INFO] Epoch: 54 , batch: 353 , training loss: 4.047402\n",
      "[INFO] Epoch: 54 , batch: 354 , training loss: 4.105886\n",
      "[INFO] Epoch: 54 , batch: 355 , training loss: 4.143098\n",
      "[INFO] Epoch: 54 , batch: 356 , training loss: 3.988837\n",
      "[INFO] Epoch: 54 , batch: 357 , training loss: 4.060445\n",
      "[INFO] Epoch: 54 , batch: 358 , training loss: 3.956645\n",
      "[INFO] Epoch: 54 , batch: 359 , training loss: 3.990766\n",
      "[INFO] Epoch: 54 , batch: 360 , training loss: 4.072071\n",
      "[INFO] Epoch: 54 , batch: 361 , training loss: 4.038066\n",
      "[INFO] Epoch: 54 , batch: 362 , training loss: 4.134893\n",
      "[INFO] Epoch: 54 , batch: 363 , training loss: 4.030402\n",
      "[INFO] Epoch: 54 , batch: 364 , training loss: 4.074450\n",
      "[INFO] Epoch: 54 , batch: 365 , training loss: 4.008150\n",
      "[INFO] Epoch: 54 , batch: 366 , training loss: 4.106135\n",
      "[INFO] Epoch: 54 , batch: 367 , training loss: 4.163681\n",
      "[INFO] Epoch: 54 , batch: 368 , training loss: 4.572367\n",
      "[INFO] Epoch: 54 , batch: 369 , training loss: 4.241325\n",
      "[INFO] Epoch: 54 , batch: 370 , training loss: 3.985954\n",
      "[INFO] Epoch: 54 , batch: 371 , training loss: 4.453124\n",
      "[INFO] Epoch: 54 , batch: 372 , training loss: 4.679135\n",
      "[INFO] Epoch: 54 , batch: 373 , training loss: 4.714273\n",
      "[INFO] Epoch: 54 , batch: 374 , training loss: 4.907217\n",
      "[INFO] Epoch: 54 , batch: 375 , training loss: 4.862782\n",
      "[INFO] Epoch: 54 , batch: 376 , training loss: 4.705337\n",
      "[INFO] Epoch: 54 , batch: 377 , training loss: 4.484668\n",
      "[INFO] Epoch: 54 , batch: 378 , training loss: 4.562269\n",
      "[INFO] Epoch: 54 , batch: 379 , training loss: 4.530392\n",
      "[INFO] Epoch: 54 , batch: 380 , training loss: 4.710772\n",
      "[INFO] Epoch: 54 , batch: 381 , training loss: 4.390699\n",
      "[INFO] Epoch: 54 , batch: 382 , training loss: 4.643596\n",
      "[INFO] Epoch: 54 , batch: 383 , training loss: 4.714834\n",
      "[INFO] Epoch: 54 , batch: 384 , training loss: 4.653173\n",
      "[INFO] Epoch: 54 , batch: 385 , training loss: 4.369621\n",
      "[INFO] Epoch: 54 , batch: 386 , training loss: 4.604079\n",
      "[INFO] Epoch: 54 , batch: 387 , training loss: 4.552498\n",
      "[INFO] Epoch: 54 , batch: 388 , training loss: 4.358929\n",
      "[INFO] Epoch: 54 , batch: 389 , training loss: 4.180618\n",
      "[INFO] Epoch: 54 , batch: 390 , training loss: 4.204324\n",
      "[INFO] Epoch: 54 , batch: 391 , training loss: 4.243889\n",
      "[INFO] Epoch: 54 , batch: 392 , training loss: 4.595979\n",
      "[INFO] Epoch: 54 , batch: 393 , training loss: 4.489568\n",
      "[INFO] Epoch: 54 , batch: 394 , training loss: 4.608933\n",
      "[INFO] Epoch: 54 , batch: 395 , training loss: 4.402597\n",
      "[INFO] Epoch: 54 , batch: 396 , training loss: 4.201270\n",
      "[INFO] Epoch: 54 , batch: 397 , training loss: 4.362371\n",
      "[INFO] Epoch: 54 , batch: 398 , training loss: 4.233914\n",
      "[INFO] Epoch: 54 , batch: 399 , training loss: 4.314733\n",
      "[INFO] Epoch: 54 , batch: 400 , training loss: 4.282080\n",
      "[INFO] Epoch: 54 , batch: 401 , training loss: 4.726242\n",
      "[INFO] Epoch: 54 , batch: 402 , training loss: 4.435817\n",
      "[INFO] Epoch: 54 , batch: 403 , training loss: 4.281958\n",
      "[INFO] Epoch: 54 , batch: 404 , training loss: 4.459240\n",
      "[INFO] Epoch: 54 , batch: 405 , training loss: 4.506198\n",
      "[INFO] Epoch: 54 , batch: 406 , training loss: 4.415977\n",
      "[INFO] Epoch: 54 , batch: 407 , training loss: 4.427579\n",
      "[INFO] Epoch: 54 , batch: 408 , training loss: 4.419793\n",
      "[INFO] Epoch: 54 , batch: 409 , training loss: 4.427364\n",
      "[INFO] Epoch: 54 , batch: 410 , training loss: 4.493494\n",
      "[INFO] Epoch: 54 , batch: 411 , training loss: 4.680745\n",
      "[INFO] Epoch: 54 , batch: 412 , training loss: 4.489232\n",
      "[INFO] Epoch: 54 , batch: 413 , training loss: 4.370251\n",
      "[INFO] Epoch: 54 , batch: 414 , training loss: 4.408910\n",
      "[INFO] Epoch: 54 , batch: 415 , training loss: 4.455862\n",
      "[INFO] Epoch: 54 , batch: 416 , training loss: 4.493493\n",
      "[INFO] Epoch: 54 , batch: 417 , training loss: 4.431134\n",
      "[INFO] Epoch: 54 , batch: 418 , training loss: 4.495012\n",
      "[INFO] Epoch: 54 , batch: 419 , training loss: 4.416987\n",
      "[INFO] Epoch: 54 , batch: 420 , training loss: 4.421882\n",
      "[INFO] Epoch: 54 , batch: 421 , training loss: 4.390430\n",
      "[INFO] Epoch: 54 , batch: 422 , training loss: 4.236479\n",
      "[INFO] Epoch: 54 , batch: 423 , training loss: 4.486541\n",
      "[INFO] Epoch: 54 , batch: 424 , training loss: 4.630438\n",
      "[INFO] Epoch: 54 , batch: 425 , training loss: 4.545932\n",
      "[INFO] Epoch: 54 , batch: 426 , training loss: 4.261992\n",
      "[INFO] Epoch: 54 , batch: 427 , training loss: 4.486580\n",
      "[INFO] Epoch: 54 , batch: 428 , training loss: 4.366917\n",
      "[INFO] Epoch: 54 , batch: 429 , training loss: 4.251933\n",
      "[INFO] Epoch: 54 , batch: 430 , training loss: 4.507376\n",
      "[INFO] Epoch: 54 , batch: 431 , training loss: 4.124452\n",
      "[INFO] Epoch: 54 , batch: 432 , training loss: 4.162517\n",
      "[INFO] Epoch: 54 , batch: 433 , training loss: 4.224065\n",
      "[INFO] Epoch: 54 , batch: 434 , training loss: 4.080246\n",
      "[INFO] Epoch: 54 , batch: 435 , training loss: 4.421223\n",
      "[INFO] Epoch: 54 , batch: 436 , training loss: 4.468891\n",
      "[INFO] Epoch: 54 , batch: 437 , training loss: 4.250814\n",
      "[INFO] Epoch: 54 , batch: 438 , training loss: 4.112472\n",
      "[INFO] Epoch: 54 , batch: 439 , training loss: 4.370097\n",
      "[INFO] Epoch: 54 , batch: 440 , training loss: 4.457303\n",
      "[INFO] Epoch: 54 , batch: 441 , training loss: 4.579633\n",
      "[INFO] Epoch: 54 , batch: 442 , training loss: 4.333285\n",
      "[INFO] Epoch: 54 , batch: 443 , training loss: 4.488562\n",
      "[INFO] Epoch: 54 , batch: 444 , training loss: 4.127114\n",
      "[INFO] Epoch: 54 , batch: 445 , training loss: 4.005730\n",
      "[INFO] Epoch: 54 , batch: 446 , training loss: 3.951590\n",
      "[INFO] Epoch: 54 , batch: 447 , training loss: 4.147563\n",
      "[INFO] Epoch: 54 , batch: 448 , training loss: 4.269690\n",
      "[INFO] Epoch: 54 , batch: 449 , training loss: 4.655726\n",
      "[INFO] Epoch: 54 , batch: 450 , training loss: 4.735830\n",
      "[INFO] Epoch: 54 , batch: 451 , training loss: 4.607282\n",
      "[INFO] Epoch: 54 , batch: 452 , training loss: 4.456182\n",
      "[INFO] Epoch: 54 , batch: 453 , training loss: 4.202613\n",
      "[INFO] Epoch: 54 , batch: 454 , training loss: 4.349601\n",
      "[INFO] Epoch: 54 , batch: 455 , training loss: 4.377366\n",
      "[INFO] Epoch: 54 , batch: 456 , training loss: 4.389669\n",
      "[INFO] Epoch: 54 , batch: 457 , training loss: 4.483943\n",
      "[INFO] Epoch: 54 , batch: 458 , training loss: 4.220492\n",
      "[INFO] Epoch: 54 , batch: 459 , training loss: 4.200992\n",
      "[INFO] Epoch: 54 , batch: 460 , training loss: 4.311848\n",
      "[INFO] Epoch: 54 , batch: 461 , training loss: 4.278451\n",
      "[INFO] Epoch: 54 , batch: 462 , training loss: 4.337808\n",
      "[INFO] Epoch: 54 , batch: 463 , training loss: 4.239858\n",
      "[INFO] Epoch: 54 , batch: 464 , training loss: 4.418477\n",
      "[INFO] Epoch: 54 , batch: 465 , training loss: 4.357068\n",
      "[INFO] Epoch: 54 , batch: 466 , training loss: 4.456724\n",
      "[INFO] Epoch: 54 , batch: 467 , training loss: 4.424274\n",
      "[INFO] Epoch: 54 , batch: 468 , training loss: 4.381889\n",
      "[INFO] Epoch: 54 , batch: 469 , training loss: 4.413361\n",
      "[INFO] Epoch: 54 , batch: 470 , training loss: 4.214920\n",
      "[INFO] Epoch: 54 , batch: 471 , training loss: 4.330626\n",
      "[INFO] Epoch: 54 , batch: 472 , training loss: 4.375308\n",
      "[INFO] Epoch: 54 , batch: 473 , training loss: 4.308309\n",
      "[INFO] Epoch: 54 , batch: 474 , training loss: 4.095151\n",
      "[INFO] Epoch: 54 , batch: 475 , training loss: 3.967117\n",
      "[INFO] Epoch: 54 , batch: 476 , training loss: 4.354805\n",
      "[INFO] Epoch: 54 , batch: 477 , training loss: 4.472007\n",
      "[INFO] Epoch: 54 , batch: 478 , training loss: 4.473141\n",
      "[INFO] Epoch: 54 , batch: 479 , training loss: 4.467218\n",
      "[INFO] Epoch: 54 , batch: 480 , training loss: 4.607706\n",
      "[INFO] Epoch: 54 , batch: 481 , training loss: 4.462058\n",
      "[INFO] Epoch: 54 , batch: 482 , training loss: 4.568942\n",
      "[INFO] Epoch: 54 , batch: 483 , training loss: 4.408533\n",
      "[INFO] Epoch: 54 , batch: 484 , training loss: 4.227075\n",
      "[INFO] Epoch: 54 , batch: 485 , training loss: 4.312043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 54 , batch: 486 , training loss: 4.212724\n",
      "[INFO] Epoch: 54 , batch: 487 , training loss: 4.197955\n",
      "[INFO] Epoch: 54 , batch: 488 , training loss: 4.355649\n",
      "[INFO] Epoch: 54 , batch: 489 , training loss: 4.282892\n",
      "[INFO] Epoch: 54 , batch: 490 , training loss: 4.344649\n",
      "[INFO] Epoch: 54 , batch: 491 , training loss: 4.275965\n",
      "[INFO] Epoch: 54 , batch: 492 , training loss: 4.226143\n",
      "[INFO] Epoch: 54 , batch: 493 , training loss: 4.399585\n",
      "[INFO] Epoch: 54 , batch: 494 , training loss: 4.296070\n",
      "[INFO] Epoch: 54 , batch: 495 , training loss: 4.474543\n",
      "[INFO] Epoch: 54 , batch: 496 , training loss: 4.345701\n",
      "[INFO] Epoch: 54 , batch: 497 , training loss: 4.363639\n",
      "[INFO] Epoch: 54 , batch: 498 , training loss: 4.346762\n",
      "[INFO] Epoch: 54 , batch: 499 , training loss: 4.429509\n",
      "[INFO] Epoch: 54 , batch: 500 , training loss: 4.575481\n",
      "[INFO] Epoch: 54 , batch: 501 , training loss: 4.905984\n",
      "[INFO] Epoch: 54 , batch: 502 , training loss: 4.923761\n",
      "[INFO] Epoch: 54 , batch: 503 , training loss: 4.566540\n",
      "[INFO] Epoch: 54 , batch: 504 , training loss: 4.746404\n",
      "[INFO] Epoch: 54 , batch: 505 , training loss: 4.684947\n",
      "[INFO] Epoch: 54 , batch: 506 , training loss: 4.667337\n",
      "[INFO] Epoch: 54 , batch: 507 , training loss: 4.722337\n",
      "[INFO] Epoch: 54 , batch: 508 , training loss: 4.633284\n",
      "[INFO] Epoch: 54 , batch: 509 , training loss: 4.443448\n",
      "[INFO] Epoch: 54 , batch: 510 , training loss: 4.531703\n",
      "[INFO] Epoch: 54 , batch: 511 , training loss: 4.466005\n",
      "[INFO] Epoch: 54 , batch: 512 , training loss: 4.528033\n",
      "[INFO] Epoch: 54 , batch: 513 , training loss: 4.802931\n",
      "[INFO] Epoch: 54 , batch: 514 , training loss: 4.432076\n",
      "[INFO] Epoch: 54 , batch: 515 , training loss: 4.690147\n",
      "[INFO] Epoch: 54 , batch: 516 , training loss: 4.484723\n",
      "[INFO] Epoch: 54 , batch: 517 , training loss: 4.441386\n",
      "[INFO] Epoch: 54 , batch: 518 , training loss: 4.401942\n",
      "[INFO] Epoch: 54 , batch: 519 , training loss: 4.244788\n",
      "[INFO] Epoch: 54 , batch: 520 , training loss: 4.484154\n",
      "[INFO] Epoch: 54 , batch: 521 , training loss: 4.448569\n",
      "[INFO] Epoch: 54 , batch: 522 , training loss: 4.548450\n",
      "[INFO] Epoch: 54 , batch: 523 , training loss: 4.474260\n",
      "[INFO] Epoch: 54 , batch: 524 , training loss: 4.751482\n",
      "[INFO] Epoch: 54 , batch: 525 , training loss: 4.636127\n",
      "[INFO] Epoch: 54 , batch: 526 , training loss: 4.416997\n",
      "[INFO] Epoch: 54 , batch: 527 , training loss: 4.445170\n",
      "[INFO] Epoch: 54 , batch: 528 , training loss: 4.464313\n",
      "[INFO] Epoch: 54 , batch: 529 , training loss: 4.432045\n",
      "[INFO] Epoch: 54 , batch: 530 , training loss: 4.290684\n",
      "[INFO] Epoch: 54 , batch: 531 , training loss: 4.463878\n",
      "[INFO] Epoch: 54 , batch: 532 , training loss: 4.357978\n",
      "[INFO] Epoch: 54 , batch: 533 , training loss: 4.472112\n",
      "[INFO] Epoch: 54 , batch: 534 , training loss: 4.475781\n",
      "[INFO] Epoch: 54 , batch: 535 , training loss: 4.472754\n",
      "[INFO] Epoch: 54 , batch: 536 , training loss: 4.340184\n",
      "[INFO] Epoch: 54 , batch: 537 , training loss: 4.309855\n",
      "[INFO] Epoch: 54 , batch: 538 , training loss: 4.410561\n",
      "[INFO] Epoch: 54 , batch: 539 , training loss: 4.494217\n",
      "[INFO] Epoch: 54 , batch: 540 , training loss: 5.023319\n",
      "[INFO] Epoch: 54 , batch: 541 , training loss: 4.861212\n",
      "[INFO] Epoch: 54 , batch: 542 , training loss: 4.737548\n",
      "[INFO] Epoch: 55 , batch: 0 , training loss: 3.781769\n",
      "[INFO] Epoch: 55 , batch: 1 , training loss: 3.616482\n",
      "[INFO] Epoch: 55 , batch: 2 , training loss: 3.746084\n",
      "[INFO] Epoch: 55 , batch: 3 , training loss: 3.672746\n",
      "[INFO] Epoch: 55 , batch: 4 , training loss: 4.004667\n",
      "[INFO] Epoch: 55 , batch: 5 , training loss: 3.664271\n",
      "[INFO] Epoch: 55 , batch: 6 , training loss: 4.095646\n",
      "[INFO] Epoch: 55 , batch: 7 , training loss: 3.961977\n",
      "[INFO] Epoch: 55 , batch: 8 , training loss: 3.621081\n",
      "[INFO] Epoch: 55 , batch: 9 , training loss: 3.883337\n",
      "[INFO] Epoch: 55 , batch: 10 , training loss: 3.847222\n",
      "[INFO] Epoch: 55 , batch: 11 , training loss: 3.785267\n",
      "[INFO] Epoch: 55 , batch: 12 , training loss: 3.688095\n",
      "[INFO] Epoch: 55 , batch: 13 , training loss: 3.704468\n",
      "[INFO] Epoch: 55 , batch: 14 , training loss: 3.615614\n",
      "[INFO] Epoch: 55 , batch: 15 , training loss: 3.834475\n",
      "[INFO] Epoch: 55 , batch: 16 , training loss: 3.638291\n",
      "[INFO] Epoch: 55 , batch: 17 , training loss: 3.772963\n",
      "[INFO] Epoch: 55 , batch: 18 , training loss: 3.726662\n",
      "[INFO] Epoch: 55 , batch: 19 , training loss: 3.491164\n",
      "[INFO] Epoch: 55 , batch: 20 , training loss: 3.482701\n",
      "[INFO] Epoch: 55 , batch: 21 , training loss: 3.599203\n",
      "[INFO] Epoch: 55 , batch: 22 , training loss: 3.515814\n",
      "[INFO] Epoch: 55 , batch: 23 , training loss: 3.713153\n",
      "[INFO] Epoch: 55 , batch: 24 , training loss: 3.570598\n",
      "[INFO] Epoch: 55 , batch: 25 , training loss: 3.662311\n",
      "[INFO] Epoch: 55 , batch: 26 , training loss: 3.526411\n",
      "[INFO] Epoch: 55 , batch: 27 , training loss: 3.506160\n",
      "[INFO] Epoch: 55 , batch: 28 , training loss: 3.699153\n",
      "[INFO] Epoch: 55 , batch: 29 , training loss: 3.546190\n",
      "[INFO] Epoch: 55 , batch: 30 , training loss: 3.542678\n",
      "[INFO] Epoch: 55 , batch: 31 , training loss: 3.635540\n",
      "[INFO] Epoch: 55 , batch: 32 , training loss: 3.606216\n",
      "[INFO] Epoch: 55 , batch: 33 , training loss: 3.624099\n",
      "[INFO] Epoch: 55 , batch: 34 , training loss: 3.663245\n",
      "[INFO] Epoch: 55 , batch: 35 , training loss: 3.562102\n",
      "[INFO] Epoch: 55 , batch: 36 , training loss: 3.669693\n",
      "[INFO] Epoch: 55 , batch: 37 , training loss: 3.521742\n",
      "[INFO] Epoch: 55 , batch: 38 , training loss: 3.628553\n",
      "[INFO] Epoch: 55 , batch: 39 , training loss: 3.450743\n",
      "[INFO] Epoch: 55 , batch: 40 , training loss: 3.630823\n",
      "[INFO] Epoch: 55 , batch: 41 , training loss: 3.629646\n",
      "[INFO] Epoch: 55 , batch: 42 , training loss: 4.113915\n",
      "[INFO] Epoch: 55 , batch: 43 , training loss: 3.829489\n",
      "[INFO] Epoch: 55 , batch: 44 , training loss: 4.176479\n",
      "[INFO] Epoch: 55 , batch: 45 , training loss: 4.098859\n",
      "[INFO] Epoch: 55 , batch: 46 , training loss: 4.082870\n",
      "[INFO] Epoch: 55 , batch: 47 , training loss: 3.654931\n",
      "[INFO] Epoch: 55 , batch: 48 , training loss: 3.678433\n",
      "[INFO] Epoch: 55 , batch: 49 , training loss: 3.834865\n",
      "[INFO] Epoch: 55 , batch: 50 , training loss: 3.590296\n",
      "[INFO] Epoch: 55 , batch: 51 , training loss: 3.811265\n",
      "[INFO] Epoch: 55 , batch: 52 , training loss: 3.636268\n",
      "[INFO] Epoch: 55 , batch: 53 , training loss: 3.764170\n",
      "[INFO] Epoch: 55 , batch: 54 , training loss: 3.789114\n",
      "[INFO] Epoch: 55 , batch: 55 , training loss: 3.830369\n",
      "[INFO] Epoch: 55 , batch: 56 , training loss: 3.685178\n",
      "[INFO] Epoch: 55 , batch: 57 , training loss: 3.625608\n",
      "[INFO] Epoch: 55 , batch: 58 , training loss: 3.675948\n",
      "[INFO] Epoch: 55 , batch: 59 , training loss: 3.751745\n",
      "[INFO] Epoch: 55 , batch: 60 , training loss: 3.683736\n",
      "[INFO] Epoch: 55 , batch: 61 , training loss: 3.775116\n",
      "[INFO] Epoch: 55 , batch: 62 , training loss: 3.624489\n",
      "[INFO] Epoch: 55 , batch: 63 , training loss: 3.846039\n",
      "[INFO] Epoch: 55 , batch: 64 , training loss: 4.054659\n",
      "[INFO] Epoch: 55 , batch: 65 , training loss: 3.756071\n",
      "[INFO] Epoch: 55 , batch: 66 , training loss: 3.579653\n",
      "[INFO] Epoch: 55 , batch: 67 , training loss: 3.633598\n",
      "[INFO] Epoch: 55 , batch: 68 , training loss: 3.783125\n",
      "[INFO] Epoch: 55 , batch: 69 , training loss: 3.716112\n",
      "[INFO] Epoch: 55 , batch: 70 , training loss: 3.950968\n",
      "[INFO] Epoch: 55 , batch: 71 , training loss: 3.794439\n",
      "[INFO] Epoch: 55 , batch: 72 , training loss: 3.840190\n",
      "[INFO] Epoch: 55 , batch: 73 , training loss: 3.818423\n",
      "[INFO] Epoch: 55 , batch: 74 , training loss: 3.874460\n",
      "[INFO] Epoch: 55 , batch: 75 , training loss: 3.787781\n",
      "[INFO] Epoch: 55 , batch: 76 , training loss: 3.877264\n",
      "[INFO] Epoch: 55 , batch: 77 , training loss: 3.821793\n",
      "[INFO] Epoch: 55 , batch: 78 , training loss: 3.914596\n",
      "[INFO] Epoch: 55 , batch: 79 , training loss: 3.763914\n",
      "[INFO] Epoch: 55 , batch: 80 , training loss: 3.957576\n",
      "[INFO] Epoch: 55 , batch: 81 , training loss: 3.858788\n",
      "[INFO] Epoch: 55 , batch: 82 , training loss: 3.854087\n",
      "[INFO] Epoch: 55 , batch: 83 , training loss: 3.925240\n",
      "[INFO] Epoch: 55 , batch: 84 , training loss: 3.944927\n",
      "[INFO] Epoch: 55 , batch: 85 , training loss: 4.002013\n",
      "[INFO] Epoch: 55 , batch: 86 , training loss: 3.930723\n",
      "[INFO] Epoch: 55 , batch: 87 , training loss: 3.887141\n",
      "[INFO] Epoch: 55 , batch: 88 , training loss: 4.031764\n",
      "[INFO] Epoch: 55 , batch: 89 , training loss: 3.815481\n",
      "[INFO] Epoch: 55 , batch: 90 , training loss: 3.902326\n",
      "[INFO] Epoch: 55 , batch: 91 , training loss: 3.824049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 55 , batch: 92 , training loss: 3.839819\n",
      "[INFO] Epoch: 55 , batch: 93 , training loss: 3.951296\n",
      "[INFO] Epoch: 55 , batch: 94 , training loss: 4.083187\n",
      "[INFO] Epoch: 55 , batch: 95 , training loss: 3.895345\n",
      "[INFO] Epoch: 55 , batch: 96 , training loss: 3.853177\n",
      "[INFO] Epoch: 55 , batch: 97 , training loss: 3.793619\n",
      "[INFO] Epoch: 55 , batch: 98 , training loss: 3.731407\n",
      "[INFO] Epoch: 55 , batch: 99 , training loss: 3.843757\n",
      "[INFO] Epoch: 55 , batch: 100 , training loss: 3.759932\n",
      "[INFO] Epoch: 55 , batch: 101 , training loss: 3.782795\n",
      "[INFO] Epoch: 55 , batch: 102 , training loss: 3.902000\n",
      "[INFO] Epoch: 55 , batch: 103 , training loss: 3.738834\n",
      "[INFO] Epoch: 55 , batch: 104 , training loss: 3.665403\n",
      "[INFO] Epoch: 55 , batch: 105 , training loss: 3.938399\n",
      "[INFO] Epoch: 55 , batch: 106 , training loss: 3.962736\n",
      "[INFO] Epoch: 55 , batch: 107 , training loss: 3.791514\n",
      "[INFO] Epoch: 55 , batch: 108 , training loss: 3.768112\n",
      "[INFO] Epoch: 55 , batch: 109 , training loss: 3.649281\n",
      "[INFO] Epoch: 55 , batch: 110 , training loss: 3.866495\n",
      "[INFO] Epoch: 55 , batch: 111 , training loss: 3.924743\n",
      "[INFO] Epoch: 55 , batch: 112 , training loss: 3.851348\n",
      "[INFO] Epoch: 55 , batch: 113 , training loss: 3.805275\n",
      "[INFO] Epoch: 55 , batch: 114 , training loss: 3.837838\n",
      "[INFO] Epoch: 55 , batch: 115 , training loss: 3.833879\n",
      "[INFO] Epoch: 55 , batch: 116 , training loss: 3.736163\n",
      "[INFO] Epoch: 55 , batch: 117 , training loss: 3.944936\n",
      "[INFO] Epoch: 55 , batch: 118 , training loss: 3.931756\n",
      "[INFO] Epoch: 55 , batch: 119 , training loss: 4.044641\n",
      "[INFO] Epoch: 55 , batch: 120 , training loss: 4.032068\n",
      "[INFO] Epoch: 55 , batch: 121 , training loss: 3.913802\n",
      "[INFO] Epoch: 55 , batch: 122 , training loss: 3.802553\n",
      "[INFO] Epoch: 55 , batch: 123 , training loss: 3.767016\n",
      "[INFO] Epoch: 55 , batch: 124 , training loss: 3.886717\n",
      "[INFO] Epoch: 55 , batch: 125 , training loss: 3.745897\n",
      "[INFO] Epoch: 55 , batch: 126 , training loss: 3.759231\n",
      "[INFO] Epoch: 55 , batch: 127 , training loss: 3.740326\n",
      "[INFO] Epoch: 55 , batch: 128 , training loss: 3.875914\n",
      "[INFO] Epoch: 55 , batch: 129 , training loss: 3.831749\n",
      "[INFO] Epoch: 55 , batch: 130 , training loss: 3.846080\n",
      "[INFO] Epoch: 55 , batch: 131 , training loss: 3.846511\n",
      "[INFO] Epoch: 55 , batch: 132 , training loss: 3.880701\n",
      "[INFO] Epoch: 55 , batch: 133 , training loss: 3.843175\n",
      "[INFO] Epoch: 55 , batch: 134 , training loss: 3.612236\n",
      "[INFO] Epoch: 55 , batch: 135 , training loss: 3.698271\n",
      "[INFO] Epoch: 55 , batch: 136 , training loss: 3.967629\n",
      "[INFO] Epoch: 55 , batch: 137 , training loss: 3.899254\n",
      "[INFO] Epoch: 55 , batch: 138 , training loss: 3.936095\n",
      "[INFO] Epoch: 55 , batch: 139 , training loss: 4.528942\n",
      "[INFO] Epoch: 55 , batch: 140 , training loss: 4.285210\n",
      "[INFO] Epoch: 55 , batch: 141 , training loss: 4.071347\n",
      "[INFO] Epoch: 55 , batch: 142 , training loss: 3.795909\n",
      "[INFO] Epoch: 55 , batch: 143 , training loss: 3.924778\n",
      "[INFO] Epoch: 55 , batch: 144 , training loss: 3.803053\n",
      "[INFO] Epoch: 55 , batch: 145 , training loss: 3.858520\n",
      "[INFO] Epoch: 55 , batch: 146 , training loss: 4.060710\n",
      "[INFO] Epoch: 55 , batch: 147 , training loss: 3.697069\n",
      "[INFO] Epoch: 55 , batch: 148 , training loss: 3.667088\n",
      "[INFO] Epoch: 55 , batch: 149 , training loss: 3.778520\n",
      "[INFO] Epoch: 55 , batch: 150 , training loss: 4.068814\n",
      "[INFO] Epoch: 55 , batch: 151 , training loss: 3.873716\n",
      "[INFO] Epoch: 55 , batch: 152 , training loss: 3.876896\n",
      "[INFO] Epoch: 55 , batch: 153 , training loss: 3.980790\n",
      "[INFO] Epoch: 55 , batch: 154 , training loss: 4.065830\n",
      "[INFO] Epoch: 55 , batch: 155 , training loss: 4.266468\n",
      "[INFO] Epoch: 55 , batch: 156 , training loss: 4.000148\n",
      "[INFO] Epoch: 55 , batch: 157 , training loss: 3.935368\n",
      "[INFO] Epoch: 55 , batch: 158 , training loss: 4.102198\n",
      "[INFO] Epoch: 55 , batch: 159 , training loss: 4.016821\n",
      "[INFO] Epoch: 55 , batch: 160 , training loss: 4.263903\n",
      "[INFO] Epoch: 55 , batch: 161 , training loss: 4.300911\n",
      "[INFO] Epoch: 55 , batch: 162 , training loss: 4.266379\n",
      "[INFO] Epoch: 55 , batch: 163 , training loss: 4.461919\n",
      "[INFO] Epoch: 55 , batch: 164 , training loss: 4.360858\n",
      "[INFO] Epoch: 55 , batch: 165 , training loss: 4.303959\n",
      "[INFO] Epoch: 55 , batch: 166 , training loss: 4.244434\n",
      "[INFO] Epoch: 55 , batch: 167 , training loss: 4.398450\n",
      "[INFO] Epoch: 55 , batch: 168 , training loss: 4.066477\n",
      "[INFO] Epoch: 55 , batch: 169 , training loss: 4.039436\n",
      "[INFO] Epoch: 55 , batch: 170 , training loss: 4.192839\n",
      "[INFO] Epoch: 55 , batch: 171 , training loss: 3.663105\n",
      "[INFO] Epoch: 55 , batch: 172 , training loss: 3.877456\n",
      "[INFO] Epoch: 55 , batch: 173 , training loss: 4.107117\n",
      "[INFO] Epoch: 55 , batch: 174 , training loss: 4.606639\n",
      "[INFO] Epoch: 55 , batch: 175 , training loss: 4.832842\n",
      "[INFO] Epoch: 55 , batch: 176 , training loss: 4.502153\n",
      "[INFO] Epoch: 55 , batch: 177 , training loss: 4.070568\n",
      "[INFO] Epoch: 55 , batch: 178 , training loss: 4.091069\n",
      "[INFO] Epoch: 55 , batch: 179 , training loss: 4.159787\n",
      "[INFO] Epoch: 55 , batch: 180 , training loss: 4.091021\n",
      "[INFO] Epoch: 55 , batch: 181 , training loss: 4.392203\n",
      "[INFO] Epoch: 55 , batch: 182 , training loss: 4.344425\n",
      "[INFO] Epoch: 55 , batch: 183 , training loss: 4.294824\n",
      "[INFO] Epoch: 55 , batch: 184 , training loss: 4.205543\n",
      "[INFO] Epoch: 55 , batch: 185 , training loss: 4.171578\n",
      "[INFO] Epoch: 55 , batch: 186 , training loss: 4.316785\n",
      "[INFO] Epoch: 55 , batch: 187 , training loss: 4.401216\n",
      "[INFO] Epoch: 55 , batch: 188 , training loss: 4.380436\n",
      "[INFO] Epoch: 55 , batch: 189 , training loss: 4.295349\n",
      "[INFO] Epoch: 55 , batch: 190 , training loss: 4.335082\n",
      "[INFO] Epoch: 55 , batch: 191 , training loss: 4.435948\n",
      "[INFO] Epoch: 55 , batch: 192 , training loss: 4.275014\n",
      "[INFO] Epoch: 55 , batch: 193 , training loss: 4.356016\n",
      "[INFO] Epoch: 55 , batch: 194 , training loss: 4.299176\n",
      "[INFO] Epoch: 55 , batch: 195 , training loss: 4.283386\n",
      "[INFO] Epoch: 55 , batch: 196 , training loss: 4.116296\n",
      "[INFO] Epoch: 55 , batch: 197 , training loss: 4.201678\n",
      "[INFO] Epoch: 55 , batch: 198 , training loss: 4.102508\n",
      "[INFO] Epoch: 55 , batch: 199 , training loss: 4.232069\n",
      "[INFO] Epoch: 55 , batch: 200 , training loss: 4.153988\n",
      "[INFO] Epoch: 55 , batch: 201 , training loss: 4.038149\n",
      "[INFO] Epoch: 55 , batch: 202 , training loss: 4.049280\n",
      "[INFO] Epoch: 55 , batch: 203 , training loss: 4.157549\n",
      "[INFO] Epoch: 55 , batch: 204 , training loss: 4.252056\n",
      "[INFO] Epoch: 55 , batch: 205 , training loss: 3.867100\n",
      "[INFO] Epoch: 55 , batch: 206 , training loss: 3.785630\n",
      "[INFO] Epoch: 55 , batch: 207 , training loss: 3.779360\n",
      "[INFO] Epoch: 55 , batch: 208 , training loss: 4.129503\n",
      "[INFO] Epoch: 55 , batch: 209 , training loss: 4.089771\n",
      "[INFO] Epoch: 55 , batch: 210 , training loss: 4.068911\n",
      "[INFO] Epoch: 55 , batch: 211 , training loss: 4.065847\n",
      "[INFO] Epoch: 55 , batch: 212 , training loss: 4.188123\n",
      "[INFO] Epoch: 55 , batch: 213 , training loss: 4.145598\n",
      "[INFO] Epoch: 55 , batch: 214 , training loss: 4.199128\n",
      "[INFO] Epoch: 55 , batch: 215 , training loss: 4.391905\n",
      "[INFO] Epoch: 55 , batch: 216 , training loss: 4.105022\n",
      "[INFO] Epoch: 55 , batch: 217 , training loss: 4.063773\n",
      "[INFO] Epoch: 55 , batch: 218 , training loss: 4.044431\n",
      "[INFO] Epoch: 55 , batch: 219 , training loss: 4.149642\n",
      "[INFO] Epoch: 55 , batch: 220 , training loss: 3.974813\n",
      "[INFO] Epoch: 55 , batch: 221 , training loss: 4.010158\n",
      "[INFO] Epoch: 55 , batch: 222 , training loss: 4.125809\n",
      "[INFO] Epoch: 55 , batch: 223 , training loss: 4.281454\n",
      "[INFO] Epoch: 55 , batch: 224 , training loss: 4.285661\n",
      "[INFO] Epoch: 55 , batch: 225 , training loss: 4.176197\n",
      "[INFO] Epoch: 55 , batch: 226 , training loss: 4.297714\n",
      "[INFO] Epoch: 55 , batch: 227 , training loss: 4.277307\n",
      "[INFO] Epoch: 55 , batch: 228 , training loss: 4.284945\n",
      "[INFO] Epoch: 55 , batch: 229 , training loss: 4.158661\n",
      "[INFO] Epoch: 55 , batch: 230 , training loss: 4.016279\n",
      "[INFO] Epoch: 55 , batch: 231 , training loss: 3.880509\n",
      "[INFO] Epoch: 55 , batch: 232 , training loss: 4.024853\n",
      "[INFO] Epoch: 55 , batch: 233 , training loss: 4.056516\n",
      "[INFO] Epoch: 55 , batch: 234 , training loss: 3.734715\n",
      "[INFO] Epoch: 55 , batch: 235 , training loss: 3.849388\n",
      "[INFO] Epoch: 55 , batch: 236 , training loss: 3.929398\n",
      "[INFO] Epoch: 55 , batch: 237 , training loss: 4.142444\n",
      "[INFO] Epoch: 55 , batch: 238 , training loss: 3.954900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 55 , batch: 239 , training loss: 3.993610\n",
      "[INFO] Epoch: 55 , batch: 240 , training loss: 4.025709\n",
      "[INFO] Epoch: 55 , batch: 241 , training loss: 3.837283\n",
      "[INFO] Epoch: 55 , batch: 242 , training loss: 3.856989\n",
      "[INFO] Epoch: 55 , batch: 243 , training loss: 4.142413\n",
      "[INFO] Epoch: 55 , batch: 244 , training loss: 4.094434\n",
      "[INFO] Epoch: 55 , batch: 245 , training loss: 4.018648\n",
      "[INFO] Epoch: 55 , batch: 246 , training loss: 3.783480\n",
      "[INFO] Epoch: 55 , batch: 247 , training loss: 3.940956\n",
      "[INFO] Epoch: 55 , batch: 248 , training loss: 4.009492\n",
      "[INFO] Epoch: 55 , batch: 249 , training loss: 3.985160\n",
      "[INFO] Epoch: 55 , batch: 250 , training loss: 3.800258\n",
      "[INFO] Epoch: 55 , batch: 251 , training loss: 4.224869\n",
      "[INFO] Epoch: 55 , batch: 252 , training loss: 3.945921\n",
      "[INFO] Epoch: 55 , batch: 253 , training loss: 3.868971\n",
      "[INFO] Epoch: 55 , batch: 254 , training loss: 4.138170\n",
      "[INFO] Epoch: 55 , batch: 255 , training loss: 4.123629\n",
      "[INFO] Epoch: 55 , batch: 256 , training loss: 4.086661\n",
      "[INFO] Epoch: 55 , batch: 257 , training loss: 4.276140\n",
      "[INFO] Epoch: 55 , batch: 258 , training loss: 4.230680\n",
      "[INFO] Epoch: 55 , batch: 259 , training loss: 4.308121\n",
      "[INFO] Epoch: 55 , batch: 260 , training loss: 4.072843\n",
      "[INFO] Epoch: 55 , batch: 261 , training loss: 4.269255\n",
      "[INFO] Epoch: 55 , batch: 262 , training loss: 4.374952\n",
      "[INFO] Epoch: 55 , batch: 263 , training loss: 4.562347\n",
      "[INFO] Epoch: 55 , batch: 264 , training loss: 3.895773\n",
      "[INFO] Epoch: 55 , batch: 265 , training loss: 4.019931\n",
      "[INFO] Epoch: 55 , batch: 266 , training loss: 4.426480\n",
      "[INFO] Epoch: 55 , batch: 267 , training loss: 4.179948\n",
      "[INFO] Epoch: 55 , batch: 268 , training loss: 4.113047\n",
      "[INFO] Epoch: 55 , batch: 269 , training loss: 4.100246\n",
      "[INFO] Epoch: 55 , batch: 270 , training loss: 4.100737\n",
      "[INFO] Epoch: 55 , batch: 271 , training loss: 4.131239\n",
      "[INFO] Epoch: 55 , batch: 272 , training loss: 4.134584\n",
      "[INFO] Epoch: 55 , batch: 273 , training loss: 4.146058\n",
      "[INFO] Epoch: 55 , batch: 274 , training loss: 4.244294\n",
      "[INFO] Epoch: 55 , batch: 275 , training loss: 4.086152\n",
      "[INFO] Epoch: 55 , batch: 276 , training loss: 4.146829\n",
      "[INFO] Epoch: 55 , batch: 277 , training loss: 4.335663\n",
      "[INFO] Epoch: 55 , batch: 278 , training loss: 4.010756\n",
      "[INFO] Epoch: 55 , batch: 279 , training loss: 4.009083\n",
      "[INFO] Epoch: 55 , batch: 280 , training loss: 3.995518\n",
      "[INFO] Epoch: 55 , batch: 281 , training loss: 4.103751\n",
      "[INFO] Epoch: 55 , batch: 282 , training loss: 4.014056\n",
      "[INFO] Epoch: 55 , batch: 283 , training loss: 4.020027\n",
      "[INFO] Epoch: 55 , batch: 284 , training loss: 4.054852\n",
      "[INFO] Epoch: 55 , batch: 285 , training loss: 4.007052\n",
      "[INFO] Epoch: 55 , batch: 286 , training loss: 4.010962\n",
      "[INFO] Epoch: 55 , batch: 287 , training loss: 3.947505\n",
      "[INFO] Epoch: 55 , batch: 288 , training loss: 3.910019\n",
      "[INFO] Epoch: 55 , batch: 289 , training loss: 3.984072\n",
      "[INFO] Epoch: 55 , batch: 290 , training loss: 3.769200\n",
      "[INFO] Epoch: 55 , batch: 291 , training loss: 3.760291\n",
      "[INFO] Epoch: 55 , batch: 292 , training loss: 3.858485\n",
      "[INFO] Epoch: 55 , batch: 293 , training loss: 3.781234\n",
      "[INFO] Epoch: 55 , batch: 294 , training loss: 4.447372\n",
      "[INFO] Epoch: 55 , batch: 295 , training loss: 4.229195\n",
      "[INFO] Epoch: 55 , batch: 296 , training loss: 4.151703\n",
      "[INFO] Epoch: 55 , batch: 297 , training loss: 4.133025\n",
      "[INFO] Epoch: 55 , batch: 298 , training loss: 3.937298\n",
      "[INFO] Epoch: 55 , batch: 299 , training loss: 3.990248\n",
      "[INFO] Epoch: 55 , batch: 300 , training loss: 3.975619\n",
      "[INFO] Epoch: 55 , batch: 301 , training loss: 3.884508\n",
      "[INFO] Epoch: 55 , batch: 302 , training loss: 4.071341\n",
      "[INFO] Epoch: 55 , batch: 303 , training loss: 4.061167\n",
      "[INFO] Epoch: 55 , batch: 304 , training loss: 4.197155\n",
      "[INFO] Epoch: 55 , batch: 305 , training loss: 4.054486\n",
      "[INFO] Epoch: 55 , batch: 306 , training loss: 4.156357\n",
      "[INFO] Epoch: 55 , batch: 307 , training loss: 4.183411\n",
      "[INFO] Epoch: 55 , batch: 308 , training loss: 4.003984\n",
      "[INFO] Epoch: 55 , batch: 309 , training loss: 3.978715\n",
      "[INFO] Epoch: 55 , batch: 310 , training loss: 3.901765\n",
      "[INFO] Epoch: 55 , batch: 311 , training loss: 3.903225\n",
      "[INFO] Epoch: 55 , batch: 312 , training loss: 3.805130\n",
      "[INFO] Epoch: 55 , batch: 313 , training loss: 3.918707\n",
      "[INFO] Epoch: 55 , batch: 314 , training loss: 3.997672\n",
      "[INFO] Epoch: 55 , batch: 315 , training loss: 4.074399\n",
      "[INFO] Epoch: 55 , batch: 316 , training loss: 4.312295\n",
      "[INFO] Epoch: 55 , batch: 317 , training loss: 4.687294\n",
      "[INFO] Epoch: 55 , batch: 318 , training loss: 4.786230\n",
      "[INFO] Epoch: 55 , batch: 319 , training loss: 4.477958\n",
      "[INFO] Epoch: 55 , batch: 320 , training loss: 4.015604\n",
      "[INFO] Epoch: 55 , batch: 321 , training loss: 3.826246\n",
      "[INFO] Epoch: 55 , batch: 322 , training loss: 3.947113\n",
      "[INFO] Epoch: 55 , batch: 323 , training loss: 3.994014\n",
      "[INFO] Epoch: 55 , batch: 324 , training loss: 3.946505\n",
      "[INFO] Epoch: 55 , batch: 325 , training loss: 4.054915\n",
      "[INFO] Epoch: 55 , batch: 326 , training loss: 4.152586\n",
      "[INFO] Epoch: 55 , batch: 327 , training loss: 4.082306\n",
      "[INFO] Epoch: 55 , batch: 328 , training loss: 4.086683\n",
      "[INFO] Epoch: 55 , batch: 329 , training loss: 3.978920\n",
      "[INFO] Epoch: 55 , batch: 330 , training loss: 3.974422\n",
      "[INFO] Epoch: 55 , batch: 331 , training loss: 4.105916\n",
      "[INFO] Epoch: 55 , batch: 332 , training loss: 3.981045\n",
      "[INFO] Epoch: 55 , batch: 333 , training loss: 3.945396\n",
      "[INFO] Epoch: 55 , batch: 334 , training loss: 3.971747\n",
      "[INFO] Epoch: 55 , batch: 335 , training loss: 4.074919\n",
      "[INFO] Epoch: 55 , batch: 336 , training loss: 4.078736\n",
      "[INFO] Epoch: 55 , batch: 337 , training loss: 4.123296\n",
      "[INFO] Epoch: 55 , batch: 338 , training loss: 4.363019\n",
      "[INFO] Epoch: 55 , batch: 339 , training loss: 4.164002\n",
      "[INFO] Epoch: 55 , batch: 340 , training loss: 4.350188\n",
      "[INFO] Epoch: 55 , batch: 341 , training loss: 4.101288\n",
      "[INFO] Epoch: 55 , batch: 342 , training loss: 3.914153\n",
      "[INFO] Epoch: 55 , batch: 343 , training loss: 3.982058\n",
      "[INFO] Epoch: 55 , batch: 344 , training loss: 3.827441\n",
      "[INFO] Epoch: 55 , batch: 345 , training loss: 3.947459\n",
      "[INFO] Epoch: 55 , batch: 346 , training loss: 4.018304\n",
      "[INFO] Epoch: 55 , batch: 347 , training loss: 3.918634\n",
      "[INFO] Epoch: 55 , batch: 348 , training loss: 4.011600\n",
      "[INFO] Epoch: 55 , batch: 349 , training loss: 4.106245\n",
      "[INFO] Epoch: 55 , batch: 350 , training loss: 3.976401\n",
      "[INFO] Epoch: 55 , batch: 351 , training loss: 4.053771\n",
      "[INFO] Epoch: 55 , batch: 352 , training loss: 4.067635\n",
      "[INFO] Epoch: 55 , batch: 353 , training loss: 4.048183\n",
      "[INFO] Epoch: 55 , batch: 354 , training loss: 4.123325\n",
      "[INFO] Epoch: 55 , batch: 355 , training loss: 4.134329\n",
      "[INFO] Epoch: 55 , batch: 356 , training loss: 4.008155\n",
      "[INFO] Epoch: 55 , batch: 357 , training loss: 4.065879\n",
      "[INFO] Epoch: 55 , batch: 358 , training loss: 3.970684\n",
      "[INFO] Epoch: 55 , batch: 359 , training loss: 3.979088\n",
      "[INFO] Epoch: 55 , batch: 360 , training loss: 4.080946\n",
      "[INFO] Epoch: 55 , batch: 361 , training loss: 4.040774\n",
      "[INFO] Epoch: 55 , batch: 362 , training loss: 4.159208\n",
      "[INFO] Epoch: 55 , batch: 363 , training loss: 4.022612\n",
      "[INFO] Epoch: 55 , batch: 364 , training loss: 4.086783\n",
      "[INFO] Epoch: 55 , batch: 365 , training loss: 4.026743\n",
      "[INFO] Epoch: 55 , batch: 366 , training loss: 4.107982\n",
      "[INFO] Epoch: 55 , batch: 367 , training loss: 4.155363\n",
      "[INFO] Epoch: 55 , batch: 368 , training loss: 4.576810\n",
      "[INFO] Epoch: 55 , batch: 369 , training loss: 4.231625\n",
      "[INFO] Epoch: 55 , batch: 370 , training loss: 4.012727\n",
      "[INFO] Epoch: 55 , batch: 371 , training loss: 4.450303\n",
      "[INFO] Epoch: 55 , batch: 372 , training loss: 4.716801\n",
      "[INFO] Epoch: 55 , batch: 373 , training loss: 4.697420\n",
      "[INFO] Epoch: 55 , batch: 374 , training loss: 4.882634\n",
      "[INFO] Epoch: 55 , batch: 375 , training loss: 4.848589\n",
      "[INFO] Epoch: 55 , batch: 376 , training loss: 4.704147\n",
      "[INFO] Epoch: 55 , batch: 377 , training loss: 4.443658\n",
      "[INFO] Epoch: 55 , batch: 378 , training loss: 4.550829\n",
      "[INFO] Epoch: 55 , batch: 379 , training loss: 4.522723\n",
      "[INFO] Epoch: 55 , batch: 380 , training loss: 4.725045\n",
      "[INFO] Epoch: 55 , batch: 381 , training loss: 4.407743\n",
      "[INFO] Epoch: 55 , batch: 382 , training loss: 4.654049\n",
      "[INFO] Epoch: 55 , batch: 383 , training loss: 4.721094\n",
      "[INFO] Epoch: 55 , batch: 384 , training loss: 4.684297\n",
      "[INFO] Epoch: 55 , batch: 385 , training loss: 4.350766\n",
      "[INFO] Epoch: 55 , batch: 386 , training loss: 4.585813\n",
      "[INFO] Epoch: 55 , batch: 387 , training loss: 4.553226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 55 , batch: 388 , training loss: 4.349954\n",
      "[INFO] Epoch: 55 , batch: 389 , training loss: 4.170595\n",
      "[INFO] Epoch: 55 , batch: 390 , training loss: 4.185713\n",
      "[INFO] Epoch: 55 , batch: 391 , training loss: 4.223241\n",
      "[INFO] Epoch: 55 , batch: 392 , training loss: 4.592072\n",
      "[INFO] Epoch: 55 , batch: 393 , training loss: 4.485446\n",
      "[INFO] Epoch: 55 , batch: 394 , training loss: 4.570390\n",
      "[INFO] Epoch: 55 , batch: 395 , training loss: 4.394953\n",
      "[INFO] Epoch: 55 , batch: 396 , training loss: 4.218216\n",
      "[INFO] Epoch: 55 , batch: 397 , training loss: 4.348470\n",
      "[INFO] Epoch: 55 , batch: 398 , training loss: 4.222342\n",
      "[INFO] Epoch: 55 , batch: 399 , training loss: 4.302132\n",
      "[INFO] Epoch: 55 , batch: 400 , training loss: 4.265905\n",
      "[INFO] Epoch: 55 , batch: 401 , training loss: 4.711975\n",
      "[INFO] Epoch: 55 , batch: 402 , training loss: 4.435390\n",
      "[INFO] Epoch: 55 , batch: 403 , training loss: 4.251636\n",
      "[INFO] Epoch: 55 , batch: 404 , training loss: 4.447899\n",
      "[INFO] Epoch: 55 , batch: 405 , training loss: 4.485645\n",
      "[INFO] Epoch: 55 , batch: 406 , training loss: 4.398601\n",
      "[INFO] Epoch: 55 , batch: 407 , training loss: 4.402101\n",
      "[INFO] Epoch: 55 , batch: 408 , training loss: 4.403681\n",
      "[INFO] Epoch: 55 , batch: 409 , training loss: 4.402587\n",
      "[INFO] Epoch: 55 , batch: 410 , training loss: 4.481027\n",
      "[INFO] Epoch: 55 , batch: 411 , training loss: 4.629401\n",
      "[INFO] Epoch: 55 , batch: 412 , training loss: 4.454963\n",
      "[INFO] Epoch: 55 , batch: 413 , training loss: 4.326723\n",
      "[INFO] Epoch: 55 , batch: 414 , training loss: 4.392588\n",
      "[INFO] Epoch: 55 , batch: 415 , training loss: 4.430786\n",
      "[INFO] Epoch: 55 , batch: 416 , training loss: 4.470896\n",
      "[INFO] Epoch: 55 , batch: 417 , training loss: 4.387468\n",
      "[INFO] Epoch: 55 , batch: 418 , training loss: 4.472919\n",
      "[INFO] Epoch: 55 , batch: 419 , training loss: 4.410829\n",
      "[INFO] Epoch: 55 , batch: 420 , training loss: 4.390418\n",
      "[INFO] Epoch: 55 , batch: 421 , training loss: 4.380143\n",
      "[INFO] Epoch: 55 , batch: 422 , training loss: 4.222745\n",
      "[INFO] Epoch: 55 , batch: 423 , training loss: 4.431700\n",
      "[INFO] Epoch: 55 , batch: 424 , training loss: 4.609641\n",
      "[INFO] Epoch: 55 , batch: 425 , training loss: 4.485752\n",
      "[INFO] Epoch: 55 , batch: 426 , training loss: 4.215866\n",
      "[INFO] Epoch: 55 , batch: 427 , training loss: 4.473150\n",
      "[INFO] Epoch: 55 , batch: 428 , training loss: 4.328785\n",
      "[INFO] Epoch: 55 , batch: 429 , training loss: 4.214923\n",
      "[INFO] Epoch: 55 , batch: 430 , training loss: 4.454762\n",
      "[INFO] Epoch: 55 , batch: 431 , training loss: 4.070955\n",
      "[INFO] Epoch: 55 , batch: 432 , training loss: 4.121363\n",
      "[INFO] Epoch: 55 , batch: 433 , training loss: 4.162867\n",
      "[INFO] Epoch: 55 , batch: 434 , training loss: 4.052696\n",
      "[INFO] Epoch: 55 , batch: 435 , training loss: 4.395142\n",
      "[INFO] Epoch: 55 , batch: 436 , training loss: 4.413719\n",
      "[INFO] Epoch: 55 , batch: 437 , training loss: 4.226454\n",
      "[INFO] Epoch: 55 , batch: 438 , training loss: 4.084903\n",
      "[INFO] Epoch: 55 , batch: 439 , training loss: 4.303269\n",
      "[INFO] Epoch: 55 , batch: 440 , training loss: 4.444480\n",
      "[INFO] Epoch: 55 , batch: 441 , training loss: 4.533728\n",
      "[INFO] Epoch: 55 , batch: 442 , training loss: 4.301550\n",
      "[INFO] Epoch: 55 , batch: 443 , training loss: 4.444081\n",
      "[INFO] Epoch: 55 , batch: 444 , training loss: 4.094952\n",
      "[INFO] Epoch: 55 , batch: 445 , training loss: 3.970428\n",
      "[INFO] Epoch: 55 , batch: 446 , training loss: 3.899569\n",
      "[INFO] Epoch: 55 , batch: 447 , training loss: 4.100495\n",
      "[INFO] Epoch: 55 , batch: 448 , training loss: 4.234740\n",
      "[INFO] Epoch: 55 , batch: 449 , training loss: 4.611224\n",
      "[INFO] Epoch: 55 , batch: 450 , training loss: 4.685222\n",
      "[INFO] Epoch: 55 , batch: 451 , training loss: 4.580954\n",
      "[INFO] Epoch: 55 , batch: 452 , training loss: 4.402111\n",
      "[INFO] Epoch: 55 , batch: 453 , training loss: 4.156719\n",
      "[INFO] Epoch: 55 , batch: 454 , training loss: 4.314653\n",
      "[INFO] Epoch: 55 , batch: 455 , training loss: 4.334054\n",
      "[INFO] Epoch: 55 , batch: 456 , training loss: 4.349048\n",
      "[INFO] Epoch: 55 , batch: 457 , training loss: 4.433013\n",
      "[INFO] Epoch: 55 , batch: 458 , training loss: 4.177221\n",
      "[INFO] Epoch: 55 , batch: 459 , training loss: 4.144645\n",
      "[INFO] Epoch: 55 , batch: 460 , training loss: 4.271278\n",
      "[INFO] Epoch: 55 , batch: 461 , training loss: 4.236109\n",
      "[INFO] Epoch: 55 , batch: 462 , training loss: 4.306567\n",
      "[INFO] Epoch: 55 , batch: 463 , training loss: 4.201062\n",
      "[INFO] Epoch: 55 , batch: 464 , training loss: 4.379888\n",
      "[INFO] Epoch: 55 , batch: 465 , training loss: 4.337930\n",
      "[INFO] Epoch: 55 , batch: 466 , training loss: 4.416771\n",
      "[INFO] Epoch: 55 , batch: 467 , training loss: 4.403871\n",
      "[INFO] Epoch: 55 , batch: 468 , training loss: 4.338696\n",
      "[INFO] Epoch: 55 , batch: 469 , training loss: 4.375476\n",
      "[INFO] Epoch: 55 , batch: 470 , training loss: 4.191398\n",
      "[INFO] Epoch: 55 , batch: 471 , training loss: 4.307762\n",
      "[INFO] Epoch: 55 , batch: 472 , training loss: 4.359566\n",
      "[INFO] Epoch: 55 , batch: 473 , training loss: 4.265968\n",
      "[INFO] Epoch: 55 , batch: 474 , training loss: 4.053959\n",
      "[INFO] Epoch: 55 , batch: 475 , training loss: 3.934139\n",
      "[INFO] Epoch: 55 , batch: 476 , training loss: 4.332739\n",
      "[INFO] Epoch: 55 , batch: 477 , training loss: 4.450310\n",
      "[INFO] Epoch: 55 , batch: 478 , training loss: 4.452916\n",
      "[INFO] Epoch: 55 , batch: 479 , training loss: 4.439876\n",
      "[INFO] Epoch: 55 , batch: 480 , training loss: 4.553203\n",
      "[INFO] Epoch: 55 , batch: 481 , training loss: 4.434001\n",
      "[INFO] Epoch: 55 , batch: 482 , training loss: 4.538290\n",
      "[INFO] Epoch: 55 , batch: 483 , training loss: 4.381521\n",
      "[INFO] Epoch: 55 , batch: 484 , training loss: 4.180982\n",
      "[INFO] Epoch: 55 , batch: 485 , training loss: 4.285485\n",
      "[INFO] Epoch: 55 , batch: 486 , training loss: 4.189801\n",
      "[INFO] Epoch: 55 , batch: 487 , training loss: 4.165807\n",
      "[INFO] Epoch: 55 , batch: 488 , training loss: 4.350061\n",
      "[INFO] Epoch: 55 , batch: 489 , training loss: 4.240999\n",
      "[INFO] Epoch: 55 , batch: 490 , training loss: 4.322214\n",
      "[INFO] Epoch: 55 , batch: 491 , training loss: 4.240537\n",
      "[INFO] Epoch: 55 , batch: 492 , training loss: 4.209303\n",
      "[INFO] Epoch: 55 , batch: 493 , training loss: 4.357956\n",
      "[INFO] Epoch: 55 , batch: 494 , training loss: 4.280927\n",
      "[INFO] Epoch: 55 , batch: 495 , training loss: 4.468763\n",
      "[INFO] Epoch: 55 , batch: 496 , training loss: 4.318963\n",
      "[INFO] Epoch: 55 , batch: 497 , training loss: 4.353192\n",
      "[INFO] Epoch: 55 , batch: 498 , training loss: 4.331197\n",
      "[INFO] Epoch: 55 , batch: 499 , training loss: 4.399902\n",
      "[INFO] Epoch: 55 , batch: 500 , training loss: 4.539613\n",
      "[INFO] Epoch: 55 , batch: 501 , training loss: 4.892908\n",
      "[INFO] Epoch: 55 , batch: 502 , training loss: 4.902284\n",
      "[INFO] Epoch: 55 , batch: 503 , training loss: 4.499043\n",
      "[INFO] Epoch: 55 , batch: 504 , training loss: 4.669887\n",
      "[INFO] Epoch: 55 , batch: 505 , training loss: 4.639161\n",
      "[INFO] Epoch: 55 , batch: 506 , training loss: 4.620247\n",
      "[INFO] Epoch: 55 , batch: 507 , training loss: 4.676413\n",
      "[INFO] Epoch: 55 , batch: 508 , training loss: 4.586523\n",
      "[INFO] Epoch: 55 , batch: 509 , training loss: 4.399894\n",
      "[INFO] Epoch: 55 , batch: 510 , training loss: 4.496874\n",
      "[INFO] Epoch: 55 , batch: 511 , training loss: 4.403340\n",
      "[INFO] Epoch: 55 , batch: 512 , training loss: 4.488707\n",
      "[INFO] Epoch: 55 , batch: 513 , training loss: 4.755475\n",
      "[INFO] Epoch: 55 , batch: 514 , training loss: 4.390274\n",
      "[INFO] Epoch: 55 , batch: 515 , training loss: 4.659172\n",
      "[INFO] Epoch: 55 , batch: 516 , training loss: 4.449178\n",
      "[INFO] Epoch: 55 , batch: 517 , training loss: 4.420605\n",
      "[INFO] Epoch: 55 , batch: 518 , training loss: 4.384630\n",
      "[INFO] Epoch: 55 , batch: 519 , training loss: 4.223665\n",
      "[INFO] Epoch: 55 , batch: 520 , training loss: 4.459398\n",
      "[INFO] Epoch: 55 , batch: 521 , training loss: 4.436635\n",
      "[INFO] Epoch: 55 , batch: 522 , training loss: 4.499517\n",
      "[INFO] Epoch: 55 , batch: 523 , training loss: 4.441355\n",
      "[INFO] Epoch: 55 , batch: 524 , training loss: 4.730249\n",
      "[INFO] Epoch: 55 , batch: 525 , training loss: 4.612794\n",
      "[INFO] Epoch: 55 , batch: 526 , training loss: 4.380279\n",
      "[INFO] Epoch: 55 , batch: 527 , training loss: 4.418406\n",
      "[INFO] Epoch: 55 , batch: 528 , training loss: 4.433015\n",
      "[INFO] Epoch: 55 , batch: 529 , training loss: 4.412025\n",
      "[INFO] Epoch: 55 , batch: 530 , training loss: 4.274093\n",
      "[INFO] Epoch: 55 , batch: 531 , training loss: 4.425990\n",
      "[INFO] Epoch: 55 , batch: 532 , training loss: 4.327461\n",
      "[INFO] Epoch: 55 , batch: 533 , training loss: 4.443419\n",
      "[INFO] Epoch: 55 , batch: 534 , training loss: 4.448753\n",
      "[INFO] Epoch: 55 , batch: 535 , training loss: 4.465363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 55 , batch: 536 , training loss: 4.313735\n",
      "[INFO] Epoch: 55 , batch: 537 , training loss: 4.292348\n",
      "[INFO] Epoch: 55 , batch: 538 , training loss: 4.384324\n",
      "[INFO] Epoch: 55 , batch: 539 , training loss: 4.458714\n",
      "[INFO] Epoch: 55 , batch: 540 , training loss: 5.030320\n",
      "[INFO] Epoch: 55 , batch: 541 , training loss: 4.821841\n",
      "[INFO] Epoch: 55 , batch: 542 , training loss: 4.715638\n",
      "[INFO] Epoch: 56 , batch: 0 , training loss: 3.778879\n",
      "[INFO] Epoch: 56 , batch: 1 , training loss: 3.572476\n",
      "[INFO] Epoch: 56 , batch: 2 , training loss: 3.738041\n",
      "[INFO] Epoch: 56 , batch: 3 , training loss: 3.615531\n",
      "[INFO] Epoch: 56 , batch: 4 , training loss: 4.003502\n",
      "[INFO] Epoch: 56 , batch: 5 , training loss: 3.662344\n",
      "[INFO] Epoch: 56 , batch: 6 , training loss: 4.103778\n",
      "[INFO] Epoch: 56 , batch: 7 , training loss: 3.969524\n",
      "[INFO] Epoch: 56 , batch: 8 , training loss: 3.605821\n",
      "[INFO] Epoch: 56 , batch: 9 , training loss: 3.861423\n",
      "[INFO] Epoch: 56 , batch: 10 , training loss: 3.830591\n",
      "[INFO] Epoch: 56 , batch: 11 , training loss: 3.778351\n",
      "[INFO] Epoch: 56 , batch: 12 , training loss: 3.674742\n",
      "[INFO] Epoch: 56 , batch: 13 , training loss: 3.660920\n",
      "[INFO] Epoch: 56 , batch: 14 , training loss: 3.621242\n",
      "[INFO] Epoch: 56 , batch: 15 , training loss: 3.805827\n",
      "[INFO] Epoch: 56 , batch: 16 , training loss: 3.678858\n",
      "[INFO] Epoch: 56 , batch: 17 , training loss: 3.774711\n",
      "[INFO] Epoch: 56 , batch: 18 , training loss: 3.736143\n",
      "[INFO] Epoch: 56 , batch: 19 , training loss: 3.504243\n",
      "[INFO] Epoch: 56 , batch: 20 , training loss: 3.482154\n",
      "[INFO] Epoch: 56 , batch: 21 , training loss: 3.586138\n",
      "[INFO] Epoch: 56 , batch: 22 , training loss: 3.472963\n",
      "[INFO] Epoch: 56 , batch: 23 , training loss: 3.685917\n",
      "[INFO] Epoch: 56 , batch: 24 , training loss: 3.562427\n",
      "[INFO] Epoch: 56 , batch: 25 , training loss: 3.637522\n",
      "[INFO] Epoch: 56 , batch: 26 , training loss: 3.512253\n",
      "[INFO] Epoch: 56 , batch: 27 , training loss: 3.528716\n",
      "[INFO] Epoch: 56 , batch: 28 , training loss: 3.688785\n",
      "[INFO] Epoch: 56 , batch: 29 , training loss: 3.525375\n",
      "[INFO] Epoch: 56 , batch: 30 , training loss: 3.515633\n",
      "[INFO] Epoch: 56 , batch: 31 , training loss: 3.602037\n",
      "[INFO] Epoch: 56 , batch: 32 , training loss: 3.590527\n",
      "[INFO] Epoch: 56 , batch: 33 , training loss: 3.614546\n",
      "[INFO] Epoch: 56 , batch: 34 , training loss: 3.599097\n",
      "[INFO] Epoch: 56 , batch: 35 , training loss: 3.554724\n",
      "[INFO] Epoch: 56 , batch: 36 , training loss: 3.662707\n",
      "[INFO] Epoch: 56 , batch: 37 , training loss: 3.543741\n",
      "[INFO] Epoch: 56 , batch: 38 , training loss: 3.600727\n",
      "[INFO] Epoch: 56 , batch: 39 , training loss: 3.435051\n",
      "[INFO] Epoch: 56 , batch: 40 , training loss: 3.612520\n",
      "[INFO] Epoch: 56 , batch: 41 , training loss: 3.589712\n",
      "[INFO] Epoch: 56 , batch: 42 , training loss: 4.036636\n",
      "[INFO] Epoch: 56 , batch: 43 , training loss: 3.811789\n",
      "[INFO] Epoch: 56 , batch: 44 , training loss: 4.132944\n",
      "[INFO] Epoch: 56 , batch: 45 , training loss: 4.085593\n",
      "[INFO] Epoch: 56 , batch: 46 , training loss: 4.080612\n",
      "[INFO] Epoch: 56 , batch: 47 , training loss: 3.631196\n",
      "[INFO] Epoch: 56 , batch: 48 , training loss: 3.650167\n",
      "[INFO] Epoch: 56 , batch: 49 , training loss: 3.824796\n",
      "[INFO] Epoch: 56 , batch: 50 , training loss: 3.612576\n",
      "[INFO] Epoch: 56 , batch: 51 , training loss: 3.832583\n",
      "[INFO] Epoch: 56 , batch: 52 , training loss: 3.628931\n",
      "[INFO] Epoch: 56 , batch: 53 , training loss: 3.760420\n",
      "[INFO] Epoch: 56 , batch: 54 , training loss: 3.777087\n",
      "[INFO] Epoch: 56 , batch: 55 , training loss: 3.841601\n",
      "[INFO] Epoch: 56 , batch: 56 , training loss: 3.675032\n",
      "[INFO] Epoch: 56 , batch: 57 , training loss: 3.620141\n",
      "[INFO] Epoch: 56 , batch: 58 , training loss: 3.642576\n",
      "[INFO] Epoch: 56 , batch: 59 , training loss: 3.734802\n",
      "[INFO] Epoch: 56 , batch: 60 , training loss: 3.669541\n",
      "[INFO] Epoch: 56 , batch: 61 , training loss: 3.749219\n",
      "[INFO] Epoch: 56 , batch: 62 , training loss: 3.644486\n",
      "[INFO] Epoch: 56 , batch: 63 , training loss: 3.829386\n",
      "[INFO] Epoch: 56 , batch: 64 , training loss: 4.034499\n",
      "[INFO] Epoch: 56 , batch: 65 , training loss: 3.741173\n",
      "[INFO] Epoch: 56 , batch: 66 , training loss: 3.594276\n",
      "[INFO] Epoch: 56 , batch: 67 , training loss: 3.626864\n",
      "[INFO] Epoch: 56 , batch: 68 , training loss: 3.751091\n",
      "[INFO] Epoch: 56 , batch: 69 , training loss: 3.749340\n",
      "[INFO] Epoch: 56 , batch: 70 , training loss: 3.955268\n",
      "[INFO] Epoch: 56 , batch: 71 , training loss: 3.771992\n",
      "[INFO] Epoch: 56 , batch: 72 , training loss: 3.834719\n",
      "[INFO] Epoch: 56 , batch: 73 , training loss: 3.809564\n",
      "[INFO] Epoch: 56 , batch: 74 , training loss: 3.886255\n",
      "[INFO] Epoch: 56 , batch: 75 , training loss: 3.778151\n",
      "[INFO] Epoch: 56 , batch: 76 , training loss: 3.867263\n",
      "[INFO] Epoch: 56 , batch: 77 , training loss: 3.813288\n",
      "[INFO] Epoch: 56 , batch: 78 , training loss: 3.884019\n",
      "[INFO] Epoch: 56 , batch: 79 , training loss: 3.748402\n",
      "[INFO] Epoch: 56 , batch: 80 , training loss: 3.959965\n",
      "[INFO] Epoch: 56 , batch: 81 , training loss: 3.865060\n",
      "[INFO] Epoch: 56 , batch: 82 , training loss: 3.849535\n",
      "[INFO] Epoch: 56 , batch: 83 , training loss: 3.936297\n",
      "[INFO] Epoch: 56 , batch: 84 , training loss: 3.930088\n",
      "[INFO] Epoch: 56 , batch: 85 , training loss: 3.995763\n",
      "[INFO] Epoch: 56 , batch: 86 , training loss: 3.939753\n",
      "[INFO] Epoch: 56 , batch: 87 , training loss: 3.885643\n",
      "[INFO] Epoch: 56 , batch: 88 , training loss: 4.041022\n",
      "[INFO] Epoch: 56 , batch: 89 , training loss: 3.786537\n",
      "[INFO] Epoch: 56 , batch: 90 , training loss: 3.887408\n",
      "[INFO] Epoch: 56 , batch: 91 , training loss: 3.821389\n",
      "[INFO] Epoch: 56 , batch: 92 , training loss: 3.846769\n",
      "[INFO] Epoch: 56 , batch: 93 , training loss: 3.958088\n",
      "[INFO] Epoch: 56 , batch: 94 , training loss: 4.077806\n",
      "[INFO] Epoch: 56 , batch: 95 , training loss: 3.857340\n",
      "[INFO] Epoch: 56 , batch: 96 , training loss: 3.856595\n",
      "[INFO] Epoch: 56 , batch: 97 , training loss: 3.785747\n",
      "[INFO] Epoch: 56 , batch: 98 , training loss: 3.736933\n",
      "[INFO] Epoch: 56 , batch: 99 , training loss: 3.843347\n",
      "[INFO] Epoch: 56 , batch: 100 , training loss: 3.775468\n",
      "[INFO] Epoch: 56 , batch: 101 , training loss: 3.775504\n",
      "[INFO] Epoch: 56 , batch: 102 , training loss: 3.904319\n",
      "[INFO] Epoch: 56 , batch: 103 , training loss: 3.710612\n",
      "[INFO] Epoch: 56 , batch: 104 , training loss: 3.675599\n",
      "[INFO] Epoch: 56 , batch: 105 , training loss: 3.925754\n",
      "[INFO] Epoch: 56 , batch: 106 , training loss: 3.967529\n",
      "[INFO] Epoch: 56 , batch: 107 , training loss: 3.797182\n",
      "[INFO] Epoch: 56 , batch: 108 , training loss: 3.753149\n",
      "[INFO] Epoch: 56 , batch: 109 , training loss: 3.662572\n",
      "[INFO] Epoch: 56 , batch: 110 , training loss: 3.848256\n",
      "[INFO] Epoch: 56 , batch: 111 , training loss: 3.929919\n",
      "[INFO] Epoch: 56 , batch: 112 , training loss: 3.843021\n",
      "[INFO] Epoch: 56 , batch: 113 , training loss: 3.799520\n",
      "[INFO] Epoch: 56 , batch: 114 , training loss: 3.809998\n",
      "[INFO] Epoch: 56 , batch: 115 , training loss: 3.836361\n",
      "[INFO] Epoch: 56 , batch: 116 , training loss: 3.740607\n",
      "[INFO] Epoch: 56 , batch: 117 , training loss: 3.941290\n",
      "[INFO] Epoch: 56 , batch: 118 , training loss: 3.914633\n",
      "[INFO] Epoch: 56 , batch: 119 , training loss: 4.058640\n",
      "[INFO] Epoch: 56 , batch: 120 , training loss: 4.025364\n",
      "[INFO] Epoch: 56 , batch: 121 , training loss: 3.929213\n",
      "[INFO] Epoch: 56 , batch: 122 , training loss: 3.812254\n",
      "[INFO] Epoch: 56 , batch: 123 , training loss: 3.793975\n",
      "[INFO] Epoch: 56 , batch: 124 , training loss: 3.906939\n",
      "[INFO] Epoch: 56 , batch: 125 , training loss: 3.730308\n",
      "[INFO] Epoch: 56 , batch: 126 , training loss: 3.747843\n",
      "[INFO] Epoch: 56 , batch: 127 , training loss: 3.757692\n",
      "[INFO] Epoch: 56 , batch: 128 , training loss: 3.870641\n",
      "[INFO] Epoch: 56 , batch: 129 , training loss: 3.812343\n",
      "[INFO] Epoch: 56 , batch: 130 , training loss: 3.822299\n",
      "[INFO] Epoch: 56 , batch: 131 , training loss: 3.842108\n",
      "[INFO] Epoch: 56 , batch: 132 , training loss: 3.843223\n",
      "[INFO] Epoch: 56 , batch: 133 , training loss: 3.806673\n",
      "[INFO] Epoch: 56 , batch: 134 , training loss: 3.592089\n",
      "[INFO] Epoch: 56 , batch: 135 , training loss: 3.655728\n",
      "[INFO] Epoch: 56 , batch: 136 , training loss: 3.923374\n",
      "[INFO] Epoch: 56 , batch: 137 , training loss: 3.853149\n",
      "[INFO] Epoch: 56 , batch: 138 , training loss: 3.917334\n",
      "[INFO] Epoch: 56 , batch: 139 , training loss: 4.498636\n",
      "[INFO] Epoch: 56 , batch: 140 , training loss: 4.232510\n",
      "[INFO] Epoch: 56 , batch: 141 , training loss: 4.049683\n",
      "[INFO] Epoch: 56 , batch: 142 , training loss: 3.753090\n",
      "[INFO] Epoch: 56 , batch: 143 , training loss: 3.872159\n",
      "[INFO] Epoch: 56 , batch: 144 , training loss: 3.741493\n",
      "[INFO] Epoch: 56 , batch: 145 , training loss: 3.790462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 56 , batch: 146 , training loss: 4.000906\n",
      "[INFO] Epoch: 56 , batch: 147 , training loss: 3.662678\n",
      "[INFO] Epoch: 56 , batch: 148 , training loss: 3.632670\n",
      "[INFO] Epoch: 56 , batch: 149 , training loss: 3.730310\n",
      "[INFO] Epoch: 56 , batch: 150 , training loss: 3.993983\n",
      "[INFO] Epoch: 56 , batch: 151 , training loss: 3.842918\n",
      "[INFO] Epoch: 56 , batch: 152 , training loss: 3.861604\n",
      "[INFO] Epoch: 56 , batch: 153 , training loss: 3.886205\n",
      "[INFO] Epoch: 56 , batch: 154 , training loss: 3.967530\n",
      "[INFO] Epoch: 56 , batch: 155 , training loss: 4.168412\n",
      "[INFO] Epoch: 56 , batch: 156 , training loss: 3.898261\n",
      "[INFO] Epoch: 56 , batch: 157 , training loss: 3.870748\n",
      "[INFO] Epoch: 56 , batch: 158 , training loss: 3.998012\n",
      "[INFO] Epoch: 56 , batch: 159 , training loss: 3.936969\n",
      "[INFO] Epoch: 56 , batch: 160 , training loss: 4.113914\n",
      "[INFO] Epoch: 56 , batch: 161 , training loss: 4.203022\n",
      "[INFO] Epoch: 56 , batch: 162 , training loss: 4.174146\n",
      "[INFO] Epoch: 56 , batch: 163 , training loss: 4.351387\n",
      "[INFO] Epoch: 56 , batch: 164 , training loss: 4.320330\n",
      "[INFO] Epoch: 56 , batch: 165 , training loss: 4.222745\n",
      "[INFO] Epoch: 56 , batch: 166 , training loss: 4.150912\n",
      "[INFO] Epoch: 56 , batch: 167 , training loss: 4.201225\n",
      "[INFO] Epoch: 56 , batch: 168 , training loss: 3.893171\n",
      "[INFO] Epoch: 56 , batch: 169 , training loss: 3.867808\n",
      "[INFO] Epoch: 56 , batch: 170 , training loss: 4.035221\n",
      "[INFO] Epoch: 56 , batch: 171 , training loss: 3.503412\n",
      "[INFO] Epoch: 56 , batch: 172 , training loss: 3.736039\n",
      "[INFO] Epoch: 56 , batch: 173 , training loss: 4.059305\n",
      "[INFO] Epoch: 56 , batch: 174 , training loss: 4.501307\n",
      "[INFO] Epoch: 56 , batch: 175 , training loss: 4.745546\n",
      "[INFO] Epoch: 56 , batch: 176 , training loss: 4.372153\n",
      "[INFO] Epoch: 56 , batch: 177 , training loss: 3.991763\n",
      "[INFO] Epoch: 56 , batch: 178 , training loss: 3.996632\n",
      "[INFO] Epoch: 56 , batch: 179 , training loss: 4.064000\n",
      "[INFO] Epoch: 56 , batch: 180 , training loss: 4.025378\n",
      "[INFO] Epoch: 56 , batch: 181 , training loss: 4.323147\n",
      "[INFO] Epoch: 56 , batch: 182 , training loss: 4.255824\n",
      "[INFO] Epoch: 56 , batch: 183 , training loss: 4.224402\n",
      "[INFO] Epoch: 56 , batch: 184 , training loss: 4.132213\n",
      "[INFO] Epoch: 56 , batch: 185 , training loss: 4.090333\n",
      "[INFO] Epoch: 56 , batch: 186 , training loss: 4.232378\n",
      "[INFO] Epoch: 56 , batch: 187 , training loss: 4.336621\n",
      "[INFO] Epoch: 56 , batch: 188 , training loss: 4.306895\n",
      "[INFO] Epoch: 56 , batch: 189 , training loss: 4.232583\n",
      "[INFO] Epoch: 56 , batch: 190 , training loss: 4.279209\n",
      "[INFO] Epoch: 56 , batch: 191 , training loss: 4.359413\n",
      "[INFO] Epoch: 56 , batch: 192 , training loss: 4.232895\n",
      "[INFO] Epoch: 56 , batch: 193 , training loss: 4.321469\n",
      "[INFO] Epoch: 56 , batch: 194 , training loss: 4.257155\n",
      "[INFO] Epoch: 56 , batch: 195 , training loss: 4.217328\n",
      "[INFO] Epoch: 56 , batch: 196 , training loss: 4.048427\n",
      "[INFO] Epoch: 56 , batch: 197 , training loss: 4.148343\n",
      "[INFO] Epoch: 56 , batch: 198 , training loss: 4.049492\n",
      "[INFO] Epoch: 56 , batch: 199 , training loss: 4.208159\n",
      "[INFO] Epoch: 56 , batch: 200 , training loss: 4.107269\n",
      "[INFO] Epoch: 56 , batch: 201 , training loss: 3.976067\n",
      "[INFO] Epoch: 56 , batch: 202 , training loss: 3.994550\n",
      "[INFO] Epoch: 56 , batch: 203 , training loss: 4.126078\n",
      "[INFO] Epoch: 56 , batch: 204 , training loss: 4.213010\n",
      "[INFO] Epoch: 56 , batch: 205 , training loss: 3.828672\n",
      "[INFO] Epoch: 56 , batch: 206 , training loss: 3.763776\n",
      "[INFO] Epoch: 56 , batch: 207 , training loss: 3.726676\n",
      "[INFO] Epoch: 56 , batch: 208 , training loss: 4.051438\n",
      "[INFO] Epoch: 56 , batch: 209 , training loss: 4.039535\n",
      "[INFO] Epoch: 56 , batch: 210 , training loss: 4.030435\n",
      "[INFO] Epoch: 56 , batch: 211 , training loss: 4.054099\n",
      "[INFO] Epoch: 56 , batch: 212 , training loss: 4.147903\n",
      "[INFO] Epoch: 56 , batch: 213 , training loss: 4.098258\n",
      "[INFO] Epoch: 56 , batch: 214 , training loss: 4.171872\n",
      "[INFO] Epoch: 56 , batch: 215 , training loss: 4.346894\n",
      "[INFO] Epoch: 56 , batch: 216 , training loss: 4.063143\n",
      "[INFO] Epoch: 56 , batch: 217 , training loss: 4.024248\n",
      "[INFO] Epoch: 56 , batch: 218 , training loss: 3.997466\n",
      "[INFO] Epoch: 56 , batch: 219 , training loss: 4.127826\n",
      "[INFO] Epoch: 56 , batch: 220 , training loss: 3.945671\n",
      "[INFO] Epoch: 56 , batch: 221 , training loss: 3.948607\n",
      "[INFO] Epoch: 56 , batch: 222 , training loss: 4.084851\n",
      "[INFO] Epoch: 56 , batch: 223 , training loss: 4.216444\n",
      "[INFO] Epoch: 56 , batch: 224 , training loss: 4.248346\n",
      "[INFO] Epoch: 56 , batch: 225 , training loss: 4.133836\n",
      "[INFO] Epoch: 56 , batch: 226 , training loss: 4.257760\n",
      "[INFO] Epoch: 56 , batch: 227 , training loss: 4.245090\n",
      "[INFO] Epoch: 56 , batch: 228 , training loss: 4.253733\n",
      "[INFO] Epoch: 56 , batch: 229 , training loss: 4.120284\n",
      "[INFO] Epoch: 56 , batch: 230 , training loss: 3.997262\n",
      "[INFO] Epoch: 56 , batch: 231 , training loss: 3.854183\n",
      "[INFO] Epoch: 56 , batch: 232 , training loss: 3.990932\n",
      "[INFO] Epoch: 56 , batch: 233 , training loss: 4.033125\n",
      "[INFO] Epoch: 56 , batch: 234 , training loss: 3.706057\n",
      "[INFO] Epoch: 56 , batch: 235 , training loss: 3.830674\n",
      "[INFO] Epoch: 56 , batch: 236 , training loss: 3.920772\n",
      "[INFO] Epoch: 56 , batch: 237 , training loss: 4.124070\n",
      "[INFO] Epoch: 56 , batch: 238 , training loss: 3.925236\n",
      "[INFO] Epoch: 56 , batch: 239 , training loss: 3.950344\n",
      "[INFO] Epoch: 56 , batch: 240 , training loss: 3.993904\n",
      "[INFO] Epoch: 56 , batch: 241 , training loss: 3.817822\n",
      "[INFO] Epoch: 56 , batch: 242 , training loss: 3.821398\n",
      "[INFO] Epoch: 56 , batch: 243 , training loss: 4.108513\n",
      "[INFO] Epoch: 56 , batch: 244 , training loss: 4.061945\n",
      "[INFO] Epoch: 56 , batch: 245 , training loss: 3.995743\n",
      "[INFO] Epoch: 56 , batch: 246 , training loss: 3.735688\n",
      "[INFO] Epoch: 56 , batch: 247 , training loss: 3.910199\n",
      "[INFO] Epoch: 56 , batch: 248 , training loss: 3.981310\n",
      "[INFO] Epoch: 56 , batch: 249 , training loss: 3.958211\n",
      "[INFO] Epoch: 56 , batch: 250 , training loss: 3.788514\n",
      "[INFO] Epoch: 56 , batch: 251 , training loss: 4.190926\n",
      "[INFO] Epoch: 56 , batch: 252 , training loss: 3.924374\n",
      "[INFO] Epoch: 56 , batch: 253 , training loss: 3.821960\n",
      "[INFO] Epoch: 56 , batch: 254 , training loss: 4.117756\n",
      "[INFO] Epoch: 56 , batch: 255 , training loss: 4.084711\n",
      "[INFO] Epoch: 56 , batch: 256 , training loss: 4.041729\n",
      "[INFO] Epoch: 56 , batch: 257 , training loss: 4.246377\n",
      "[INFO] Epoch: 56 , batch: 258 , training loss: 4.229524\n",
      "[INFO] Epoch: 56 , batch: 259 , training loss: 4.256948\n",
      "[INFO] Epoch: 56 , batch: 260 , training loss: 4.040083\n",
      "[INFO] Epoch: 56 , batch: 261 , training loss: 4.220575\n",
      "[INFO] Epoch: 56 , batch: 262 , training loss: 4.345480\n",
      "[INFO] Epoch: 56 , batch: 263 , training loss: 4.522561\n",
      "[INFO] Epoch: 56 , batch: 264 , training loss: 3.874371\n",
      "[INFO] Epoch: 56 , batch: 265 , training loss: 3.990901\n",
      "[INFO] Epoch: 56 , batch: 266 , training loss: 4.401306\n",
      "[INFO] Epoch: 56 , batch: 267 , training loss: 4.139468\n",
      "[INFO] Epoch: 56 , batch: 268 , training loss: 4.076540\n",
      "[INFO] Epoch: 56 , batch: 269 , training loss: 4.048814\n",
      "[INFO] Epoch: 56 , batch: 270 , training loss: 4.074006\n",
      "[INFO] Epoch: 56 , batch: 271 , training loss: 4.095591\n",
      "[INFO] Epoch: 56 , batch: 272 , training loss: 4.103312\n",
      "[INFO] Epoch: 56 , batch: 273 , training loss: 4.100203\n",
      "[INFO] Epoch: 56 , batch: 274 , training loss: 4.188892\n",
      "[INFO] Epoch: 56 , batch: 275 , training loss: 4.067666\n",
      "[INFO] Epoch: 56 , batch: 276 , training loss: 4.117154\n",
      "[INFO] Epoch: 56 , batch: 277 , training loss: 4.285967\n",
      "[INFO] Epoch: 56 , batch: 278 , training loss: 3.976024\n",
      "[INFO] Epoch: 56 , batch: 279 , training loss: 3.972642\n",
      "[INFO] Epoch: 56 , batch: 280 , training loss: 3.959479\n",
      "[INFO] Epoch: 56 , batch: 281 , training loss: 4.080683\n",
      "[INFO] Epoch: 56 , batch: 282 , training loss: 3.997489\n",
      "[INFO] Epoch: 56 , batch: 283 , training loss: 3.981941\n",
      "[INFO] Epoch: 56 , batch: 284 , training loss: 4.033984\n",
      "[INFO] Epoch: 56 , batch: 285 , training loss: 3.951907\n",
      "[INFO] Epoch: 56 , batch: 286 , training loss: 3.973700\n",
      "[INFO] Epoch: 56 , batch: 287 , training loss: 3.926922\n",
      "[INFO] Epoch: 56 , batch: 288 , training loss: 3.889604\n",
      "[INFO] Epoch: 56 , batch: 289 , training loss: 3.963319\n",
      "[INFO] Epoch: 56 , batch: 290 , training loss: 3.747309\n",
      "[INFO] Epoch: 56 , batch: 291 , training loss: 3.736610\n",
      "[INFO] Epoch: 56 , batch: 292 , training loss: 3.841566\n",
      "[INFO] Epoch: 56 , batch: 293 , training loss: 3.762141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 56 , batch: 294 , training loss: 4.416354\n",
      "[INFO] Epoch: 56 , batch: 295 , training loss: 4.200614\n",
      "[INFO] Epoch: 56 , batch: 296 , training loss: 4.116880\n",
      "[INFO] Epoch: 56 , batch: 297 , training loss: 4.092784\n",
      "[INFO] Epoch: 56 , batch: 298 , training loss: 3.929626\n",
      "[INFO] Epoch: 56 , batch: 299 , training loss: 3.972197\n",
      "[INFO] Epoch: 56 , batch: 300 , training loss: 3.941239\n",
      "[INFO] Epoch: 56 , batch: 301 , training loss: 3.856560\n",
      "[INFO] Epoch: 56 , batch: 302 , training loss: 4.039042\n",
      "[INFO] Epoch: 56 , batch: 303 , training loss: 4.038990\n",
      "[INFO] Epoch: 56 , batch: 304 , training loss: 4.181533\n",
      "[INFO] Epoch: 56 , batch: 305 , training loss: 4.036088\n",
      "[INFO] Epoch: 56 , batch: 306 , training loss: 4.143988\n",
      "[INFO] Epoch: 56 , batch: 307 , training loss: 4.158947\n",
      "[INFO] Epoch: 56 , batch: 308 , training loss: 3.975438\n",
      "[INFO] Epoch: 56 , batch: 309 , training loss: 3.959047\n",
      "[INFO] Epoch: 56 , batch: 310 , training loss: 3.886297\n",
      "[INFO] Epoch: 56 , batch: 311 , training loss: 3.893225\n",
      "[INFO] Epoch: 56 , batch: 312 , training loss: 3.792159\n",
      "[INFO] Epoch: 56 , batch: 313 , training loss: 3.893402\n",
      "[INFO] Epoch: 56 , batch: 314 , training loss: 3.968179\n",
      "[INFO] Epoch: 56 , batch: 315 , training loss: 4.048231\n",
      "[INFO] Epoch: 56 , batch: 316 , training loss: 4.281170\n",
      "[INFO] Epoch: 56 , batch: 317 , training loss: 4.614596\n",
      "[INFO] Epoch: 56 , batch: 318 , training loss: 4.767911\n",
      "[INFO] Epoch: 56 , batch: 319 , training loss: 4.442855\n",
      "[INFO] Epoch: 56 , batch: 320 , training loss: 3.988719\n",
      "[INFO] Epoch: 56 , batch: 321 , training loss: 3.818553\n",
      "[INFO] Epoch: 56 , batch: 322 , training loss: 3.933314\n",
      "[INFO] Epoch: 56 , batch: 323 , training loss: 3.955288\n",
      "[INFO] Epoch: 56 , batch: 324 , training loss: 3.919179\n",
      "[INFO] Epoch: 56 , batch: 325 , training loss: 4.040919\n",
      "[INFO] Epoch: 56 , batch: 326 , training loss: 4.119396\n",
      "[INFO] Epoch: 56 , batch: 327 , training loss: 4.044108\n",
      "[INFO] Epoch: 56 , batch: 328 , training loss: 4.069236\n",
      "[INFO] Epoch: 56 , batch: 329 , training loss: 3.959933\n",
      "[INFO] Epoch: 56 , batch: 330 , training loss: 3.932971\n",
      "[INFO] Epoch: 56 , batch: 331 , training loss: 4.081390\n",
      "[INFO] Epoch: 56 , batch: 332 , training loss: 3.945803\n",
      "[INFO] Epoch: 56 , batch: 333 , training loss: 3.919361\n",
      "[INFO] Epoch: 56 , batch: 334 , training loss: 3.938001\n",
      "[INFO] Epoch: 56 , batch: 335 , training loss: 4.067336\n",
      "[INFO] Epoch: 56 , batch: 336 , training loss: 4.073113\n",
      "[INFO] Epoch: 56 , batch: 337 , training loss: 4.098449\n",
      "[INFO] Epoch: 56 , batch: 338 , training loss: 4.323311\n",
      "[INFO] Epoch: 56 , batch: 339 , training loss: 4.137009\n",
      "[INFO] Epoch: 56 , batch: 340 , training loss: 4.336532\n",
      "[INFO] Epoch: 56 , batch: 341 , training loss: 4.096358\n",
      "[INFO] Epoch: 56 , batch: 342 , training loss: 3.875159\n",
      "[INFO] Epoch: 56 , batch: 343 , training loss: 3.946644\n",
      "[INFO] Epoch: 56 , batch: 344 , training loss: 3.816079\n",
      "[INFO] Epoch: 56 , batch: 345 , training loss: 3.929840\n",
      "[INFO] Epoch: 56 , batch: 346 , training loss: 3.996459\n",
      "[INFO] Epoch: 56 , batch: 347 , training loss: 3.891280\n",
      "[INFO] Epoch: 56 , batch: 348 , training loss: 3.978027\n",
      "[INFO] Epoch: 56 , batch: 349 , training loss: 4.069790\n",
      "[INFO] Epoch: 56 , batch: 350 , training loss: 3.956415\n",
      "[INFO] Epoch: 56 , batch: 351 , training loss: 4.031610\n",
      "[INFO] Epoch: 56 , batch: 352 , training loss: 4.035898\n",
      "[INFO] Epoch: 56 , batch: 353 , training loss: 4.026649\n",
      "[INFO] Epoch: 56 , batch: 354 , training loss: 4.110547\n",
      "[INFO] Epoch: 56 , batch: 355 , training loss: 4.101331\n",
      "[INFO] Epoch: 56 , batch: 356 , training loss: 3.983090\n",
      "[INFO] Epoch: 56 , batch: 357 , training loss: 4.026242\n",
      "[INFO] Epoch: 56 , batch: 358 , training loss: 3.953126\n",
      "[INFO] Epoch: 56 , batch: 359 , training loss: 3.959184\n",
      "[INFO] Epoch: 56 , batch: 360 , training loss: 4.062898\n",
      "[INFO] Epoch: 56 , batch: 361 , training loss: 4.036737\n",
      "[INFO] Epoch: 56 , batch: 362 , training loss: 4.138022\n",
      "[INFO] Epoch: 56 , batch: 363 , training loss: 4.031158\n",
      "[INFO] Epoch: 56 , batch: 364 , training loss: 4.067403\n",
      "[INFO] Epoch: 56 , batch: 365 , training loss: 4.000586\n",
      "[INFO] Epoch: 56 , batch: 366 , training loss: 4.095769\n",
      "[INFO] Epoch: 56 , batch: 367 , training loss: 4.135900\n",
      "[INFO] Epoch: 56 , batch: 368 , training loss: 4.544268\n",
      "[INFO] Epoch: 56 , batch: 369 , training loss: 4.222687\n",
      "[INFO] Epoch: 56 , batch: 370 , training loss: 3.992220\n",
      "[INFO] Epoch: 56 , batch: 371 , training loss: 4.395694\n",
      "[INFO] Epoch: 56 , batch: 372 , training loss: 4.668237\n",
      "[INFO] Epoch: 56 , batch: 373 , training loss: 4.642924\n",
      "[INFO] Epoch: 56 , batch: 374 , training loss: 4.821865\n",
      "[INFO] Epoch: 56 , batch: 375 , training loss: 4.819816\n",
      "[INFO] Epoch: 56 , batch: 376 , training loss: 4.671416\n",
      "[INFO] Epoch: 56 , batch: 377 , training loss: 4.439997\n",
      "[INFO] Epoch: 56 , batch: 378 , training loss: 4.559385\n",
      "[INFO] Epoch: 56 , batch: 379 , training loss: 4.521338\n",
      "[INFO] Epoch: 56 , batch: 380 , training loss: 4.716586\n",
      "[INFO] Epoch: 56 , batch: 381 , training loss: 4.386759\n",
      "[INFO] Epoch: 56 , batch: 382 , training loss: 4.644124\n",
      "[INFO] Epoch: 56 , batch: 383 , training loss: 4.696629\n",
      "[INFO] Epoch: 56 , batch: 384 , training loss: 4.669328\n",
      "[INFO] Epoch: 56 , batch: 385 , training loss: 4.354469\n",
      "[INFO] Epoch: 56 , batch: 386 , training loss: 4.600882\n",
      "[INFO] Epoch: 56 , batch: 387 , training loss: 4.576903\n",
      "[INFO] Epoch: 56 , batch: 388 , training loss: 4.389925\n",
      "[INFO] Epoch: 56 , batch: 389 , training loss: 4.185821\n",
      "[INFO] Epoch: 56 , batch: 390 , training loss: 4.209597\n",
      "[INFO] Epoch: 56 , batch: 391 , training loss: 4.233621\n",
      "[INFO] Epoch: 56 , batch: 392 , training loss: 4.624809\n",
      "[INFO] Epoch: 56 , batch: 393 , training loss: 4.492918\n",
      "[INFO] Epoch: 56 , batch: 394 , training loss: 4.600440\n",
      "[INFO] Epoch: 56 , batch: 395 , training loss: 4.406945\n",
      "[INFO] Epoch: 56 , batch: 396 , training loss: 4.231371\n",
      "[INFO] Epoch: 56 , batch: 397 , training loss: 4.384821\n",
      "[INFO] Epoch: 56 , batch: 398 , training loss: 4.236965\n",
      "[INFO] Epoch: 56 , batch: 399 , training loss: 4.329628\n",
      "[INFO] Epoch: 56 , batch: 400 , training loss: 4.294983\n",
      "[INFO] Epoch: 56 , batch: 401 , training loss: 4.701483\n",
      "[INFO] Epoch: 56 , batch: 402 , training loss: 4.438317\n",
      "[INFO] Epoch: 56 , batch: 403 , training loss: 4.277816\n",
      "[INFO] Epoch: 56 , batch: 404 , training loss: 4.472044\n",
      "[INFO] Epoch: 56 , batch: 405 , training loss: 4.497772\n",
      "[INFO] Epoch: 56 , batch: 406 , training loss: 4.402443\n",
      "[INFO] Epoch: 56 , batch: 407 , training loss: 4.428072\n",
      "[INFO] Epoch: 56 , batch: 408 , training loss: 4.413198\n",
      "[INFO] Epoch: 56 , batch: 409 , training loss: 4.411105\n",
      "[INFO] Epoch: 56 , batch: 410 , training loss: 4.484799\n",
      "[INFO] Epoch: 56 , batch: 411 , training loss: 4.650461\n",
      "[INFO] Epoch: 56 , batch: 412 , training loss: 4.465007\n",
      "[INFO] Epoch: 56 , batch: 413 , training loss: 4.334053\n",
      "[INFO] Epoch: 56 , batch: 414 , training loss: 4.383207\n",
      "[INFO] Epoch: 56 , batch: 415 , training loss: 4.433627\n",
      "[INFO] Epoch: 56 , batch: 416 , training loss: 4.482772\n",
      "[INFO] Epoch: 56 , batch: 417 , training loss: 4.401969\n",
      "[INFO] Epoch: 56 , batch: 418 , training loss: 4.469657\n",
      "[INFO] Epoch: 56 , batch: 419 , training loss: 4.397954\n",
      "[INFO] Epoch: 56 , batch: 420 , training loss: 4.397255\n",
      "[INFO] Epoch: 56 , batch: 421 , training loss: 4.378747\n",
      "[INFO] Epoch: 56 , batch: 422 , training loss: 4.225286\n",
      "[INFO] Epoch: 56 , batch: 423 , training loss: 4.447423\n",
      "[INFO] Epoch: 56 , batch: 424 , training loss: 4.609931\n",
      "[INFO] Epoch: 56 , batch: 425 , training loss: 4.497863\n",
      "[INFO] Epoch: 56 , batch: 426 , training loss: 4.229634\n",
      "[INFO] Epoch: 56 , batch: 427 , training loss: 4.461220\n",
      "[INFO] Epoch: 56 , batch: 428 , training loss: 4.335671\n",
      "[INFO] Epoch: 56 , batch: 429 , training loss: 4.223411\n",
      "[INFO] Epoch: 56 , batch: 430 , training loss: 4.470393\n",
      "[INFO] Epoch: 56 , batch: 431 , training loss: 4.088527\n",
      "[INFO] Epoch: 56 , batch: 432 , training loss: 4.138629\n",
      "[INFO] Epoch: 56 , batch: 433 , training loss: 4.171579\n",
      "[INFO] Epoch: 56 , batch: 434 , training loss: 4.063191\n",
      "[INFO] Epoch: 56 , batch: 435 , training loss: 4.398150\n",
      "[INFO] Epoch: 56 , batch: 436 , training loss: 4.429882\n",
      "[INFO] Epoch: 56 , batch: 437 , training loss: 4.233960\n",
      "[INFO] Epoch: 56 , batch: 438 , training loss: 4.086435\n",
      "[INFO] Epoch: 56 , batch: 439 , training loss: 4.318928\n",
      "[INFO] Epoch: 56 , batch: 440 , training loss: 4.441376\n",
      "[INFO] Epoch: 56 , batch: 441 , training loss: 4.553596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 56 , batch: 442 , training loss: 4.311554\n",
      "[INFO] Epoch: 56 , batch: 443 , training loss: 4.460518\n",
      "[INFO] Epoch: 56 , batch: 444 , training loss: 4.101393\n",
      "[INFO] Epoch: 56 , batch: 445 , training loss: 3.979531\n",
      "[INFO] Epoch: 56 , batch: 446 , training loss: 3.921801\n",
      "[INFO] Epoch: 56 , batch: 447 , training loss: 4.113568\n",
      "[INFO] Epoch: 56 , batch: 448 , training loss: 4.246809\n",
      "[INFO] Epoch: 56 , batch: 449 , training loss: 4.622828\n",
      "[INFO] Epoch: 56 , batch: 450 , training loss: 4.697426\n",
      "[INFO] Epoch: 56 , batch: 451 , training loss: 4.573720\n",
      "[INFO] Epoch: 56 , batch: 452 , training loss: 4.401928\n",
      "[INFO] Epoch: 56 , batch: 453 , training loss: 4.171699\n",
      "[INFO] Epoch: 56 , batch: 454 , training loss: 4.335243\n",
      "[INFO] Epoch: 56 , batch: 455 , training loss: 4.343440\n",
      "[INFO] Epoch: 56 , batch: 456 , training loss: 4.361372\n",
      "[INFO] Epoch: 56 , batch: 457 , training loss: 4.439866\n",
      "[INFO] Epoch: 56 , batch: 458 , training loss: 4.178342\n",
      "[INFO] Epoch: 56 , batch: 459 , training loss: 4.163958\n",
      "[INFO] Epoch: 56 , batch: 460 , training loss: 4.277158\n",
      "[INFO] Epoch: 56 , batch: 461 , training loss: 4.227931\n",
      "[INFO] Epoch: 56 , batch: 462 , training loss: 4.317537\n",
      "[INFO] Epoch: 56 , batch: 463 , training loss: 4.207106\n",
      "[INFO] Epoch: 56 , batch: 464 , training loss: 4.396565\n",
      "[INFO] Epoch: 56 , batch: 465 , training loss: 4.330922\n",
      "[INFO] Epoch: 56 , batch: 466 , training loss: 4.405235\n",
      "[INFO] Epoch: 56 , batch: 467 , training loss: 4.382280\n",
      "[INFO] Epoch: 56 , batch: 468 , training loss: 4.336623\n",
      "[INFO] Epoch: 56 , batch: 469 , training loss: 4.387600\n",
      "[INFO] Epoch: 56 , batch: 470 , training loss: 4.194189\n",
      "[INFO] Epoch: 56 , batch: 471 , training loss: 4.291516\n",
      "[INFO] Epoch: 56 , batch: 472 , training loss: 4.356455\n",
      "[INFO] Epoch: 56 , batch: 473 , training loss: 4.272728\n",
      "[INFO] Epoch: 56 , batch: 474 , training loss: 4.056798\n",
      "[INFO] Epoch: 56 , batch: 475 , training loss: 3.926673\n",
      "[INFO] Epoch: 56 , batch: 476 , training loss: 4.326214\n",
      "[INFO] Epoch: 56 , batch: 477 , training loss: 4.441524\n",
      "[INFO] Epoch: 56 , batch: 478 , training loss: 4.452842\n",
      "[INFO] Epoch: 56 , batch: 479 , training loss: 4.434894\n",
      "[INFO] Epoch: 56 , batch: 480 , training loss: 4.572574\n",
      "[INFO] Epoch: 56 , batch: 481 , training loss: 4.436059\n",
      "[INFO] Epoch: 56 , batch: 482 , training loss: 4.527461\n",
      "[INFO] Epoch: 56 , batch: 483 , training loss: 4.376585\n",
      "[INFO] Epoch: 56 , batch: 484 , training loss: 4.192482\n",
      "[INFO] Epoch: 56 , batch: 485 , training loss: 4.302507\n",
      "[INFO] Epoch: 56 , batch: 486 , training loss: 4.174005\n",
      "[INFO] Epoch: 56 , batch: 487 , training loss: 4.183781\n",
      "[INFO] Epoch: 56 , batch: 488 , training loss: 4.349924\n",
      "[INFO] Epoch: 56 , batch: 489 , training loss: 4.241342\n",
      "[INFO] Epoch: 56 , batch: 490 , training loss: 4.308411\n",
      "[INFO] Epoch: 56 , batch: 491 , training loss: 4.237488\n",
      "[INFO] Epoch: 56 , batch: 492 , training loss: 4.220726\n",
      "[INFO] Epoch: 56 , batch: 493 , training loss: 4.360944\n",
      "[INFO] Epoch: 56 , batch: 494 , training loss: 4.287922\n",
      "[INFO] Epoch: 56 , batch: 495 , training loss: 4.446853\n",
      "[INFO] Epoch: 56 , batch: 496 , training loss: 4.301446\n",
      "[INFO] Epoch: 56 , batch: 497 , training loss: 4.346052\n",
      "[INFO] Epoch: 56 , batch: 498 , training loss: 4.312603\n",
      "[INFO] Epoch: 56 , batch: 499 , training loss: 4.402060\n",
      "[INFO] Epoch: 56 , batch: 500 , training loss: 4.554302\n",
      "[INFO] Epoch: 56 , batch: 501 , training loss: 4.901476\n",
      "[INFO] Epoch: 56 , batch: 502 , training loss: 4.890074\n",
      "[INFO] Epoch: 56 , batch: 503 , training loss: 4.522069\n",
      "[INFO] Epoch: 56 , batch: 504 , training loss: 4.670103\n",
      "[INFO] Epoch: 56 , batch: 505 , training loss: 4.638094\n",
      "[INFO] Epoch: 56 , batch: 506 , training loss: 4.622102\n",
      "[INFO] Epoch: 56 , batch: 507 , training loss: 4.692049\n",
      "[INFO] Epoch: 56 , batch: 508 , training loss: 4.587322\n",
      "[INFO] Epoch: 56 , batch: 509 , training loss: 4.404927\n",
      "[INFO] Epoch: 56 , batch: 510 , training loss: 4.482728\n",
      "[INFO] Epoch: 56 , batch: 511 , training loss: 4.417565\n",
      "[INFO] Epoch: 56 , batch: 512 , training loss: 4.512968\n",
      "[INFO] Epoch: 56 , batch: 513 , training loss: 4.775423\n",
      "[INFO] Epoch: 56 , batch: 514 , training loss: 4.399806\n",
      "[INFO] Epoch: 56 , batch: 515 , training loss: 4.665877\n",
      "[INFO] Epoch: 56 , batch: 516 , training loss: 4.465838\n",
      "[INFO] Epoch: 56 , batch: 517 , training loss: 4.436936\n",
      "[INFO] Epoch: 56 , batch: 518 , training loss: 4.386807\n",
      "[INFO] Epoch: 56 , batch: 519 , training loss: 4.220106\n",
      "[INFO] Epoch: 56 , batch: 520 , training loss: 4.460988\n",
      "[INFO] Epoch: 56 , batch: 521 , training loss: 4.455207\n",
      "[INFO] Epoch: 56 , batch: 522 , training loss: 4.526245\n",
      "[INFO] Epoch: 56 , batch: 523 , training loss: 4.438777\n",
      "[INFO] Epoch: 56 , batch: 524 , training loss: 4.735421\n",
      "[INFO] Epoch: 56 , batch: 525 , training loss: 4.621669\n",
      "[INFO] Epoch: 56 , batch: 526 , training loss: 4.386806\n",
      "[INFO] Epoch: 56 , batch: 527 , training loss: 4.433896\n",
      "[INFO] Epoch: 56 , batch: 528 , training loss: 4.448425\n",
      "[INFO] Epoch: 56 , batch: 529 , training loss: 4.422066\n",
      "[INFO] Epoch: 56 , batch: 530 , training loss: 4.271896\n",
      "[INFO] Epoch: 56 , batch: 531 , training loss: 4.422249\n",
      "[INFO] Epoch: 56 , batch: 532 , training loss: 4.339818\n",
      "[INFO] Epoch: 56 , batch: 533 , training loss: 4.467448\n",
      "[INFO] Epoch: 56 , batch: 534 , training loss: 4.461284\n",
      "[INFO] Epoch: 56 , batch: 535 , training loss: 4.460348\n",
      "[INFO] Epoch: 56 , batch: 536 , training loss: 4.314339\n",
      "[INFO] Epoch: 56 , batch: 537 , training loss: 4.303131\n",
      "[INFO] Epoch: 56 , batch: 538 , training loss: 4.390591\n",
      "[INFO] Epoch: 56 , batch: 539 , training loss: 4.464059\n",
      "[INFO] Epoch: 56 , batch: 540 , training loss: 4.990040\n",
      "[INFO] Epoch: 56 , batch: 541 , training loss: 4.828629\n",
      "[INFO] Epoch: 56 , batch: 542 , training loss: 4.717928\n",
      "[INFO] Epoch: 57 , batch: 0 , training loss: 3.616673\n",
      "[INFO] Epoch: 57 , batch: 1 , training loss: 3.540677\n",
      "[INFO] Epoch: 57 , batch: 2 , training loss: 3.692371\n",
      "[INFO] Epoch: 57 , batch: 3 , training loss: 3.616464\n",
      "[INFO] Epoch: 57 , batch: 4 , training loss: 3.980460\n",
      "[INFO] Epoch: 57 , batch: 5 , training loss: 3.625503\n",
      "[INFO] Epoch: 57 , batch: 6 , training loss: 4.087309\n",
      "[INFO] Epoch: 57 , batch: 7 , training loss: 3.936321\n",
      "[INFO] Epoch: 57 , batch: 8 , training loss: 3.593825\n",
      "[INFO] Epoch: 57 , batch: 9 , training loss: 3.883086\n",
      "[INFO] Epoch: 57 , batch: 10 , training loss: 3.823800\n",
      "[INFO] Epoch: 57 , batch: 11 , training loss: 3.768174\n",
      "[INFO] Epoch: 57 , batch: 12 , training loss: 3.671372\n",
      "[INFO] Epoch: 57 , batch: 13 , training loss: 3.675937\n",
      "[INFO] Epoch: 57 , batch: 14 , training loss: 3.634440\n",
      "[INFO] Epoch: 57 , batch: 15 , training loss: 3.808246\n",
      "[INFO] Epoch: 57 , batch: 16 , training loss: 3.652238\n",
      "[INFO] Epoch: 57 , batch: 17 , training loss: 3.775499\n",
      "[INFO] Epoch: 57 , batch: 18 , training loss: 3.728682\n",
      "[INFO] Epoch: 57 , batch: 19 , training loss: 3.478481\n",
      "[INFO] Epoch: 57 , batch: 20 , training loss: 3.453360\n",
      "[INFO] Epoch: 57 , batch: 21 , training loss: 3.584113\n",
      "[INFO] Epoch: 57 , batch: 22 , training loss: 3.460104\n",
      "[INFO] Epoch: 57 , batch: 23 , training loss: 3.690001\n",
      "[INFO] Epoch: 57 , batch: 24 , training loss: 3.540099\n",
      "[INFO] Epoch: 57 , batch: 25 , training loss: 3.652950\n",
      "[INFO] Epoch: 57 , batch: 26 , training loss: 3.532778\n",
      "[INFO] Epoch: 57 , batch: 27 , training loss: 3.509514\n",
      "[INFO] Epoch: 57 , batch: 28 , training loss: 3.685379\n",
      "[INFO] Epoch: 57 , batch: 29 , training loss: 3.491810\n",
      "[INFO] Epoch: 57 , batch: 30 , training loss: 3.516329\n",
      "[INFO] Epoch: 57 , batch: 31 , training loss: 3.615082\n",
      "[INFO] Epoch: 57 , batch: 32 , training loss: 3.583496\n",
      "[INFO] Epoch: 57 , batch: 33 , training loss: 3.616755\n",
      "[INFO] Epoch: 57 , batch: 34 , training loss: 3.612738\n",
      "[INFO] Epoch: 57 , batch: 35 , training loss: 3.562787\n",
      "[INFO] Epoch: 57 , batch: 36 , training loss: 3.644900\n",
      "[INFO] Epoch: 57 , batch: 37 , training loss: 3.514965\n",
      "[INFO] Epoch: 57 , batch: 38 , training loss: 3.582400\n",
      "[INFO] Epoch: 57 , batch: 39 , training loss: 3.418958\n",
      "[INFO] Epoch: 57 , batch: 40 , training loss: 3.611386\n",
      "[INFO] Epoch: 57 , batch: 41 , training loss: 3.591311\n",
      "[INFO] Epoch: 57 , batch: 42 , training loss: 4.024435\n",
      "[INFO] Epoch: 57 , batch: 43 , training loss: 3.811843\n",
      "[INFO] Epoch: 57 , batch: 44 , training loss: 4.112545\n",
      "[INFO] Epoch: 57 , batch: 45 , training loss: 4.033434\n",
      "[INFO] Epoch: 57 , batch: 46 , training loss: 4.030793\n",
      "[INFO] Epoch: 57 , batch: 47 , training loss: 3.632749\n",
      "[INFO] Epoch: 57 , batch: 48 , training loss: 3.639858\n",
      "[INFO] Epoch: 57 , batch: 49 , training loss: 3.833724\n",
      "[INFO] Epoch: 57 , batch: 50 , training loss: 3.591866\n",
      "[INFO] Epoch: 57 , batch: 51 , training loss: 3.816765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 57 , batch: 52 , training loss: 3.619490\n",
      "[INFO] Epoch: 57 , batch: 53 , training loss: 3.753616\n",
      "[INFO] Epoch: 57 , batch: 54 , training loss: 3.739665\n",
      "[INFO] Epoch: 57 , batch: 55 , training loss: 3.816001\n",
      "[INFO] Epoch: 57 , batch: 56 , training loss: 3.669344\n",
      "[INFO] Epoch: 57 , batch: 57 , training loss: 3.602231\n",
      "[INFO] Epoch: 57 , batch: 58 , training loss: 3.663204\n",
      "[INFO] Epoch: 57 , batch: 59 , training loss: 3.716778\n",
      "[INFO] Epoch: 57 , batch: 60 , training loss: 3.649035\n",
      "[INFO] Epoch: 57 , batch: 61 , training loss: 3.749956\n",
      "[INFO] Epoch: 57 , batch: 62 , training loss: 3.614094\n",
      "[INFO] Epoch: 57 , batch: 63 , training loss: 3.817692\n",
      "[INFO] Epoch: 57 , batch: 64 , training loss: 4.021233\n",
      "[INFO] Epoch: 57 , batch: 65 , training loss: 3.738163\n",
      "[INFO] Epoch: 57 , batch: 66 , training loss: 3.596615\n",
      "[INFO] Epoch: 57 , batch: 67 , training loss: 3.617242\n",
      "[INFO] Epoch: 57 , batch: 68 , training loss: 3.756208\n",
      "[INFO] Epoch: 57 , batch: 69 , training loss: 3.689269\n",
      "[INFO] Epoch: 57 , batch: 70 , training loss: 3.942873\n",
      "[INFO] Epoch: 57 , batch: 71 , training loss: 3.766233\n",
      "[INFO] Epoch: 57 , batch: 72 , training loss: 3.829881\n",
      "[INFO] Epoch: 57 , batch: 73 , training loss: 3.778470\n",
      "[INFO] Epoch: 57 , batch: 74 , training loss: 3.885089\n",
      "[INFO] Epoch: 57 , batch: 75 , training loss: 3.747446\n",
      "[INFO] Epoch: 57 , batch: 76 , training loss: 3.852576\n",
      "[INFO] Epoch: 57 , batch: 77 , training loss: 3.800301\n",
      "[INFO] Epoch: 57 , batch: 78 , training loss: 3.886336\n",
      "[INFO] Epoch: 57 , batch: 79 , training loss: 3.748764\n",
      "[INFO] Epoch: 57 , batch: 80 , training loss: 3.931056\n",
      "[INFO] Epoch: 57 , batch: 81 , training loss: 3.844017\n",
      "[INFO] Epoch: 57 , batch: 82 , training loss: 3.807999\n",
      "[INFO] Epoch: 57 , batch: 83 , training loss: 3.899034\n",
      "[INFO] Epoch: 57 , batch: 84 , training loss: 3.926386\n",
      "[INFO] Epoch: 57 , batch: 85 , training loss: 3.970887\n",
      "[INFO] Epoch: 57 , batch: 86 , training loss: 3.908032\n",
      "[INFO] Epoch: 57 , batch: 87 , training loss: 3.853933\n",
      "[INFO] Epoch: 57 , batch: 88 , training loss: 3.999370\n",
      "[INFO] Epoch: 57 , batch: 89 , training loss: 3.768112\n",
      "[INFO] Epoch: 57 , batch: 90 , training loss: 3.868250\n",
      "[INFO] Epoch: 57 , batch: 91 , training loss: 3.819232\n",
      "[INFO] Epoch: 57 , batch: 92 , training loss: 3.833905\n",
      "[INFO] Epoch: 57 , batch: 93 , training loss: 3.943635\n",
      "[INFO] Epoch: 57 , batch: 94 , training loss: 4.039410\n",
      "[INFO] Epoch: 57 , batch: 95 , training loss: 3.866083\n",
      "[INFO] Epoch: 57 , batch: 96 , training loss: 3.832385\n",
      "[INFO] Epoch: 57 , batch: 97 , training loss: 3.767697\n",
      "[INFO] Epoch: 57 , batch: 98 , training loss: 3.705700\n",
      "[INFO] Epoch: 57 , batch: 99 , training loss: 3.821205\n",
      "[INFO] Epoch: 57 , batch: 100 , training loss: 3.751064\n",
      "[INFO] Epoch: 57 , batch: 101 , training loss: 3.768581\n",
      "[INFO] Epoch: 57 , batch: 102 , training loss: 3.918341\n",
      "[INFO] Epoch: 57 , batch: 103 , training loss: 3.700856\n",
      "[INFO] Epoch: 57 , batch: 104 , training loss: 3.650888\n",
      "[INFO] Epoch: 57 , batch: 105 , training loss: 3.893348\n",
      "[INFO] Epoch: 57 , batch: 106 , training loss: 3.939573\n",
      "[INFO] Epoch: 57 , batch: 107 , training loss: 3.772057\n",
      "[INFO] Epoch: 57 , batch: 108 , training loss: 3.714378\n",
      "[INFO] Epoch: 57 , batch: 109 , training loss: 3.632980\n",
      "[INFO] Epoch: 57 , batch: 110 , training loss: 3.858534\n",
      "[INFO] Epoch: 57 , batch: 111 , training loss: 3.910908\n",
      "[INFO] Epoch: 57 , batch: 112 , training loss: 3.805856\n",
      "[INFO] Epoch: 57 , batch: 113 , training loss: 3.784278\n",
      "[INFO] Epoch: 57 , batch: 114 , training loss: 3.801143\n",
      "[INFO] Epoch: 57 , batch: 115 , training loss: 3.816430\n",
      "[INFO] Epoch: 57 , batch: 116 , training loss: 3.712675\n",
      "[INFO] Epoch: 57 , batch: 117 , training loss: 3.919838\n",
      "[INFO] Epoch: 57 , batch: 118 , training loss: 3.900597\n",
      "[INFO] Epoch: 57 , batch: 119 , training loss: 4.025376\n",
      "[INFO] Epoch: 57 , batch: 120 , training loss: 4.009960\n",
      "[INFO] Epoch: 57 , batch: 121 , training loss: 3.913118\n",
      "[INFO] Epoch: 57 , batch: 122 , training loss: 3.783638\n",
      "[INFO] Epoch: 57 , batch: 123 , training loss: 3.780420\n",
      "[INFO] Epoch: 57 , batch: 124 , training loss: 3.886465\n",
      "[INFO] Epoch: 57 , batch: 125 , training loss: 3.709523\n",
      "[INFO] Epoch: 57 , batch: 126 , training loss: 3.728653\n",
      "[INFO] Epoch: 57 , batch: 127 , training loss: 3.715574\n",
      "[INFO] Epoch: 57 , batch: 128 , training loss: 3.854969\n",
      "[INFO] Epoch: 57 , batch: 129 , training loss: 3.809698\n",
      "[INFO] Epoch: 57 , batch: 130 , training loss: 3.824270\n",
      "[INFO] Epoch: 57 , batch: 131 , training loss: 3.834253\n",
      "[INFO] Epoch: 57 , batch: 132 , training loss: 3.833085\n",
      "[INFO] Epoch: 57 , batch: 133 , training loss: 3.799898\n",
      "[INFO] Epoch: 57 , batch: 134 , training loss: 3.600059\n",
      "[INFO] Epoch: 57 , batch: 135 , training loss: 3.643743\n",
      "[INFO] Epoch: 57 , batch: 136 , training loss: 3.913257\n",
      "[INFO] Epoch: 57 , batch: 137 , training loss: 3.853406\n",
      "[INFO] Epoch: 57 , batch: 138 , training loss: 3.913706\n",
      "[INFO] Epoch: 57 , batch: 139 , training loss: 4.453198\n",
      "[INFO] Epoch: 57 , batch: 140 , training loss: 4.274595\n",
      "[INFO] Epoch: 57 , batch: 141 , training loss: 4.000786\n",
      "[INFO] Epoch: 57 , batch: 142 , training loss: 3.736364\n",
      "[INFO] Epoch: 57 , batch: 143 , training loss: 3.871848\n",
      "[INFO] Epoch: 57 , batch: 144 , training loss: 3.730392\n",
      "[INFO] Epoch: 57 , batch: 145 , training loss: 3.782875\n",
      "[INFO] Epoch: 57 , batch: 146 , training loss: 3.988694\n",
      "[INFO] Epoch: 57 , batch: 147 , training loss: 3.664562\n",
      "[INFO] Epoch: 57 , batch: 148 , training loss: 3.608124\n",
      "[INFO] Epoch: 57 , batch: 149 , training loss: 3.707914\n",
      "[INFO] Epoch: 57 , batch: 150 , training loss: 3.966794\n",
      "[INFO] Epoch: 57 , batch: 151 , training loss: 3.831867\n",
      "[INFO] Epoch: 57 , batch: 152 , training loss: 3.834610\n",
      "[INFO] Epoch: 57 , batch: 153 , training loss: 3.860284\n",
      "[INFO] Epoch: 57 , batch: 154 , training loss: 3.939097\n",
      "[INFO] Epoch: 57 , batch: 155 , training loss: 4.129781\n",
      "[INFO] Epoch: 57 , batch: 156 , training loss: 3.896718\n",
      "[INFO] Epoch: 57 , batch: 157 , training loss: 3.869702\n",
      "[INFO] Epoch: 57 , batch: 158 , training loss: 3.957031\n",
      "[INFO] Epoch: 57 , batch: 159 , training loss: 3.903549\n",
      "[INFO] Epoch: 57 , batch: 160 , training loss: 4.125462\n",
      "[INFO] Epoch: 57 , batch: 161 , training loss: 4.243120\n",
      "[INFO] Epoch: 57 , batch: 162 , training loss: 4.214530\n",
      "[INFO] Epoch: 57 , batch: 163 , training loss: 4.395503\n",
      "[INFO] Epoch: 57 , batch: 164 , training loss: 4.338685\n",
      "[INFO] Epoch: 57 , batch: 165 , training loss: 4.222636\n",
      "[INFO] Epoch: 57 , batch: 166 , training loss: 4.183644\n",
      "[INFO] Epoch: 57 , batch: 167 , training loss: 4.222503\n",
      "[INFO] Epoch: 57 , batch: 168 , training loss: 3.933184\n",
      "[INFO] Epoch: 57 , batch: 169 , training loss: 3.893803\n",
      "[INFO] Epoch: 57 , batch: 170 , training loss: 4.110081\n",
      "[INFO] Epoch: 57 , batch: 171 , training loss: 3.526862\n",
      "[INFO] Epoch: 57 , batch: 172 , training loss: 3.746262\n",
      "[INFO] Epoch: 57 , batch: 173 , training loss: 4.059002\n",
      "[INFO] Epoch: 57 , batch: 174 , training loss: 4.496779\n",
      "[INFO] Epoch: 57 , batch: 175 , training loss: 4.769751\n",
      "[INFO] Epoch: 57 , batch: 176 , training loss: 4.424145\n",
      "[INFO] Epoch: 57 , batch: 177 , training loss: 3.999775\n",
      "[INFO] Epoch: 57 , batch: 178 , training loss: 4.030660\n",
      "[INFO] Epoch: 57 , batch: 179 , training loss: 4.086431\n",
      "[INFO] Epoch: 57 , batch: 180 , training loss: 4.040075\n",
      "[INFO] Epoch: 57 , batch: 181 , training loss: 4.346390\n",
      "[INFO] Epoch: 57 , batch: 182 , training loss: 4.287412\n",
      "[INFO] Epoch: 57 , batch: 183 , training loss: 4.262252\n",
      "[INFO] Epoch: 57 , batch: 184 , training loss: 4.146857\n",
      "[INFO] Epoch: 57 , batch: 185 , training loss: 4.100368\n",
      "[INFO] Epoch: 57 , batch: 186 , training loss: 4.253211\n",
      "[INFO] Epoch: 57 , batch: 187 , training loss: 4.340433\n",
      "[INFO] Epoch: 57 , batch: 188 , training loss: 4.323839\n",
      "[INFO] Epoch: 57 , batch: 189 , training loss: 4.262155\n",
      "[INFO] Epoch: 57 , batch: 190 , training loss: 4.278930\n",
      "[INFO] Epoch: 57 , batch: 191 , training loss: 4.411232\n",
      "[INFO] Epoch: 57 , batch: 192 , training loss: 4.246953\n",
      "[INFO] Epoch: 57 , batch: 193 , training loss: 4.346841\n",
      "[INFO] Epoch: 57 , batch: 194 , training loss: 4.282448\n",
      "[INFO] Epoch: 57 , batch: 195 , training loss: 4.232797\n",
      "[INFO] Epoch: 57 , batch: 196 , training loss: 4.064896\n",
      "[INFO] Epoch: 57 , batch: 197 , training loss: 4.152606\n",
      "[INFO] Epoch: 57 , batch: 198 , training loss: 4.069640\n",
      "[INFO] Epoch: 57 , batch: 199 , training loss: 4.211506\n",
      "[INFO] Epoch: 57 , batch: 200 , training loss: 4.118846\n",
      "[INFO] Epoch: 57 , batch: 201 , training loss: 3.997545\n",
      "[INFO] Epoch: 57 , batch: 202 , training loss: 3.994369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 57 , batch: 203 , training loss: 4.139840\n",
      "[INFO] Epoch: 57 , batch: 204 , training loss: 4.211557\n",
      "[INFO] Epoch: 57 , batch: 205 , training loss: 3.839720\n",
      "[INFO] Epoch: 57 , batch: 206 , training loss: 3.764269\n",
      "[INFO] Epoch: 57 , batch: 207 , training loss: 3.752478\n",
      "[INFO] Epoch: 57 , batch: 208 , training loss: 4.062160\n",
      "[INFO] Epoch: 57 , batch: 209 , training loss: 4.058107\n",
      "[INFO] Epoch: 57 , batch: 210 , training loss: 4.050051\n",
      "[INFO] Epoch: 57 , batch: 211 , training loss: 4.056182\n",
      "[INFO] Epoch: 57 , batch: 212 , training loss: 4.144705\n",
      "[INFO] Epoch: 57 , batch: 213 , training loss: 4.093955\n",
      "[INFO] Epoch: 57 , batch: 214 , training loss: 4.165680\n",
      "[INFO] Epoch: 57 , batch: 215 , training loss: 4.344364\n",
      "[INFO] Epoch: 57 , batch: 216 , training loss: 4.068804\n",
      "[INFO] Epoch: 57 , batch: 217 , training loss: 4.024361\n",
      "[INFO] Epoch: 57 , batch: 218 , training loss: 4.023795\n",
      "[INFO] Epoch: 57 , batch: 219 , training loss: 4.127060\n",
      "[INFO] Epoch: 57 , batch: 220 , training loss: 3.976053\n",
      "[INFO] Epoch: 57 , batch: 221 , training loss: 3.961647\n",
      "[INFO] Epoch: 57 , batch: 222 , training loss: 4.088823\n",
      "[INFO] Epoch: 57 , batch: 223 , training loss: 4.240328\n",
      "[INFO] Epoch: 57 , batch: 224 , training loss: 4.262142\n",
      "[INFO] Epoch: 57 , batch: 225 , training loss: 4.131015\n",
      "[INFO] Epoch: 57 , batch: 226 , training loss: 4.257799\n",
      "[INFO] Epoch: 57 , batch: 227 , training loss: 4.250017\n",
      "[INFO] Epoch: 57 , batch: 228 , training loss: 4.257308\n",
      "[INFO] Epoch: 57 , batch: 229 , training loss: 4.128973\n",
      "[INFO] Epoch: 57 , batch: 230 , training loss: 3.981270\n",
      "[INFO] Epoch: 57 , batch: 231 , training loss: 3.860779\n",
      "[INFO] Epoch: 57 , batch: 232 , training loss: 3.977396\n",
      "[INFO] Epoch: 57 , batch: 233 , training loss: 4.033977\n",
      "[INFO] Epoch: 57 , batch: 234 , training loss: 3.699803\n",
      "[INFO] Epoch: 57 , batch: 235 , training loss: 3.827133\n",
      "[INFO] Epoch: 57 , batch: 236 , training loss: 3.917430\n",
      "[INFO] Epoch: 57 , batch: 237 , training loss: 4.125987\n",
      "[INFO] Epoch: 57 , batch: 238 , training loss: 3.930556\n",
      "[INFO] Epoch: 57 , batch: 239 , training loss: 3.935462\n",
      "[INFO] Epoch: 57 , batch: 240 , training loss: 4.017295\n",
      "[INFO] Epoch: 57 , batch: 241 , training loss: 3.821117\n",
      "[INFO] Epoch: 57 , batch: 242 , training loss: 3.824499\n",
      "[INFO] Epoch: 57 , batch: 243 , training loss: 4.120674\n",
      "[INFO] Epoch: 57 , batch: 244 , training loss: 4.060576\n",
      "[INFO] Epoch: 57 , batch: 245 , training loss: 4.019162\n",
      "[INFO] Epoch: 57 , batch: 246 , training loss: 3.757705\n",
      "[INFO] Epoch: 57 , batch: 247 , training loss: 3.909503\n",
      "[INFO] Epoch: 57 , batch: 248 , training loss: 3.982191\n",
      "[INFO] Epoch: 57 , batch: 249 , training loss: 3.969289\n",
      "[INFO] Epoch: 57 , batch: 250 , training loss: 3.780249\n",
      "[INFO] Epoch: 57 , batch: 251 , training loss: 4.208001\n",
      "[INFO] Epoch: 57 , batch: 252 , training loss: 3.906803\n",
      "[INFO] Epoch: 57 , batch: 253 , training loss: 3.841047\n",
      "[INFO] Epoch: 57 , batch: 254 , training loss: 4.108971\n",
      "[INFO] Epoch: 57 , batch: 255 , training loss: 4.096913\n",
      "[INFO] Epoch: 57 , batch: 256 , training loss: 4.056881\n",
      "[INFO] Epoch: 57 , batch: 257 , training loss: 4.244417\n",
      "[INFO] Epoch: 57 , batch: 258 , training loss: 4.226175\n",
      "[INFO] Epoch: 57 , batch: 259 , training loss: 4.264755\n",
      "[INFO] Epoch: 57 , batch: 260 , training loss: 4.043223\n",
      "[INFO] Epoch: 57 , batch: 261 , training loss: 4.227235\n",
      "[INFO] Epoch: 57 , batch: 262 , training loss: 4.338597\n",
      "[INFO] Epoch: 57 , batch: 263 , training loss: 4.514738\n",
      "[INFO] Epoch: 57 , batch: 264 , training loss: 3.880958\n",
      "[INFO] Epoch: 57 , batch: 265 , training loss: 3.995287\n",
      "[INFO] Epoch: 57 , batch: 266 , training loss: 4.398341\n",
      "[INFO] Epoch: 57 , batch: 267 , training loss: 4.146511\n",
      "[INFO] Epoch: 57 , batch: 268 , training loss: 4.076670\n",
      "[INFO] Epoch: 57 , batch: 269 , training loss: 4.060923\n",
      "[INFO] Epoch: 57 , batch: 270 , training loss: 4.056421\n",
      "[INFO] Epoch: 57 , batch: 271 , training loss: 4.103549\n",
      "[INFO] Epoch: 57 , batch: 272 , training loss: 4.095891\n",
      "[INFO] Epoch: 57 , batch: 273 , training loss: 4.107925\n",
      "[INFO] Epoch: 57 , batch: 274 , training loss: 4.201598\n",
      "[INFO] Epoch: 57 , batch: 275 , training loss: 4.060836\n",
      "[INFO] Epoch: 57 , batch: 276 , training loss: 4.104358\n",
      "[INFO] Epoch: 57 , batch: 277 , training loss: 4.284503\n",
      "[INFO] Epoch: 57 , batch: 278 , training loss: 3.974092\n",
      "[INFO] Epoch: 57 , batch: 279 , training loss: 3.969445\n",
      "[INFO] Epoch: 57 , batch: 280 , training loss: 3.967240\n",
      "[INFO] Epoch: 57 , batch: 281 , training loss: 4.086926\n",
      "[INFO] Epoch: 57 , batch: 282 , training loss: 4.004772\n",
      "[INFO] Epoch: 57 , batch: 283 , training loss: 4.000473\n",
      "[INFO] Epoch: 57 , batch: 284 , training loss: 4.034012\n",
      "[INFO] Epoch: 57 , batch: 285 , training loss: 3.960832\n",
      "[INFO] Epoch: 57 , batch: 286 , training loss: 3.981639\n",
      "[INFO] Epoch: 57 , batch: 287 , training loss: 3.921434\n",
      "[INFO] Epoch: 57 , batch: 288 , training loss: 3.875134\n",
      "[INFO] Epoch: 57 , batch: 289 , training loss: 3.971375\n",
      "[INFO] Epoch: 57 , batch: 290 , training loss: 3.742552\n",
      "[INFO] Epoch: 57 , batch: 291 , training loss: 3.736993\n",
      "[INFO] Epoch: 57 , batch: 292 , training loss: 3.834243\n",
      "[INFO] Epoch: 57 , batch: 293 , training loss: 3.769844\n",
      "[INFO] Epoch: 57 , batch: 294 , training loss: 4.411197\n",
      "[INFO] Epoch: 57 , batch: 295 , training loss: 4.190804\n",
      "[INFO] Epoch: 57 , batch: 296 , training loss: 4.131088\n",
      "[INFO] Epoch: 57 , batch: 297 , training loss: 4.080892\n",
      "[INFO] Epoch: 57 , batch: 298 , training loss: 3.915328\n",
      "[INFO] Epoch: 57 , batch: 299 , training loss: 3.965448\n",
      "[INFO] Epoch: 57 , batch: 300 , training loss: 3.952167\n",
      "[INFO] Epoch: 57 , batch: 301 , training loss: 3.858715\n",
      "[INFO] Epoch: 57 , batch: 302 , training loss: 4.055404\n",
      "[INFO] Epoch: 57 , batch: 303 , training loss: 4.048862\n",
      "[INFO] Epoch: 57 , batch: 304 , training loss: 4.190382\n",
      "[INFO] Epoch: 57 , batch: 305 , training loss: 4.029528\n",
      "[INFO] Epoch: 57 , batch: 306 , training loss: 4.155323\n",
      "[INFO] Epoch: 57 , batch: 307 , training loss: 4.168064\n",
      "[INFO] Epoch: 57 , batch: 308 , training loss: 3.995508\n",
      "[INFO] Epoch: 57 , batch: 309 , training loss: 3.978489\n",
      "[INFO] Epoch: 57 , batch: 310 , training loss: 3.907707\n",
      "[INFO] Epoch: 57 , batch: 311 , training loss: 3.902194\n",
      "[INFO] Epoch: 57 , batch: 312 , training loss: 3.793345\n",
      "[INFO] Epoch: 57 , batch: 313 , training loss: 3.925284\n",
      "[INFO] Epoch: 57 , batch: 314 , training loss: 3.987991\n",
      "[INFO] Epoch: 57 , batch: 315 , training loss: 4.046524\n",
      "[INFO] Epoch: 57 , batch: 316 , training loss: 4.292255\n",
      "[INFO] Epoch: 57 , batch: 317 , training loss: 4.672184\n",
      "[INFO] Epoch: 57 , batch: 318 , training loss: 4.797247\n",
      "[INFO] Epoch: 57 , batch: 319 , training loss: 4.463347\n",
      "[INFO] Epoch: 57 , batch: 320 , training loss: 4.015312\n",
      "[INFO] Epoch: 57 , batch: 321 , training loss: 3.846864\n",
      "[INFO] Epoch: 57 , batch: 322 , training loss: 3.956704\n",
      "[INFO] Epoch: 57 , batch: 323 , training loss: 3.981892\n",
      "[INFO] Epoch: 57 , batch: 324 , training loss: 3.951784\n",
      "[INFO] Epoch: 57 , batch: 325 , training loss: 4.047449\n",
      "[INFO] Epoch: 57 , batch: 326 , training loss: 4.140514\n",
      "[INFO] Epoch: 57 , batch: 327 , training loss: 4.065894\n",
      "[INFO] Epoch: 57 , batch: 328 , training loss: 4.084931\n",
      "[INFO] Epoch: 57 , batch: 329 , training loss: 3.990799\n",
      "[INFO] Epoch: 57 , batch: 330 , training loss: 3.952715\n",
      "[INFO] Epoch: 57 , batch: 331 , training loss: 4.107855\n",
      "[INFO] Epoch: 57 , batch: 332 , training loss: 3.944434\n",
      "[INFO] Epoch: 57 , batch: 333 , training loss: 3.931156\n",
      "[INFO] Epoch: 57 , batch: 334 , training loss: 3.966097\n",
      "[INFO] Epoch: 57 , batch: 335 , training loss: 4.066939\n",
      "[INFO] Epoch: 57 , batch: 336 , training loss: 4.077710\n",
      "[INFO] Epoch: 57 , batch: 337 , training loss: 4.121903\n",
      "[INFO] Epoch: 57 , batch: 338 , training loss: 4.348402\n",
      "[INFO] Epoch: 57 , batch: 339 , training loss: 4.138915\n",
      "[INFO] Epoch: 57 , batch: 340 , training loss: 4.353124\n",
      "[INFO] Epoch: 57 , batch: 341 , training loss: 4.112488\n",
      "[INFO] Epoch: 57 , batch: 342 , training loss: 3.897150\n",
      "[INFO] Epoch: 57 , batch: 343 , training loss: 3.959254\n",
      "[INFO] Epoch: 57 , batch: 344 , training loss: 3.817897\n",
      "[INFO] Epoch: 57 , batch: 345 , training loss: 3.946049\n",
      "[INFO] Epoch: 57 , batch: 346 , training loss: 4.015318\n",
      "[INFO] Epoch: 57 , batch: 347 , training loss: 3.916116\n",
      "[INFO] Epoch: 57 , batch: 348 , training loss: 4.021223\n",
      "[INFO] Epoch: 57 , batch: 349 , training loss: 4.094856\n",
      "[INFO] Epoch: 57 , batch: 350 , training loss: 3.970119\n",
      "[INFO] Epoch: 57 , batch: 351 , training loss: 4.050205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 57 , batch: 352 , training loss: 4.046158\n",
      "[INFO] Epoch: 57 , batch: 353 , training loss: 4.043594\n",
      "[INFO] Epoch: 57 , batch: 354 , training loss: 4.119707\n",
      "[INFO] Epoch: 57 , batch: 355 , training loss: 4.118585\n",
      "[INFO] Epoch: 57 , batch: 356 , training loss: 3.989555\n",
      "[INFO] Epoch: 57 , batch: 357 , training loss: 4.048146\n",
      "[INFO] Epoch: 57 , batch: 358 , training loss: 3.985025\n",
      "[INFO] Epoch: 57 , batch: 359 , training loss: 3.987058\n",
      "[INFO] Epoch: 57 , batch: 360 , training loss: 4.063374\n",
      "[INFO] Epoch: 57 , batch: 361 , training loss: 4.037827\n",
      "[INFO] Epoch: 57 , batch: 362 , training loss: 4.148288\n",
      "[INFO] Epoch: 57 , batch: 363 , training loss: 4.033825\n",
      "[INFO] Epoch: 57 , batch: 364 , training loss: 4.096376\n",
      "[INFO] Epoch: 57 , batch: 365 , training loss: 4.008707\n",
      "[INFO] Epoch: 57 , batch: 366 , training loss: 4.107296\n",
      "[INFO] Epoch: 57 , batch: 367 , training loss: 4.161804\n",
      "[INFO] Epoch: 57 , batch: 368 , training loss: 4.577541\n",
      "[INFO] Epoch: 57 , batch: 369 , training loss: 4.233693\n",
      "[INFO] Epoch: 57 , batch: 370 , training loss: 3.994358\n",
      "[INFO] Epoch: 57 , batch: 371 , training loss: 4.430432\n",
      "[INFO] Epoch: 57 , batch: 372 , training loss: 4.704003\n",
      "[INFO] Epoch: 57 , batch: 373 , training loss: 4.679332\n",
      "[INFO] Epoch: 57 , batch: 374 , training loss: 4.875412\n",
      "[INFO] Epoch: 57 , batch: 375 , training loss: 4.818659\n",
      "[INFO] Epoch: 57 , batch: 376 , training loss: 4.681513\n",
      "[INFO] Epoch: 57 , batch: 377 , training loss: 4.442431\n",
      "[INFO] Epoch: 57 , batch: 378 , training loss: 4.566213\n",
      "[INFO] Epoch: 57 , batch: 379 , training loss: 4.517599\n",
      "[INFO] Epoch: 57 , batch: 380 , training loss: 4.722297\n",
      "[INFO] Epoch: 57 , batch: 381 , training loss: 4.374074\n",
      "[INFO] Epoch: 57 , batch: 382 , training loss: 4.658691\n",
      "[INFO] Epoch: 57 , batch: 383 , training loss: 4.700957\n",
      "[INFO] Epoch: 57 , batch: 384 , training loss: 4.630719\n",
      "[INFO] Epoch: 57 , batch: 385 , training loss: 4.336965\n",
      "[INFO] Epoch: 57 , batch: 386 , training loss: 4.580321\n",
      "[INFO] Epoch: 57 , batch: 387 , training loss: 4.534079\n",
      "[INFO] Epoch: 57 , batch: 388 , training loss: 4.370602\n",
      "[INFO] Epoch: 57 , batch: 389 , training loss: 4.181557\n",
      "[INFO] Epoch: 57 , batch: 390 , training loss: 4.193654\n",
      "[INFO] Epoch: 57 , batch: 391 , training loss: 4.233887\n",
      "[INFO] Epoch: 57 , batch: 392 , training loss: 4.582872\n",
      "[INFO] Epoch: 57 , batch: 393 , training loss: 4.451978\n",
      "[INFO] Epoch: 57 , batch: 394 , training loss: 4.576060\n",
      "[INFO] Epoch: 57 , batch: 395 , training loss: 4.376722\n",
      "[INFO] Epoch: 57 , batch: 396 , training loss: 4.211558\n",
      "[INFO] Epoch: 57 , batch: 397 , training loss: 4.350697\n",
      "[INFO] Epoch: 57 , batch: 398 , training loss: 4.211748\n",
      "[INFO] Epoch: 57 , batch: 399 , training loss: 4.298806\n",
      "[INFO] Epoch: 57 , batch: 400 , training loss: 4.258365\n",
      "[INFO] Epoch: 57 , batch: 401 , training loss: 4.684117\n",
      "[INFO] Epoch: 57 , batch: 402 , training loss: 4.414826\n",
      "[INFO] Epoch: 57 , batch: 403 , training loss: 4.246008\n",
      "[INFO] Epoch: 57 , batch: 404 , training loss: 4.437922\n",
      "[INFO] Epoch: 57 , batch: 405 , training loss: 4.475951\n",
      "[INFO] Epoch: 57 , batch: 406 , training loss: 4.366447\n",
      "[INFO] Epoch: 57 , batch: 407 , training loss: 4.389732\n",
      "[INFO] Epoch: 57 , batch: 408 , training loss: 4.393960\n",
      "[INFO] Epoch: 57 , batch: 409 , training loss: 4.384277\n",
      "[INFO] Epoch: 57 , batch: 410 , training loss: 4.462530\n",
      "[INFO] Epoch: 57 , batch: 411 , training loss: 4.616811\n",
      "[INFO] Epoch: 57 , batch: 412 , training loss: 4.452067\n",
      "[INFO] Epoch: 57 , batch: 413 , training loss: 4.305727\n",
      "[INFO] Epoch: 57 , batch: 414 , training loss: 4.364758\n",
      "[INFO] Epoch: 57 , batch: 415 , training loss: 4.398872\n",
      "[INFO] Epoch: 57 , batch: 416 , training loss: 4.466150\n",
      "[INFO] Epoch: 57 , batch: 417 , training loss: 4.387419\n",
      "[INFO] Epoch: 57 , batch: 418 , training loss: 4.452749\n",
      "[INFO] Epoch: 57 , batch: 419 , training loss: 4.402242\n",
      "[INFO] Epoch: 57 , batch: 420 , training loss: 4.387395\n",
      "[INFO] Epoch: 57 , batch: 421 , training loss: 4.364432\n",
      "[INFO] Epoch: 57 , batch: 422 , training loss: 4.211370\n",
      "[INFO] Epoch: 57 , batch: 423 , training loss: 4.423671\n",
      "[INFO] Epoch: 57 , batch: 424 , training loss: 4.585521\n",
      "[INFO] Epoch: 57 , batch: 425 , training loss: 4.480309\n",
      "[INFO] Epoch: 57 , batch: 426 , training loss: 4.218367\n",
      "[INFO] Epoch: 57 , batch: 427 , training loss: 4.454014\n",
      "[INFO] Epoch: 57 , batch: 428 , training loss: 4.317718\n",
      "[INFO] Epoch: 57 , batch: 429 , training loss: 4.221430\n",
      "[INFO] Epoch: 57 , batch: 430 , training loss: 4.453743\n",
      "[INFO] Epoch: 57 , batch: 431 , training loss: 4.074317\n",
      "[INFO] Epoch: 57 , batch: 432 , training loss: 4.119146\n",
      "[INFO] Epoch: 57 , batch: 433 , training loss: 4.151500\n",
      "[INFO] Epoch: 57 , batch: 434 , training loss: 4.030389\n",
      "[INFO] Epoch: 57 , batch: 435 , training loss: 4.377657\n",
      "[INFO] Epoch: 57 , batch: 436 , training loss: 4.421798\n",
      "[INFO] Epoch: 57 , batch: 437 , training loss: 4.220192\n",
      "[INFO] Epoch: 57 , batch: 438 , training loss: 4.075067\n",
      "[INFO] Epoch: 57 , batch: 439 , training loss: 4.299372\n",
      "[INFO] Epoch: 57 , batch: 440 , training loss: 4.424530\n",
      "[INFO] Epoch: 57 , batch: 441 , training loss: 4.525668\n",
      "[INFO] Epoch: 57 , batch: 442 , training loss: 4.296240\n",
      "[INFO] Epoch: 57 , batch: 443 , training loss: 4.423625\n",
      "[INFO] Epoch: 57 , batch: 444 , training loss: 4.085249\n",
      "[INFO] Epoch: 57 , batch: 445 , training loss: 3.958875\n",
      "[INFO] Epoch: 57 , batch: 446 , training loss: 3.898091\n",
      "[INFO] Epoch: 57 , batch: 447 , training loss: 4.099023\n",
      "[INFO] Epoch: 57 , batch: 448 , training loss: 4.218086\n",
      "[INFO] Epoch: 57 , batch: 449 , training loss: 4.617965\n",
      "[INFO] Epoch: 57 , batch: 450 , training loss: 4.679462\n",
      "[INFO] Epoch: 57 , batch: 451 , training loss: 4.558391\n",
      "[INFO] Epoch: 57 , batch: 452 , training loss: 4.385456\n",
      "[INFO] Epoch: 57 , batch: 453 , training loss: 4.143180\n",
      "[INFO] Epoch: 57 , batch: 454 , training loss: 4.295486\n",
      "[INFO] Epoch: 57 , batch: 455 , training loss: 4.333238\n",
      "[INFO] Epoch: 57 , batch: 456 , training loss: 4.346134\n",
      "[INFO] Epoch: 57 , batch: 457 , training loss: 4.422293\n",
      "[INFO] Epoch: 57 , batch: 458 , training loss: 4.158726\n",
      "[INFO] Epoch: 57 , batch: 459 , training loss: 4.142600\n",
      "[INFO] Epoch: 57 , batch: 460 , training loss: 4.269547\n",
      "[INFO] Epoch: 57 , batch: 461 , training loss: 4.210355\n",
      "[INFO] Epoch: 57 , batch: 462 , training loss: 4.292066\n",
      "[INFO] Epoch: 57 , batch: 463 , training loss: 4.204887\n",
      "[INFO] Epoch: 57 , batch: 464 , training loss: 4.365626\n",
      "[INFO] Epoch: 57 , batch: 465 , training loss: 4.310212\n",
      "[INFO] Epoch: 57 , batch: 466 , training loss: 4.383590\n",
      "[INFO] Epoch: 57 , batch: 467 , training loss: 4.368313\n",
      "[INFO] Epoch: 57 , batch: 468 , training loss: 4.342802\n",
      "[INFO] Epoch: 57 , batch: 469 , training loss: 4.358681\n",
      "[INFO] Epoch: 57 , batch: 470 , training loss: 4.188075\n",
      "[INFO] Epoch: 57 , batch: 471 , training loss: 4.283936\n",
      "[INFO] Epoch: 57 , batch: 472 , training loss: 4.339065\n",
      "[INFO] Epoch: 57 , batch: 473 , training loss: 4.250558\n",
      "[INFO] Epoch: 57 , batch: 474 , training loss: 4.048186\n",
      "[INFO] Epoch: 57 , batch: 475 , training loss: 3.923228\n",
      "[INFO] Epoch: 57 , batch: 476 , training loss: 4.315499\n",
      "[INFO] Epoch: 57 , batch: 477 , training loss: 4.437965\n",
      "[INFO] Epoch: 57 , batch: 478 , training loss: 4.437545\n",
      "[INFO] Epoch: 57 , batch: 479 , training loss: 4.430124\n",
      "[INFO] Epoch: 57 , batch: 480 , training loss: 4.551973\n",
      "[INFO] Epoch: 57 , batch: 481 , training loss: 4.414668\n",
      "[INFO] Epoch: 57 , batch: 482 , training loss: 4.519760\n",
      "[INFO] Epoch: 57 , batch: 483 , training loss: 4.365715\n",
      "[INFO] Epoch: 57 , batch: 484 , training loss: 4.174054\n",
      "[INFO] Epoch: 57 , batch: 485 , training loss: 4.272010\n",
      "[INFO] Epoch: 57 , batch: 486 , training loss: 4.173694\n",
      "[INFO] Epoch: 57 , batch: 487 , training loss: 4.154959\n",
      "[INFO] Epoch: 57 , batch: 488 , training loss: 4.329131\n",
      "[INFO] Epoch: 57 , batch: 489 , training loss: 4.234734\n",
      "[INFO] Epoch: 57 , batch: 490 , training loss: 4.308210\n",
      "[INFO] Epoch: 57 , batch: 491 , training loss: 4.206915\n",
      "[INFO] Epoch: 57 , batch: 492 , training loss: 4.202250\n",
      "[INFO] Epoch: 57 , batch: 493 , training loss: 4.358129\n",
      "[INFO] Epoch: 57 , batch: 494 , training loss: 4.268635\n",
      "[INFO] Epoch: 57 , batch: 495 , training loss: 4.447932\n",
      "[INFO] Epoch: 57 , batch: 496 , training loss: 4.300762\n",
      "[INFO] Epoch: 57 , batch: 497 , training loss: 4.331703\n",
      "[INFO] Epoch: 57 , batch: 498 , training loss: 4.308430\n",
      "[INFO] Epoch: 57 , batch: 499 , training loss: 4.385206\n",
      "[INFO] Epoch: 57 , batch: 500 , training loss: 4.528498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 57 , batch: 501 , training loss: 4.871758\n",
      "[INFO] Epoch: 57 , batch: 502 , training loss: 4.899757\n",
      "[INFO] Epoch: 57 , batch: 503 , training loss: 4.509704\n",
      "[INFO] Epoch: 57 , batch: 504 , training loss: 4.650455\n",
      "[INFO] Epoch: 57 , batch: 505 , training loss: 4.634280\n",
      "[INFO] Epoch: 57 , batch: 506 , training loss: 4.655069\n",
      "[INFO] Epoch: 57 , batch: 507 , training loss: 4.700099\n",
      "[INFO] Epoch: 57 , batch: 508 , training loss: 4.611518\n",
      "[INFO] Epoch: 57 , batch: 509 , training loss: 4.431013\n",
      "[INFO] Epoch: 57 , batch: 510 , training loss: 4.524572\n",
      "[INFO] Epoch: 57 , batch: 511 , training loss: 4.454509\n",
      "[INFO] Epoch: 57 , batch: 512 , training loss: 4.544796\n",
      "[INFO] Epoch: 57 , batch: 513 , training loss: 4.824207\n",
      "[INFO] Epoch: 57 , batch: 514 , training loss: 4.455092\n",
      "[INFO] Epoch: 57 , batch: 515 , training loss: 4.722538\n",
      "[INFO] Epoch: 57 , batch: 516 , training loss: 4.491644\n",
      "[INFO] Epoch: 57 , batch: 517 , training loss: 4.488937\n",
      "[INFO] Epoch: 57 , batch: 518 , training loss: 4.465242\n",
      "[INFO] Epoch: 57 , batch: 519 , training loss: 4.279939\n",
      "[INFO] Epoch: 57 , batch: 520 , training loss: 4.541729\n",
      "[INFO] Epoch: 57 , batch: 521 , training loss: 4.501790\n",
      "[INFO] Epoch: 57 , batch: 522 , training loss: 4.605600\n",
      "[INFO] Epoch: 57 , batch: 523 , training loss: 4.504177\n",
      "[INFO] Epoch: 57 , batch: 524 , training loss: 4.801909\n",
      "[INFO] Epoch: 57 , batch: 525 , training loss: 4.708031\n",
      "[INFO] Epoch: 57 , batch: 526 , training loss: 4.468640\n",
      "[INFO] Epoch: 57 , batch: 527 , training loss: 4.496743\n",
      "[INFO] Epoch: 57 , batch: 528 , training loss: 4.527939\n",
      "[INFO] Epoch: 57 , batch: 529 , training loss: 4.498192\n",
      "[INFO] Epoch: 57 , batch: 530 , training loss: 4.343650\n",
      "[INFO] Epoch: 57 , batch: 531 , training loss: 4.511592\n",
      "[INFO] Epoch: 57 , batch: 532 , training loss: 4.386336\n",
      "[INFO] Epoch: 57 , batch: 533 , training loss: 4.528626\n",
      "[INFO] Epoch: 57 , batch: 534 , training loss: 4.529878\n",
      "[INFO] Epoch: 57 , batch: 535 , training loss: 4.542665\n",
      "[INFO] Epoch: 57 , batch: 536 , training loss: 4.388952\n",
      "[INFO] Epoch: 57 , batch: 537 , training loss: 4.361465\n",
      "[INFO] Epoch: 57 , batch: 538 , training loss: 4.451655\n",
      "[INFO] Epoch: 57 , batch: 539 , training loss: 4.569070\n",
      "[INFO] Epoch: 57 , batch: 540 , training loss: 5.139216\n",
      "[INFO] Epoch: 57 , batch: 541 , training loss: 4.969878\n",
      "[INFO] Epoch: 57 , batch: 542 , training loss: 4.843243\n",
      "[INFO] Epoch: 58 , batch: 0 , training loss: 3.782689\n",
      "[INFO] Epoch: 58 , batch: 1 , training loss: 3.699921\n",
      "[INFO] Epoch: 58 , batch: 2 , training loss: 3.857870\n",
      "[INFO] Epoch: 58 , batch: 3 , training loss: 3.732939\n",
      "[INFO] Epoch: 58 , batch: 4 , training loss: 4.154102\n",
      "[INFO] Epoch: 58 , batch: 5 , training loss: 3.733886\n",
      "[INFO] Epoch: 58 , batch: 6 , training loss: 4.202359\n",
      "[INFO] Epoch: 58 , batch: 7 , training loss: 4.015268\n",
      "[INFO] Epoch: 58 , batch: 8 , training loss: 3.724565\n",
      "[INFO] Epoch: 58 , batch: 9 , training loss: 3.965168\n",
      "[INFO] Epoch: 58 , batch: 10 , training loss: 3.932701\n",
      "[INFO] Epoch: 58 , batch: 11 , training loss: 3.850212\n",
      "[INFO] Epoch: 58 , batch: 12 , training loss: 3.777193\n",
      "[INFO] Epoch: 58 , batch: 13 , training loss: 3.756007\n",
      "[INFO] Epoch: 58 , batch: 14 , training loss: 3.719694\n",
      "[INFO] Epoch: 58 , batch: 15 , training loss: 3.871546\n",
      "[INFO] Epoch: 58 , batch: 16 , training loss: 3.748888\n",
      "[INFO] Epoch: 58 , batch: 17 , training loss: 3.864354\n",
      "[INFO] Epoch: 58 , batch: 18 , training loss: 3.793550\n",
      "[INFO] Epoch: 58 , batch: 19 , training loss: 3.568780\n",
      "[INFO] Epoch: 58 , batch: 20 , training loss: 3.547137\n",
      "[INFO] Epoch: 58 , batch: 21 , training loss: 3.655947\n",
      "[INFO] Epoch: 58 , batch: 22 , training loss: 3.587292\n",
      "[INFO] Epoch: 58 , batch: 23 , training loss: 3.786345\n",
      "[INFO] Epoch: 58 , batch: 24 , training loss: 3.655249\n",
      "[INFO] Epoch: 58 , batch: 25 , training loss: 3.722681\n",
      "[INFO] Epoch: 58 , batch: 26 , training loss: 3.605131\n",
      "[INFO] Epoch: 58 , batch: 27 , training loss: 3.579583\n",
      "[INFO] Epoch: 58 , batch: 28 , training loss: 3.781275\n",
      "[INFO] Epoch: 58 , batch: 29 , training loss: 3.576013\n",
      "[INFO] Epoch: 58 , batch: 30 , training loss: 3.630845\n",
      "[INFO] Epoch: 58 , batch: 31 , training loss: 3.689096\n",
      "[INFO] Epoch: 58 , batch: 32 , training loss: 3.673402\n",
      "[INFO] Epoch: 58 , batch: 33 , training loss: 3.705064\n",
      "[INFO] Epoch: 58 , batch: 34 , training loss: 3.724325\n",
      "[INFO] Epoch: 58 , batch: 35 , training loss: 3.630196\n",
      "[INFO] Epoch: 58 , batch: 36 , training loss: 3.721092\n",
      "[INFO] Epoch: 58 , batch: 37 , training loss: 3.602302\n",
      "[INFO] Epoch: 58 , batch: 38 , training loss: 3.681354\n",
      "[INFO] Epoch: 58 , batch: 39 , training loss: 3.512594\n",
      "[INFO] Epoch: 58 , batch: 40 , training loss: 3.688079\n",
      "[INFO] Epoch: 58 , batch: 41 , training loss: 3.681446\n",
      "[INFO] Epoch: 58 , batch: 42 , training loss: 4.196739\n",
      "[INFO] Epoch: 58 , batch: 43 , training loss: 3.999938\n",
      "[INFO] Epoch: 58 , batch: 44 , training loss: 4.257646\n",
      "[INFO] Epoch: 58 , batch: 45 , training loss: 4.249500\n",
      "[INFO] Epoch: 58 , batch: 46 , training loss: 4.286490\n",
      "[INFO] Epoch: 58 , batch: 47 , training loss: 3.737383\n",
      "[INFO] Epoch: 58 , batch: 48 , training loss: 3.819650\n",
      "[INFO] Epoch: 58 , batch: 49 , training loss: 3.991102\n",
      "[INFO] Epoch: 58 , batch: 50 , training loss: 3.731242\n",
      "[INFO] Epoch: 58 , batch: 51 , training loss: 3.908472\n",
      "[INFO] Epoch: 58 , batch: 52 , training loss: 3.697688\n",
      "[INFO] Epoch: 58 , batch: 53 , training loss: 3.824082\n",
      "[INFO] Epoch: 58 , batch: 54 , training loss: 3.857748\n",
      "[INFO] Epoch: 58 , batch: 55 , training loss: 3.944649\n",
      "[INFO] Epoch: 58 , batch: 56 , training loss: 3.761385\n",
      "[INFO] Epoch: 58 , batch: 57 , training loss: 3.682006\n",
      "[INFO] Epoch: 58 , batch: 58 , training loss: 3.712574\n",
      "[INFO] Epoch: 58 , batch: 59 , training loss: 3.802889\n",
      "[INFO] Epoch: 58 , batch: 60 , training loss: 3.694417\n",
      "[INFO] Epoch: 58 , batch: 61 , training loss: 3.839256\n",
      "[INFO] Epoch: 58 , batch: 62 , training loss: 3.712443\n",
      "[INFO] Epoch: 58 , batch: 63 , training loss: 3.902149\n",
      "[INFO] Epoch: 58 , batch: 64 , training loss: 4.067417\n",
      "[INFO] Epoch: 58 , batch: 65 , training loss: 3.806172\n",
      "[INFO] Epoch: 58 , batch: 66 , training loss: 3.659494\n",
      "[INFO] Epoch: 58 , batch: 67 , training loss: 3.701410\n",
      "[INFO] Epoch: 58 , batch: 68 , training loss: 3.877659\n",
      "[INFO] Epoch: 58 , batch: 69 , training loss: 3.772236\n",
      "[INFO] Epoch: 58 , batch: 70 , training loss: 4.000994\n",
      "[INFO] Epoch: 58 , batch: 71 , training loss: 3.824653\n",
      "[INFO] Epoch: 58 , batch: 72 , training loss: 3.899747\n",
      "[INFO] Epoch: 58 , batch: 73 , training loss: 3.859195\n",
      "[INFO] Epoch: 58 , batch: 74 , training loss: 3.955864\n",
      "[INFO] Epoch: 58 , batch: 75 , training loss: 3.834401\n",
      "[INFO] Epoch: 58 , batch: 76 , training loss: 3.953084\n",
      "[INFO] Epoch: 58 , batch: 77 , training loss: 3.873741\n",
      "[INFO] Epoch: 58 , batch: 78 , training loss: 3.960725\n",
      "[INFO] Epoch: 58 , batch: 79 , training loss: 3.802432\n",
      "[INFO] Epoch: 58 , batch: 80 , training loss: 3.990799\n",
      "[INFO] Epoch: 58 , batch: 81 , training loss: 3.946153\n",
      "[INFO] Epoch: 58 , batch: 82 , training loss: 3.922223\n",
      "[INFO] Epoch: 58 , batch: 83 , training loss: 3.987147\n",
      "[INFO] Epoch: 58 , batch: 84 , training loss: 4.042613\n",
      "[INFO] Epoch: 58 , batch: 85 , training loss: 4.056141\n",
      "[INFO] Epoch: 58 , batch: 86 , training loss: 4.014122\n",
      "[INFO] Epoch: 58 , batch: 87 , training loss: 3.926094\n",
      "[INFO] Epoch: 58 , batch: 88 , training loss: 4.112440\n",
      "[INFO] Epoch: 58 , batch: 89 , training loss: 3.873523\n",
      "[INFO] Epoch: 58 , batch: 90 , training loss: 3.951802\n",
      "[INFO] Epoch: 58 , batch: 91 , training loss: 3.905334\n",
      "[INFO] Epoch: 58 , batch: 92 , training loss: 3.936667\n",
      "[INFO] Epoch: 58 , batch: 93 , training loss: 4.027770\n",
      "[INFO] Epoch: 58 , batch: 94 , training loss: 4.137629\n",
      "[INFO] Epoch: 58 , batch: 95 , training loss: 3.915130\n",
      "[INFO] Epoch: 58 , batch: 96 , training loss: 3.919145\n",
      "[INFO] Epoch: 58 , batch: 97 , training loss: 3.853505\n",
      "[INFO] Epoch: 58 , batch: 98 , training loss: 3.817141\n",
      "[INFO] Epoch: 58 , batch: 99 , training loss: 3.938032\n",
      "[INFO] Epoch: 58 , batch: 100 , training loss: 3.851328\n",
      "[INFO] Epoch: 58 , batch: 101 , training loss: 3.833330\n",
      "[INFO] Epoch: 58 , batch: 102 , training loss: 3.995697\n",
      "[INFO] Epoch: 58 , batch: 103 , training loss: 3.768428\n",
      "[INFO] Epoch: 58 , batch: 104 , training loss: 3.752941\n",
      "[INFO] Epoch: 58 , batch: 105 , training loss: 3.970316\n",
      "[INFO] Epoch: 58 , batch: 106 , training loss: 4.037299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 58 , batch: 107 , training loss: 3.854776\n",
      "[INFO] Epoch: 58 , batch: 108 , training loss: 3.807561\n",
      "[INFO] Epoch: 58 , batch: 109 , training loss: 3.720042\n",
      "[INFO] Epoch: 58 , batch: 110 , training loss: 3.880784\n",
      "[INFO] Epoch: 58 , batch: 111 , training loss: 3.982124\n",
      "[INFO] Epoch: 58 , batch: 112 , training loss: 3.922260\n",
      "[INFO] Epoch: 58 , batch: 113 , training loss: 3.858440\n",
      "[INFO] Epoch: 58 , batch: 114 , training loss: 3.894027\n",
      "[INFO] Epoch: 58 , batch: 115 , training loss: 3.891335\n",
      "[INFO] Epoch: 58 , batch: 116 , training loss: 3.820378\n",
      "[INFO] Epoch: 58 , batch: 117 , training loss: 4.026004\n",
      "[INFO] Epoch: 58 , batch: 118 , training loss: 4.018532\n",
      "[INFO] Epoch: 58 , batch: 119 , training loss: 4.116992\n",
      "[INFO] Epoch: 58 , batch: 120 , training loss: 4.081753\n",
      "[INFO] Epoch: 58 , batch: 121 , training loss: 3.976103\n",
      "[INFO] Epoch: 58 , batch: 122 , training loss: 3.917417\n",
      "[INFO] Epoch: 58 , batch: 123 , training loss: 3.848369\n",
      "[INFO] Epoch: 58 , batch: 124 , training loss: 4.005608\n",
      "[INFO] Epoch: 58 , batch: 125 , training loss: 3.789979\n",
      "[INFO] Epoch: 58 , batch: 126 , training loss: 3.794213\n",
      "[INFO] Epoch: 58 , batch: 127 , training loss: 3.783016\n",
      "[INFO] Epoch: 58 , batch: 128 , training loss: 3.936041\n",
      "[INFO] Epoch: 58 , batch: 129 , training loss: 3.876658\n",
      "[INFO] Epoch: 58 , batch: 130 , training loss: 3.888932\n",
      "[INFO] Epoch: 58 , batch: 131 , training loss: 3.922773\n",
      "[INFO] Epoch: 58 , batch: 132 , training loss: 3.894081\n",
      "[INFO] Epoch: 58 , batch: 133 , training loss: 3.860217\n",
      "[INFO] Epoch: 58 , batch: 134 , training loss: 3.658202\n",
      "[INFO] Epoch: 58 , batch: 135 , training loss: 3.689401\n",
      "[INFO] Epoch: 58 , batch: 136 , training loss: 3.994610\n",
      "[INFO] Epoch: 58 , batch: 137 , training loss: 3.957311\n",
      "[INFO] Epoch: 58 , batch: 138 , training loss: 3.982229\n",
      "[INFO] Epoch: 58 , batch: 139 , training loss: 4.596214\n",
      "[INFO] Epoch: 58 , batch: 140 , training loss: 4.391084\n",
      "[INFO] Epoch: 58 , batch: 141 , training loss: 4.137132\n",
      "[INFO] Epoch: 58 , batch: 142 , training loss: 3.823916\n",
      "[INFO] Epoch: 58 , batch: 143 , training loss: 3.956815\n",
      "[INFO] Epoch: 58 , batch: 144 , training loss: 3.816644\n",
      "[INFO] Epoch: 58 , batch: 145 , training loss: 3.881721\n",
      "[INFO] Epoch: 58 , batch: 146 , training loss: 4.074643\n",
      "[INFO] Epoch: 58 , batch: 147 , training loss: 3.722095\n",
      "[INFO] Epoch: 58 , batch: 148 , training loss: 3.697102\n",
      "[INFO] Epoch: 58 , batch: 149 , training loss: 3.798157\n",
      "[INFO] Epoch: 58 , batch: 150 , training loss: 4.041041\n",
      "[INFO] Epoch: 58 , batch: 151 , training loss: 3.886836\n",
      "[INFO] Epoch: 58 , batch: 152 , training loss: 3.875833\n",
      "[INFO] Epoch: 58 , batch: 153 , training loss: 3.962177\n",
      "[INFO] Epoch: 58 , batch: 154 , training loss: 4.017201\n",
      "[INFO] Epoch: 58 , batch: 155 , training loss: 4.231258\n",
      "[INFO] Epoch: 58 , batch: 156 , training loss: 4.009227\n",
      "[INFO] Epoch: 58 , batch: 157 , training loss: 3.930681\n",
      "[INFO] Epoch: 58 , batch: 158 , training loss: 4.086838\n",
      "[INFO] Epoch: 58 , batch: 159 , training loss: 4.016361\n",
      "[INFO] Epoch: 58 , batch: 160 , training loss: 4.274783\n",
      "[INFO] Epoch: 58 , batch: 161 , training loss: 4.234368\n",
      "[INFO] Epoch: 58 , batch: 162 , training loss: 4.239467\n",
      "[INFO] Epoch: 58 , batch: 163 , training loss: 4.418711\n",
      "[INFO] Epoch: 58 , batch: 164 , training loss: 4.363985\n",
      "[INFO] Epoch: 58 , batch: 165 , training loss: 4.237808\n",
      "[INFO] Epoch: 58 , batch: 166 , training loss: 4.232306\n",
      "[INFO] Epoch: 58 , batch: 167 , training loss: 4.254421\n",
      "[INFO] Epoch: 58 , batch: 168 , training loss: 3.977946\n",
      "[INFO] Epoch: 58 , batch: 169 , training loss: 3.955521\n",
      "[INFO] Epoch: 58 , batch: 170 , training loss: 4.145031\n",
      "[INFO] Epoch: 58 , batch: 171 , training loss: 3.648898\n",
      "[INFO] Epoch: 58 , batch: 172 , training loss: 3.880488\n",
      "[INFO] Epoch: 58 , batch: 173 , training loss: 4.118706\n",
      "[INFO] Epoch: 58 , batch: 174 , training loss: 4.562130\n",
      "[INFO] Epoch: 58 , batch: 175 , training loss: 4.799716\n",
      "[INFO] Epoch: 58 , batch: 176 , training loss: 4.453992\n",
      "[INFO] Epoch: 58 , batch: 177 , training loss: 4.057562\n",
      "[INFO] Epoch: 58 , batch: 178 , training loss: 4.064020\n",
      "[INFO] Epoch: 58 , batch: 179 , training loss: 4.089281\n",
      "[INFO] Epoch: 58 , batch: 180 , training loss: 4.047531\n",
      "[INFO] Epoch: 58 , batch: 181 , training loss: 4.373635\n",
      "[INFO] Epoch: 58 , batch: 182 , training loss: 4.303346\n",
      "[INFO] Epoch: 58 , batch: 183 , training loss: 4.282118\n",
      "[INFO] Epoch: 58 , batch: 184 , training loss: 4.160780\n",
      "[INFO] Epoch: 58 , batch: 185 , training loss: 4.152460\n",
      "[INFO] Epoch: 58 , batch: 186 , training loss: 4.257272\n",
      "[INFO] Epoch: 58 , batch: 187 , training loss: 4.376225\n",
      "[INFO] Epoch: 58 , batch: 188 , training loss: 4.349531\n",
      "[INFO] Epoch: 58 , batch: 189 , training loss: 4.263453\n",
      "[INFO] Epoch: 58 , batch: 190 , training loss: 4.322637\n",
      "[INFO] Epoch: 58 , batch: 191 , training loss: 4.408795\n",
      "[INFO] Epoch: 58 , batch: 192 , training loss: 4.283611\n",
      "[INFO] Epoch: 58 , batch: 193 , training loss: 4.353774\n",
      "[INFO] Epoch: 58 , batch: 194 , training loss: 4.284116\n",
      "[INFO] Epoch: 58 , batch: 195 , training loss: 4.244237\n",
      "[INFO] Epoch: 58 , batch: 196 , training loss: 4.097654\n",
      "[INFO] Epoch: 58 , batch: 197 , training loss: 4.164285\n",
      "[INFO] Epoch: 58 , batch: 198 , training loss: 4.100233\n",
      "[INFO] Epoch: 58 , batch: 199 , training loss: 4.229717\n",
      "[INFO] Epoch: 58 , batch: 200 , training loss: 4.128006\n",
      "[INFO] Epoch: 58 , batch: 201 , training loss: 4.041321\n",
      "[INFO] Epoch: 58 , batch: 202 , training loss: 4.007849\n",
      "[INFO] Epoch: 58 , batch: 203 , training loss: 4.162724\n",
      "[INFO] Epoch: 58 , batch: 204 , training loss: 4.228013\n",
      "[INFO] Epoch: 58 , batch: 205 , training loss: 3.840821\n",
      "[INFO] Epoch: 58 , batch: 206 , training loss: 3.782675\n",
      "[INFO] Epoch: 58 , batch: 207 , training loss: 3.756078\n",
      "[INFO] Epoch: 58 , batch: 208 , training loss: 4.096990\n",
      "[INFO] Epoch: 58 , batch: 209 , training loss: 4.071996\n",
      "[INFO] Epoch: 58 , batch: 210 , training loss: 4.087452\n",
      "[INFO] Epoch: 58 , batch: 211 , training loss: 4.080575\n",
      "[INFO] Epoch: 58 , batch: 212 , training loss: 4.163805\n",
      "[INFO] Epoch: 58 , batch: 213 , training loss: 4.120344\n",
      "[INFO] Epoch: 58 , batch: 214 , training loss: 4.205189\n",
      "[INFO] Epoch: 58 , batch: 215 , training loss: 4.378537\n",
      "[INFO] Epoch: 58 , batch: 216 , training loss: 4.084175\n",
      "[INFO] Epoch: 58 , batch: 217 , training loss: 4.050920\n",
      "[INFO] Epoch: 58 , batch: 218 , training loss: 4.045907\n",
      "[INFO] Epoch: 58 , batch: 219 , training loss: 4.145556\n",
      "[INFO] Epoch: 58 , batch: 220 , training loss: 3.991879\n",
      "[INFO] Epoch: 58 , batch: 221 , training loss: 3.988383\n",
      "[INFO] Epoch: 58 , batch: 222 , training loss: 4.110415\n",
      "[INFO] Epoch: 58 , batch: 223 , training loss: 4.270483\n",
      "[INFO] Epoch: 58 , batch: 224 , training loss: 4.269910\n",
      "[INFO] Epoch: 58 , batch: 225 , training loss: 4.153954\n",
      "[INFO] Epoch: 58 , batch: 226 , training loss: 4.295239\n",
      "[INFO] Epoch: 58 , batch: 227 , training loss: 4.258442\n",
      "[INFO] Epoch: 58 , batch: 228 , training loss: 4.266202\n",
      "[INFO] Epoch: 58 , batch: 229 , training loss: 4.137726\n",
      "[INFO] Epoch: 58 , batch: 230 , training loss: 4.008376\n",
      "[INFO] Epoch: 58 , batch: 231 , training loss: 3.878285\n",
      "[INFO] Epoch: 58 , batch: 232 , training loss: 4.015558\n",
      "[INFO] Epoch: 58 , batch: 233 , training loss: 4.055022\n",
      "[INFO] Epoch: 58 , batch: 234 , training loss: 3.729355\n",
      "[INFO] Epoch: 58 , batch: 235 , training loss: 3.832595\n",
      "[INFO] Epoch: 58 , batch: 236 , training loss: 3.949379\n",
      "[INFO] Epoch: 58 , batch: 237 , training loss: 4.145613\n",
      "[INFO] Epoch: 58 , batch: 238 , training loss: 3.952305\n",
      "[INFO] Epoch: 58 , batch: 239 , training loss: 3.975642\n",
      "[INFO] Epoch: 58 , batch: 240 , training loss: 4.014281\n",
      "[INFO] Epoch: 58 , batch: 241 , training loss: 3.846094\n",
      "[INFO] Epoch: 58 , batch: 242 , training loss: 3.842671\n",
      "[INFO] Epoch: 58 , batch: 243 , training loss: 4.133247\n",
      "[INFO] Epoch: 58 , batch: 244 , training loss: 4.073774\n",
      "[INFO] Epoch: 58 , batch: 245 , training loss: 4.031704\n",
      "[INFO] Epoch: 58 , batch: 246 , training loss: 3.755154\n",
      "[INFO] Epoch: 58 , batch: 247 , training loss: 3.923019\n",
      "[INFO] Epoch: 58 , batch: 248 , training loss: 4.002710\n",
      "[INFO] Epoch: 58 , batch: 249 , training loss: 3.981794\n",
      "[INFO] Epoch: 58 , batch: 250 , training loss: 3.792212\n",
      "[INFO] Epoch: 58 , batch: 251 , training loss: 4.217776\n",
      "[INFO] Epoch: 58 , batch: 252 , training loss: 3.935493\n",
      "[INFO] Epoch: 58 , batch: 253 , training loss: 3.835088\n",
      "[INFO] Epoch: 58 , batch: 254 , training loss: 4.117609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 58 , batch: 255 , training loss: 4.106740\n",
      "[INFO] Epoch: 58 , batch: 256 , training loss: 4.060811\n",
      "[INFO] Epoch: 58 , batch: 257 , training loss: 4.261635\n",
      "[INFO] Epoch: 58 , batch: 258 , training loss: 4.252214\n",
      "[INFO] Epoch: 58 , batch: 259 , training loss: 4.288792\n",
      "[INFO] Epoch: 58 , batch: 260 , training loss: 4.063776\n",
      "[INFO] Epoch: 58 , batch: 261 , training loss: 4.250385\n",
      "[INFO] Epoch: 58 , batch: 262 , training loss: 4.376326\n",
      "[INFO] Epoch: 58 , batch: 263 , training loss: 4.542504\n",
      "[INFO] Epoch: 58 , batch: 264 , training loss: 3.903024\n",
      "[INFO] Epoch: 58 , batch: 265 , training loss: 3.999596\n",
      "[INFO] Epoch: 58 , batch: 266 , training loss: 4.411915\n",
      "[INFO] Epoch: 58 , batch: 267 , training loss: 4.173561\n",
      "[INFO] Epoch: 58 , batch: 268 , training loss: 4.080970\n",
      "[INFO] Epoch: 58 , batch: 269 , training loss: 4.089761\n",
      "[INFO] Epoch: 58 , batch: 270 , training loss: 4.079669\n",
      "[INFO] Epoch: 58 , batch: 271 , training loss: 4.110079\n",
      "[INFO] Epoch: 58 , batch: 272 , training loss: 4.127615\n",
      "[INFO] Epoch: 58 , batch: 273 , training loss: 4.144612\n",
      "[INFO] Epoch: 58 , batch: 274 , training loss: 4.229205\n",
      "[INFO] Epoch: 58 , batch: 275 , training loss: 4.079770\n",
      "[INFO] Epoch: 58 , batch: 276 , training loss: 4.141482\n",
      "[INFO] Epoch: 58 , batch: 277 , training loss: 4.300292\n",
      "[INFO] Epoch: 58 , batch: 278 , training loss: 3.979362\n",
      "[INFO] Epoch: 58 , batch: 279 , training loss: 3.975585\n",
      "[INFO] Epoch: 58 , batch: 280 , training loss: 3.977260\n",
      "[INFO] Epoch: 58 , batch: 281 , training loss: 4.090941\n",
      "[INFO] Epoch: 58 , batch: 282 , training loss: 4.014401\n",
      "[INFO] Epoch: 58 , batch: 283 , training loss: 4.023389\n",
      "[INFO] Epoch: 58 , batch: 284 , training loss: 4.028460\n",
      "[INFO] Epoch: 58 , batch: 285 , training loss: 3.987017\n",
      "[INFO] Epoch: 58 , batch: 286 , training loss: 3.998152\n",
      "[INFO] Epoch: 58 , batch: 287 , training loss: 3.936148\n",
      "[INFO] Epoch: 58 , batch: 288 , training loss: 3.896955\n",
      "[INFO] Epoch: 58 , batch: 289 , training loss: 3.974755\n",
      "[INFO] Epoch: 58 , batch: 290 , training loss: 3.749918\n",
      "[INFO] Epoch: 58 , batch: 291 , training loss: 3.754722\n",
      "[INFO] Epoch: 58 , batch: 292 , training loss: 3.861478\n",
      "[INFO] Epoch: 58 , batch: 293 , training loss: 3.783956\n",
      "[INFO] Epoch: 58 , batch: 294 , training loss: 4.424028\n",
      "[INFO] Epoch: 58 , batch: 295 , training loss: 4.202933\n",
      "[INFO] Epoch: 58 , batch: 296 , training loss: 4.134825\n",
      "[INFO] Epoch: 58 , batch: 297 , training loss: 4.091782\n",
      "[INFO] Epoch: 58 , batch: 298 , training loss: 3.921063\n",
      "[INFO] Epoch: 58 , batch: 299 , training loss: 3.989885\n",
      "[INFO] Epoch: 58 , batch: 300 , training loss: 3.967986\n",
      "[INFO] Epoch: 58 , batch: 301 , training loss: 3.880661\n",
      "[INFO] Epoch: 58 , batch: 302 , training loss: 4.058474\n",
      "[INFO] Epoch: 58 , batch: 303 , training loss: 4.069491\n",
      "[INFO] Epoch: 58 , batch: 304 , training loss: 4.191412\n",
      "[INFO] Epoch: 58 , batch: 305 , training loss: 4.030269\n",
      "[INFO] Epoch: 58 , batch: 306 , training loss: 4.160908\n",
      "[INFO] Epoch: 58 , batch: 307 , training loss: 4.172014\n",
      "[INFO] Epoch: 58 , batch: 308 , training loss: 3.980039\n",
      "[INFO] Epoch: 58 , batch: 309 , training loss: 3.978844\n",
      "[INFO] Epoch: 58 , batch: 310 , training loss: 3.922222\n",
      "[INFO] Epoch: 58 , batch: 311 , training loss: 3.902706\n",
      "[INFO] Epoch: 58 , batch: 312 , training loss: 3.804709\n",
      "[INFO] Epoch: 58 , batch: 313 , training loss: 3.906985\n",
      "[INFO] Epoch: 58 , batch: 314 , training loss: 3.980061\n",
      "[INFO] Epoch: 58 , batch: 315 , training loss: 4.046713\n",
      "[INFO] Epoch: 58 , batch: 316 , training loss: 4.300026\n",
      "[INFO] Epoch: 58 , batch: 317 , training loss: 4.659190\n",
      "[INFO] Epoch: 58 , batch: 318 , training loss: 4.789396\n",
      "[INFO] Epoch: 58 , batch: 319 , training loss: 4.461241\n",
      "[INFO] Epoch: 58 , batch: 320 , training loss: 4.019189\n",
      "[INFO] Epoch: 58 , batch: 321 , training loss: 3.842297\n",
      "[INFO] Epoch: 58 , batch: 322 , training loss: 3.944180\n",
      "[INFO] Epoch: 58 , batch: 323 , training loss: 3.974589\n",
      "[INFO] Epoch: 58 , batch: 324 , training loss: 3.957729\n",
      "[INFO] Epoch: 58 , batch: 325 , training loss: 4.053567\n",
      "[INFO] Epoch: 58 , batch: 326 , training loss: 4.126631\n",
      "[INFO] Epoch: 58 , batch: 327 , training loss: 4.053868\n",
      "[INFO] Epoch: 58 , batch: 328 , training loss: 4.074514\n",
      "[INFO] Epoch: 58 , batch: 329 , training loss: 3.968769\n",
      "[INFO] Epoch: 58 , batch: 330 , training loss: 3.947259\n",
      "[INFO] Epoch: 58 , batch: 331 , training loss: 4.095836\n",
      "[INFO] Epoch: 58 , batch: 332 , training loss: 3.937220\n",
      "[INFO] Epoch: 58 , batch: 333 , training loss: 3.922406\n",
      "[INFO] Epoch: 58 , batch: 334 , training loss: 3.956096\n",
      "[INFO] Epoch: 58 , batch: 335 , training loss: 4.062706\n",
      "[INFO] Epoch: 58 , batch: 336 , training loss: 4.077870\n",
      "[INFO] Epoch: 58 , batch: 337 , training loss: 4.119897\n",
      "[INFO] Epoch: 58 , batch: 338 , training loss: 4.325109\n",
      "[INFO] Epoch: 58 , batch: 339 , training loss: 4.167650\n",
      "[INFO] Epoch: 58 , batch: 340 , training loss: 4.343958\n",
      "[INFO] Epoch: 58 , batch: 341 , training loss: 4.101185\n",
      "[INFO] Epoch: 58 , batch: 342 , training loss: 3.896231\n",
      "[INFO] Epoch: 58 , batch: 343 , training loss: 3.965039\n",
      "[INFO] Epoch: 58 , batch: 344 , training loss: 3.821767\n",
      "[INFO] Epoch: 58 , batch: 345 , training loss: 3.950797\n",
      "[INFO] Epoch: 58 , batch: 346 , training loss: 4.011177\n",
      "[INFO] Epoch: 58 , batch: 347 , training loss: 3.912873\n",
      "[INFO] Epoch: 58 , batch: 348 , training loss: 3.998447\n",
      "[INFO] Epoch: 58 , batch: 349 , training loss: 4.106996\n",
      "[INFO] Epoch: 58 , batch: 350 , training loss: 3.976721\n",
      "[INFO] Epoch: 58 , batch: 351 , training loss: 4.044891\n",
      "[INFO] Epoch: 58 , batch: 352 , training loss: 4.052859\n",
      "[INFO] Epoch: 58 , batch: 353 , training loss: 4.034121\n",
      "[INFO] Epoch: 58 , batch: 354 , training loss: 4.107132\n",
      "[INFO] Epoch: 58 , batch: 355 , training loss: 4.129297\n",
      "[INFO] Epoch: 58 , batch: 356 , training loss: 3.991605\n",
      "[INFO] Epoch: 58 , batch: 357 , training loss: 4.061984\n",
      "[INFO] Epoch: 58 , batch: 358 , training loss: 4.007957\n",
      "[INFO] Epoch: 58 , batch: 359 , training loss: 3.984317\n",
      "[INFO] Epoch: 58 , batch: 360 , training loss: 4.065087\n",
      "[INFO] Epoch: 58 , batch: 361 , training loss: 4.049460\n",
      "[INFO] Epoch: 58 , batch: 362 , training loss: 4.162204\n",
      "[INFO] Epoch: 58 , batch: 363 , training loss: 4.035706\n",
      "[INFO] Epoch: 58 , batch: 364 , training loss: 4.092799\n",
      "[INFO] Epoch: 58 , batch: 365 , training loss: 4.015913\n",
      "[INFO] Epoch: 58 , batch: 366 , training loss: 4.115400\n",
      "[INFO] Epoch: 58 , batch: 367 , training loss: 4.173257\n",
      "[INFO] Epoch: 58 , batch: 368 , training loss: 4.600831\n",
      "[INFO] Epoch: 58 , batch: 369 , training loss: 4.251752\n",
      "[INFO] Epoch: 58 , batch: 370 , training loss: 4.015928\n",
      "[INFO] Epoch: 58 , batch: 371 , training loss: 4.456405\n",
      "[INFO] Epoch: 58 , batch: 372 , training loss: 4.720700\n",
      "[INFO] Epoch: 58 , batch: 373 , training loss: 4.689081\n",
      "[INFO] Epoch: 58 , batch: 374 , training loss: 4.853610\n",
      "[INFO] Epoch: 58 , batch: 375 , training loss: 4.829092\n",
      "[INFO] Epoch: 58 , batch: 376 , training loss: 4.668852\n",
      "[INFO] Epoch: 58 , batch: 377 , training loss: 4.457198\n",
      "[INFO] Epoch: 58 , batch: 378 , training loss: 4.558193\n",
      "[INFO] Epoch: 58 , batch: 379 , training loss: 4.535094\n",
      "[INFO] Epoch: 58 , batch: 380 , training loss: 4.722536\n",
      "[INFO] Epoch: 58 , batch: 381 , training loss: 4.379969\n",
      "[INFO] Epoch: 58 , batch: 382 , training loss: 4.621326\n",
      "[INFO] Epoch: 58 , batch: 383 , training loss: 4.714765\n",
      "[INFO] Epoch: 58 , batch: 384 , training loss: 4.665689\n",
      "[INFO] Epoch: 58 , batch: 385 , training loss: 4.359009\n",
      "[INFO] Epoch: 58 , batch: 386 , training loss: 4.577783\n",
      "[INFO] Epoch: 58 , batch: 387 , training loss: 4.520134\n",
      "[INFO] Epoch: 58 , batch: 388 , training loss: 4.353060\n",
      "[INFO] Epoch: 58 , batch: 389 , training loss: 4.177737\n",
      "[INFO] Epoch: 58 , batch: 390 , training loss: 4.194176\n",
      "[INFO] Epoch: 58 , batch: 391 , training loss: 4.234130\n",
      "[INFO] Epoch: 58 , batch: 392 , training loss: 4.582775\n",
      "[INFO] Epoch: 58 , batch: 393 , training loss: 4.450398\n",
      "[INFO] Epoch: 58 , batch: 394 , training loss: 4.592949\n",
      "[INFO] Epoch: 58 , batch: 395 , training loss: 4.365045\n",
      "[INFO] Epoch: 58 , batch: 396 , training loss: 4.215755\n",
      "[INFO] Epoch: 58 , batch: 397 , training loss: 4.354159\n",
      "[INFO] Epoch: 58 , batch: 398 , training loss: 4.217813\n",
      "[INFO] Epoch: 58 , batch: 399 , training loss: 4.294226\n",
      "[INFO] Epoch: 58 , batch: 400 , training loss: 4.284469\n",
      "[INFO] Epoch: 58 , batch: 401 , training loss: 4.703054\n",
      "[INFO] Epoch: 58 , batch: 402 , training loss: 4.419582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 58 , batch: 403 , training loss: 4.264062\n",
      "[INFO] Epoch: 58 , batch: 404 , training loss: 4.441775\n",
      "[INFO] Epoch: 58 , batch: 405 , training loss: 4.474555\n",
      "[INFO] Epoch: 58 , batch: 406 , training loss: 4.388364\n",
      "[INFO] Epoch: 58 , batch: 407 , training loss: 4.398829\n",
      "[INFO] Epoch: 58 , batch: 408 , training loss: 4.387001\n",
      "[INFO] Epoch: 58 , batch: 409 , training loss: 4.421924\n",
      "[INFO] Epoch: 58 , batch: 410 , training loss: 4.458900\n",
      "[INFO] Epoch: 58 , batch: 411 , training loss: 4.622896\n",
      "[INFO] Epoch: 58 , batch: 412 , training loss: 4.460495\n",
      "[INFO] Epoch: 58 , batch: 413 , training loss: 4.318290\n",
      "[INFO] Epoch: 58 , batch: 414 , training loss: 4.368704\n",
      "[INFO] Epoch: 58 , batch: 415 , training loss: 4.419990\n",
      "[INFO] Epoch: 58 , batch: 416 , training loss: 4.474844\n",
      "[INFO] Epoch: 58 , batch: 417 , training loss: 4.407739\n",
      "[INFO] Epoch: 58 , batch: 418 , training loss: 4.460831\n",
      "[INFO] Epoch: 58 , batch: 419 , training loss: 4.397020\n",
      "[INFO] Epoch: 58 , batch: 420 , training loss: 4.385527\n",
      "[INFO] Epoch: 58 , batch: 421 , training loss: 4.377386\n",
      "[INFO] Epoch: 58 , batch: 422 , training loss: 4.216372\n",
      "[INFO] Epoch: 58 , batch: 423 , training loss: 4.454821\n",
      "[INFO] Epoch: 58 , batch: 424 , training loss: 4.593143\n",
      "[INFO] Epoch: 58 , batch: 425 , training loss: 4.481797\n",
      "[INFO] Epoch: 58 , batch: 426 , training loss: 4.222178\n",
      "[INFO] Epoch: 58 , batch: 427 , training loss: 4.465277\n",
      "[INFO] Epoch: 58 , batch: 428 , training loss: 4.323502\n",
      "[INFO] Epoch: 58 , batch: 429 , training loss: 4.228887\n",
      "[INFO] Epoch: 58 , batch: 430 , training loss: 4.460680\n",
      "[INFO] Epoch: 58 , batch: 431 , training loss: 4.069042\n",
      "[INFO] Epoch: 58 , batch: 432 , training loss: 4.128094\n",
      "[INFO] Epoch: 58 , batch: 433 , training loss: 4.170637\n",
      "[INFO] Epoch: 58 , batch: 434 , training loss: 4.035225\n",
      "[INFO] Epoch: 58 , batch: 435 , training loss: 4.395591\n",
      "[INFO] Epoch: 58 , batch: 436 , training loss: 4.424845\n",
      "[INFO] Epoch: 58 , batch: 437 , training loss: 4.231984\n",
      "[INFO] Epoch: 58 , batch: 438 , training loss: 4.088555\n",
      "[INFO] Epoch: 58 , batch: 439 , training loss: 4.312249\n",
      "[INFO] Epoch: 58 , batch: 440 , training loss: 4.443448\n",
      "[INFO] Epoch: 58 , batch: 441 , training loss: 4.537323\n",
      "[INFO] Epoch: 58 , batch: 442 , training loss: 4.304046\n",
      "[INFO] Epoch: 58 , batch: 443 , training loss: 4.460908\n",
      "[INFO] Epoch: 58 , batch: 444 , training loss: 4.096323\n",
      "[INFO] Epoch: 58 , batch: 445 , training loss: 3.974718\n",
      "[INFO] Epoch: 58 , batch: 446 , training loss: 3.913100\n",
      "[INFO] Epoch: 58 , batch: 447 , training loss: 4.127161\n",
      "[INFO] Epoch: 58 , batch: 448 , training loss: 4.247035\n",
      "[INFO] Epoch: 58 , batch: 449 , training loss: 4.616784\n",
      "[INFO] Epoch: 58 , batch: 450 , training loss: 4.694726\n",
      "[INFO] Epoch: 58 , batch: 451 , training loss: 4.565773\n",
      "[INFO] Epoch: 58 , batch: 452 , training loss: 4.404116\n",
      "[INFO] Epoch: 58 , batch: 453 , training loss: 4.149555\n",
      "[INFO] Epoch: 58 , batch: 454 , training loss: 4.310869\n",
      "[INFO] Epoch: 58 , batch: 455 , training loss: 4.346268\n",
      "[INFO] Epoch: 58 , batch: 456 , training loss: 4.355813\n",
      "[INFO] Epoch: 58 , batch: 457 , training loss: 4.437370\n",
      "[INFO] Epoch: 58 , batch: 458 , training loss: 4.163360\n",
      "[INFO] Epoch: 58 , batch: 459 , training loss: 4.146653\n",
      "[INFO] Epoch: 58 , batch: 460 , training loss: 4.269888\n",
      "[INFO] Epoch: 58 , batch: 461 , training loss: 4.217945\n",
      "[INFO] Epoch: 58 , batch: 462 , training loss: 4.306172\n",
      "[INFO] Epoch: 58 , batch: 463 , training loss: 4.200639\n",
      "[INFO] Epoch: 58 , batch: 464 , training loss: 4.388830\n",
      "[INFO] Epoch: 58 , batch: 465 , training loss: 4.317426\n",
      "[INFO] Epoch: 58 , batch: 466 , training loss: 4.404570\n",
      "[INFO] Epoch: 58 , batch: 467 , training loss: 4.386549\n",
      "[INFO] Epoch: 58 , batch: 468 , training loss: 4.351656\n",
      "[INFO] Epoch: 58 , batch: 469 , training loss: 4.370121\n",
      "[INFO] Epoch: 58 , batch: 470 , training loss: 4.199175\n",
      "[INFO] Epoch: 58 , batch: 471 , training loss: 4.289225\n",
      "[INFO] Epoch: 58 , batch: 472 , training loss: 4.341172\n",
      "[INFO] Epoch: 58 , batch: 473 , training loss: 4.273137\n",
      "[INFO] Epoch: 58 , batch: 474 , training loss: 4.057867\n",
      "[INFO] Epoch: 58 , batch: 475 , training loss: 3.919971\n",
      "[INFO] Epoch: 58 , batch: 476 , training loss: 4.330637\n",
      "[INFO] Epoch: 58 , batch: 477 , training loss: 4.439147\n",
      "[INFO] Epoch: 58 , batch: 478 , training loss: 4.447823\n",
      "[INFO] Epoch: 58 , batch: 479 , training loss: 4.425229\n",
      "[INFO] Epoch: 58 , batch: 480 , training loss: 4.563784\n",
      "[INFO] Epoch: 58 , batch: 481 , training loss: 4.431542\n",
      "[INFO] Epoch: 58 , batch: 482 , training loss: 4.540082\n",
      "[INFO] Epoch: 58 , batch: 483 , training loss: 4.363865\n",
      "[INFO] Epoch: 58 , batch: 484 , training loss: 4.180069\n",
      "[INFO] Epoch: 58 , batch: 485 , training loss: 4.277162\n",
      "[INFO] Epoch: 58 , batch: 486 , training loss: 4.169142\n",
      "[INFO] Epoch: 58 , batch: 487 , training loss: 4.164336\n",
      "[INFO] Epoch: 58 , batch: 488 , training loss: 4.336910\n",
      "[INFO] Epoch: 58 , batch: 489 , training loss: 4.239393\n",
      "[INFO] Epoch: 58 , batch: 490 , training loss: 4.307368\n",
      "[INFO] Epoch: 58 , batch: 491 , training loss: 4.207598\n",
      "[INFO] Epoch: 58 , batch: 492 , training loss: 4.202544\n",
      "[INFO] Epoch: 58 , batch: 493 , training loss: 4.352705\n",
      "[INFO] Epoch: 58 , batch: 494 , training loss: 4.259664\n",
      "[INFO] Epoch: 58 , batch: 495 , training loss: 4.458886\n",
      "[INFO] Epoch: 58 , batch: 496 , training loss: 4.295898\n",
      "[INFO] Epoch: 58 , batch: 497 , training loss: 4.337762\n",
      "[INFO] Epoch: 58 , batch: 498 , training loss: 4.312865\n",
      "[INFO] Epoch: 58 , batch: 499 , training loss: 4.382268\n",
      "[INFO] Epoch: 58 , batch: 500 , training loss: 4.534671\n",
      "[INFO] Epoch: 58 , batch: 501 , training loss: 4.867124\n",
      "[INFO] Epoch: 58 , batch: 502 , training loss: 4.908818\n",
      "[INFO] Epoch: 58 , batch: 503 , training loss: 4.508773\n",
      "[INFO] Epoch: 58 , batch: 504 , training loss: 4.644130\n",
      "[INFO] Epoch: 58 , batch: 505 , training loss: 4.633792\n",
      "[INFO] Epoch: 58 , batch: 506 , training loss: 4.608266\n",
      "[INFO] Epoch: 58 , batch: 507 , training loss: 4.673128\n",
      "[INFO] Epoch: 58 , batch: 508 , training loss: 4.581456\n",
      "[INFO] Epoch: 58 , batch: 509 , training loss: 4.381239\n",
      "[INFO] Epoch: 58 , batch: 510 , training loss: 4.472794\n",
      "[INFO] Epoch: 58 , batch: 511 , training loss: 4.387053\n",
      "[INFO] Epoch: 58 , batch: 512 , training loss: 4.488383\n",
      "[INFO] Epoch: 58 , batch: 513 , training loss: 4.754191\n",
      "[INFO] Epoch: 58 , batch: 514 , training loss: 4.398171\n",
      "[INFO] Epoch: 58 , batch: 515 , training loss: 4.657884\n",
      "[INFO] Epoch: 58 , batch: 516 , training loss: 4.437000\n",
      "[INFO] Epoch: 58 , batch: 517 , training loss: 4.415742\n",
      "[INFO] Epoch: 58 , batch: 518 , training loss: 4.391604\n",
      "[INFO] Epoch: 58 , batch: 519 , training loss: 4.198461\n",
      "[INFO] Epoch: 58 , batch: 520 , training loss: 4.443787\n",
      "[INFO] Epoch: 58 , batch: 521 , training loss: 4.430946\n",
      "[INFO] Epoch: 58 , batch: 522 , training loss: 4.512726\n",
      "[INFO] Epoch: 58 , batch: 523 , training loss: 4.422044\n",
      "[INFO] Epoch: 58 , batch: 524 , training loss: 4.726107\n",
      "[INFO] Epoch: 58 , batch: 525 , training loss: 4.614776\n",
      "[INFO] Epoch: 58 , batch: 526 , training loss: 4.404456\n",
      "[INFO] Epoch: 58 , batch: 527 , training loss: 4.417316\n",
      "[INFO] Epoch: 58 , batch: 528 , training loss: 4.428493\n",
      "[INFO] Epoch: 58 , batch: 529 , training loss: 4.421156\n",
      "[INFO] Epoch: 58 , batch: 530 , training loss: 4.285383\n",
      "[INFO] Epoch: 58 , batch: 531 , training loss: 4.436846\n",
      "[INFO] Epoch: 58 , batch: 532 , training loss: 4.334132\n",
      "[INFO] Epoch: 58 , batch: 533 , training loss: 4.448300\n",
      "[INFO] Epoch: 58 , batch: 534 , training loss: 4.451939\n",
      "[INFO] Epoch: 58 , batch: 535 , training loss: 4.436409\n",
      "[INFO] Epoch: 58 , batch: 536 , training loss: 4.323461\n",
      "[INFO] Epoch: 58 , batch: 537 , training loss: 4.298225\n",
      "[INFO] Epoch: 58 , batch: 538 , training loss: 4.393458\n",
      "[INFO] Epoch: 58 , batch: 539 , training loss: 4.449088\n",
      "[INFO] Epoch: 58 , batch: 540 , training loss: 4.999234\n",
      "[INFO] Epoch: 58 , batch: 541 , training loss: 4.813933\n",
      "[INFO] Epoch: 58 , batch: 542 , training loss: 4.711059\n",
      "[INFO] Epoch: 59 , batch: 0 , training loss: 3.666634\n",
      "[INFO] Epoch: 59 , batch: 1 , training loss: 3.513125\n",
      "[INFO] Epoch: 59 , batch: 2 , training loss: 3.731904\n",
      "[INFO] Epoch: 59 , batch: 3 , training loss: 3.620693\n",
      "[INFO] Epoch: 59 , batch: 4 , training loss: 3.961766\n",
      "[INFO] Epoch: 59 , batch: 5 , training loss: 3.616470\n",
      "[INFO] Epoch: 59 , batch: 6 , training loss: 3.987366\n",
      "[INFO] Epoch: 59 , batch: 7 , training loss: 3.946679\n",
      "[INFO] Epoch: 59 , batch: 8 , training loss: 3.599475\n",
      "[INFO] Epoch: 59 , batch: 9 , training loss: 3.878073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 59 , batch: 10 , training loss: 3.858046\n",
      "[INFO] Epoch: 59 , batch: 11 , training loss: 3.755111\n",
      "[INFO] Epoch: 59 , batch: 12 , training loss: 3.680003\n",
      "[INFO] Epoch: 59 , batch: 13 , training loss: 3.658701\n",
      "[INFO] Epoch: 59 , batch: 14 , training loss: 3.585741\n",
      "[INFO] Epoch: 59 , batch: 15 , training loss: 3.796121\n",
      "[INFO] Epoch: 59 , batch: 16 , training loss: 3.641958\n",
      "[INFO] Epoch: 59 , batch: 17 , training loss: 3.765424\n",
      "[INFO] Epoch: 59 , batch: 18 , training loss: 3.728329\n",
      "[INFO] Epoch: 59 , batch: 19 , training loss: 3.482273\n",
      "[INFO] Epoch: 59 , batch: 20 , training loss: 3.464205\n",
      "[INFO] Epoch: 59 , batch: 21 , training loss: 3.583491\n",
      "[INFO] Epoch: 59 , batch: 22 , training loss: 3.468703\n",
      "[INFO] Epoch: 59 , batch: 23 , training loss: 3.699479\n",
      "[INFO] Epoch: 59 , batch: 24 , training loss: 3.538862\n",
      "[INFO] Epoch: 59 , batch: 25 , training loss: 3.633859\n",
      "[INFO] Epoch: 59 , batch: 26 , training loss: 3.495637\n",
      "[INFO] Epoch: 59 , batch: 27 , training loss: 3.507630\n",
      "[INFO] Epoch: 59 , batch: 28 , training loss: 3.671355\n",
      "[INFO] Epoch: 59 , batch: 29 , training loss: 3.497388\n",
      "[INFO] Epoch: 59 , batch: 30 , training loss: 3.521581\n",
      "[INFO] Epoch: 59 , batch: 31 , training loss: 3.606575\n",
      "[INFO] Epoch: 59 , batch: 32 , training loss: 3.574819\n",
      "[INFO] Epoch: 59 , batch: 33 , training loss: 3.630164\n",
      "[INFO] Epoch: 59 , batch: 34 , training loss: 3.607496\n",
      "[INFO] Epoch: 59 , batch: 35 , training loss: 3.550440\n",
      "[INFO] Epoch: 59 , batch: 36 , training loss: 3.641392\n",
      "[INFO] Epoch: 59 , batch: 37 , training loss: 3.519052\n",
      "[INFO] Epoch: 59 , batch: 38 , training loss: 3.565034\n",
      "[INFO] Epoch: 59 , batch: 39 , training loss: 3.427727\n",
      "[INFO] Epoch: 59 , batch: 40 , training loss: 3.621475\n",
      "[INFO] Epoch: 59 , batch: 41 , training loss: 3.580660\n",
      "[INFO] Epoch: 59 , batch: 42 , training loss: 4.062553\n",
      "[INFO] Epoch: 59 , batch: 43 , training loss: 3.804054\n",
      "[INFO] Epoch: 59 , batch: 44 , training loss: 4.168854\n",
      "[INFO] Epoch: 59 , batch: 45 , training loss: 4.045585\n",
      "[INFO] Epoch: 59 , batch: 46 , training loss: 4.086320\n",
      "[INFO] Epoch: 59 , batch: 47 , training loss: 3.628053\n",
      "[INFO] Epoch: 59 , batch: 48 , training loss: 3.680691\n",
      "[INFO] Epoch: 59 , batch: 49 , training loss: 3.888488\n",
      "[INFO] Epoch: 59 , batch: 50 , training loss: 3.646330\n",
      "[INFO] Epoch: 59 , batch: 51 , training loss: 3.858356\n",
      "[INFO] Epoch: 59 , batch: 52 , training loss: 3.633139\n",
      "[INFO] Epoch: 59 , batch: 53 , training loss: 3.798430\n",
      "[INFO] Epoch: 59 , batch: 54 , training loss: 3.828622\n",
      "[INFO] Epoch: 59 , batch: 55 , training loss: 3.875229\n",
      "[INFO] Epoch: 59 , batch: 56 , training loss: 3.717360\n",
      "[INFO] Epoch: 59 , batch: 57 , training loss: 3.651033\n",
      "[INFO] Epoch: 59 , batch: 58 , training loss: 3.698172\n",
      "[INFO] Epoch: 59 , batch: 59 , training loss: 3.807492\n",
      "[INFO] Epoch: 59 , batch: 60 , training loss: 3.695458\n",
      "[INFO] Epoch: 59 , batch: 61 , training loss: 3.808515\n",
      "[INFO] Epoch: 59 , batch: 62 , training loss: 3.702117\n",
      "[INFO] Epoch: 59 , batch: 63 , training loss: 3.888722\n",
      "[INFO] Epoch: 59 , batch: 64 , training loss: 4.052106\n",
      "[INFO] Epoch: 59 , batch: 65 , training loss: 3.799726\n",
      "[INFO] Epoch: 59 , batch: 66 , training loss: 3.647995\n",
      "[INFO] Epoch: 59 , batch: 67 , training loss: 3.688496\n",
      "[INFO] Epoch: 59 , batch: 68 , training loss: 3.830327\n",
      "[INFO] Epoch: 59 , batch: 69 , training loss: 3.749800\n",
      "[INFO] Epoch: 59 , batch: 70 , training loss: 3.999907\n",
      "[INFO] Epoch: 59 , batch: 71 , training loss: 3.825551\n",
      "[INFO] Epoch: 59 , batch: 72 , training loss: 3.873075\n",
      "[INFO] Epoch: 59 , batch: 73 , training loss: 3.847706\n",
      "[INFO] Epoch: 59 , batch: 74 , training loss: 3.940246\n",
      "[INFO] Epoch: 59 , batch: 75 , training loss: 3.804682\n",
      "[INFO] Epoch: 59 , batch: 76 , training loss: 3.923532\n",
      "[INFO] Epoch: 59 , batch: 77 , training loss: 3.842063\n",
      "[INFO] Epoch: 59 , batch: 78 , training loss: 3.954939\n",
      "[INFO] Epoch: 59 , batch: 79 , training loss: 3.816115\n",
      "[INFO] Epoch: 59 , batch: 80 , training loss: 3.990185\n",
      "[INFO] Epoch: 59 , batch: 81 , training loss: 3.923234\n",
      "[INFO] Epoch: 59 , batch: 82 , training loss: 3.903529\n",
      "[INFO] Epoch: 59 , batch: 83 , training loss: 3.984601\n",
      "[INFO] Epoch: 59 , batch: 84 , training loss: 3.973173\n",
      "[INFO] Epoch: 59 , batch: 85 , training loss: 4.018072\n",
      "[INFO] Epoch: 59 , batch: 86 , training loss: 3.986541\n",
      "[INFO] Epoch: 59 , batch: 87 , training loss: 3.918965\n",
      "[INFO] Epoch: 59 , batch: 88 , training loss: 4.066826\n",
      "[INFO] Epoch: 59 , batch: 89 , training loss: 3.843593\n",
      "[INFO] Epoch: 59 , batch: 90 , training loss: 3.916836\n",
      "[INFO] Epoch: 59 , batch: 91 , training loss: 3.848577\n",
      "[INFO] Epoch: 59 , batch: 92 , training loss: 3.894244\n",
      "[INFO] Epoch: 59 , batch: 93 , training loss: 3.973455\n",
      "[INFO] Epoch: 59 , batch: 94 , training loss: 4.101889\n",
      "[INFO] Epoch: 59 , batch: 95 , training loss: 3.885662\n",
      "[INFO] Epoch: 59 , batch: 96 , training loss: 3.870146\n",
      "[INFO] Epoch: 59 , batch: 97 , training loss: 3.819759\n",
      "[INFO] Epoch: 59 , batch: 98 , training loss: 3.764433\n",
      "[INFO] Epoch: 59 , batch: 99 , training loss: 3.906378\n",
      "[INFO] Epoch: 59 , batch: 100 , training loss: 3.813743\n",
      "[INFO] Epoch: 59 , batch: 101 , training loss: 3.800383\n",
      "[INFO] Epoch: 59 , batch: 102 , training loss: 3.968930\n",
      "[INFO] Epoch: 59 , batch: 103 , training loss: 3.742229\n",
      "[INFO] Epoch: 59 , batch: 104 , training loss: 3.705568\n",
      "[INFO] Epoch: 59 , batch: 105 , training loss: 3.959291\n",
      "[INFO] Epoch: 59 , batch: 106 , training loss: 3.986539\n",
      "[INFO] Epoch: 59 , batch: 107 , training loss: 3.834509\n",
      "[INFO] Epoch: 59 , batch: 108 , training loss: 3.771325\n",
      "[INFO] Epoch: 59 , batch: 109 , training loss: 3.679899\n",
      "[INFO] Epoch: 59 , batch: 110 , training loss: 3.872708\n",
      "[INFO] Epoch: 59 , batch: 111 , training loss: 3.960016\n",
      "[INFO] Epoch: 59 , batch: 112 , training loss: 3.895873\n",
      "[INFO] Epoch: 59 , batch: 113 , training loss: 3.835259\n",
      "[INFO] Epoch: 59 , batch: 114 , training loss: 3.869726\n",
      "[INFO] Epoch: 59 , batch: 115 , training loss: 3.882382\n",
      "[INFO] Epoch: 59 , batch: 116 , training loss: 3.756888\n",
      "[INFO] Epoch: 59 , batch: 117 , training loss: 3.997522\n",
      "[INFO] Epoch: 59 , batch: 118 , training loss: 3.971821\n",
      "[INFO] Epoch: 59 , batch: 119 , training loss: 4.111678\n",
      "[INFO] Epoch: 59 , batch: 120 , training loss: 4.051854\n",
      "[INFO] Epoch: 59 , batch: 121 , training loss: 3.965862\n",
      "[INFO] Epoch: 59 , batch: 122 , training loss: 3.861107\n",
      "[INFO] Epoch: 59 , batch: 123 , training loss: 3.832129\n",
      "[INFO] Epoch: 59 , batch: 124 , training loss: 3.980844\n",
      "[INFO] Epoch: 59 , batch: 125 , training loss: 3.744381\n",
      "[INFO] Epoch: 59 , batch: 126 , training loss: 3.760092\n",
      "[INFO] Epoch: 59 , batch: 127 , training loss: 3.761208\n",
      "[INFO] Epoch: 59 , batch: 128 , training loss: 3.908480\n",
      "[INFO] Epoch: 59 , batch: 129 , training loss: 3.847049\n",
      "[INFO] Epoch: 59 , batch: 130 , training loss: 3.860346\n",
      "[INFO] Epoch: 59 , batch: 131 , training loss: 3.877336\n",
      "[INFO] Epoch: 59 , batch: 132 , training loss: 3.885744\n",
      "[INFO] Epoch: 59 , batch: 133 , training loss: 3.842667\n",
      "[INFO] Epoch: 59 , batch: 134 , training loss: 3.625309\n",
      "[INFO] Epoch: 59 , batch: 135 , training loss: 3.688050\n",
      "[INFO] Epoch: 59 , batch: 136 , training loss: 3.992826\n",
      "[INFO] Epoch: 59 , batch: 137 , training loss: 3.912367\n",
      "[INFO] Epoch: 59 , batch: 138 , training loss: 3.936028\n",
      "[INFO] Epoch: 59 , batch: 139 , training loss: 4.524574\n",
      "[INFO] Epoch: 59 , batch: 140 , training loss: 4.319829\n",
      "[INFO] Epoch: 59 , batch: 141 , training loss: 4.098744\n",
      "[INFO] Epoch: 59 , batch: 142 , training loss: 3.807405\n",
      "[INFO] Epoch: 59 , batch: 143 , training loss: 3.937919\n",
      "[INFO] Epoch: 59 , batch: 144 , training loss: 3.760336\n",
      "[INFO] Epoch: 59 , batch: 145 , training loss: 3.837719\n",
      "[INFO] Epoch: 59 , batch: 146 , training loss: 4.040682\n",
      "[INFO] Epoch: 59 , batch: 147 , training loss: 3.703163\n",
      "[INFO] Epoch: 59 , batch: 148 , training loss: 3.658601\n",
      "[INFO] Epoch: 59 , batch: 149 , training loss: 3.752545\n",
      "[INFO] Epoch: 59 , batch: 150 , training loss: 4.021881\n",
      "[INFO] Epoch: 59 , batch: 151 , training loss: 3.879612\n",
      "[INFO] Epoch: 59 , batch: 152 , training loss: 3.875949\n",
      "[INFO] Epoch: 59 , batch: 153 , training loss: 3.905872\n",
      "[INFO] Epoch: 59 , batch: 154 , training loss: 4.020803\n",
      "[INFO] Epoch: 59 , batch: 155 , training loss: 4.234571\n",
      "[INFO] Epoch: 59 , batch: 156 , training loss: 3.941416\n",
      "[INFO] Epoch: 59 , batch: 157 , training loss: 3.920956\n",
      "[INFO] Epoch: 59 , batch: 158 , training loss: 4.074917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 59 , batch: 159 , training loss: 4.013987\n",
      "[INFO] Epoch: 59 , batch: 160 , training loss: 4.199535\n",
      "[INFO] Epoch: 59 , batch: 161 , training loss: 4.221202\n",
      "[INFO] Epoch: 59 , batch: 162 , training loss: 4.227261\n",
      "[INFO] Epoch: 59 , batch: 163 , training loss: 4.424793\n",
      "[INFO] Epoch: 59 , batch: 164 , training loss: 4.349060\n",
      "[INFO] Epoch: 59 , batch: 165 , training loss: 4.270915\n",
      "[INFO] Epoch: 59 , batch: 166 , training loss: 4.220504\n",
      "[INFO] Epoch: 59 , batch: 167 , training loss: 4.248243\n",
      "[INFO] Epoch: 59 , batch: 168 , training loss: 3.980163\n",
      "[INFO] Epoch: 59 , batch: 169 , training loss: 3.914592\n",
      "[INFO] Epoch: 59 , batch: 170 , training loss: 4.116997\n",
      "[INFO] Epoch: 59 , batch: 171 , training loss: 3.563836\n",
      "[INFO] Epoch: 59 , batch: 172 , training loss: 3.828413\n",
      "[INFO] Epoch: 59 , batch: 173 , training loss: 4.099324\n",
      "[INFO] Epoch: 59 , batch: 174 , training loss: 4.547915\n",
      "[INFO] Epoch: 59 , batch: 175 , training loss: 4.765400\n",
      "[INFO] Epoch: 59 , batch: 176 , training loss: 4.455915\n",
      "[INFO] Epoch: 59 , batch: 177 , training loss: 4.030061\n",
      "[INFO] Epoch: 59 , batch: 178 , training loss: 4.055127\n",
      "[INFO] Epoch: 59 , batch: 179 , training loss: 4.103017\n",
      "[INFO] Epoch: 59 , batch: 180 , training loss: 4.044896\n",
      "[INFO] Epoch: 59 , batch: 181 , training loss: 4.347188\n",
      "[INFO] Epoch: 59 , batch: 182 , training loss: 4.268522\n",
      "[INFO] Epoch: 59 , batch: 183 , training loss: 4.259819\n",
      "[INFO] Epoch: 59 , batch: 184 , training loss: 4.147461\n",
      "[INFO] Epoch: 59 , batch: 185 , training loss: 4.105900\n",
      "[INFO] Epoch: 59 , batch: 186 , training loss: 4.256285\n",
      "[INFO] Epoch: 59 , batch: 187 , training loss: 4.352653\n",
      "[INFO] Epoch: 59 , batch: 188 , training loss: 4.325975\n",
      "[INFO] Epoch: 59 , batch: 189 , training loss: 4.259513\n",
      "[INFO] Epoch: 59 , batch: 190 , training loss: 4.299536\n",
      "[INFO] Epoch: 59 , batch: 191 , training loss: 4.398067\n",
      "[INFO] Epoch: 59 , batch: 192 , training loss: 4.266257\n",
      "[INFO] Epoch: 59 , batch: 193 , training loss: 4.350853\n",
      "[INFO] Epoch: 59 , batch: 194 , training loss: 4.274109\n",
      "[INFO] Epoch: 59 , batch: 195 , training loss: 4.210297\n",
      "[INFO] Epoch: 59 , batch: 196 , training loss: 4.059692\n",
      "[INFO] Epoch: 59 , batch: 197 , training loss: 4.150295\n",
      "[INFO] Epoch: 59 , batch: 198 , training loss: 4.063130\n",
      "[INFO] Epoch: 59 , batch: 199 , training loss: 4.207584\n",
      "[INFO] Epoch: 59 , batch: 200 , training loss: 4.106874\n",
      "[INFO] Epoch: 59 , batch: 201 , training loss: 4.005048\n",
      "[INFO] Epoch: 59 , batch: 202 , training loss: 3.993105\n",
      "[INFO] Epoch: 59 , batch: 203 , training loss: 4.139556\n",
      "[INFO] Epoch: 59 , batch: 204 , training loss: 4.205823\n",
      "[INFO] Epoch: 59 , batch: 205 , training loss: 3.832557\n",
      "[INFO] Epoch: 59 , batch: 206 , training loss: 3.765128\n",
      "[INFO] Epoch: 59 , batch: 207 , training loss: 3.733678\n",
      "[INFO] Epoch: 59 , batch: 208 , training loss: 4.083994\n",
      "[INFO] Epoch: 59 , batch: 209 , training loss: 4.055115\n",
      "[INFO] Epoch: 59 , batch: 210 , training loss: 4.079640\n",
      "[INFO] Epoch: 59 , batch: 211 , training loss: 4.045955\n",
      "[INFO] Epoch: 59 , batch: 212 , training loss: 4.160119\n",
      "[INFO] Epoch: 59 , batch: 213 , training loss: 4.094925\n",
      "[INFO] Epoch: 59 , batch: 214 , training loss: 4.161182\n",
      "[INFO] Epoch: 59 , batch: 215 , training loss: 4.386237\n",
      "[INFO] Epoch: 59 , batch: 216 , training loss: 4.072082\n",
      "[INFO] Epoch: 59 , batch: 217 , training loss: 4.043165\n",
      "[INFO] Epoch: 59 , batch: 218 , training loss: 4.027729\n",
      "[INFO] Epoch: 59 , batch: 219 , training loss: 4.122485\n",
      "[INFO] Epoch: 59 , batch: 220 , training loss: 3.967592\n",
      "[INFO] Epoch: 59 , batch: 221 , training loss: 3.967139\n",
      "[INFO] Epoch: 59 , batch: 222 , training loss: 4.089817\n",
      "[INFO] Epoch: 59 , batch: 223 , training loss: 4.231763\n",
      "[INFO] Epoch: 59 , batch: 224 , training loss: 4.261774\n",
      "[INFO] Epoch: 59 , batch: 225 , training loss: 4.147274\n",
      "[INFO] Epoch: 59 , batch: 226 , training loss: 4.284243\n",
      "[INFO] Epoch: 59 , batch: 227 , training loss: 4.260416\n",
      "[INFO] Epoch: 59 , batch: 228 , training loss: 4.298967\n",
      "[INFO] Epoch: 59 , batch: 229 , training loss: 4.120434\n",
      "[INFO] Epoch: 59 , batch: 230 , training loss: 3.996596\n",
      "[INFO] Epoch: 59 , batch: 231 , training loss: 3.843800\n",
      "[INFO] Epoch: 59 , batch: 232 , training loss: 3.988298\n",
      "[INFO] Epoch: 59 , batch: 233 , training loss: 4.038765\n",
      "[INFO] Epoch: 59 , batch: 234 , training loss: 3.717051\n",
      "[INFO] Epoch: 59 , batch: 235 , training loss: 3.814754\n",
      "[INFO] Epoch: 59 , batch: 236 , training loss: 3.924596\n",
      "[INFO] Epoch: 59 , batch: 237 , training loss: 4.164774\n",
      "[INFO] Epoch: 59 , batch: 238 , training loss: 3.929046\n",
      "[INFO] Epoch: 59 , batch: 239 , training loss: 3.958845\n",
      "[INFO] Epoch: 59 , batch: 240 , training loss: 4.005782\n",
      "[INFO] Epoch: 59 , batch: 241 , training loss: 3.820384\n",
      "[INFO] Epoch: 59 , batch: 242 , training loss: 3.816985\n",
      "[INFO] Epoch: 59 , batch: 243 , training loss: 4.129346\n",
      "[INFO] Epoch: 59 , batch: 244 , training loss: 4.064957\n",
      "[INFO] Epoch: 59 , batch: 245 , training loss: 4.012369\n",
      "[INFO] Epoch: 59 , batch: 246 , training loss: 3.745564\n",
      "[INFO] Epoch: 59 , batch: 247 , training loss: 3.923551\n",
      "[INFO] Epoch: 59 , batch: 248 , training loss: 3.988914\n",
      "[INFO] Epoch: 59 , batch: 249 , training loss: 3.960766\n",
      "[INFO] Epoch: 59 , batch: 250 , training loss: 3.783803\n",
      "[INFO] Epoch: 59 , batch: 251 , training loss: 4.207665\n",
      "[INFO] Epoch: 59 , batch: 252 , training loss: 3.931500\n",
      "[INFO] Epoch: 59 , batch: 253 , training loss: 3.817665\n",
      "[INFO] Epoch: 59 , batch: 254 , training loss: 4.090570\n",
      "[INFO] Epoch: 59 , batch: 255 , training loss: 4.093736\n",
      "[INFO] Epoch: 59 , batch: 256 , training loss: 4.064183\n",
      "[INFO] Epoch: 59 , batch: 257 , training loss: 4.234007\n",
      "[INFO] Epoch: 59 , batch: 258 , training loss: 4.224311\n",
      "[INFO] Epoch: 59 , batch: 259 , training loss: 4.265468\n",
      "[INFO] Epoch: 59 , batch: 260 , training loss: 4.057918\n",
      "[INFO] Epoch: 59 , batch: 261 , training loss: 4.238116\n",
      "[INFO] Epoch: 59 , batch: 262 , training loss: 4.365004\n",
      "[INFO] Epoch: 59 , batch: 263 , training loss: 4.534355\n",
      "[INFO] Epoch: 59 , batch: 264 , training loss: 3.888506\n",
      "[INFO] Epoch: 59 , batch: 265 , training loss: 3.989552\n",
      "[INFO] Epoch: 59 , batch: 266 , training loss: 4.393818\n",
      "[INFO] Epoch: 59 , batch: 267 , training loss: 4.166994\n",
      "[INFO] Epoch: 59 , batch: 268 , training loss: 4.074255\n",
      "[INFO] Epoch: 59 , batch: 269 , training loss: 4.062803\n",
      "[INFO] Epoch: 59 , batch: 270 , training loss: 4.080449\n",
      "[INFO] Epoch: 59 , batch: 271 , training loss: 4.098973\n",
      "[INFO] Epoch: 59 , batch: 272 , training loss: 4.115938\n",
      "[INFO] Epoch: 59 , batch: 273 , training loss: 4.121738\n",
      "[INFO] Epoch: 59 , batch: 274 , training loss: 4.223275\n",
      "[INFO] Epoch: 59 , batch: 275 , training loss: 4.069002\n",
      "[INFO] Epoch: 59 , batch: 276 , training loss: 4.131161\n",
      "[INFO] Epoch: 59 , batch: 277 , training loss: 4.300232\n",
      "[INFO] Epoch: 59 , batch: 278 , training loss: 3.967263\n",
      "[INFO] Epoch: 59 , batch: 279 , training loss: 3.965666\n",
      "[INFO] Epoch: 59 , batch: 280 , training loss: 3.978259\n",
      "[INFO] Epoch: 59 , batch: 281 , training loss: 4.100562\n",
      "[INFO] Epoch: 59 , batch: 282 , training loss: 4.022951\n",
      "[INFO] Epoch: 59 , batch: 283 , training loss: 4.010814\n",
      "[INFO] Epoch: 59 , batch: 284 , training loss: 4.021303\n",
      "[INFO] Epoch: 59 , batch: 285 , training loss: 3.975104\n",
      "[INFO] Epoch: 59 , batch: 286 , training loss: 3.984135\n",
      "[INFO] Epoch: 59 , batch: 287 , training loss: 3.923348\n",
      "[INFO] Epoch: 59 , batch: 288 , training loss: 3.886945\n",
      "[INFO] Epoch: 59 , batch: 289 , training loss: 3.975144\n",
      "[INFO] Epoch: 59 , batch: 290 , training loss: 3.755269\n",
      "[INFO] Epoch: 59 , batch: 291 , training loss: 3.758036\n",
      "[INFO] Epoch: 59 , batch: 292 , training loss: 3.843740\n",
      "[INFO] Epoch: 59 , batch: 293 , training loss: 3.782518\n",
      "[INFO] Epoch: 59 , batch: 294 , training loss: 4.414104\n",
      "[INFO] Epoch: 59 , batch: 295 , training loss: 4.199868\n",
      "[INFO] Epoch: 59 , batch: 296 , training loss: 4.139467\n",
      "[INFO] Epoch: 59 , batch: 297 , training loss: 4.085624\n",
      "[INFO] Epoch: 59 , batch: 298 , training loss: 3.923189\n",
      "[INFO] Epoch: 59 , batch: 299 , training loss: 3.957659\n",
      "[INFO] Epoch: 59 , batch: 300 , training loss: 3.957916\n",
      "[INFO] Epoch: 59 , batch: 301 , training loss: 3.877792\n",
      "[INFO] Epoch: 59 , batch: 302 , training loss: 4.041110\n",
      "[INFO] Epoch: 59 , batch: 303 , training loss: 4.062428\n",
      "[INFO] Epoch: 59 , batch: 304 , training loss: 4.169645\n",
      "[INFO] Epoch: 59 , batch: 305 , training loss: 4.041127\n",
      "[INFO] Epoch: 59 , batch: 306 , training loss: 4.142230\n",
      "[INFO] Epoch: 59 , batch: 307 , training loss: 4.160903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 59 , batch: 308 , training loss: 3.983204\n",
      "[INFO] Epoch: 59 , batch: 309 , training loss: 3.950543\n",
      "[INFO] Epoch: 59 , batch: 310 , training loss: 3.914265\n",
      "[INFO] Epoch: 59 , batch: 311 , training loss: 3.893192\n",
      "[INFO] Epoch: 59 , batch: 312 , training loss: 3.781359\n",
      "[INFO] Epoch: 59 , batch: 313 , training loss: 3.898954\n",
      "[INFO] Epoch: 59 , batch: 314 , training loss: 3.980568\n",
      "[INFO] Epoch: 59 , batch: 315 , training loss: 4.043311\n",
      "[INFO] Epoch: 59 , batch: 316 , training loss: 4.276764\n",
      "[INFO] Epoch: 59 , batch: 317 , training loss: 4.654461\n",
      "[INFO] Epoch: 59 , batch: 318 , training loss: 4.745182\n",
      "[INFO] Epoch: 59 , batch: 319 , training loss: 4.436717\n",
      "[INFO] Epoch: 59 , batch: 320 , training loss: 3.990113\n",
      "[INFO] Epoch: 59 , batch: 321 , training loss: 3.836703\n",
      "[INFO] Epoch: 59 , batch: 322 , training loss: 3.927100\n",
      "[INFO] Epoch: 59 , batch: 323 , training loss: 3.971312\n",
      "[INFO] Epoch: 59 , batch: 324 , training loss: 3.928636\n",
      "[INFO] Epoch: 59 , batch: 325 , training loss: 4.055165\n",
      "[INFO] Epoch: 59 , batch: 326 , training loss: 4.107077\n",
      "[INFO] Epoch: 59 , batch: 327 , training loss: 4.049736\n",
      "[INFO] Epoch: 59 , batch: 328 , training loss: 4.064587\n",
      "[INFO] Epoch: 59 , batch: 329 , training loss: 3.972677\n",
      "[INFO] Epoch: 59 , batch: 330 , training loss: 3.952893\n",
      "[INFO] Epoch: 59 , batch: 331 , training loss: 4.100657\n",
      "[INFO] Epoch: 59 , batch: 332 , training loss: 3.942369\n",
      "[INFO] Epoch: 59 , batch: 333 , training loss: 3.912612\n",
      "[INFO] Epoch: 59 , batch: 334 , training loss: 3.952196\n",
      "[INFO] Epoch: 59 , batch: 335 , training loss: 4.065103\n",
      "[INFO] Epoch: 59 , batch: 336 , training loss: 4.087914\n",
      "[INFO] Epoch: 59 , batch: 337 , training loss: 4.122773\n",
      "[INFO] Epoch: 59 , batch: 338 , training loss: 4.319213\n",
      "[INFO] Epoch: 59 , batch: 339 , training loss: 4.147603\n",
      "[INFO] Epoch: 59 , batch: 340 , training loss: 4.332670\n",
      "[INFO] Epoch: 59 , batch: 341 , training loss: 4.087615\n",
      "[INFO] Epoch: 59 , batch: 342 , training loss: 3.885917\n",
      "[INFO] Epoch: 59 , batch: 343 , training loss: 3.945575\n",
      "[INFO] Epoch: 59 , batch: 344 , training loss: 3.805696\n",
      "[INFO] Epoch: 59 , batch: 345 , training loss: 3.931756\n",
      "[INFO] Epoch: 59 , batch: 346 , training loss: 4.002672\n",
      "[INFO] Epoch: 59 , batch: 347 , training loss: 3.909980\n",
      "[INFO] Epoch: 59 , batch: 348 , training loss: 3.998041\n",
      "[INFO] Epoch: 59 , batch: 349 , training loss: 4.104241\n",
      "[INFO] Epoch: 59 , batch: 350 , training loss: 3.966020\n",
      "[INFO] Epoch: 59 , batch: 351 , training loss: 4.041528\n",
      "[INFO] Epoch: 59 , batch: 352 , training loss: 4.045543\n",
      "[INFO] Epoch: 59 , batch: 353 , training loss: 4.029018\n",
      "[INFO] Epoch: 59 , batch: 354 , training loss: 4.110590\n",
      "[INFO] Epoch: 59 , batch: 355 , training loss: 4.113714\n",
      "[INFO] Epoch: 59 , batch: 356 , training loss: 3.976468\n",
      "[INFO] Epoch: 59 , batch: 357 , training loss: 4.035148\n",
      "[INFO] Epoch: 59 , batch: 358 , training loss: 3.968762\n",
      "[INFO] Epoch: 59 , batch: 359 , training loss: 3.968590\n",
      "[INFO] Epoch: 59 , batch: 360 , training loss: 4.060640\n",
      "[INFO] Epoch: 59 , batch: 361 , training loss: 4.030533\n",
      "[INFO] Epoch: 59 , batch: 362 , training loss: 4.137999\n",
      "[INFO] Epoch: 59 , batch: 363 , training loss: 4.019550\n",
      "[INFO] Epoch: 59 , batch: 364 , training loss: 4.080564\n",
      "[INFO] Epoch: 59 , batch: 365 , training loss: 4.005953\n",
      "[INFO] Epoch: 59 , batch: 366 , training loss: 4.108155\n",
      "[INFO] Epoch: 59 , batch: 367 , training loss: 4.148371\n",
      "[INFO] Epoch: 59 , batch: 368 , training loss: 4.553545\n",
      "[INFO] Epoch: 59 , batch: 369 , training loss: 4.222435\n",
      "[INFO] Epoch: 59 , batch: 370 , training loss: 3.999878\n",
      "[INFO] Epoch: 59 , batch: 371 , training loss: 4.392614\n",
      "[INFO] Epoch: 59 , batch: 372 , training loss: 4.684266\n",
      "[INFO] Epoch: 59 , batch: 373 , training loss: 4.666904\n",
      "[INFO] Epoch: 59 , batch: 374 , training loss: 4.835520\n",
      "[INFO] Epoch: 59 , batch: 375 , training loss: 4.836919\n",
      "[INFO] Epoch: 59 , batch: 376 , training loss: 4.669392\n",
      "[INFO] Epoch: 59 , batch: 377 , training loss: 4.445166\n",
      "[INFO] Epoch: 59 , batch: 378 , training loss: 4.531848\n",
      "[INFO] Epoch: 59 , batch: 379 , training loss: 4.518540\n",
      "[INFO] Epoch: 59 , batch: 380 , training loss: 4.695522\n",
      "[INFO] Epoch: 59 , batch: 381 , training loss: 4.360503\n",
      "[INFO] Epoch: 59 , batch: 382 , training loss: 4.620788\n",
      "[INFO] Epoch: 59 , batch: 383 , training loss: 4.681571\n",
      "[INFO] Epoch: 59 , batch: 384 , training loss: 4.618536\n",
      "[INFO] Epoch: 59 , batch: 385 , training loss: 4.321587\n",
      "[INFO] Epoch: 59 , batch: 386 , training loss: 4.575126\n",
      "[INFO] Epoch: 59 , batch: 387 , training loss: 4.506915\n",
      "[INFO] Epoch: 59 , batch: 388 , training loss: 4.344169\n",
      "[INFO] Epoch: 59 , batch: 389 , training loss: 4.168083\n",
      "[INFO] Epoch: 59 , batch: 390 , training loss: 4.189410\n",
      "[INFO] Epoch: 59 , batch: 391 , training loss: 4.217704\n",
      "[INFO] Epoch: 59 , batch: 392 , training loss: 4.574307\n",
      "[INFO] Epoch: 59 , batch: 393 , training loss: 4.456461\n",
      "[INFO] Epoch: 59 , batch: 394 , training loss: 4.578297\n",
      "[INFO] Epoch: 59 , batch: 395 , training loss: 4.370770\n",
      "[INFO] Epoch: 59 , batch: 396 , training loss: 4.205914\n",
      "[INFO] Epoch: 59 , batch: 397 , training loss: 4.355832\n",
      "[INFO] Epoch: 59 , batch: 398 , training loss: 4.197880\n",
      "[INFO] Epoch: 59 , batch: 399 , training loss: 4.290879\n",
      "[INFO] Epoch: 59 , batch: 400 , training loss: 4.252268\n",
      "[INFO] Epoch: 59 , batch: 401 , training loss: 4.689996\n",
      "[INFO] Epoch: 59 , batch: 402 , training loss: 4.401338\n",
      "[INFO] Epoch: 59 , batch: 403 , training loss: 4.256974\n",
      "[INFO] Epoch: 59 , batch: 404 , training loss: 4.425341\n",
      "[INFO] Epoch: 59 , batch: 405 , training loss: 4.443022\n",
      "[INFO] Epoch: 59 , batch: 406 , training loss: 4.365776\n",
      "[INFO] Epoch: 59 , batch: 407 , training loss: 4.382947\n",
      "[INFO] Epoch: 59 , batch: 408 , training loss: 4.379639\n",
      "[INFO] Epoch: 59 , batch: 409 , training loss: 4.392051\n",
      "[INFO] Epoch: 59 , batch: 410 , training loss: 4.435272\n",
      "[INFO] Epoch: 59 , batch: 411 , training loss: 4.604877\n",
      "[INFO] Epoch: 59 , batch: 412 , training loss: 4.451869\n",
      "[INFO] Epoch: 59 , batch: 413 , training loss: 4.321739\n",
      "[INFO] Epoch: 59 , batch: 414 , training loss: 4.343880\n",
      "[INFO] Epoch: 59 , batch: 415 , training loss: 4.418999\n",
      "[INFO] Epoch: 59 , batch: 416 , training loss: 4.465942\n",
      "[INFO] Epoch: 59 , batch: 417 , training loss: 4.373367\n",
      "[INFO] Epoch: 59 , batch: 418 , training loss: 4.438550\n",
      "[INFO] Epoch: 59 , batch: 419 , training loss: 4.384812\n",
      "[INFO] Epoch: 59 , batch: 420 , training loss: 4.373518\n",
      "[INFO] Epoch: 59 , batch: 421 , training loss: 4.357339\n",
      "[INFO] Epoch: 59 , batch: 422 , training loss: 4.202601\n",
      "[INFO] Epoch: 59 , batch: 423 , training loss: 4.433239\n",
      "[INFO] Epoch: 59 , batch: 424 , training loss: 4.591710\n",
      "[INFO] Epoch: 59 , batch: 425 , training loss: 4.456204\n",
      "[INFO] Epoch: 59 , batch: 426 , training loss: 4.222685\n",
      "[INFO] Epoch: 59 , batch: 427 , training loss: 4.433987\n",
      "[INFO] Epoch: 59 , batch: 428 , training loss: 4.299163\n",
      "[INFO] Epoch: 59 , batch: 429 , training loss: 4.220677\n",
      "[INFO] Epoch: 59 , batch: 430 , training loss: 4.439531\n",
      "[INFO] Epoch: 59 , batch: 431 , training loss: 4.049581\n",
      "[INFO] Epoch: 59 , batch: 432 , training loss: 4.097234\n",
      "[INFO] Epoch: 59 , batch: 433 , training loss: 4.144876\n",
      "[INFO] Epoch: 59 , batch: 434 , training loss: 4.026060\n",
      "[INFO] Epoch: 59 , batch: 435 , training loss: 4.370288\n",
      "[INFO] Epoch: 59 , batch: 436 , training loss: 4.415376\n",
      "[INFO] Epoch: 59 , batch: 437 , training loss: 4.203264\n",
      "[INFO] Epoch: 59 , batch: 438 , training loss: 4.065781\n",
      "[INFO] Epoch: 59 , batch: 439 , training loss: 4.291662\n",
      "[INFO] Epoch: 59 , batch: 440 , training loss: 4.432224\n",
      "[INFO] Epoch: 59 , batch: 441 , training loss: 4.516628\n",
      "[INFO] Epoch: 59 , batch: 442 , training loss: 4.282190\n",
      "[INFO] Epoch: 59 , batch: 443 , training loss: 4.451184\n",
      "[INFO] Epoch: 59 , batch: 444 , training loss: 4.069812\n",
      "[INFO] Epoch: 59 , batch: 445 , training loss: 3.958411\n",
      "[INFO] Epoch: 59 , batch: 446 , training loss: 3.902580\n",
      "[INFO] Epoch: 59 , batch: 447 , training loss: 4.092342\n",
      "[INFO] Epoch: 59 , batch: 448 , training loss: 4.218948\n",
      "[INFO] Epoch: 59 , batch: 449 , training loss: 4.606488\n",
      "[INFO] Epoch: 59 , batch: 450 , training loss: 4.675842\n",
      "[INFO] Epoch: 59 , batch: 451 , training loss: 4.570555\n",
      "[INFO] Epoch: 59 , batch: 452 , training loss: 4.388527\n",
      "[INFO] Epoch: 59 , batch: 453 , training loss: 4.143823\n",
      "[INFO] Epoch: 59 , batch: 454 , training loss: 4.300716\n",
      "[INFO] Epoch: 59 , batch: 455 , training loss: 4.332498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 59 , batch: 456 , training loss: 4.344516\n",
      "[INFO] Epoch: 59 , batch: 457 , training loss: 4.418599\n",
      "[INFO] Epoch: 59 , batch: 458 , training loss: 4.163683\n",
      "[INFO] Epoch: 59 , batch: 459 , training loss: 4.140625\n",
      "[INFO] Epoch: 59 , batch: 460 , training loss: 4.258723\n",
      "[INFO] Epoch: 59 , batch: 461 , training loss: 4.215068\n",
      "[INFO] Epoch: 59 , batch: 462 , training loss: 4.289156\n",
      "[INFO] Epoch: 59 , batch: 463 , training loss: 4.203961\n",
      "[INFO] Epoch: 59 , batch: 464 , training loss: 4.371907\n",
      "[INFO] Epoch: 59 , batch: 465 , training loss: 4.314752\n",
      "[INFO] Epoch: 59 , batch: 466 , training loss: 4.390507\n",
      "[INFO] Epoch: 59 , batch: 467 , training loss: 4.361484\n",
      "[INFO] Epoch: 59 , batch: 468 , training loss: 4.324205\n",
      "[INFO] Epoch: 59 , batch: 469 , training loss: 4.362588\n",
      "[INFO] Epoch: 59 , batch: 470 , training loss: 4.177770\n",
      "[INFO] Epoch: 59 , batch: 471 , training loss: 4.293092\n",
      "[INFO] Epoch: 59 , batch: 472 , training loss: 4.342181\n",
      "[INFO] Epoch: 59 , batch: 473 , training loss: 4.252911\n",
      "[INFO] Epoch: 59 , batch: 474 , training loss: 4.043323\n",
      "[INFO] Epoch: 59 , batch: 475 , training loss: 3.922364\n",
      "[INFO] Epoch: 59 , batch: 476 , training loss: 4.313571\n",
      "[INFO] Epoch: 59 , batch: 477 , training loss: 4.424583\n",
      "[INFO] Epoch: 59 , batch: 478 , training loss: 4.430781\n",
      "[INFO] Epoch: 59 , batch: 479 , training loss: 4.415198\n",
      "[INFO] Epoch: 59 , batch: 480 , training loss: 4.552649\n",
      "[INFO] Epoch: 59 , batch: 481 , training loss: 4.420202\n",
      "[INFO] Epoch: 59 , batch: 482 , training loss: 4.532265\n",
      "[INFO] Epoch: 59 , batch: 483 , training loss: 4.361417\n",
      "[INFO] Epoch: 59 , batch: 484 , training loss: 4.168217\n",
      "[INFO] Epoch: 59 , batch: 485 , training loss: 4.271029\n",
      "[INFO] Epoch: 59 , batch: 486 , training loss: 4.168949\n",
      "[INFO] Epoch: 59 , batch: 487 , training loss: 4.156634\n",
      "[INFO] Epoch: 59 , batch: 488 , training loss: 4.335273\n",
      "[INFO] Epoch: 59 , batch: 489 , training loss: 4.245518\n",
      "[INFO] Epoch: 59 , batch: 490 , training loss: 4.316411\n",
      "[INFO] Epoch: 59 , batch: 491 , training loss: 4.215999\n",
      "[INFO] Epoch: 59 , batch: 492 , training loss: 4.201536\n",
      "[INFO] Epoch: 59 , batch: 493 , training loss: 4.353978\n",
      "[INFO] Epoch: 59 , batch: 494 , training loss: 4.271574\n",
      "[INFO] Epoch: 59 , batch: 495 , training loss: 4.454947\n",
      "[INFO] Epoch: 59 , batch: 496 , training loss: 4.306218\n",
      "[INFO] Epoch: 59 , batch: 497 , training loss: 4.333886\n",
      "[INFO] Epoch: 59 , batch: 498 , training loss: 4.322545\n",
      "[INFO] Epoch: 59 , batch: 499 , training loss: 4.382549\n",
      "[INFO] Epoch: 59 , batch: 500 , training loss: 4.522213\n",
      "[INFO] Epoch: 59 , batch: 501 , training loss: 4.887510\n",
      "[INFO] Epoch: 59 , batch: 502 , training loss: 4.880169\n",
      "[INFO] Epoch: 59 , batch: 503 , training loss: 4.491227\n",
      "[INFO] Epoch: 59 , batch: 504 , training loss: 4.649613\n",
      "[INFO] Epoch: 59 , batch: 505 , training loss: 4.633327\n",
      "[INFO] Epoch: 59 , batch: 506 , training loss: 4.628585\n",
      "[INFO] Epoch: 59 , batch: 507 , training loss: 4.690336\n",
      "[INFO] Epoch: 59 , batch: 508 , training loss: 4.611913\n",
      "[INFO] Epoch: 59 , batch: 509 , training loss: 4.401083\n",
      "[INFO] Epoch: 59 , batch: 510 , training loss: 4.506267\n",
      "[INFO] Epoch: 59 , batch: 511 , training loss: 4.420112\n",
      "[INFO] Epoch: 59 , batch: 512 , training loss: 4.507741\n",
      "[INFO] Epoch: 59 , batch: 513 , training loss: 4.784094\n",
      "[INFO] Epoch: 59 , batch: 514 , training loss: 4.408751\n",
      "[INFO] Epoch: 59 , batch: 515 , training loss: 4.678166\n",
      "[INFO] Epoch: 59 , batch: 516 , training loss: 4.454104\n",
      "[INFO] Epoch: 59 , batch: 517 , training loss: 4.433116\n",
      "[INFO] Epoch: 59 , batch: 518 , training loss: 4.393424\n",
      "[INFO] Epoch: 59 , batch: 519 , training loss: 4.230086\n",
      "[INFO] Epoch: 59 , batch: 520 , training loss: 4.460512\n",
      "[INFO] Epoch: 59 , batch: 521 , training loss: 4.433474\n",
      "[INFO] Epoch: 59 , batch: 522 , training loss: 4.542192\n",
      "[INFO] Epoch: 59 , batch: 523 , training loss: 4.450892\n",
      "[INFO] Epoch: 59 , batch: 524 , training loss: 4.746192\n",
      "[INFO] Epoch: 59 , batch: 525 , training loss: 4.623845\n",
      "[INFO] Epoch: 59 , batch: 526 , training loss: 4.410494\n",
      "[INFO] Epoch: 59 , batch: 527 , training loss: 4.434938\n",
      "[INFO] Epoch: 59 , batch: 528 , training loss: 4.461487\n",
      "[INFO] Epoch: 59 , batch: 529 , training loss: 4.425742\n",
      "[INFO] Epoch: 59 , batch: 530 , training loss: 4.292477\n",
      "[INFO] Epoch: 59 , batch: 531 , training loss: 4.440228\n",
      "[INFO] Epoch: 59 , batch: 532 , training loss: 4.340502\n",
      "[INFO] Epoch: 59 , batch: 533 , training loss: 4.440044\n",
      "[INFO] Epoch: 59 , batch: 534 , training loss: 4.461751\n",
      "[INFO] Epoch: 59 , batch: 535 , training loss: 4.460465\n",
      "[INFO] Epoch: 59 , batch: 536 , training loss: 4.340308\n",
      "[INFO] Epoch: 59 , batch: 537 , training loss: 4.307744\n",
      "[INFO] Epoch: 59 , batch: 538 , training loss: 4.391347\n",
      "[INFO] Epoch: 59 , batch: 539 , training loss: 4.481047\n",
      "[INFO] Epoch: 59 , batch: 540 , training loss: 5.003149\n",
      "[INFO] Epoch: 59 , batch: 541 , training loss: 4.840303\n",
      "[INFO] Epoch: 59 , batch: 542 , training loss: 4.731836\n",
      "[INFO] Epoch: 60 , batch: 0 , training loss: 3.723228\n",
      "[INFO] Epoch: 60 , batch: 1 , training loss: 3.593939\n",
      "[INFO] Epoch: 60 , batch: 2 , training loss: 3.766153\n",
      "[INFO] Epoch: 60 , batch: 3 , training loss: 3.630165\n",
      "[INFO] Epoch: 60 , batch: 4 , training loss: 4.002445\n",
      "[INFO] Epoch: 60 , batch: 5 , training loss: 3.640347\n",
      "[INFO] Epoch: 60 , batch: 6 , training loss: 4.079161\n",
      "[INFO] Epoch: 60 , batch: 7 , training loss: 3.955984\n",
      "[INFO] Epoch: 60 , batch: 8 , training loss: 3.611916\n",
      "[INFO] Epoch: 60 , batch: 9 , training loss: 3.869742\n",
      "[INFO] Epoch: 60 , batch: 10 , training loss: 3.852457\n",
      "[INFO] Epoch: 60 , batch: 11 , training loss: 3.784550\n",
      "[INFO] Epoch: 60 , batch: 12 , training loss: 3.689626\n",
      "[INFO] Epoch: 60 , batch: 13 , training loss: 3.666841\n",
      "[INFO] Epoch: 60 , batch: 14 , training loss: 3.603635\n",
      "[INFO] Epoch: 60 , batch: 15 , training loss: 3.828934\n",
      "[INFO] Epoch: 60 , batch: 16 , training loss: 3.637378\n",
      "[INFO] Epoch: 60 , batch: 17 , training loss: 3.747909\n",
      "[INFO] Epoch: 60 , batch: 18 , training loss: 3.718857\n",
      "[INFO] Epoch: 60 , batch: 19 , training loss: 3.488804\n",
      "[INFO] Epoch: 60 , batch: 20 , training loss: 3.453285\n",
      "[INFO] Epoch: 60 , batch: 21 , training loss: 3.610414\n",
      "[INFO] Epoch: 60 , batch: 22 , training loss: 3.499367\n",
      "[INFO] Epoch: 60 , batch: 23 , training loss: 3.680874\n",
      "[INFO] Epoch: 60 , batch: 24 , training loss: 3.564535\n",
      "[INFO] Epoch: 60 , batch: 25 , training loss: 3.648564\n",
      "[INFO] Epoch: 60 , batch: 26 , training loss: 3.527591\n",
      "[INFO] Epoch: 60 , batch: 27 , training loss: 3.511061\n",
      "[INFO] Epoch: 60 , batch: 28 , training loss: 3.695582\n",
      "[INFO] Epoch: 60 , batch: 29 , training loss: 3.503592\n",
      "[INFO] Epoch: 60 , batch: 30 , training loss: 3.540030\n",
      "[INFO] Epoch: 60 , batch: 31 , training loss: 3.619086\n",
      "[INFO] Epoch: 60 , batch: 32 , training loss: 3.575176\n",
      "[INFO] Epoch: 60 , batch: 33 , training loss: 3.633693\n",
      "[INFO] Epoch: 60 , batch: 34 , training loss: 3.614567\n",
      "[INFO] Epoch: 60 , batch: 35 , training loss: 3.604233\n",
      "[INFO] Epoch: 60 , batch: 36 , training loss: 3.654830\n",
      "[INFO] Epoch: 60 , batch: 37 , training loss: 3.542073\n",
      "[INFO] Epoch: 60 , batch: 38 , training loss: 3.598200\n",
      "[INFO] Epoch: 60 , batch: 39 , training loss: 3.417425\n",
      "[INFO] Epoch: 60 , batch: 40 , training loss: 3.621083\n",
      "[INFO] Epoch: 60 , batch: 41 , training loss: 3.609307\n",
      "[INFO] Epoch: 60 , batch: 42 , training loss: 4.072018\n",
      "[INFO] Epoch: 60 , batch: 43 , training loss: 3.825509\n",
      "[INFO] Epoch: 60 , batch: 44 , training loss: 4.187338\n",
      "[INFO] Epoch: 60 , batch: 45 , training loss: 4.096569\n",
      "[INFO] Epoch: 60 , batch: 46 , training loss: 4.106688\n",
      "[INFO] Epoch: 60 , batch: 47 , training loss: 3.635311\n",
      "[INFO] Epoch: 60 , batch: 48 , training loss: 3.678694\n",
      "[INFO] Epoch: 60 , batch: 49 , training loss: 3.851958\n",
      "[INFO] Epoch: 60 , batch: 50 , training loss: 3.637750\n",
      "[INFO] Epoch: 60 , batch: 51 , training loss: 3.850288\n",
      "[INFO] Epoch: 60 , batch: 52 , training loss: 3.592843\n",
      "[INFO] Epoch: 60 , batch: 53 , training loss: 3.777874\n",
      "[INFO] Epoch: 60 , batch: 54 , training loss: 3.753072\n",
      "[INFO] Epoch: 60 , batch: 55 , training loss: 3.813310\n",
      "[INFO] Epoch: 60 , batch: 56 , training loss: 3.685633\n",
      "[INFO] Epoch: 60 , batch: 57 , training loss: 3.596182\n",
      "[INFO] Epoch: 60 , batch: 58 , training loss: 3.650007\n",
      "[INFO] Epoch: 60 , batch: 59 , training loss: 3.734490\n",
      "[INFO] Epoch: 60 , batch: 60 , training loss: 3.655225\n",
      "[INFO] Epoch: 60 , batch: 61 , training loss: 3.779619\n",
      "[INFO] Epoch: 60 , batch: 62 , training loss: 3.619280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 60 , batch: 63 , training loss: 3.838094\n",
      "[INFO] Epoch: 60 , batch: 64 , training loss: 4.016670\n",
      "[INFO] Epoch: 60 , batch: 65 , training loss: 3.744637\n",
      "[INFO] Epoch: 60 , batch: 66 , training loss: 3.607214\n",
      "[INFO] Epoch: 60 , batch: 67 , training loss: 3.632357\n",
      "[INFO] Epoch: 60 , batch: 68 , training loss: 3.769167\n",
      "[INFO] Epoch: 60 , batch: 69 , training loss: 3.724738\n",
      "[INFO] Epoch: 60 , batch: 70 , training loss: 3.922207\n",
      "[INFO] Epoch: 60 , batch: 71 , training loss: 3.762239\n",
      "[INFO] Epoch: 60 , batch: 72 , training loss: 3.793145\n",
      "[INFO] Epoch: 60 , batch: 73 , training loss: 3.784642\n",
      "[INFO] Epoch: 60 , batch: 74 , training loss: 3.872361\n",
      "[INFO] Epoch: 60 , batch: 75 , training loss: 3.783613\n",
      "[INFO] Epoch: 60 , batch: 76 , training loss: 3.844043\n",
      "[INFO] Epoch: 60 , batch: 77 , training loss: 3.787253\n",
      "[INFO] Epoch: 60 , batch: 78 , training loss: 3.896545\n",
      "[INFO] Epoch: 60 , batch: 79 , training loss: 3.762736\n",
      "[INFO] Epoch: 60 , batch: 80 , training loss: 3.952414\n",
      "[INFO] Epoch: 60 , batch: 81 , training loss: 3.856480\n",
      "[INFO] Epoch: 60 , batch: 82 , training loss: 3.859334\n",
      "[INFO] Epoch: 60 , batch: 83 , training loss: 3.901469\n",
      "[INFO] Epoch: 60 , batch: 84 , training loss: 3.903783\n",
      "[INFO] Epoch: 60 , batch: 85 , training loss: 3.975392\n",
      "[INFO] Epoch: 60 , batch: 86 , training loss: 3.908991\n",
      "[INFO] Epoch: 60 , batch: 87 , training loss: 3.846178\n",
      "[INFO] Epoch: 60 , batch: 88 , training loss: 4.023325\n",
      "[INFO] Epoch: 60 , batch: 89 , training loss: 3.777082\n",
      "[INFO] Epoch: 60 , batch: 90 , training loss: 3.883313\n",
      "[INFO] Epoch: 60 , batch: 91 , training loss: 3.835357\n",
      "[INFO] Epoch: 60 , batch: 92 , training loss: 3.842051\n",
      "[INFO] Epoch: 60 , batch: 93 , training loss: 3.950471\n",
      "[INFO] Epoch: 60 , batch: 94 , training loss: 4.062938\n",
      "[INFO] Epoch: 60 , batch: 95 , training loss: 3.822778\n",
      "[INFO] Epoch: 60 , batch: 96 , training loss: 3.863815\n",
      "[INFO] Epoch: 60 , batch: 97 , training loss: 3.762812\n",
      "[INFO] Epoch: 60 , batch: 98 , training loss: 3.713027\n",
      "[INFO] Epoch: 60 , batch: 99 , training loss: 3.859706\n",
      "[INFO] Epoch: 60 , batch: 100 , training loss: 3.763536\n",
      "[INFO] Epoch: 60 , batch: 101 , training loss: 3.756538\n",
      "[INFO] Epoch: 60 , batch: 102 , training loss: 3.910448\n",
      "[INFO] Epoch: 60 , batch: 103 , training loss: 3.714658\n",
      "[INFO] Epoch: 60 , batch: 104 , training loss: 3.660154\n",
      "[INFO] Epoch: 60 , batch: 105 , training loss: 3.901230\n",
      "[INFO] Epoch: 60 , batch: 106 , training loss: 3.962247\n",
      "[INFO] Epoch: 60 , batch: 107 , training loss: 3.787398\n",
      "[INFO] Epoch: 60 , batch: 108 , training loss: 3.734096\n",
      "[INFO] Epoch: 60 , batch: 109 , training loss: 3.647837\n",
      "[INFO] Epoch: 60 , batch: 110 , training loss: 3.822960\n",
      "[INFO] Epoch: 60 , batch: 111 , training loss: 3.918779\n",
      "[INFO] Epoch: 60 , batch: 112 , training loss: 3.839171\n",
      "[INFO] Epoch: 60 , batch: 113 , training loss: 3.783625\n",
      "[INFO] Epoch: 60 , batch: 114 , training loss: 3.803498\n",
      "[INFO] Epoch: 60 , batch: 115 , training loss: 3.821783\n",
      "[INFO] Epoch: 60 , batch: 116 , training loss: 3.714512\n",
      "[INFO] Epoch: 60 , batch: 117 , training loss: 3.928038\n",
      "[INFO] Epoch: 60 , batch: 118 , training loss: 3.919554\n",
      "[INFO] Epoch: 60 , batch: 119 , training loss: 4.048533\n",
      "[INFO] Epoch: 60 , batch: 120 , training loss: 4.006618\n",
      "[INFO] Epoch: 60 , batch: 121 , training loss: 3.918656\n",
      "[INFO] Epoch: 60 , batch: 122 , training loss: 3.799206\n",
      "[INFO] Epoch: 60 , batch: 123 , training loss: 3.782936\n",
      "[INFO] Epoch: 60 , batch: 124 , training loss: 3.886305\n",
      "[INFO] Epoch: 60 , batch: 125 , training loss: 3.744542\n",
      "[INFO] Epoch: 60 , batch: 126 , training loss: 3.742921\n",
      "[INFO] Epoch: 60 , batch: 127 , training loss: 3.724948\n",
      "[INFO] Epoch: 60 , batch: 128 , training loss: 3.854033\n",
      "[INFO] Epoch: 60 , batch: 129 , training loss: 3.806524\n",
      "[INFO] Epoch: 60 , batch: 130 , training loss: 3.824623\n",
      "[INFO] Epoch: 60 , batch: 131 , training loss: 3.859146\n",
      "[INFO] Epoch: 60 , batch: 132 , training loss: 3.838402\n",
      "[INFO] Epoch: 60 , batch: 133 , training loss: 3.821184\n",
      "[INFO] Epoch: 60 , batch: 134 , training loss: 3.596128\n",
      "[INFO] Epoch: 60 , batch: 135 , training loss: 3.640533\n",
      "[INFO] Epoch: 60 , batch: 136 , training loss: 3.922489\n",
      "[INFO] Epoch: 60 , batch: 137 , training loss: 3.879700\n",
      "[INFO] Epoch: 60 , batch: 138 , training loss: 3.914153\n",
      "[INFO] Epoch: 60 , batch: 139 , training loss: 4.459462\n",
      "[INFO] Epoch: 60 , batch: 140 , training loss: 4.264081\n",
      "[INFO] Epoch: 60 , batch: 141 , training loss: 4.048578\n",
      "[INFO] Epoch: 60 , batch: 142 , training loss: 3.769729\n",
      "[INFO] Epoch: 60 , batch: 143 , training loss: 3.905974\n",
      "[INFO] Epoch: 60 , batch: 144 , training loss: 3.745681\n",
      "[INFO] Epoch: 60 , batch: 145 , training loss: 3.792643\n",
      "[INFO] Epoch: 60 , batch: 146 , training loss: 3.999071\n",
      "[INFO] Epoch: 60 , batch: 147 , training loss: 3.681282\n",
      "[INFO] Epoch: 60 , batch: 148 , training loss: 3.649806\n",
      "[INFO] Epoch: 60 , batch: 149 , training loss: 3.718489\n",
      "[INFO] Epoch: 60 , batch: 150 , training loss: 4.012365\n",
      "[INFO] Epoch: 60 , batch: 151 , training loss: 3.858722\n",
      "[INFO] Epoch: 60 , batch: 152 , training loss: 3.849802\n",
      "[INFO] Epoch: 60 , batch: 153 , training loss: 3.917770\n",
      "[INFO] Epoch: 60 , batch: 154 , training loss: 3.971215\n",
      "[INFO] Epoch: 60 , batch: 155 , training loss: 4.170018\n",
      "[INFO] Epoch: 60 , batch: 156 , training loss: 3.926050\n",
      "[INFO] Epoch: 60 , batch: 157 , training loss: 3.890645\n",
      "[INFO] Epoch: 60 , batch: 158 , training loss: 4.034850\n",
      "[INFO] Epoch: 60 , batch: 159 , training loss: 3.950845\n",
      "[INFO] Epoch: 60 , batch: 160 , training loss: 4.140917\n",
      "[INFO] Epoch: 60 , batch: 161 , training loss: 4.192818\n",
      "[INFO] Epoch: 60 , batch: 162 , training loss: 4.176240\n",
      "[INFO] Epoch: 60 , batch: 163 , training loss: 4.333766\n",
      "[INFO] Epoch: 60 , batch: 164 , training loss: 4.292770\n",
      "[INFO] Epoch: 60 , batch: 165 , training loss: 4.208525\n",
      "[INFO] Epoch: 60 , batch: 166 , training loss: 4.138134\n",
      "[INFO] Epoch: 60 , batch: 167 , training loss: 4.176774\n",
      "[INFO] Epoch: 60 , batch: 168 , training loss: 3.884703\n",
      "[INFO] Epoch: 60 , batch: 169 , training loss: 3.843261\n",
      "[INFO] Epoch: 60 , batch: 170 , training loss: 4.032378\n",
      "[INFO] Epoch: 60 , batch: 171 , training loss: 3.508470\n",
      "[INFO] Epoch: 60 , batch: 172 , training loss: 3.756766\n",
      "[INFO] Epoch: 60 , batch: 173 , training loss: 4.062104\n",
      "[INFO] Epoch: 60 , batch: 174 , training loss: 4.493238\n",
      "[INFO] Epoch: 60 , batch: 175 , training loss: 4.723480\n",
      "[INFO] Epoch: 60 , batch: 176 , training loss: 4.392689\n",
      "[INFO] Epoch: 60 , batch: 177 , training loss: 3.974756\n",
      "[INFO] Epoch: 60 , batch: 178 , training loss: 4.009203\n",
      "[INFO] Epoch: 60 , batch: 179 , training loss: 4.074483\n",
      "[INFO] Epoch: 60 , batch: 180 , training loss: 4.016132\n",
      "[INFO] Epoch: 60 , batch: 181 , training loss: 4.312281\n",
      "[INFO] Epoch: 60 , batch: 182 , training loss: 4.258516\n",
      "[INFO] Epoch: 60 , batch: 183 , training loss: 4.230970\n",
      "[INFO] Epoch: 60 , batch: 184 , training loss: 4.109347\n",
      "[INFO] Epoch: 60 , batch: 185 , training loss: 4.075464\n",
      "[INFO] Epoch: 60 , batch: 186 , training loss: 4.233466\n",
      "[INFO] Epoch: 60 , batch: 187 , training loss: 4.318717\n",
      "[INFO] Epoch: 60 , batch: 188 , training loss: 4.297908\n",
      "[INFO] Epoch: 60 , batch: 189 , training loss: 4.217544\n",
      "[INFO] Epoch: 60 , batch: 190 , training loss: 4.257435\n",
      "[INFO] Epoch: 60 , batch: 191 , training loss: 4.349252\n",
      "[INFO] Epoch: 60 , batch: 192 , training loss: 4.228708\n",
      "[INFO] Epoch: 60 , batch: 193 , training loss: 4.317883\n",
      "[INFO] Epoch: 60 , batch: 194 , training loss: 4.242988\n",
      "[INFO] Epoch: 60 , batch: 195 , training loss: 4.182487\n",
      "[INFO] Epoch: 60 , batch: 196 , training loss: 4.034241\n",
      "[INFO] Epoch: 60 , batch: 197 , training loss: 4.126498\n",
      "[INFO] Epoch: 60 , batch: 198 , training loss: 4.040892\n",
      "[INFO] Epoch: 60 , batch: 199 , training loss: 4.169331\n",
      "[INFO] Epoch: 60 , batch: 200 , training loss: 4.070986\n",
      "[INFO] Epoch: 60 , batch: 201 , training loss: 3.983480\n",
      "[INFO] Epoch: 60 , batch: 202 , training loss: 3.978750\n",
      "[INFO] Epoch: 60 , batch: 203 , training loss: 4.131671\n",
      "[INFO] Epoch: 60 , batch: 204 , training loss: 4.187346\n",
      "[INFO] Epoch: 60 , batch: 205 , training loss: 3.817461\n",
      "[INFO] Epoch: 60 , batch: 206 , training loss: 3.741061\n",
      "[INFO] Epoch: 60 , batch: 207 , training loss: 3.711780\n",
      "[INFO] Epoch: 60 , batch: 208 , training loss: 4.057799\n",
      "[INFO] Epoch: 60 , batch: 209 , training loss: 4.052501\n",
      "[INFO] Epoch: 60 , batch: 210 , training loss: 4.033958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 60 , batch: 211 , training loss: 4.046180\n",
      "[INFO] Epoch: 60 , batch: 212 , training loss: 4.133890\n",
      "[INFO] Epoch: 60 , batch: 213 , training loss: 4.074025\n",
      "[INFO] Epoch: 60 , batch: 214 , training loss: 4.149230\n",
      "[INFO] Epoch: 60 , batch: 215 , training loss: 4.347132\n",
      "[INFO] Epoch: 60 , batch: 216 , training loss: 4.059173\n",
      "[INFO] Epoch: 60 , batch: 217 , training loss: 4.006725\n",
      "[INFO] Epoch: 60 , batch: 218 , training loss: 4.020829\n",
      "[INFO] Epoch: 60 , batch: 219 , training loss: 4.104733\n",
      "[INFO] Epoch: 60 , batch: 220 , training loss: 3.936007\n",
      "[INFO] Epoch: 60 , batch: 221 , training loss: 3.921229\n",
      "[INFO] Epoch: 60 , batch: 222 , training loss: 4.086128\n",
      "[INFO] Epoch: 60 , batch: 223 , training loss: 4.217035\n",
      "[INFO] Epoch: 60 , batch: 224 , training loss: 4.251889\n",
      "[INFO] Epoch: 60 , batch: 225 , training loss: 4.123874\n",
      "[INFO] Epoch: 60 , batch: 226 , training loss: 4.258890\n",
      "[INFO] Epoch: 60 , batch: 227 , training loss: 4.242409\n",
      "[INFO] Epoch: 60 , batch: 228 , training loss: 4.248144\n",
      "[INFO] Epoch: 60 , batch: 229 , training loss: 4.100112\n",
      "[INFO] Epoch: 60 , batch: 230 , training loss: 3.966249\n",
      "[INFO] Epoch: 60 , batch: 231 , training loss: 3.846019\n",
      "[INFO] Epoch: 60 , batch: 232 , training loss: 3.977164\n",
      "[INFO] Epoch: 60 , batch: 233 , training loss: 4.022156\n",
      "[INFO] Epoch: 60 , batch: 234 , training loss: 3.686316\n",
      "[INFO] Epoch: 60 , batch: 235 , training loss: 3.804451\n",
      "[INFO] Epoch: 60 , batch: 236 , training loss: 3.895341\n",
      "[INFO] Epoch: 60 , batch: 237 , training loss: 4.122856\n",
      "[INFO] Epoch: 60 , batch: 238 , training loss: 3.913620\n",
      "[INFO] Epoch: 60 , batch: 239 , training loss: 3.918311\n",
      "[INFO] Epoch: 60 , batch: 240 , training loss: 3.978187\n",
      "[INFO] Epoch: 60 , batch: 241 , training loss: 3.790281\n",
      "[INFO] Epoch: 60 , batch: 242 , training loss: 3.802094\n",
      "[INFO] Epoch: 60 , batch: 243 , training loss: 4.114962\n",
      "[INFO] Epoch: 60 , batch: 244 , training loss: 4.044394\n",
      "[INFO] Epoch: 60 , batch: 245 , training loss: 3.983536\n",
      "[INFO] Epoch: 60 , batch: 246 , training loss: 3.742790\n",
      "[INFO] Epoch: 60 , batch: 247 , training loss: 3.898787\n",
      "[INFO] Epoch: 60 , batch: 248 , training loss: 3.971914\n",
      "[INFO] Epoch: 60 , batch: 249 , training loss: 3.952325\n",
      "[INFO] Epoch: 60 , batch: 250 , training loss: 3.760447\n",
      "[INFO] Epoch: 60 , batch: 251 , training loss: 4.198122\n",
      "[INFO] Epoch: 60 , batch: 252 , training loss: 3.912483\n",
      "[INFO] Epoch: 60 , batch: 253 , training loss: 3.811497\n",
      "[INFO] Epoch: 60 , batch: 254 , training loss: 4.078652\n",
      "[INFO] Epoch: 60 , batch: 255 , training loss: 4.065749\n",
      "[INFO] Epoch: 60 , batch: 256 , training loss: 4.051871\n",
      "[INFO] Epoch: 60 , batch: 257 , training loss: 4.227686\n",
      "[INFO] Epoch: 60 , batch: 258 , training loss: 4.218827\n",
      "[INFO] Epoch: 60 , batch: 259 , training loss: 4.249465\n",
      "[INFO] Epoch: 60 , batch: 260 , training loss: 4.030145\n",
      "[INFO] Epoch: 60 , batch: 261 , training loss: 4.207935\n",
      "[INFO] Epoch: 60 , batch: 262 , training loss: 4.344053\n",
      "[INFO] Epoch: 60 , batch: 263 , training loss: 4.500684\n",
      "[INFO] Epoch: 60 , batch: 264 , training loss: 3.876621\n",
      "[INFO] Epoch: 60 , batch: 265 , training loss: 3.970392\n",
      "[INFO] Epoch: 60 , batch: 266 , training loss: 4.368537\n",
      "[INFO] Epoch: 60 , batch: 267 , training loss: 4.150127\n",
      "[INFO] Epoch: 60 , batch: 268 , training loss: 4.058500\n",
      "[INFO] Epoch: 60 , batch: 269 , training loss: 4.044261\n",
      "[INFO] Epoch: 60 , batch: 270 , training loss: 4.058300\n",
      "[INFO] Epoch: 60 , batch: 271 , training loss: 4.079063\n",
      "[INFO] Epoch: 60 , batch: 272 , training loss: 4.098799\n",
      "[INFO] Epoch: 60 , batch: 273 , training loss: 4.101251\n",
      "[INFO] Epoch: 60 , batch: 274 , training loss: 4.205366\n",
      "[INFO] Epoch: 60 , batch: 275 , training loss: 4.045932\n",
      "[INFO] Epoch: 60 , batch: 276 , training loss: 4.100653\n",
      "[INFO] Epoch: 60 , batch: 277 , training loss: 4.283165\n",
      "[INFO] Epoch: 60 , batch: 278 , training loss: 3.954362\n",
      "[INFO] Epoch: 60 , batch: 279 , training loss: 3.952209\n",
      "[INFO] Epoch: 60 , batch: 280 , training loss: 3.955350\n",
      "[INFO] Epoch: 60 , batch: 281 , training loss: 4.076267\n",
      "[INFO] Epoch: 60 , batch: 282 , training loss: 4.004194\n",
      "[INFO] Epoch: 60 , batch: 283 , training loss: 3.995126\n",
      "[INFO] Epoch: 60 , batch: 284 , training loss: 4.008753\n",
      "[INFO] Epoch: 60 , batch: 285 , training loss: 3.942594\n",
      "[INFO] Epoch: 60 , batch: 286 , training loss: 3.982444\n",
      "[INFO] Epoch: 60 , batch: 287 , training loss: 3.906755\n",
      "[INFO] Epoch: 60 , batch: 288 , training loss: 3.864196\n",
      "[INFO] Epoch: 60 , batch: 289 , training loss: 3.953659\n",
      "[INFO] Epoch: 60 , batch: 290 , training loss: 3.730215\n",
      "[INFO] Epoch: 60 , batch: 291 , training loss: 3.736915\n",
      "[INFO] Epoch: 60 , batch: 292 , training loss: 3.824895\n",
      "[INFO] Epoch: 60 , batch: 293 , training loss: 3.754517\n",
      "[INFO] Epoch: 60 , batch: 294 , training loss: 4.406878\n",
      "[INFO] Epoch: 60 , batch: 295 , training loss: 4.177191\n",
      "[INFO] Epoch: 60 , batch: 296 , training loss: 4.111748\n",
      "[INFO] Epoch: 60 , batch: 297 , training loss: 4.070376\n",
      "[INFO] Epoch: 60 , batch: 298 , training loss: 3.910805\n",
      "[INFO] Epoch: 60 , batch: 299 , training loss: 3.959199\n",
      "[INFO] Epoch: 60 , batch: 300 , training loss: 3.947452\n",
      "[INFO] Epoch: 60 , batch: 301 , training loss: 3.891369\n",
      "[INFO] Epoch: 60 , batch: 302 , training loss: 4.034337\n",
      "[INFO] Epoch: 60 , batch: 303 , training loss: 4.044347\n",
      "[INFO] Epoch: 60 , batch: 304 , training loss: 4.146492\n",
      "[INFO] Epoch: 60 , batch: 305 , training loss: 4.010775\n",
      "[INFO] Epoch: 60 , batch: 306 , training loss: 4.127845\n",
      "[INFO] Epoch: 60 , batch: 307 , training loss: 4.150779\n",
      "[INFO] Epoch: 60 , batch: 308 , training loss: 3.968949\n",
      "[INFO] Epoch: 60 , batch: 309 , training loss: 3.942163\n",
      "[INFO] Epoch: 60 , batch: 310 , training loss: 3.894736\n",
      "[INFO] Epoch: 60 , batch: 311 , training loss: 3.876137\n",
      "[INFO] Epoch: 60 , batch: 312 , training loss: 3.766443\n",
      "[INFO] Epoch: 60 , batch: 313 , training loss: 3.860324\n",
      "[INFO] Epoch: 60 , batch: 314 , training loss: 3.954883\n",
      "[INFO] Epoch: 60 , batch: 315 , training loss: 4.029851\n",
      "[INFO] Epoch: 60 , batch: 316 , training loss: 4.258279\n",
      "[INFO] Epoch: 60 , batch: 317 , training loss: 4.596595\n",
      "[INFO] Epoch: 60 , batch: 318 , training loss: 4.715873\n",
      "[INFO] Epoch: 60 , batch: 319 , training loss: 4.419133\n",
      "[INFO] Epoch: 60 , batch: 320 , training loss: 3.973322\n",
      "[INFO] Epoch: 60 , batch: 321 , training loss: 3.823539\n",
      "[INFO] Epoch: 60 , batch: 322 , training loss: 3.914139\n",
      "[INFO] Epoch: 60 , batch: 323 , training loss: 3.947634\n",
      "[INFO] Epoch: 60 , batch: 324 , training loss: 3.928027\n",
      "[INFO] Epoch: 60 , batch: 325 , training loss: 4.034656\n",
      "[INFO] Epoch: 60 , batch: 326 , training loss: 4.097241\n",
      "[INFO] Epoch: 60 , batch: 327 , training loss: 4.040857\n",
      "[INFO] Epoch: 60 , batch: 328 , training loss: 4.044391\n",
      "[INFO] Epoch: 60 , batch: 329 , training loss: 3.947326\n",
      "[INFO] Epoch: 60 , batch: 330 , training loss: 3.942927\n",
      "[INFO] Epoch: 60 , batch: 331 , training loss: 4.073841\n",
      "[INFO] Epoch: 60 , batch: 332 , training loss: 3.936328\n",
      "[INFO] Epoch: 60 , batch: 333 , training loss: 3.904228\n",
      "[INFO] Epoch: 60 , batch: 334 , training loss: 3.933315\n",
      "[INFO] Epoch: 60 , batch: 335 , training loss: 4.036652\n",
      "[INFO] Epoch: 60 , batch: 336 , training loss: 4.066362\n",
      "[INFO] Epoch: 60 , batch: 337 , training loss: 4.109488\n",
      "[INFO] Epoch: 60 , batch: 338 , training loss: 4.300354\n",
      "[INFO] Epoch: 60 , batch: 339 , training loss: 4.133951\n",
      "[INFO] Epoch: 60 , batch: 340 , training loss: 4.328475\n",
      "[INFO] Epoch: 60 , batch: 341 , training loss: 4.080335\n",
      "[INFO] Epoch: 60 , batch: 342 , training loss: 3.871619\n",
      "[INFO] Epoch: 60 , batch: 343 , training loss: 3.933451\n",
      "[INFO] Epoch: 60 , batch: 344 , training loss: 3.793347\n",
      "[INFO] Epoch: 60 , batch: 345 , training loss: 3.906301\n",
      "[INFO] Epoch: 60 , batch: 346 , training loss: 3.982802\n",
      "[INFO] Epoch: 60 , batch: 347 , training loss: 3.877398\n",
      "[INFO] Epoch: 60 , batch: 348 , training loss: 3.971893\n",
      "[INFO] Epoch: 60 , batch: 349 , training loss: 4.066283\n",
      "[INFO] Epoch: 60 , batch: 350 , training loss: 3.949159\n",
      "[INFO] Epoch: 60 , batch: 351 , training loss: 4.024343\n",
      "[INFO] Epoch: 60 , batch: 352 , training loss: 4.029873\n",
      "[INFO] Epoch: 60 , batch: 353 , training loss: 4.029631\n",
      "[INFO] Epoch: 60 , batch: 354 , training loss: 4.098009\n",
      "[INFO] Epoch: 60 , batch: 355 , training loss: 4.097903\n",
      "[INFO] Epoch: 60 , batch: 356 , training loss: 3.960615\n",
      "[INFO] Epoch: 60 , batch: 357 , training loss: 4.017897\n",
      "[INFO] Epoch: 60 , batch: 358 , training loss: 3.958086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 60 , batch: 359 , training loss: 3.953338\n",
      "[INFO] Epoch: 60 , batch: 360 , training loss: 4.045179\n",
      "[INFO] Epoch: 60 , batch: 361 , training loss: 4.025414\n",
      "[INFO] Epoch: 60 , batch: 362 , training loss: 4.135010\n",
      "[INFO] Epoch: 60 , batch: 363 , training loss: 4.000564\n",
      "[INFO] Epoch: 60 , batch: 364 , training loss: 4.060615\n",
      "[INFO] Epoch: 60 , batch: 365 , training loss: 3.995341\n",
      "[INFO] Epoch: 60 , batch: 366 , training loss: 4.081660\n",
      "[INFO] Epoch: 60 , batch: 367 , training loss: 4.130285\n",
      "[INFO] Epoch: 60 , batch: 368 , training loss: 4.529833\n",
      "[INFO] Epoch: 60 , batch: 369 , training loss: 4.194848\n",
      "[INFO] Epoch: 60 , batch: 370 , training loss: 3.966751\n",
      "[INFO] Epoch: 60 , batch: 371 , training loss: 4.400625\n",
      "[INFO] Epoch: 60 , batch: 372 , training loss: 4.636089\n",
      "[INFO] Epoch: 60 , batch: 373 , training loss: 4.651833\n",
      "[INFO] Epoch: 60 , batch: 374 , training loss: 4.789046\n",
      "[INFO] Epoch: 60 , batch: 375 , training loss: 4.802886\n",
      "[INFO] Epoch: 60 , batch: 376 , training loss: 4.663881\n",
      "[INFO] Epoch: 60 , batch: 377 , training loss: 4.431555\n",
      "[INFO] Epoch: 60 , batch: 378 , training loss: 4.524160\n",
      "[INFO] Epoch: 60 , batch: 379 , training loss: 4.501831\n",
      "[INFO] Epoch: 60 , batch: 380 , training loss: 4.689326\n",
      "[INFO] Epoch: 60 , batch: 381 , training loss: 4.359765\n",
      "[INFO] Epoch: 60 , batch: 382 , training loss: 4.618027\n",
      "[INFO] Epoch: 60 , batch: 383 , training loss: 4.653306\n",
      "[INFO] Epoch: 60 , batch: 384 , training loss: 4.619371\n",
      "[INFO] Epoch: 60 , batch: 385 , training loss: 4.310233\n",
      "[INFO] Epoch: 60 , batch: 386 , training loss: 4.549478\n",
      "[INFO] Epoch: 60 , batch: 387 , training loss: 4.512029\n",
      "[INFO] Epoch: 60 , batch: 388 , training loss: 4.338433\n",
      "[INFO] Epoch: 60 , batch: 389 , training loss: 4.158332\n",
      "[INFO] Epoch: 60 , batch: 390 , training loss: 4.177053\n",
      "[INFO] Epoch: 60 , batch: 391 , training loss: 4.204803\n",
      "[INFO] Epoch: 60 , batch: 392 , training loss: 4.552015\n",
      "[INFO] Epoch: 60 , batch: 393 , training loss: 4.446489\n",
      "[INFO] Epoch: 60 , batch: 394 , training loss: 4.559797\n",
      "[INFO] Epoch: 60 , batch: 395 , training loss: 4.363114\n",
      "[INFO] Epoch: 60 , batch: 396 , training loss: 4.187582\n",
      "[INFO] Epoch: 60 , batch: 397 , training loss: 4.332044\n",
      "[INFO] Epoch: 60 , batch: 398 , training loss: 4.178517\n",
      "[INFO] Epoch: 60 , batch: 399 , training loss: 4.272008\n",
      "[INFO] Epoch: 60 , batch: 400 , training loss: 4.240531\n",
      "[INFO] Epoch: 60 , batch: 401 , training loss: 4.666363\n",
      "[INFO] Epoch: 60 , batch: 402 , training loss: 4.388405\n",
      "[INFO] Epoch: 60 , batch: 403 , training loss: 4.245372\n",
      "[INFO] Epoch: 60 , batch: 404 , training loss: 4.418979\n",
      "[INFO] Epoch: 60 , batch: 405 , training loss: 4.447715\n",
      "[INFO] Epoch: 60 , batch: 406 , training loss: 4.361653\n",
      "[INFO] Epoch: 60 , batch: 407 , training loss: 4.378105\n",
      "[INFO] Epoch: 60 , batch: 408 , training loss: 4.358378\n",
      "[INFO] Epoch: 60 , batch: 409 , training loss: 4.372981\n",
      "[INFO] Epoch: 60 , batch: 410 , training loss: 4.426437\n",
      "[INFO] Epoch: 60 , batch: 411 , training loss: 4.590238\n",
      "[INFO] Epoch: 60 , batch: 412 , training loss: 4.436140\n",
      "[INFO] Epoch: 60 , batch: 413 , training loss: 4.294131\n",
      "[INFO] Epoch: 60 , batch: 414 , training loss: 4.351799\n",
      "[INFO] Epoch: 60 , batch: 415 , training loss: 4.388944\n",
      "[INFO] Epoch: 60 , batch: 416 , training loss: 4.450668\n",
      "[INFO] Epoch: 60 , batch: 417 , training loss: 4.378038\n",
      "[INFO] Epoch: 60 , batch: 418 , training loss: 4.431361\n",
      "[INFO] Epoch: 60 , batch: 419 , training loss: 4.376966\n",
      "[INFO] Epoch: 60 , batch: 420 , training loss: 4.358569\n",
      "[INFO] Epoch: 60 , batch: 421 , training loss: 4.351162\n",
      "[INFO] Epoch: 60 , batch: 422 , training loss: 4.204547\n",
      "[INFO] Epoch: 60 , batch: 423 , training loss: 4.410727\n",
      "[INFO] Epoch: 60 , batch: 424 , training loss: 4.577416\n",
      "[INFO] Epoch: 60 , batch: 425 , training loss: 4.447669\n",
      "[INFO] Epoch: 60 , batch: 426 , training loss: 4.208311\n",
      "[INFO] Epoch: 60 , batch: 427 , training loss: 4.432550\n",
      "[INFO] Epoch: 60 , batch: 428 , training loss: 4.288368\n",
      "[INFO] Epoch: 60 , batch: 429 , training loss: 4.200394\n",
      "[INFO] Epoch: 60 , batch: 430 , training loss: 4.432761\n",
      "[INFO] Epoch: 60 , batch: 431 , training loss: 4.050773\n",
      "[INFO] Epoch: 60 , batch: 432 , training loss: 4.091177\n",
      "[INFO] Epoch: 60 , batch: 433 , training loss: 4.138867\n",
      "[INFO] Epoch: 60 , batch: 434 , training loss: 4.030757\n",
      "[INFO] Epoch: 60 , batch: 435 , training loss: 4.374065\n",
      "[INFO] Epoch: 60 , batch: 436 , training loss: 4.396120\n",
      "[INFO] Epoch: 60 , batch: 437 , training loss: 4.193461\n",
      "[INFO] Epoch: 60 , batch: 438 , training loss: 4.060227\n",
      "[INFO] Epoch: 60 , batch: 439 , training loss: 4.294829\n",
      "[INFO] Epoch: 60 , batch: 440 , training loss: 4.406505\n",
      "[INFO] Epoch: 60 , batch: 441 , training loss: 4.506584\n",
      "[INFO] Epoch: 60 , batch: 442 , training loss: 4.266617\n",
      "[INFO] Epoch: 60 , batch: 443 , training loss: 4.440679\n",
      "[INFO] Epoch: 60 , batch: 444 , training loss: 4.068908\n",
      "[INFO] Epoch: 60 , batch: 445 , training loss: 3.949249\n",
      "[INFO] Epoch: 60 , batch: 446 , training loss: 3.895832\n",
      "[INFO] Epoch: 60 , batch: 447 , training loss: 4.094939\n",
      "[INFO] Epoch: 60 , batch: 448 , training loss: 4.211211\n",
      "[INFO] Epoch: 60 , batch: 449 , training loss: 4.596539\n",
      "[INFO] Epoch: 60 , batch: 450 , training loss: 4.653473\n",
      "[INFO] Epoch: 60 , batch: 451 , training loss: 4.559158\n",
      "[INFO] Epoch: 60 , batch: 452 , training loss: 4.374924\n",
      "[INFO] Epoch: 60 , batch: 453 , training loss: 4.138607\n",
      "[INFO] Epoch: 60 , batch: 454 , training loss: 4.289009\n",
      "[INFO] Epoch: 60 , batch: 455 , training loss: 4.320594\n",
      "[INFO] Epoch: 60 , batch: 456 , training loss: 4.334042\n",
      "[INFO] Epoch: 60 , batch: 457 , training loss: 4.418807\n",
      "[INFO] Epoch: 60 , batch: 458 , training loss: 4.159081\n",
      "[INFO] Epoch: 60 , batch: 459 , training loss: 4.119330\n",
      "[INFO] Epoch: 60 , batch: 460 , training loss: 4.257548\n",
      "[INFO] Epoch: 60 , batch: 461 , training loss: 4.200454\n",
      "[INFO] Epoch: 60 , batch: 462 , training loss: 4.276213\n",
      "[INFO] Epoch: 60 , batch: 463 , training loss: 4.194954\n",
      "[INFO] Epoch: 60 , batch: 464 , training loss: 4.348938\n",
      "[INFO] Epoch: 60 , batch: 465 , training loss: 4.307323\n",
      "[INFO] Epoch: 60 , batch: 466 , training loss: 4.381525\n",
      "[INFO] Epoch: 60 , batch: 467 , training loss: 4.345447\n",
      "[INFO] Epoch: 60 , batch: 468 , training loss: 4.316181\n",
      "[INFO] Epoch: 60 , batch: 469 , training loss: 4.350879\n",
      "[INFO] Epoch: 60 , batch: 470 , training loss: 4.167298\n",
      "[INFO] Epoch: 60 , batch: 471 , training loss: 4.284282\n",
      "[INFO] Epoch: 60 , batch: 472 , training loss: 4.320041\n",
      "[INFO] Epoch: 60 , batch: 473 , training loss: 4.248107\n",
      "[INFO] Epoch: 60 , batch: 474 , training loss: 4.040846\n",
      "[INFO] Epoch: 60 , batch: 475 , training loss: 3.914230\n",
      "[INFO] Epoch: 60 , batch: 476 , training loss: 4.300085\n",
      "[INFO] Epoch: 60 , batch: 477 , training loss: 4.422325\n",
      "[INFO] Epoch: 60 , batch: 478 , training loss: 4.421885\n",
      "[INFO] Epoch: 60 , batch: 479 , training loss: 4.408106\n",
      "[INFO] Epoch: 60 , batch: 480 , training loss: 4.538324\n",
      "[INFO] Epoch: 60 , batch: 481 , training loss: 4.401644\n",
      "[INFO] Epoch: 60 , batch: 482 , training loss: 4.521408\n",
      "[INFO] Epoch: 60 , batch: 483 , training loss: 4.343134\n",
      "[INFO] Epoch: 60 , batch: 484 , training loss: 4.153411\n",
      "[INFO] Epoch: 60 , batch: 485 , training loss: 4.253537\n",
      "[INFO] Epoch: 60 , batch: 486 , training loss: 4.158090\n",
      "[INFO] Epoch: 60 , batch: 487 , training loss: 4.133756\n",
      "[INFO] Epoch: 60 , batch: 488 , training loss: 4.312812\n",
      "[INFO] Epoch: 60 , batch: 489 , training loss: 4.222874\n",
      "[INFO] Epoch: 60 , batch: 490 , training loss: 4.279791\n",
      "[INFO] Epoch: 60 , batch: 491 , training loss: 4.193046\n",
      "[INFO] Epoch: 60 , batch: 492 , training loss: 4.188726\n",
      "[INFO] Epoch: 60 , batch: 493 , training loss: 4.340607\n",
      "[INFO] Epoch: 60 , batch: 494 , training loss: 4.245762\n",
      "[INFO] Epoch: 60 , batch: 495 , training loss: 4.444350\n",
      "[INFO] Epoch: 60 , batch: 496 , training loss: 4.274796\n",
      "[INFO] Epoch: 60 , batch: 497 , training loss: 4.323405\n",
      "[INFO] Epoch: 60 , batch: 498 , training loss: 4.293694\n",
      "[INFO] Epoch: 60 , batch: 499 , training loss: 4.357553\n",
      "[INFO] Epoch: 60 , batch: 500 , training loss: 4.510032\n",
      "[INFO] Epoch: 60 , batch: 501 , training loss: 4.846964\n",
      "[INFO] Epoch: 60 , batch: 502 , training loss: 4.841173\n",
      "[INFO] Epoch: 60 , batch: 503 , training loss: 4.458669\n",
      "[INFO] Epoch: 60 , batch: 504 , training loss: 4.635427\n",
      "[INFO] Epoch: 60 , batch: 505 , training loss: 4.599544\n",
      "[INFO] Epoch: 60 , batch: 506 , training loss: 4.589891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 60 , batch: 507 , training loss: 4.628386\n",
      "[INFO] Epoch: 60 , batch: 508 , training loss: 4.562175\n",
      "[INFO] Epoch: 60 , batch: 509 , training loss: 4.383283\n",
      "[INFO] Epoch: 60 , batch: 510 , training loss: 4.462215\n",
      "[INFO] Epoch: 60 , batch: 511 , training loss: 4.392938\n",
      "[INFO] Epoch: 60 , batch: 512 , training loss: 4.490571\n",
      "[INFO] Epoch: 60 , batch: 513 , training loss: 4.763111\n",
      "[INFO] Epoch: 60 , batch: 514 , training loss: 4.397442\n",
      "[INFO] Epoch: 60 , batch: 515 , training loss: 4.659752\n",
      "[INFO] Epoch: 60 , batch: 516 , training loss: 4.438318\n",
      "[INFO] Epoch: 60 , batch: 517 , training loss: 4.417752\n",
      "[INFO] Epoch: 60 , batch: 518 , training loss: 4.385586\n",
      "[INFO] Epoch: 60 , batch: 519 , training loss: 4.212087\n",
      "[INFO] Epoch: 60 , batch: 520 , training loss: 4.452143\n",
      "[INFO] Epoch: 60 , batch: 521 , training loss: 4.434112\n",
      "[INFO] Epoch: 60 , batch: 522 , training loss: 4.522215\n",
      "[INFO] Epoch: 60 , batch: 523 , training loss: 4.442338\n",
      "[INFO] Epoch: 60 , batch: 524 , training loss: 4.726981\n",
      "[INFO] Epoch: 60 , batch: 525 , training loss: 4.632839\n",
      "[INFO] Epoch: 60 , batch: 526 , training loss: 4.390022\n",
      "[INFO] Epoch: 60 , batch: 527 , training loss: 4.435413\n",
      "[INFO] Epoch: 60 , batch: 528 , training loss: 4.436142\n",
      "[INFO] Epoch: 60 , batch: 529 , training loss: 4.418501\n",
      "[INFO] Epoch: 60 , batch: 530 , training loss: 4.285717\n",
      "[INFO] Epoch: 60 , batch: 531 , training loss: 4.434451\n",
      "[INFO] Epoch: 60 , batch: 532 , training loss: 4.334859\n",
      "[INFO] Epoch: 60 , batch: 533 , training loss: 4.434221\n",
      "[INFO] Epoch: 60 , batch: 534 , training loss: 4.462803\n",
      "[INFO] Epoch: 60 , batch: 535 , training loss: 4.457794\n",
      "[INFO] Epoch: 60 , batch: 536 , training loss: 4.328502\n",
      "[INFO] Epoch: 60 , batch: 537 , training loss: 4.312375\n",
      "[INFO] Epoch: 60 , batch: 538 , training loss: 4.385857\n",
      "[INFO] Epoch: 60 , batch: 539 , training loss: 4.487307\n",
      "[INFO] Epoch: 60 , batch: 540 , training loss: 4.986561\n",
      "[INFO] Epoch: 60 , batch: 541 , training loss: 4.811829\n",
      "[INFO] Epoch: 60 , batch: 542 , training loss: 4.714352\n",
      "[INFO] Epoch: 61 , batch: 0 , training loss: 3.673634\n",
      "[INFO] Epoch: 61 , batch: 1 , training loss: 3.573435\n",
      "[INFO] Epoch: 61 , batch: 2 , training loss: 3.782290\n",
      "[INFO] Epoch: 61 , batch: 3 , training loss: 3.630192\n",
      "[INFO] Epoch: 61 , batch: 4 , training loss: 3.972165\n",
      "[INFO] Epoch: 61 , batch: 5 , training loss: 3.594814\n",
      "[INFO] Epoch: 61 , batch: 6 , training loss: 4.024421\n",
      "[INFO] Epoch: 61 , batch: 7 , training loss: 3.939686\n",
      "[INFO] Epoch: 61 , batch: 8 , training loss: 3.617135\n",
      "[INFO] Epoch: 61 , batch: 9 , training loss: 3.848611\n",
      "[INFO] Epoch: 61 , batch: 10 , training loss: 3.858279\n",
      "[INFO] Epoch: 61 , batch: 11 , training loss: 3.762006\n",
      "[INFO] Epoch: 61 , batch: 12 , training loss: 3.673867\n",
      "[INFO] Epoch: 61 , batch: 13 , training loss: 3.674357\n",
      "[INFO] Epoch: 61 , batch: 14 , training loss: 3.592339\n",
      "[INFO] Epoch: 61 , batch: 15 , training loss: 3.799033\n",
      "[INFO] Epoch: 61 , batch: 16 , training loss: 3.636411\n",
      "[INFO] Epoch: 61 , batch: 17 , training loss: 3.753222\n",
      "[INFO] Epoch: 61 , batch: 18 , training loss: 3.725085\n",
      "[INFO] Epoch: 61 , batch: 19 , training loss: 3.469443\n",
      "[INFO] Epoch: 61 , batch: 20 , training loss: 3.453393\n",
      "[INFO] Epoch: 61 , batch: 21 , training loss: 3.581075\n",
      "[INFO] Epoch: 61 , batch: 22 , training loss: 3.447846\n",
      "[INFO] Epoch: 61 , batch: 23 , training loss: 3.706257\n",
      "[INFO] Epoch: 61 , batch: 24 , training loss: 3.520293\n",
      "[INFO] Epoch: 61 , batch: 25 , training loss: 3.642801\n",
      "[INFO] Epoch: 61 , batch: 26 , training loss: 3.503379\n",
      "[INFO] Epoch: 61 , batch: 27 , training loss: 3.527579\n",
      "[INFO] Epoch: 61 , batch: 28 , training loss: 3.659877\n",
      "[INFO] Epoch: 61 , batch: 29 , training loss: 3.501669\n",
      "[INFO] Epoch: 61 , batch: 30 , training loss: 3.540800\n",
      "[INFO] Epoch: 61 , batch: 31 , training loss: 3.580939\n",
      "[INFO] Epoch: 61 , batch: 32 , training loss: 3.603461\n",
      "[INFO] Epoch: 61 , batch: 33 , training loss: 3.636690\n",
      "[INFO] Epoch: 61 , batch: 34 , training loss: 3.606723\n",
      "[INFO] Epoch: 61 , batch: 35 , training loss: 3.574733\n",
      "[INFO] Epoch: 61 , batch: 36 , training loss: 3.645321\n",
      "[INFO] Epoch: 61 , batch: 37 , training loss: 3.530756\n",
      "[INFO] Epoch: 61 , batch: 38 , training loss: 3.593776\n",
      "[INFO] Epoch: 61 , batch: 39 , training loss: 3.422979\n",
      "[INFO] Epoch: 61 , batch: 40 , training loss: 3.624863\n",
      "[INFO] Epoch: 61 , batch: 41 , training loss: 3.595655\n",
      "[INFO] Epoch: 61 , batch: 42 , training loss: 4.029376\n",
      "[INFO] Epoch: 61 , batch: 43 , training loss: 3.795681\n",
      "[INFO] Epoch: 61 , batch: 44 , training loss: 4.133102\n",
      "[INFO] Epoch: 61 , batch: 45 , training loss: 4.037583\n",
      "[INFO] Epoch: 61 , batch: 46 , training loss: 4.043044\n",
      "[INFO] Epoch: 61 , batch: 47 , training loss: 3.639530\n",
      "[INFO] Epoch: 61 , batch: 48 , training loss: 3.642205\n",
      "[INFO] Epoch: 61 , batch: 49 , training loss: 3.837517\n",
      "[INFO] Epoch: 61 , batch: 50 , training loss: 3.604601\n",
      "[INFO] Epoch: 61 , batch: 51 , training loss: 3.821320\n",
      "[INFO] Epoch: 61 , batch: 52 , training loss: 3.614079\n",
      "[INFO] Epoch: 61 , batch: 53 , training loss: 3.738863\n",
      "[INFO] Epoch: 61 , batch: 54 , training loss: 3.747981\n",
      "[INFO] Epoch: 61 , batch: 55 , training loss: 3.811188\n",
      "[INFO] Epoch: 61 , batch: 56 , training loss: 3.668523\n",
      "[INFO] Epoch: 61 , batch: 57 , training loss: 3.595916\n",
      "[INFO] Epoch: 61 , batch: 58 , training loss: 3.635345\n",
      "[INFO] Epoch: 61 , batch: 59 , training loss: 3.713129\n",
      "[INFO] Epoch: 61 , batch: 60 , training loss: 3.682250\n",
      "[INFO] Epoch: 61 , batch: 61 , training loss: 3.786678\n",
      "[INFO] Epoch: 61 , batch: 62 , training loss: 3.612362\n",
      "[INFO] Epoch: 61 , batch: 63 , training loss: 3.837295\n",
      "[INFO] Epoch: 61 , batch: 64 , training loss: 4.022680\n",
      "[INFO] Epoch: 61 , batch: 65 , training loss: 3.745205\n",
      "[INFO] Epoch: 61 , batch: 66 , training loss: 3.600073\n",
      "[INFO] Epoch: 61 , batch: 67 , training loss: 3.634492\n",
      "[INFO] Epoch: 61 , batch: 68 , training loss: 3.763539\n",
      "[INFO] Epoch: 61 , batch: 69 , training loss: 3.699111\n",
      "[INFO] Epoch: 61 , batch: 70 , training loss: 3.916880\n",
      "[INFO] Epoch: 61 , batch: 71 , training loss: 3.757483\n",
      "[INFO] Epoch: 61 , batch: 72 , training loss: 3.815577\n",
      "[INFO] Epoch: 61 , batch: 73 , training loss: 3.819667\n",
      "[INFO] Epoch: 61 , batch: 74 , training loss: 3.858303\n",
      "[INFO] Epoch: 61 , batch: 75 , training loss: 3.759105\n",
      "[INFO] Epoch: 61 , batch: 76 , training loss: 3.862268\n",
      "[INFO] Epoch: 61 , batch: 77 , training loss: 3.773959\n",
      "[INFO] Epoch: 61 , batch: 78 , training loss: 3.896755\n",
      "[INFO] Epoch: 61 , batch: 79 , training loss: 3.762433\n",
      "[INFO] Epoch: 61 , batch: 80 , training loss: 3.928131\n",
      "[INFO] Epoch: 61 , batch: 81 , training loss: 3.869190\n",
      "[INFO] Epoch: 61 , batch: 82 , training loss: 3.873714\n",
      "[INFO] Epoch: 61 , batch: 83 , training loss: 3.902433\n",
      "[INFO] Epoch: 61 , batch: 84 , training loss: 3.918663\n",
      "[INFO] Epoch: 61 , batch: 85 , training loss: 3.953938\n",
      "[INFO] Epoch: 61 , batch: 86 , training loss: 3.909204\n",
      "[INFO] Epoch: 61 , batch: 87 , training loss: 3.853426\n",
      "[INFO] Epoch: 61 , batch: 88 , training loss: 4.032357\n",
      "[INFO] Epoch: 61 , batch: 89 , training loss: 3.777819\n",
      "[INFO] Epoch: 61 , batch: 90 , training loss: 3.871959\n",
      "[INFO] Epoch: 61 , batch: 91 , training loss: 3.813148\n",
      "[INFO] Epoch: 61 , batch: 92 , training loss: 3.845283\n",
      "[INFO] Epoch: 61 , batch: 93 , training loss: 3.947117\n",
      "[INFO] Epoch: 61 , batch: 94 , training loss: 4.036632\n",
      "[INFO] Epoch: 61 , batch: 95 , training loss: 3.846694\n",
      "[INFO] Epoch: 61 , batch: 96 , training loss: 3.832520\n",
      "[INFO] Epoch: 61 , batch: 97 , training loss: 3.756498\n",
      "[INFO] Epoch: 61 , batch: 98 , training loss: 3.709091\n",
      "[INFO] Epoch: 61 , batch: 99 , training loss: 3.844257\n",
      "[INFO] Epoch: 61 , batch: 100 , training loss: 3.766500\n",
      "[INFO] Epoch: 61 , batch: 101 , training loss: 3.756379\n",
      "[INFO] Epoch: 61 , batch: 102 , training loss: 3.931335\n",
      "[INFO] Epoch: 61 , batch: 103 , training loss: 3.715298\n",
      "[INFO] Epoch: 61 , batch: 104 , training loss: 3.664461\n",
      "[INFO] Epoch: 61 , batch: 105 , training loss: 3.916636\n",
      "[INFO] Epoch: 61 , batch: 106 , training loss: 3.938137\n",
      "[INFO] Epoch: 61 , batch: 107 , training loss: 3.772020\n",
      "[INFO] Epoch: 61 , batch: 108 , training loss: 3.719223\n",
      "[INFO] Epoch: 61 , batch: 109 , training loss: 3.666956\n",
      "[INFO] Epoch: 61 , batch: 110 , training loss: 3.848540\n",
      "[INFO] Epoch: 61 , batch: 111 , training loss: 3.898158\n",
      "[INFO] Epoch: 61 , batch: 112 , training loss: 3.810096\n",
      "[INFO] Epoch: 61 , batch: 113 , training loss: 3.808294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 61 , batch: 114 , training loss: 3.774252\n",
      "[INFO] Epoch: 61 , batch: 115 , training loss: 3.812680\n",
      "[INFO] Epoch: 61 , batch: 116 , training loss: 3.703190\n",
      "[INFO] Epoch: 61 , batch: 117 , training loss: 3.940789\n",
      "[INFO] Epoch: 61 , batch: 118 , training loss: 3.946838\n",
      "[INFO] Epoch: 61 , batch: 119 , training loss: 4.034330\n",
      "[INFO] Epoch: 61 , batch: 120 , training loss: 4.013900\n",
      "[INFO] Epoch: 61 , batch: 121 , training loss: 3.893053\n",
      "[INFO] Epoch: 61 , batch: 122 , training loss: 3.795218\n",
      "[INFO] Epoch: 61 , batch: 123 , training loss: 3.775021\n",
      "[INFO] Epoch: 61 , batch: 124 , training loss: 3.901772\n",
      "[INFO] Epoch: 61 , batch: 125 , training loss: 3.714872\n",
      "[INFO] Epoch: 61 , batch: 126 , training loss: 3.697400\n",
      "[INFO] Epoch: 61 , batch: 127 , training loss: 3.722544\n",
      "[INFO] Epoch: 61 , batch: 128 , training loss: 3.840739\n",
      "[INFO] Epoch: 61 , batch: 129 , training loss: 3.820695\n",
      "[INFO] Epoch: 61 , batch: 130 , training loss: 3.815703\n",
      "[INFO] Epoch: 61 , batch: 131 , training loss: 3.815993\n",
      "[INFO] Epoch: 61 , batch: 132 , training loss: 3.834915\n",
      "[INFO] Epoch: 61 , batch: 133 , training loss: 3.806969\n",
      "[INFO] Epoch: 61 , batch: 134 , training loss: 3.603515\n",
      "[INFO] Epoch: 61 , batch: 135 , training loss: 3.660285\n",
      "[INFO] Epoch: 61 , batch: 136 , training loss: 3.925924\n",
      "[INFO] Epoch: 61 , batch: 137 , training loss: 3.862719\n",
      "[INFO] Epoch: 61 , batch: 138 , training loss: 3.892416\n",
      "[INFO] Epoch: 61 , batch: 139 , training loss: 4.469163\n",
      "[INFO] Epoch: 61 , batch: 140 , training loss: 4.265248\n",
      "[INFO] Epoch: 61 , batch: 141 , training loss: 3.996443\n",
      "[INFO] Epoch: 61 , batch: 142 , training loss: 3.745941\n",
      "[INFO] Epoch: 61 , batch: 143 , training loss: 3.904847\n",
      "[INFO] Epoch: 61 , batch: 144 , training loss: 3.723572\n",
      "[INFO] Epoch: 61 , batch: 145 , training loss: 3.796246\n",
      "[INFO] Epoch: 61 , batch: 146 , training loss: 3.995204\n",
      "[INFO] Epoch: 61 , batch: 147 , training loss: 3.674545\n",
      "[INFO] Epoch: 61 , batch: 148 , training loss: 3.624223\n",
      "[INFO] Epoch: 61 , batch: 149 , training loss: 3.722629\n",
      "[INFO] Epoch: 61 , batch: 150 , training loss: 3.954858\n",
      "[INFO] Epoch: 61 , batch: 151 , training loss: 3.837379\n",
      "[INFO] Epoch: 61 , batch: 152 , training loss: 3.845408\n",
      "[INFO] Epoch: 61 , batch: 153 , training loss: 3.879933\n",
      "[INFO] Epoch: 61 , batch: 154 , training loss: 3.945459\n",
      "[INFO] Epoch: 61 , batch: 155 , training loss: 4.147207\n",
      "[INFO] Epoch: 61 , batch: 156 , training loss: 3.902894\n",
      "[INFO] Epoch: 61 , batch: 157 , training loss: 3.869757\n",
      "[INFO] Epoch: 61 , batch: 158 , training loss: 3.972901\n",
      "[INFO] Epoch: 61 , batch: 159 , training loss: 3.958466\n",
      "[INFO] Epoch: 61 , batch: 160 , training loss: 4.099705\n",
      "[INFO] Epoch: 61 , batch: 161 , training loss: 4.155468\n",
      "[INFO] Epoch: 61 , batch: 162 , training loss: 4.156109\n",
      "[INFO] Epoch: 61 , batch: 163 , training loss: 4.313252\n",
      "[INFO] Epoch: 61 , batch: 164 , training loss: 4.290410\n",
      "[INFO] Epoch: 61 , batch: 165 , training loss: 4.177269\n",
      "[INFO] Epoch: 61 , batch: 166 , training loss: 4.124752\n",
      "[INFO] Epoch: 61 , batch: 167 , training loss: 4.121566\n",
      "[INFO] Epoch: 61 , batch: 168 , training loss: 3.819725\n",
      "[INFO] Epoch: 61 , batch: 169 , training loss: 3.799514\n",
      "[INFO] Epoch: 61 , batch: 170 , training loss: 3.976159\n",
      "[INFO] Epoch: 61 , batch: 171 , training loss: 3.487016\n",
      "[INFO] Epoch: 61 , batch: 172 , training loss: 3.692787\n",
      "[INFO] Epoch: 61 , batch: 173 , training loss: 3.999793\n",
      "[INFO] Epoch: 61 , batch: 174 , training loss: 4.434764\n",
      "[INFO] Epoch: 61 , batch: 175 , training loss: 4.728172\n",
      "[INFO] Epoch: 61 , batch: 176 , training loss: 4.349783\n",
      "[INFO] Epoch: 61 , batch: 177 , training loss: 3.964607\n",
      "[INFO] Epoch: 61 , batch: 178 , training loss: 3.967223\n",
      "[INFO] Epoch: 61 , batch: 179 , training loss: 4.038581\n",
      "[INFO] Epoch: 61 , batch: 180 , training loss: 3.992923\n",
      "[INFO] Epoch: 61 , batch: 181 , training loss: 4.295504\n",
      "[INFO] Epoch: 61 , batch: 182 , training loss: 4.233145\n",
      "[INFO] Epoch: 61 , batch: 183 , training loss: 4.229554\n",
      "[INFO] Epoch: 61 , batch: 184 , training loss: 4.087262\n",
      "[INFO] Epoch: 61 , batch: 185 , training loss: 4.049666\n",
      "[INFO] Epoch: 61 , batch: 186 , training loss: 4.214183\n",
      "[INFO] Epoch: 61 , batch: 187 , training loss: 4.298034\n",
      "[INFO] Epoch: 61 , batch: 188 , training loss: 4.279689\n",
      "[INFO] Epoch: 61 , batch: 189 , training loss: 4.203860\n",
      "[INFO] Epoch: 61 , batch: 190 , training loss: 4.237473\n",
      "[INFO] Epoch: 61 , batch: 191 , training loss: 4.359317\n",
      "[INFO] Epoch: 61 , batch: 192 , training loss: 4.207482\n",
      "[INFO] Epoch: 61 , batch: 193 , training loss: 4.292785\n",
      "[INFO] Epoch: 61 , batch: 194 , training loss: 4.225609\n",
      "[INFO] Epoch: 61 , batch: 195 , training loss: 4.179305\n",
      "[INFO] Epoch: 61 , batch: 196 , training loss: 4.007278\n",
      "[INFO] Epoch: 61 , batch: 197 , training loss: 4.104515\n",
      "[INFO] Epoch: 61 , batch: 198 , training loss: 4.047363\n",
      "[INFO] Epoch: 61 , batch: 199 , training loss: 4.164830\n",
      "[INFO] Epoch: 61 , batch: 200 , training loss: 4.065237\n",
      "[INFO] Epoch: 61 , batch: 201 , training loss: 3.972023\n",
      "[INFO] Epoch: 61 , batch: 202 , training loss: 3.980602\n",
      "[INFO] Epoch: 61 , batch: 203 , training loss: 4.131100\n",
      "[INFO] Epoch: 61 , batch: 204 , training loss: 4.163626\n",
      "[INFO] Epoch: 61 , batch: 205 , training loss: 3.803565\n",
      "[INFO] Epoch: 61 , batch: 206 , training loss: 3.738816\n",
      "[INFO] Epoch: 61 , batch: 207 , training loss: 3.701957\n",
      "[INFO] Epoch: 61 , batch: 208 , training loss: 4.026436\n",
      "[INFO] Epoch: 61 , batch: 209 , training loss: 4.029173\n",
      "[INFO] Epoch: 61 , batch: 210 , training loss: 4.014321\n",
      "[INFO] Epoch: 61 , batch: 211 , training loss: 4.032590\n",
      "[INFO] Epoch: 61 , batch: 212 , training loss: 4.130988\n",
      "[INFO] Epoch: 61 , batch: 213 , training loss: 4.067958\n",
      "[INFO] Epoch: 61 , batch: 214 , training loss: 4.148182\n",
      "[INFO] Epoch: 61 , batch: 215 , training loss: 4.332958\n",
      "[INFO] Epoch: 61 , batch: 216 , training loss: 4.042825\n",
      "[INFO] Epoch: 61 , batch: 217 , training loss: 4.002733\n",
      "[INFO] Epoch: 61 , batch: 218 , training loss: 3.995090\n",
      "[INFO] Epoch: 61 , batch: 219 , training loss: 4.103349\n",
      "[INFO] Epoch: 61 , batch: 220 , training loss: 3.937488\n",
      "[INFO] Epoch: 61 , batch: 221 , training loss: 3.930401\n",
      "[INFO] Epoch: 61 , batch: 222 , training loss: 4.053170\n",
      "[INFO] Epoch: 61 , batch: 223 , training loss: 4.211237\n",
      "[INFO] Epoch: 61 , batch: 224 , training loss: 4.215467\n",
      "[INFO] Epoch: 61 , batch: 225 , training loss: 4.123553\n",
      "[INFO] Epoch: 61 , batch: 226 , training loss: 4.249884\n",
      "[INFO] Epoch: 61 , batch: 227 , training loss: 4.232038\n",
      "[INFO] Epoch: 61 , batch: 228 , training loss: 4.236704\n",
      "[INFO] Epoch: 61 , batch: 229 , training loss: 4.104050\n",
      "[INFO] Epoch: 61 , batch: 230 , training loss: 3.968168\n",
      "[INFO] Epoch: 61 , batch: 231 , training loss: 3.844962\n",
      "[INFO] Epoch: 61 , batch: 232 , training loss: 3.964606\n",
      "[INFO] Epoch: 61 , batch: 233 , training loss: 4.001828\n",
      "[INFO] Epoch: 61 , batch: 234 , training loss: 3.673653\n",
      "[INFO] Epoch: 61 , batch: 235 , training loss: 3.791895\n",
      "[INFO] Epoch: 61 , batch: 236 , training loss: 3.894387\n",
      "[INFO] Epoch: 61 , batch: 237 , training loss: 4.109119\n",
      "[INFO] Epoch: 61 , batch: 238 , training loss: 3.906537\n",
      "[INFO] Epoch: 61 , batch: 239 , training loss: 3.919783\n",
      "[INFO] Epoch: 61 , batch: 240 , training loss: 3.982311\n",
      "[INFO] Epoch: 61 , batch: 241 , training loss: 3.786485\n",
      "[INFO] Epoch: 61 , batch: 242 , training loss: 3.804681\n",
      "[INFO] Epoch: 61 , batch: 243 , training loss: 4.098561\n",
      "[INFO] Epoch: 61 , batch: 244 , training loss: 4.031498\n",
      "[INFO] Epoch: 61 , batch: 245 , training loss: 3.986634\n",
      "[INFO] Epoch: 61 , batch: 246 , training loss: 3.735600\n",
      "[INFO] Epoch: 61 , batch: 247 , training loss: 3.898493\n",
      "[INFO] Epoch: 61 , batch: 248 , training loss: 3.952258\n",
      "[INFO] Epoch: 61 , batch: 249 , training loss: 3.944318\n",
      "[INFO] Epoch: 61 , batch: 250 , training loss: 3.758533\n",
      "[INFO] Epoch: 61 , batch: 251 , training loss: 4.178366\n",
      "[INFO] Epoch: 61 , batch: 252 , training loss: 3.904321\n",
      "[INFO] Epoch: 61 , batch: 253 , training loss: 3.802272\n",
      "[INFO] Epoch: 61 , batch: 254 , training loss: 4.078508\n",
      "[INFO] Epoch: 61 , batch: 255 , training loss: 4.042704\n",
      "[INFO] Epoch: 61 , batch: 256 , training loss: 4.026383\n",
      "[INFO] Epoch: 61 , batch: 257 , training loss: 4.199674\n",
      "[INFO] Epoch: 61 , batch: 258 , training loss: 4.208018\n",
      "[INFO] Epoch: 61 , batch: 259 , training loss: 4.232362\n",
      "[INFO] Epoch: 61 , batch: 260 , training loss: 4.025652\n",
      "[INFO] Epoch: 61 , batch: 261 , training loss: 4.205310\n",
      "[INFO] Epoch: 61 , batch: 262 , training loss: 4.318023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 61 , batch: 263 , training loss: 4.482118\n",
      "[INFO] Epoch: 61 , batch: 264 , training loss: 3.877698\n",
      "[INFO] Epoch: 61 , batch: 265 , training loss: 3.963096\n",
      "[INFO] Epoch: 61 , batch: 266 , training loss: 4.375746\n",
      "[INFO] Epoch: 61 , batch: 267 , training loss: 4.140653\n",
      "[INFO] Epoch: 61 , batch: 268 , training loss: 4.047499\n",
      "[INFO] Epoch: 61 , batch: 269 , training loss: 4.020069\n",
      "[INFO] Epoch: 61 , batch: 270 , training loss: 4.047447\n",
      "[INFO] Epoch: 61 , batch: 271 , training loss: 4.067879\n",
      "[INFO] Epoch: 61 , batch: 272 , training loss: 4.079603\n",
      "[INFO] Epoch: 61 , batch: 273 , training loss: 4.103681\n",
      "[INFO] Epoch: 61 , batch: 274 , training loss: 4.203444\n",
      "[INFO] Epoch: 61 , batch: 275 , training loss: 4.054971\n",
      "[INFO] Epoch: 61 , batch: 276 , training loss: 4.091127\n",
      "[INFO] Epoch: 61 , batch: 277 , training loss: 4.267841\n",
      "[INFO] Epoch: 61 , batch: 278 , training loss: 3.936963\n",
      "[INFO] Epoch: 61 , batch: 279 , training loss: 3.947364\n",
      "[INFO] Epoch: 61 , batch: 280 , training loss: 3.935619\n",
      "[INFO] Epoch: 61 , batch: 281 , training loss: 4.052355\n",
      "[INFO] Epoch: 61 , batch: 282 , training loss: 3.997587\n",
      "[INFO] Epoch: 61 , batch: 283 , training loss: 3.993277\n",
      "[INFO] Epoch: 61 , batch: 284 , training loss: 4.005316\n",
      "[INFO] Epoch: 61 , batch: 285 , training loss: 3.936099\n",
      "[INFO] Epoch: 61 , batch: 286 , training loss: 3.960387\n",
      "[INFO] Epoch: 61 , batch: 287 , training loss: 3.905838\n",
      "[INFO] Epoch: 61 , batch: 288 , training loss: 3.857486\n",
      "[INFO] Epoch: 61 , batch: 289 , training loss: 3.943959\n",
      "[INFO] Epoch: 61 , batch: 290 , training loss: 3.731119\n",
      "[INFO] Epoch: 61 , batch: 291 , training loss: 3.720129\n",
      "[INFO] Epoch: 61 , batch: 292 , training loss: 3.826244\n",
      "[INFO] Epoch: 61 , batch: 293 , training loss: 3.757722\n",
      "[INFO] Epoch: 61 , batch: 294 , training loss: 4.383997\n",
      "[INFO] Epoch: 61 , batch: 295 , training loss: 4.184425\n",
      "[INFO] Epoch: 61 , batch: 296 , training loss: 4.118644\n",
      "[INFO] Epoch: 61 , batch: 297 , training loss: 4.063059\n",
      "[INFO] Epoch: 61 , batch: 298 , training loss: 3.901650\n",
      "[INFO] Epoch: 61 , batch: 299 , training loss: 3.944167\n",
      "[INFO] Epoch: 61 , batch: 300 , training loss: 3.939865\n",
      "[INFO] Epoch: 61 , batch: 301 , training loss: 3.861503\n",
      "[INFO] Epoch: 61 , batch: 302 , training loss: 4.013574\n",
      "[INFO] Epoch: 61 , batch: 303 , training loss: 4.030117\n",
      "[INFO] Epoch: 61 , batch: 304 , training loss: 4.142600\n",
      "[INFO] Epoch: 61 , batch: 305 , training loss: 4.009358\n",
      "[INFO] Epoch: 61 , batch: 306 , training loss: 4.121961\n",
      "[INFO] Epoch: 61 , batch: 307 , training loss: 4.144901\n",
      "[INFO] Epoch: 61 , batch: 308 , training loss: 3.966228\n",
      "[INFO] Epoch: 61 , batch: 309 , training loss: 3.924908\n",
      "[INFO] Epoch: 61 , batch: 310 , training loss: 3.889247\n",
      "[INFO] Epoch: 61 , batch: 311 , training loss: 3.868305\n",
      "[INFO] Epoch: 61 , batch: 312 , training loss: 3.767915\n",
      "[INFO] Epoch: 61 , batch: 313 , training loss: 3.867007\n",
      "[INFO] Epoch: 61 , batch: 314 , training loss: 3.941065\n",
      "[INFO] Epoch: 61 , batch: 315 , training loss: 4.023144\n",
      "[INFO] Epoch: 61 , batch: 316 , training loss: 4.251615\n",
      "[INFO] Epoch: 61 , batch: 317 , training loss: 4.598343\n",
      "[INFO] Epoch: 61 , batch: 318 , training loss: 4.718716\n",
      "[INFO] Epoch: 61 , batch: 319 , training loss: 4.405952\n",
      "[INFO] Epoch: 61 , batch: 320 , training loss: 3.968810\n",
      "[INFO] Epoch: 61 , batch: 321 , training loss: 3.812305\n",
      "[INFO] Epoch: 61 , batch: 322 , training loss: 3.905454\n",
      "[INFO] Epoch: 61 , batch: 323 , training loss: 3.934715\n",
      "[INFO] Epoch: 61 , batch: 324 , training loss: 3.923007\n",
      "[INFO] Epoch: 61 , batch: 325 , training loss: 4.013879\n",
      "[INFO] Epoch: 61 , batch: 326 , training loss: 4.090475\n",
      "[INFO] Epoch: 61 , batch: 327 , training loss: 4.034380\n",
      "[INFO] Epoch: 61 , batch: 328 , training loss: 4.033667\n",
      "[INFO] Epoch: 61 , batch: 329 , training loss: 3.934825\n",
      "[INFO] Epoch: 61 , batch: 330 , training loss: 3.924907\n",
      "[INFO] Epoch: 61 , batch: 331 , training loss: 4.055400\n",
      "[INFO] Epoch: 61 , batch: 332 , training loss: 3.922594\n",
      "[INFO] Epoch: 61 , batch: 333 , training loss: 3.901010\n",
      "[INFO] Epoch: 61 , batch: 334 , training loss: 3.925103\n",
      "[INFO] Epoch: 61 , batch: 335 , training loss: 4.044405\n",
      "[INFO] Epoch: 61 , batch: 336 , training loss: 4.055949\n",
      "[INFO] Epoch: 61 , batch: 337 , training loss: 4.093204\n",
      "[INFO] Epoch: 61 , batch: 338 , training loss: 4.311214\n",
      "[INFO] Epoch: 61 , batch: 339 , training loss: 4.126328\n",
      "[INFO] Epoch: 61 , batch: 340 , training loss: 4.306179\n",
      "[INFO] Epoch: 61 , batch: 341 , training loss: 4.068590\n",
      "[INFO] Epoch: 61 , batch: 342 , training loss: 3.856526\n",
      "[INFO] Epoch: 61 , batch: 343 , training loss: 3.929086\n",
      "[INFO] Epoch: 61 , batch: 344 , training loss: 3.795226\n",
      "[INFO] Epoch: 61 , batch: 345 , training loss: 3.899000\n",
      "[INFO] Epoch: 61 , batch: 346 , training loss: 3.966057\n",
      "[INFO] Epoch: 61 , batch: 347 , training loss: 3.870821\n",
      "[INFO] Epoch: 61 , batch: 348 , training loss: 3.966158\n",
      "[INFO] Epoch: 61 , batch: 349 , training loss: 4.050723\n",
      "[INFO] Epoch: 61 , batch: 350 , training loss: 3.947206\n",
      "[INFO] Epoch: 61 , batch: 351 , training loss: 4.015017\n",
      "[INFO] Epoch: 61 , batch: 352 , training loss: 4.026232\n",
      "[INFO] Epoch: 61 , batch: 353 , training loss: 4.007034\n",
      "[INFO] Epoch: 61 , batch: 354 , training loss: 4.087354\n",
      "[INFO] Epoch: 61 , batch: 355 , training loss: 4.095677\n",
      "[INFO] Epoch: 61 , batch: 356 , training loss: 3.947942\n",
      "[INFO] Epoch: 61 , batch: 357 , training loss: 3.998412\n",
      "[INFO] Epoch: 61 , batch: 358 , training loss: 3.951887\n",
      "[INFO] Epoch: 61 , batch: 359 , training loss: 3.937610\n",
      "[INFO] Epoch: 61 , batch: 360 , training loss: 4.040648\n",
      "[INFO] Epoch: 61 , batch: 361 , training loss: 4.011061\n",
      "[INFO] Epoch: 61 , batch: 362 , training loss: 4.136948\n",
      "[INFO] Epoch: 61 , batch: 363 , training loss: 4.002723\n",
      "[INFO] Epoch: 61 , batch: 364 , training loss: 4.053090\n",
      "[INFO] Epoch: 61 , batch: 365 , training loss: 3.991769\n",
      "[INFO] Epoch: 61 , batch: 366 , training loss: 4.075913\n",
      "[INFO] Epoch: 61 , batch: 367 , training loss: 4.121945\n",
      "[INFO] Epoch: 61 , batch: 368 , training loss: 4.524812\n",
      "[INFO] Epoch: 61 , batch: 369 , training loss: 4.200297\n",
      "[INFO] Epoch: 61 , batch: 370 , training loss: 3.958734\n",
      "[INFO] Epoch: 61 , batch: 371 , training loss: 4.384758\n",
      "[INFO] Epoch: 61 , batch: 372 , training loss: 4.619927\n",
      "[INFO] Epoch: 61 , batch: 373 , training loss: 4.627528\n",
      "[INFO] Epoch: 61 , batch: 374 , training loss: 4.790243\n",
      "[INFO] Epoch: 61 , batch: 375 , training loss: 4.797599\n",
      "[INFO] Epoch: 61 , batch: 376 , training loss: 4.686705\n",
      "[INFO] Epoch: 61 , batch: 377 , training loss: 4.436221\n",
      "[INFO] Epoch: 61 , batch: 378 , training loss: 4.534650\n",
      "[INFO] Epoch: 61 , batch: 379 , training loss: 4.509516\n",
      "[INFO] Epoch: 61 , batch: 380 , training loss: 4.696403\n",
      "[INFO] Epoch: 61 , batch: 381 , training loss: 4.380129\n",
      "[INFO] Epoch: 61 , batch: 382 , training loss: 4.628322\n",
      "[INFO] Epoch: 61 , batch: 383 , training loss: 4.686403\n",
      "[INFO] Epoch: 61 , batch: 384 , training loss: 4.629955\n",
      "[INFO] Epoch: 61 , batch: 385 , training loss: 4.296103\n",
      "[INFO] Epoch: 61 , batch: 386 , training loss: 4.569449\n",
      "[INFO] Epoch: 61 , batch: 387 , training loss: 4.505783\n",
      "[INFO] Epoch: 61 , batch: 388 , training loss: 4.355101\n",
      "[INFO] Epoch: 61 , batch: 389 , training loss: 4.147425\n",
      "[INFO] Epoch: 61 , batch: 390 , training loss: 4.183251\n",
      "[INFO] Epoch: 61 , batch: 391 , training loss: 4.204930\n",
      "[INFO] Epoch: 61 , batch: 392 , training loss: 4.561047\n",
      "[INFO] Epoch: 61 , batch: 393 , training loss: 4.435207\n",
      "[INFO] Epoch: 61 , batch: 394 , training loss: 4.565593\n",
      "[INFO] Epoch: 61 , batch: 395 , training loss: 4.366094\n",
      "[INFO] Epoch: 61 , batch: 396 , training loss: 4.191350\n",
      "[INFO] Epoch: 61 , batch: 397 , training loss: 4.348242\n",
      "[INFO] Epoch: 61 , batch: 398 , training loss: 4.197382\n",
      "[INFO] Epoch: 61 , batch: 399 , training loss: 4.276898\n",
      "[INFO] Epoch: 61 , batch: 400 , training loss: 4.248681\n",
      "[INFO] Epoch: 61 , batch: 401 , training loss: 4.667651\n",
      "[INFO] Epoch: 61 , batch: 402 , training loss: 4.393851\n",
      "[INFO] Epoch: 61 , batch: 403 , training loss: 4.236189\n",
      "[INFO] Epoch: 61 , batch: 404 , training loss: 4.409060\n",
      "[INFO] Epoch: 61 , batch: 405 , training loss: 4.476291\n",
      "[INFO] Epoch: 61 , batch: 406 , training loss: 4.366698\n",
      "[INFO] Epoch: 61 , batch: 407 , training loss: 4.379630\n",
      "[INFO] Epoch: 61 , batch: 408 , training loss: 4.356680\n",
      "[INFO] Epoch: 61 , batch: 409 , training loss: 4.386146\n",
      "[INFO] Epoch: 61 , batch: 410 , training loss: 4.436941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 61 , batch: 411 , training loss: 4.591846\n",
      "[INFO] Epoch: 61 , batch: 412 , training loss: 4.446879\n",
      "[INFO] Epoch: 61 , batch: 413 , training loss: 4.306630\n",
      "[INFO] Epoch: 61 , batch: 414 , training loss: 4.341821\n",
      "[INFO] Epoch: 61 , batch: 415 , training loss: 4.404916\n",
      "[INFO] Epoch: 61 , batch: 416 , training loss: 4.460290\n",
      "[INFO] Epoch: 61 , batch: 417 , training loss: 4.380605\n",
      "[INFO] Epoch: 61 , batch: 418 , training loss: 4.448436\n",
      "[INFO] Epoch: 61 , batch: 419 , training loss: 4.396503\n",
      "[INFO] Epoch: 61 , batch: 420 , training loss: 4.368807\n",
      "[INFO] Epoch: 61 , batch: 421 , training loss: 4.356961\n",
      "[INFO] Epoch: 61 , batch: 422 , training loss: 4.195247\n",
      "[INFO] Epoch: 61 , batch: 423 , training loss: 4.422631\n",
      "[INFO] Epoch: 61 , batch: 424 , training loss: 4.585487\n",
      "[INFO] Epoch: 61 , batch: 425 , training loss: 4.450922\n",
      "[INFO] Epoch: 61 , batch: 426 , training loss: 4.187285\n",
      "[INFO] Epoch: 61 , batch: 427 , training loss: 4.436457\n",
      "[INFO] Epoch: 61 , batch: 428 , training loss: 4.293373\n",
      "[INFO] Epoch: 61 , batch: 429 , training loss: 4.219987\n",
      "[INFO] Epoch: 61 , batch: 430 , training loss: 4.461223\n",
      "[INFO] Epoch: 61 , batch: 431 , training loss: 4.053174\n",
      "[INFO] Epoch: 61 , batch: 432 , training loss: 4.104384\n",
      "[INFO] Epoch: 61 , batch: 433 , training loss: 4.145813\n",
      "[INFO] Epoch: 61 , batch: 434 , training loss: 4.033226\n",
      "[INFO] Epoch: 61 , batch: 435 , training loss: 4.377371\n",
      "[INFO] Epoch: 61 , batch: 436 , training loss: 4.403927\n",
      "[INFO] Epoch: 61 , batch: 437 , training loss: 4.210429\n",
      "[INFO] Epoch: 61 , batch: 438 , training loss: 4.071689\n",
      "[INFO] Epoch: 61 , batch: 439 , training loss: 4.290748\n",
      "[INFO] Epoch: 61 , batch: 440 , training loss: 4.421445\n",
      "[INFO] Epoch: 61 , batch: 441 , training loss: 4.509727\n",
      "[INFO] Epoch: 61 , batch: 442 , training loss: 4.266017\n",
      "[INFO] Epoch: 61 , batch: 443 , training loss: 4.436428\n",
      "[INFO] Epoch: 61 , batch: 444 , training loss: 4.082367\n",
      "[INFO] Epoch: 61 , batch: 445 , training loss: 3.954933\n",
      "[INFO] Epoch: 61 , batch: 446 , training loss: 3.908786\n",
      "[INFO] Epoch: 61 , batch: 447 , training loss: 4.103682\n",
      "[INFO] Epoch: 61 , batch: 448 , training loss: 4.212616\n",
      "[INFO] Epoch: 61 , batch: 449 , training loss: 4.609086\n",
      "[INFO] Epoch: 61 , batch: 450 , training loss: 4.675585\n",
      "[INFO] Epoch: 61 , batch: 451 , training loss: 4.566512\n",
      "[INFO] Epoch: 61 , batch: 452 , training loss: 4.389791\n",
      "[INFO] Epoch: 61 , batch: 453 , training loss: 4.135083\n",
      "[INFO] Epoch: 61 , batch: 454 , training loss: 4.293314\n",
      "[INFO] Epoch: 61 , batch: 455 , training loss: 4.321785\n",
      "[INFO] Epoch: 61 , batch: 456 , training loss: 4.344115\n",
      "[INFO] Epoch: 61 , batch: 457 , training loss: 4.425045\n",
      "[INFO] Epoch: 61 , batch: 458 , training loss: 4.168375\n",
      "[INFO] Epoch: 61 , batch: 459 , training loss: 4.131699\n",
      "[INFO] Epoch: 61 , batch: 460 , training loss: 4.265525\n",
      "[INFO] Epoch: 61 , batch: 461 , training loss: 4.204912\n",
      "[INFO] Epoch: 61 , batch: 462 , training loss: 4.285275\n",
      "[INFO] Epoch: 61 , batch: 463 , training loss: 4.194638\n",
      "[INFO] Epoch: 61 , batch: 464 , training loss: 4.357713\n",
      "[INFO] Epoch: 61 , batch: 465 , training loss: 4.308769\n",
      "[INFO] Epoch: 61 , batch: 466 , training loss: 4.392797\n",
      "[INFO] Epoch: 61 , batch: 467 , training loss: 4.366178\n",
      "[INFO] Epoch: 61 , batch: 468 , training loss: 4.325901\n",
      "[INFO] Epoch: 61 , batch: 469 , training loss: 4.382447\n",
      "[INFO] Epoch: 61 , batch: 470 , training loss: 4.194516\n",
      "[INFO] Epoch: 61 , batch: 471 , training loss: 4.283593\n",
      "[INFO] Epoch: 61 , batch: 472 , training loss: 4.336006\n",
      "[INFO] Epoch: 61 , batch: 473 , training loss: 4.264122\n",
      "[INFO] Epoch: 61 , batch: 474 , training loss: 4.044224\n",
      "[INFO] Epoch: 61 , batch: 475 , training loss: 3.932307\n",
      "[INFO] Epoch: 61 , batch: 476 , training loss: 4.313886\n",
      "[INFO] Epoch: 61 , batch: 477 , training loss: 4.441470\n",
      "[INFO] Epoch: 61 , batch: 478 , training loss: 4.427506\n",
      "[INFO] Epoch: 61 , batch: 479 , training loss: 4.424985\n",
      "[INFO] Epoch: 61 , batch: 480 , training loss: 4.549895\n",
      "[INFO] Epoch: 61 , batch: 481 , training loss: 4.411371\n",
      "[INFO] Epoch: 61 , batch: 482 , training loss: 4.535573\n",
      "[INFO] Epoch: 61 , batch: 483 , training loss: 4.350607\n",
      "[INFO] Epoch: 61 , batch: 484 , training loss: 4.173245\n",
      "[INFO] Epoch: 61 , batch: 485 , training loss: 4.256166\n",
      "[INFO] Epoch: 61 , batch: 486 , training loss: 4.163514\n",
      "[INFO] Epoch: 61 , batch: 487 , training loss: 4.141515\n",
      "[INFO] Epoch: 61 , batch: 488 , training loss: 4.327174\n",
      "[INFO] Epoch: 61 , batch: 489 , training loss: 4.232713\n",
      "[INFO] Epoch: 61 , batch: 490 , training loss: 4.306885\n",
      "[INFO] Epoch: 61 , batch: 491 , training loss: 4.209456\n",
      "[INFO] Epoch: 61 , batch: 492 , training loss: 4.196991\n",
      "[INFO] Epoch: 61 , batch: 493 , training loss: 4.350997\n",
      "[INFO] Epoch: 61 , batch: 494 , training loss: 4.270386\n",
      "[INFO] Epoch: 61 , batch: 495 , training loss: 4.447242\n",
      "[INFO] Epoch: 61 , batch: 496 , training loss: 4.294381\n",
      "[INFO] Epoch: 61 , batch: 497 , training loss: 4.330614\n",
      "[INFO] Epoch: 61 , batch: 498 , training loss: 4.319189\n",
      "[INFO] Epoch: 61 , batch: 499 , training loss: 4.379659\n",
      "[INFO] Epoch: 61 , batch: 500 , training loss: 4.519456\n",
      "[INFO] Epoch: 61 , batch: 501 , training loss: 4.858506\n",
      "[INFO] Epoch: 61 , batch: 502 , training loss: 4.882229\n",
      "[INFO] Epoch: 61 , batch: 503 , training loss: 4.487844\n",
      "[INFO] Epoch: 61 , batch: 504 , training loss: 4.625692\n",
      "[INFO] Epoch: 61 , batch: 505 , training loss: 4.615365\n",
      "[INFO] Epoch: 61 , batch: 506 , training loss: 4.586397\n",
      "[INFO] Epoch: 61 , batch: 507 , training loss: 4.636260\n",
      "[INFO] Epoch: 61 , batch: 508 , training loss: 4.567272\n",
      "[INFO] Epoch: 61 , batch: 509 , training loss: 4.386162\n",
      "[INFO] Epoch: 61 , batch: 510 , training loss: 4.466424\n",
      "[INFO] Epoch: 61 , batch: 511 , training loss: 4.384289\n",
      "[INFO] Epoch: 61 , batch: 512 , training loss: 4.487695\n",
      "[INFO] Epoch: 61 , batch: 513 , training loss: 4.755138\n",
      "[INFO] Epoch: 61 , batch: 514 , training loss: 4.388179\n",
      "[INFO] Epoch: 61 , batch: 515 , training loss: 4.639082\n",
      "[INFO] Epoch: 61 , batch: 516 , training loss: 4.424330\n",
      "[INFO] Epoch: 61 , batch: 517 , training loss: 4.406785\n",
      "[INFO] Epoch: 61 , batch: 518 , training loss: 4.363432\n",
      "[INFO] Epoch: 61 , batch: 519 , training loss: 4.188795\n",
      "[INFO] Epoch: 61 , batch: 520 , training loss: 4.440883\n",
      "[INFO] Epoch: 61 , batch: 521 , training loss: 4.419600\n",
      "[INFO] Epoch: 61 , batch: 522 , training loss: 4.497230\n",
      "[INFO] Epoch: 61 , batch: 523 , training loss: 4.427483\n",
      "[INFO] Epoch: 61 , batch: 524 , training loss: 4.729103\n",
      "[INFO] Epoch: 61 , batch: 525 , training loss: 4.593798\n",
      "[INFO] Epoch: 61 , batch: 526 , training loss: 4.364906\n",
      "[INFO] Epoch: 61 , batch: 527 , training loss: 4.408021\n",
      "[INFO] Epoch: 61 , batch: 528 , training loss: 4.425213\n",
      "[INFO] Epoch: 61 , batch: 529 , training loss: 4.387384\n",
      "[INFO] Epoch: 61 , batch: 530 , training loss: 4.262774\n",
      "[INFO] Epoch: 61 , batch: 531 , training loss: 4.398310\n",
      "[INFO] Epoch: 61 , batch: 532 , training loss: 4.328569\n",
      "[INFO] Epoch: 61 , batch: 533 , training loss: 4.420347\n",
      "[INFO] Epoch: 61 , batch: 534 , training loss: 4.448246\n",
      "[INFO] Epoch: 61 , batch: 535 , training loss: 4.449080\n",
      "[INFO] Epoch: 61 , batch: 536 , training loss: 4.307250\n",
      "[INFO] Epoch: 61 , batch: 537 , training loss: 4.300953\n",
      "[INFO] Epoch: 61 , batch: 538 , training loss: 4.369814\n",
      "[INFO] Epoch: 61 , batch: 539 , training loss: 4.457510\n",
      "[INFO] Epoch: 61 , batch: 540 , training loss: 4.930944\n",
      "[INFO] Epoch: 61 , batch: 541 , training loss: 4.788080\n",
      "[INFO] Epoch: 61 , batch: 542 , training loss: 4.688191\n",
      "[INFO] Epoch: 62 , batch: 0 , training loss: 3.547037\n",
      "[INFO] Epoch: 62 , batch: 1 , training loss: 3.459795\n",
      "[INFO] Epoch: 62 , batch: 2 , training loss: 3.707759\n",
      "[INFO] Epoch: 62 , batch: 3 , training loss: 3.585849\n",
      "[INFO] Epoch: 62 , batch: 4 , training loss: 3.940918\n",
      "[INFO] Epoch: 62 , batch: 5 , training loss: 3.581893\n",
      "[INFO] Epoch: 62 , batch: 6 , training loss: 3.947083\n",
      "[INFO] Epoch: 62 , batch: 7 , training loss: 3.926701\n",
      "[INFO] Epoch: 62 , batch: 8 , training loss: 3.570098\n",
      "[INFO] Epoch: 62 , batch: 9 , training loss: 3.823399\n",
      "[INFO] Epoch: 62 , batch: 10 , training loss: 3.805374\n",
      "[INFO] Epoch: 62 , batch: 11 , training loss: 3.729297\n",
      "[INFO] Epoch: 62 , batch: 12 , training loss: 3.651403\n",
      "[INFO] Epoch: 62 , batch: 13 , training loss: 3.650483\n",
      "[INFO] Epoch: 62 , batch: 14 , training loss: 3.574189\n",
      "[INFO] Epoch: 62 , batch: 15 , training loss: 3.791619\n",
      "[INFO] Epoch: 62 , batch: 16 , training loss: 3.635063\n",
      "[INFO] Epoch: 62 , batch: 17 , training loss: 3.740906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 62 , batch: 18 , training loss: 3.708352\n",
      "[INFO] Epoch: 62 , batch: 19 , training loss: 3.464626\n",
      "[INFO] Epoch: 62 , batch: 20 , training loss: 3.454640\n",
      "[INFO] Epoch: 62 , batch: 21 , training loss: 3.551657\n",
      "[INFO] Epoch: 62 , batch: 22 , training loss: 3.440908\n",
      "[INFO] Epoch: 62 , batch: 23 , training loss: 3.673489\n",
      "[INFO] Epoch: 62 , batch: 24 , training loss: 3.495963\n",
      "[INFO] Epoch: 62 , batch: 25 , training loss: 3.639874\n",
      "[INFO] Epoch: 62 , batch: 26 , training loss: 3.491053\n",
      "[INFO] Epoch: 62 , batch: 27 , training loss: 3.499727\n",
      "[INFO] Epoch: 62 , batch: 28 , training loss: 3.656248\n",
      "[INFO] Epoch: 62 , batch: 29 , training loss: 3.456081\n",
      "[INFO] Epoch: 62 , batch: 30 , training loss: 3.507309\n",
      "[INFO] Epoch: 62 , batch: 31 , training loss: 3.608013\n",
      "[INFO] Epoch: 62 , batch: 32 , training loss: 3.553616\n",
      "[INFO] Epoch: 62 , batch: 33 , training loss: 3.606780\n",
      "[INFO] Epoch: 62 , batch: 34 , training loss: 3.600342\n",
      "[INFO] Epoch: 62 , batch: 35 , training loss: 3.554262\n",
      "[INFO] Epoch: 62 , batch: 36 , training loss: 3.638609\n",
      "[INFO] Epoch: 62 , batch: 37 , training loss: 3.482451\n",
      "[INFO] Epoch: 62 , batch: 38 , training loss: 3.560014\n",
      "[INFO] Epoch: 62 , batch: 39 , training loss: 3.395431\n",
      "[INFO] Epoch: 62 , batch: 40 , training loss: 3.588777\n",
      "[INFO] Epoch: 62 , batch: 41 , training loss: 3.568329\n",
      "[INFO] Epoch: 62 , batch: 42 , training loss: 3.966101\n",
      "[INFO] Epoch: 62 , batch: 43 , training loss: 3.777762\n",
      "[INFO] Epoch: 62 , batch: 44 , training loss: 4.078463\n",
      "[INFO] Epoch: 62 , batch: 45 , training loss: 4.037893\n",
      "[INFO] Epoch: 62 , batch: 46 , training loss: 3.999784\n",
      "[INFO] Epoch: 62 , batch: 47 , training loss: 3.577668\n",
      "[INFO] Epoch: 62 , batch: 48 , training loss: 3.584001\n",
      "[INFO] Epoch: 62 , batch: 49 , training loss: 3.840898\n",
      "[INFO] Epoch: 62 , batch: 50 , training loss: 3.598081\n",
      "[INFO] Epoch: 62 , batch: 51 , training loss: 3.827034\n",
      "[INFO] Epoch: 62 , batch: 52 , training loss: 3.592496\n",
      "[INFO] Epoch: 62 , batch: 53 , training loss: 3.731779\n",
      "[INFO] Epoch: 62 , batch: 54 , training loss: 3.737308\n",
      "[INFO] Epoch: 62 , batch: 55 , training loss: 3.794314\n",
      "[INFO] Epoch: 62 , batch: 56 , training loss: 3.656972\n",
      "[INFO] Epoch: 62 , batch: 57 , training loss: 3.580472\n",
      "[INFO] Epoch: 62 , batch: 58 , training loss: 3.631796\n",
      "[INFO] Epoch: 62 , batch: 59 , training loss: 3.676702\n",
      "[INFO] Epoch: 62 , batch: 60 , training loss: 3.631819\n",
      "[INFO] Epoch: 62 , batch: 61 , training loss: 3.760399\n",
      "[INFO] Epoch: 62 , batch: 62 , training loss: 3.614362\n",
      "[INFO] Epoch: 62 , batch: 63 , training loss: 3.823568\n",
      "[INFO] Epoch: 62 , batch: 64 , training loss: 3.995073\n",
      "[INFO] Epoch: 62 , batch: 65 , training loss: 3.725539\n",
      "[INFO] Epoch: 62 , batch: 66 , training loss: 3.588796\n",
      "[INFO] Epoch: 62 , batch: 67 , training loss: 3.627208\n",
      "[INFO] Epoch: 62 , batch: 68 , training loss: 3.770645\n",
      "[INFO] Epoch: 62 , batch: 69 , training loss: 3.692036\n",
      "[INFO] Epoch: 62 , batch: 70 , training loss: 3.896696\n",
      "[INFO] Epoch: 62 , batch: 71 , training loss: 3.732526\n",
      "[INFO] Epoch: 62 , batch: 72 , training loss: 3.799543\n",
      "[INFO] Epoch: 62 , batch: 73 , training loss: 3.795692\n",
      "[INFO] Epoch: 62 , batch: 74 , training loss: 3.849236\n",
      "[INFO] Epoch: 62 , batch: 75 , training loss: 3.748226\n",
      "[INFO] Epoch: 62 , batch: 76 , training loss: 3.849256\n",
      "[INFO] Epoch: 62 , batch: 77 , training loss: 3.769381\n",
      "[INFO] Epoch: 62 , batch: 78 , training loss: 3.877310\n",
      "[INFO] Epoch: 62 , batch: 79 , training loss: 3.744670\n",
      "[INFO] Epoch: 62 , batch: 80 , training loss: 3.914577\n",
      "[INFO] Epoch: 62 , batch: 81 , training loss: 3.850748\n",
      "[INFO] Epoch: 62 , batch: 82 , training loss: 3.813406\n",
      "[INFO] Epoch: 62 , batch: 83 , training loss: 3.889426\n",
      "[INFO] Epoch: 62 , batch: 84 , training loss: 3.900423\n",
      "[INFO] Epoch: 62 , batch: 85 , training loss: 3.958124\n",
      "[INFO] Epoch: 62 , batch: 86 , training loss: 3.880666\n",
      "[INFO] Epoch: 62 , batch: 87 , training loss: 3.858879\n",
      "[INFO] Epoch: 62 , batch: 88 , training loss: 4.020133\n",
      "[INFO] Epoch: 62 , batch: 89 , training loss: 3.778462\n",
      "[INFO] Epoch: 62 , batch: 90 , training loss: 3.856123\n",
      "[INFO] Epoch: 62 , batch: 91 , training loss: 3.808654\n",
      "[INFO] Epoch: 62 , batch: 92 , training loss: 3.837922\n",
      "[INFO] Epoch: 62 , batch: 93 , training loss: 3.941483\n",
      "[INFO] Epoch: 62 , batch: 94 , training loss: 4.037972\n",
      "[INFO] Epoch: 62 , batch: 95 , training loss: 3.807242\n",
      "[INFO] Epoch: 62 , batch: 96 , training loss: 3.831897\n",
      "[INFO] Epoch: 62 , batch: 97 , training loss: 3.756041\n",
      "[INFO] Epoch: 62 , batch: 98 , training loss: 3.720064\n",
      "[INFO] Epoch: 62 , batch: 99 , training loss: 3.841672\n",
      "[INFO] Epoch: 62 , batch: 100 , training loss: 3.751540\n",
      "[INFO] Epoch: 62 , batch: 101 , training loss: 3.743959\n",
      "[INFO] Epoch: 62 , batch: 102 , training loss: 3.874669\n",
      "[INFO] Epoch: 62 , batch: 103 , training loss: 3.685135\n",
      "[INFO] Epoch: 62 , batch: 104 , training loss: 3.652323\n",
      "[INFO] Epoch: 62 , batch: 105 , training loss: 3.889375\n",
      "[INFO] Epoch: 62 , batch: 106 , training loss: 3.942573\n",
      "[INFO] Epoch: 62 , batch: 107 , training loss: 3.782288\n",
      "[INFO] Epoch: 62 , batch: 108 , training loss: 3.701959\n",
      "[INFO] Epoch: 62 , batch: 109 , training loss: 3.629801\n",
      "[INFO] Epoch: 62 , batch: 110 , training loss: 3.827956\n",
      "[INFO] Epoch: 62 , batch: 111 , training loss: 3.916853\n",
      "[INFO] Epoch: 62 , batch: 112 , training loss: 3.801912\n",
      "[INFO] Epoch: 62 , batch: 113 , training loss: 3.769983\n",
      "[INFO] Epoch: 62 , batch: 114 , training loss: 3.787645\n",
      "[INFO] Epoch: 62 , batch: 115 , training loss: 3.819829\n",
      "[INFO] Epoch: 62 , batch: 116 , training loss: 3.704467\n",
      "[INFO] Epoch: 62 , batch: 117 , training loss: 3.918839\n",
      "[INFO] Epoch: 62 , batch: 118 , training loss: 3.928099\n",
      "[INFO] Epoch: 62 , batch: 119 , training loss: 4.040548\n",
      "[INFO] Epoch: 62 , batch: 120 , training loss: 4.004774\n",
      "[INFO] Epoch: 62 , batch: 121 , training loss: 3.895119\n",
      "[INFO] Epoch: 62 , batch: 122 , training loss: 3.773064\n",
      "[INFO] Epoch: 62 , batch: 123 , training loss: 3.762966\n",
      "[INFO] Epoch: 62 , batch: 124 , training loss: 3.880668\n",
      "[INFO] Epoch: 62 , batch: 125 , training loss: 3.708667\n",
      "[INFO] Epoch: 62 , batch: 126 , training loss: 3.711409\n",
      "[INFO] Epoch: 62 , batch: 127 , training loss: 3.717992\n",
      "[INFO] Epoch: 62 , batch: 128 , training loss: 3.839535\n",
      "[INFO] Epoch: 62 , batch: 129 , training loss: 3.793689\n",
      "[INFO] Epoch: 62 , batch: 130 , training loss: 3.820646\n",
      "[INFO] Epoch: 62 , batch: 131 , training loss: 3.824945\n",
      "[INFO] Epoch: 62 , batch: 132 , training loss: 3.835189\n",
      "[INFO] Epoch: 62 , batch: 133 , training loss: 3.803561\n",
      "[INFO] Epoch: 62 , batch: 134 , training loss: 3.592461\n",
      "[INFO] Epoch: 62 , batch: 135 , training loss: 3.645737\n",
      "[INFO] Epoch: 62 , batch: 136 , training loss: 3.900798\n",
      "[INFO] Epoch: 62 , batch: 137 , training loss: 3.843679\n",
      "[INFO] Epoch: 62 , batch: 138 , training loss: 3.871248\n",
      "[INFO] Epoch: 62 , batch: 139 , training loss: 4.431766\n",
      "[INFO] Epoch: 62 , batch: 140 , training loss: 4.215549\n",
      "[INFO] Epoch: 62 , batch: 141 , training loss: 4.037356\n",
      "[INFO] Epoch: 62 , batch: 142 , training loss: 3.741617\n",
      "[INFO] Epoch: 62 , batch: 143 , training loss: 3.889102\n",
      "[INFO] Epoch: 62 , batch: 144 , training loss: 3.715359\n",
      "[INFO] Epoch: 62 , batch: 145 , training loss: 3.770418\n",
      "[INFO] Epoch: 62 , batch: 146 , training loss: 4.007492\n",
      "[INFO] Epoch: 62 , batch: 147 , training loss: 3.656202\n",
      "[INFO] Epoch: 62 , batch: 148 , training loss: 3.590813\n",
      "[INFO] Epoch: 62 , batch: 149 , training loss: 3.719399\n",
      "[INFO] Epoch: 62 , batch: 150 , training loss: 3.951782\n",
      "[INFO] Epoch: 62 , batch: 151 , training loss: 3.834201\n",
      "[INFO] Epoch: 62 , batch: 152 , training loss: 3.832531\n",
      "[INFO] Epoch: 62 , batch: 153 , training loss: 3.880833\n",
      "[INFO] Epoch: 62 , batch: 154 , training loss: 3.965720\n",
      "[INFO] Epoch: 62 , batch: 155 , training loss: 4.145752\n",
      "[INFO] Epoch: 62 , batch: 156 , training loss: 3.898447\n",
      "[INFO] Epoch: 62 , batch: 157 , training loss: 3.851354\n",
      "[INFO] Epoch: 62 , batch: 158 , training loss: 3.968841\n",
      "[INFO] Epoch: 62 , batch: 159 , training loss: 3.904191\n",
      "[INFO] Epoch: 62 , batch: 160 , training loss: 4.085386\n",
      "[INFO] Epoch: 62 , batch: 161 , training loss: 4.142249\n",
      "[INFO] Epoch: 62 , batch: 162 , training loss: 4.142475\n",
      "[INFO] Epoch: 62 , batch: 163 , training loss: 4.320566\n",
      "[INFO] Epoch: 62 , batch: 164 , training loss: 4.255751\n",
      "[INFO] Epoch: 62 , batch: 165 , training loss: 4.174053\n",
      "[INFO] Epoch: 62 , batch: 166 , training loss: 4.108739\n",
      "[INFO] Epoch: 62 , batch: 167 , training loss: 4.105970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 62 , batch: 168 , training loss: 3.792985\n",
      "[INFO] Epoch: 62 , batch: 169 , training loss: 3.777747\n",
      "[INFO] Epoch: 62 , batch: 170 , training loss: 3.974119\n",
      "[INFO] Epoch: 62 , batch: 171 , training loss: 3.478383\n",
      "[INFO] Epoch: 62 , batch: 172 , training loss: 3.675446\n",
      "[INFO] Epoch: 62 , batch: 173 , training loss: 3.984481\n",
      "[INFO] Epoch: 62 , batch: 174 , training loss: 4.422431\n",
      "[INFO] Epoch: 62 , batch: 175 , training loss: 4.713967\n",
      "[INFO] Epoch: 62 , batch: 176 , training loss: 4.332784\n",
      "[INFO] Epoch: 62 , batch: 177 , training loss: 3.972530\n",
      "[INFO] Epoch: 62 , batch: 178 , training loss: 3.986824\n",
      "[INFO] Epoch: 62 , batch: 179 , training loss: 4.021765\n",
      "[INFO] Epoch: 62 , batch: 180 , training loss: 3.980441\n",
      "[INFO] Epoch: 62 , batch: 181 , training loss: 4.260456\n",
      "[INFO] Epoch: 62 , batch: 182 , training loss: 4.223433\n",
      "[INFO] Epoch: 62 , batch: 183 , training loss: 4.196985\n",
      "[INFO] Epoch: 62 , batch: 184 , training loss: 4.100603\n",
      "[INFO] Epoch: 62 , batch: 185 , training loss: 4.046697\n",
      "[INFO] Epoch: 62 , batch: 186 , training loss: 4.210174\n",
      "[INFO] Epoch: 62 , batch: 187 , training loss: 4.291553\n",
      "[INFO] Epoch: 62 , batch: 188 , training loss: 4.267236\n",
      "[INFO] Epoch: 62 , batch: 189 , training loss: 4.195326\n",
      "[INFO] Epoch: 62 , batch: 190 , training loss: 4.231615\n",
      "[INFO] Epoch: 62 , batch: 191 , training loss: 4.327616\n",
      "[INFO] Epoch: 62 , batch: 192 , training loss: 4.207911\n",
      "[INFO] Epoch: 62 , batch: 193 , training loss: 4.287836\n",
      "[INFO] Epoch: 62 , batch: 194 , training loss: 4.229262\n",
      "[INFO] Epoch: 62 , batch: 195 , training loss: 4.166270\n",
      "[INFO] Epoch: 62 , batch: 196 , training loss: 4.008778\n",
      "[INFO] Epoch: 62 , batch: 197 , training loss: 4.087421\n",
      "[INFO] Epoch: 62 , batch: 198 , training loss: 4.039846\n",
      "[INFO] Epoch: 62 , batch: 199 , training loss: 4.169436\n",
      "[INFO] Epoch: 62 , batch: 200 , training loss: 4.062878\n",
      "[INFO] Epoch: 62 , batch: 201 , training loss: 3.969974\n",
      "[INFO] Epoch: 62 , batch: 202 , training loss: 3.962130\n",
      "[INFO] Epoch: 62 , batch: 203 , training loss: 4.108401\n",
      "[INFO] Epoch: 62 , batch: 204 , training loss: 4.151485\n",
      "[INFO] Epoch: 62 , batch: 205 , training loss: 3.810444\n",
      "[INFO] Epoch: 62 , batch: 206 , training loss: 3.745937\n",
      "[INFO] Epoch: 62 , batch: 207 , training loss: 3.718782\n",
      "[INFO] Epoch: 62 , batch: 208 , training loss: 4.025049\n",
      "[INFO] Epoch: 62 , batch: 209 , training loss: 4.018051\n",
      "[INFO] Epoch: 62 , batch: 210 , training loss: 4.025388\n",
      "[INFO] Epoch: 62 , batch: 211 , training loss: 4.030892\n",
      "[INFO] Epoch: 62 , batch: 212 , training loss: 4.109848\n",
      "[INFO] Epoch: 62 , batch: 213 , training loss: 4.059937\n",
      "[INFO] Epoch: 62 , batch: 214 , training loss: 4.158358\n",
      "[INFO] Epoch: 62 , batch: 215 , training loss: 4.350500\n",
      "[INFO] Epoch: 62 , batch: 216 , training loss: 4.082182\n",
      "[INFO] Epoch: 62 , batch: 217 , training loss: 4.004868\n",
      "[INFO] Epoch: 62 , batch: 218 , training loss: 3.993937\n",
      "[INFO] Epoch: 62 , batch: 219 , training loss: 4.089836\n",
      "[INFO] Epoch: 62 , batch: 220 , training loss: 3.937098\n",
      "[INFO] Epoch: 62 , batch: 221 , training loss: 3.932245\n",
      "[INFO] Epoch: 62 , batch: 222 , training loss: 4.066818\n",
      "[INFO] Epoch: 62 , batch: 223 , training loss: 4.197206\n",
      "[INFO] Epoch: 62 , batch: 224 , training loss: 4.213996\n",
      "[INFO] Epoch: 62 , batch: 225 , training loss: 4.123466\n",
      "[INFO] Epoch: 62 , batch: 226 , training loss: 4.226043\n",
      "[INFO] Epoch: 62 , batch: 227 , training loss: 4.217823\n",
      "[INFO] Epoch: 62 , batch: 228 , training loss: 4.240282\n",
      "[INFO] Epoch: 62 , batch: 229 , training loss: 4.090446\n",
      "[INFO] Epoch: 62 , batch: 230 , training loss: 3.951279\n",
      "[INFO] Epoch: 62 , batch: 231 , training loss: 3.834624\n",
      "[INFO] Epoch: 62 , batch: 232 , training loss: 3.980650\n",
      "[INFO] Epoch: 62 , batch: 233 , training loss: 4.000641\n",
      "[INFO] Epoch: 62 , batch: 234 , training loss: 3.689414\n",
      "[INFO] Epoch: 62 , batch: 235 , training loss: 3.799525\n",
      "[INFO] Epoch: 62 , batch: 236 , training loss: 3.891643\n",
      "[INFO] Epoch: 62 , batch: 237 , training loss: 4.107833\n",
      "[INFO] Epoch: 62 , batch: 238 , training loss: 3.901357\n",
      "[INFO] Epoch: 62 , batch: 239 , training loss: 3.929626\n",
      "[INFO] Epoch: 62 , batch: 240 , training loss: 3.981945\n",
      "[INFO] Epoch: 62 , batch: 241 , training loss: 3.791279\n",
      "[INFO] Epoch: 62 , batch: 242 , training loss: 3.815962\n",
      "[INFO] Epoch: 62 , batch: 243 , training loss: 4.110096\n",
      "[INFO] Epoch: 62 , batch: 244 , training loss: 4.050600\n",
      "[INFO] Epoch: 62 , batch: 245 , training loss: 3.992889\n",
      "[INFO] Epoch: 62 , batch: 246 , training loss: 3.710406\n",
      "[INFO] Epoch: 62 , batch: 247 , training loss: 3.890162\n",
      "[INFO] Epoch: 62 , batch: 248 , training loss: 3.953852\n",
      "[INFO] Epoch: 62 , batch: 249 , training loss: 3.938745\n",
      "[INFO] Epoch: 62 , batch: 250 , training loss: 3.747804\n",
      "[INFO] Epoch: 62 , batch: 251 , training loss: 4.177526\n",
      "[INFO] Epoch: 62 , batch: 252 , training loss: 3.918907\n",
      "[INFO] Epoch: 62 , batch: 253 , training loss: 3.795219\n",
      "[INFO] Epoch: 62 , batch: 254 , training loss: 4.071594\n",
      "[INFO] Epoch: 62 , batch: 255 , training loss: 4.057042\n",
      "[INFO] Epoch: 62 , batch: 256 , training loss: 4.034572\n",
      "[INFO] Epoch: 62 , batch: 257 , training loss: 4.214808\n",
      "[INFO] Epoch: 62 , batch: 258 , training loss: 4.202668\n",
      "[INFO] Epoch: 62 , batch: 259 , training loss: 4.235799\n",
      "[INFO] Epoch: 62 , batch: 260 , training loss: 4.018040\n",
      "[INFO] Epoch: 62 , batch: 261 , training loss: 4.218072\n",
      "[INFO] Epoch: 62 , batch: 262 , training loss: 4.325066\n",
      "[INFO] Epoch: 62 , batch: 263 , training loss: 4.507584\n",
      "[INFO] Epoch: 62 , batch: 264 , training loss: 3.878840\n",
      "[INFO] Epoch: 62 , batch: 265 , training loss: 3.960232\n",
      "[INFO] Epoch: 62 , batch: 266 , training loss: 4.367812\n",
      "[INFO] Epoch: 62 , batch: 267 , training loss: 4.123670\n",
      "[INFO] Epoch: 62 , batch: 268 , training loss: 4.050383\n",
      "[INFO] Epoch: 62 , batch: 269 , training loss: 4.022205\n",
      "[INFO] Epoch: 62 , batch: 270 , training loss: 4.053504\n",
      "[INFO] Epoch: 62 , batch: 271 , training loss: 4.076272\n",
      "[INFO] Epoch: 62 , batch: 272 , training loss: 4.078179\n",
      "[INFO] Epoch: 62 , batch: 273 , training loss: 4.077005\n",
      "[INFO] Epoch: 62 , batch: 274 , training loss: 4.183359\n",
      "[INFO] Epoch: 62 , batch: 275 , training loss: 4.043341\n",
      "[INFO] Epoch: 62 , batch: 276 , training loss: 4.099460\n",
      "[INFO] Epoch: 62 , batch: 277 , training loss: 4.266229\n",
      "[INFO] Epoch: 62 , batch: 278 , training loss: 3.962639\n",
      "[INFO] Epoch: 62 , batch: 279 , training loss: 3.942209\n",
      "[INFO] Epoch: 62 , batch: 280 , training loss: 3.946572\n",
      "[INFO] Epoch: 62 , batch: 281 , training loss: 4.066453\n",
      "[INFO] Epoch: 62 , batch: 282 , training loss: 4.005485\n",
      "[INFO] Epoch: 62 , batch: 283 , training loss: 3.983170\n",
      "[INFO] Epoch: 62 , batch: 284 , training loss: 4.006527\n",
      "[INFO] Epoch: 62 , batch: 285 , training loss: 3.949997\n",
      "[INFO] Epoch: 62 , batch: 286 , training loss: 3.953233\n",
      "[INFO] Epoch: 62 , batch: 287 , training loss: 3.910880\n",
      "[INFO] Epoch: 62 , batch: 288 , training loss: 3.861656\n",
      "[INFO] Epoch: 62 , batch: 289 , training loss: 3.947888\n",
      "[INFO] Epoch: 62 , batch: 290 , training loss: 3.727282\n",
      "[INFO] Epoch: 62 , batch: 291 , training loss: 3.722341\n",
      "[INFO] Epoch: 62 , batch: 292 , training loss: 3.819688\n",
      "[INFO] Epoch: 62 , batch: 293 , training loss: 3.739030\n",
      "[INFO] Epoch: 62 , batch: 294 , training loss: 4.396186\n",
      "[INFO] Epoch: 62 , batch: 295 , training loss: 4.164120\n",
      "[INFO] Epoch: 62 , batch: 296 , training loss: 4.102501\n",
      "[INFO] Epoch: 62 , batch: 297 , training loss: 4.065002\n",
      "[INFO] Epoch: 62 , batch: 298 , training loss: 3.907406\n",
      "[INFO] Epoch: 62 , batch: 299 , training loss: 3.946898\n",
      "[INFO] Epoch: 62 , batch: 300 , training loss: 3.948408\n",
      "[INFO] Epoch: 62 , batch: 301 , training loss: 3.857620\n",
      "[INFO] Epoch: 62 , batch: 302 , training loss: 4.034640\n",
      "[INFO] Epoch: 62 , batch: 303 , training loss: 4.029016\n",
      "[INFO] Epoch: 62 , batch: 304 , training loss: 4.166235\n",
      "[INFO] Epoch: 62 , batch: 305 , training loss: 4.015715\n",
      "[INFO] Epoch: 62 , batch: 306 , training loss: 4.110816\n",
      "[INFO] Epoch: 62 , batch: 307 , training loss: 4.138085\n",
      "[INFO] Epoch: 62 , batch: 308 , training loss: 3.944474\n",
      "[INFO] Epoch: 62 , batch: 309 , training loss: 3.931645\n",
      "[INFO] Epoch: 62 , batch: 310 , training loss: 3.883292\n",
      "[INFO] Epoch: 62 , batch: 311 , training loss: 3.880359\n",
      "[INFO] Epoch: 62 , batch: 312 , training loss: 3.767775\n",
      "[INFO] Epoch: 62 , batch: 313 , training loss: 3.866413\n",
      "[INFO] Epoch: 62 , batch: 314 , training loss: 3.950412\n",
      "[INFO] Epoch: 62 , batch: 315 , training loss: 4.026996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 62 , batch: 316 , training loss: 4.252702\n",
      "[INFO] Epoch: 62 , batch: 317 , training loss: 4.591269\n",
      "[INFO] Epoch: 62 , batch: 318 , training loss: 4.707379\n",
      "[INFO] Epoch: 62 , batch: 319 , training loss: 4.402225\n",
      "[INFO] Epoch: 62 , batch: 320 , training loss: 3.970767\n",
      "[INFO] Epoch: 62 , batch: 321 , training loss: 3.800750\n",
      "[INFO] Epoch: 62 , batch: 322 , training loss: 3.896336\n",
      "[INFO] Epoch: 62 , batch: 323 , training loss: 3.931018\n",
      "[INFO] Epoch: 62 , batch: 324 , training loss: 3.911171\n",
      "[INFO] Epoch: 62 , batch: 325 , training loss: 4.014464\n",
      "[INFO] Epoch: 62 , batch: 326 , training loss: 4.079282\n",
      "[INFO] Epoch: 62 , batch: 327 , training loss: 4.031122\n",
      "[INFO] Epoch: 62 , batch: 328 , training loss: 4.041623\n",
      "[INFO] Epoch: 62 , batch: 329 , training loss: 3.930639\n",
      "[INFO] Epoch: 62 , batch: 330 , training loss: 3.935139\n",
      "[INFO] Epoch: 62 , batch: 331 , training loss: 4.052151\n",
      "[INFO] Epoch: 62 , batch: 332 , training loss: 3.924459\n",
      "[INFO] Epoch: 62 , batch: 333 , training loss: 3.909385\n",
      "[INFO] Epoch: 62 , batch: 334 , training loss: 3.936010\n",
      "[INFO] Epoch: 62 , batch: 335 , training loss: 4.046104\n",
      "[INFO] Epoch: 62 , batch: 336 , training loss: 4.055648\n",
      "[INFO] Epoch: 62 , batch: 337 , training loss: 4.099580\n",
      "[INFO] Epoch: 62 , batch: 338 , training loss: 4.303488\n",
      "[INFO] Epoch: 62 , batch: 339 , training loss: 4.117810\n",
      "[INFO] Epoch: 62 , batch: 340 , training loss: 4.322260\n",
      "[INFO] Epoch: 62 , batch: 341 , training loss: 4.057733\n",
      "[INFO] Epoch: 62 , batch: 342 , training loss: 3.866993\n",
      "[INFO] Epoch: 62 , batch: 343 , training loss: 3.928826\n",
      "[INFO] Epoch: 62 , batch: 344 , training loss: 3.793368\n",
      "[INFO] Epoch: 62 , batch: 345 , training loss: 3.905194\n",
      "[INFO] Epoch: 62 , batch: 346 , training loss: 3.962651\n",
      "[INFO] Epoch: 62 , batch: 347 , training loss: 3.871763\n",
      "[INFO] Epoch: 62 , batch: 348 , training loss: 3.979426\n",
      "[INFO] Epoch: 62 , batch: 349 , training loss: 4.048724\n",
      "[INFO] Epoch: 62 , batch: 350 , training loss: 3.943648\n",
      "[INFO] Epoch: 62 , batch: 351 , training loss: 4.006455\n",
      "[INFO] Epoch: 62 , batch: 352 , training loss: 4.034037\n",
      "[INFO] Epoch: 62 , batch: 353 , training loss: 4.010377\n",
      "[INFO] Epoch: 62 , batch: 354 , training loss: 4.082358\n",
      "[INFO] Epoch: 62 , batch: 355 , training loss: 4.102860\n",
      "[INFO] Epoch: 62 , batch: 356 , training loss: 3.950746\n",
      "[INFO] Epoch: 62 , batch: 357 , training loss: 4.015849\n",
      "[INFO] Epoch: 62 , batch: 358 , training loss: 3.931140\n",
      "[INFO] Epoch: 62 , batch: 359 , training loss: 3.942364\n",
      "[INFO] Epoch: 62 , batch: 360 , training loss: 4.038042\n",
      "[INFO] Epoch: 62 , batch: 361 , training loss: 4.003566\n",
      "[INFO] Epoch: 62 , batch: 362 , training loss: 4.124520\n",
      "[INFO] Epoch: 62 , batch: 363 , training loss: 4.001007\n",
      "[INFO] Epoch: 62 , batch: 364 , training loss: 4.067638\n",
      "[INFO] Epoch: 62 , batch: 365 , training loss: 3.986552\n",
      "[INFO] Epoch: 62 , batch: 366 , training loss: 4.083754\n",
      "[INFO] Epoch: 62 , batch: 367 , training loss: 4.109572\n",
      "[INFO] Epoch: 62 , batch: 368 , training loss: 4.525451\n",
      "[INFO] Epoch: 62 , batch: 369 , training loss: 4.175583\n",
      "[INFO] Epoch: 62 , batch: 370 , training loss: 3.954195\n",
      "[INFO] Epoch: 62 , batch: 371 , training loss: 4.362526\n",
      "[INFO] Epoch: 62 , batch: 372 , training loss: 4.635647\n",
      "[INFO] Epoch: 62 , batch: 373 , training loss: 4.645891\n",
      "[INFO] Epoch: 62 , batch: 374 , training loss: 4.773539\n",
      "[INFO] Epoch: 62 , batch: 375 , training loss: 4.799376\n",
      "[INFO] Epoch: 62 , batch: 376 , training loss: 4.657857\n",
      "[INFO] Epoch: 62 , batch: 377 , training loss: 4.408736\n",
      "[INFO] Epoch: 62 , batch: 378 , training loss: 4.529651\n",
      "[INFO] Epoch: 62 , batch: 379 , training loss: 4.500432\n",
      "[INFO] Epoch: 62 , batch: 380 , training loss: 4.677584\n",
      "[INFO] Epoch: 62 , batch: 381 , training loss: 4.365943\n",
      "[INFO] Epoch: 62 , batch: 382 , training loss: 4.594101\n",
      "[INFO] Epoch: 62 , batch: 383 , training loss: 4.634571\n",
      "[INFO] Epoch: 62 , batch: 384 , training loss: 4.611511\n",
      "[INFO] Epoch: 62 , batch: 385 , training loss: 4.299857\n",
      "[INFO] Epoch: 62 , batch: 386 , training loss: 4.535014\n",
      "[INFO] Epoch: 62 , batch: 387 , training loss: 4.476041\n",
      "[INFO] Epoch: 62 , batch: 388 , training loss: 4.305441\n",
      "[INFO] Epoch: 62 , batch: 389 , training loss: 4.145498\n",
      "[INFO] Epoch: 62 , batch: 390 , training loss: 4.170574\n",
      "[INFO] Epoch: 62 , batch: 391 , training loss: 4.189412\n",
      "[INFO] Epoch: 62 , batch: 392 , training loss: 4.545753\n",
      "[INFO] Epoch: 62 , batch: 393 , training loss: 4.435438\n",
      "[INFO] Epoch: 62 , batch: 394 , training loss: 4.567430\n",
      "[INFO] Epoch: 62 , batch: 395 , training loss: 4.351383\n",
      "[INFO] Epoch: 62 , batch: 396 , training loss: 4.182010\n",
      "[INFO] Epoch: 62 , batch: 397 , training loss: 4.326342\n",
      "[INFO] Epoch: 62 , batch: 398 , training loss: 4.180215\n",
      "[INFO] Epoch: 62 , batch: 399 , training loss: 4.250688\n",
      "[INFO] Epoch: 62 , batch: 400 , training loss: 4.231558\n",
      "[INFO] Epoch: 62 , batch: 401 , training loss: 4.656698\n",
      "[INFO] Epoch: 62 , batch: 402 , training loss: 4.374976\n",
      "[INFO] Epoch: 62 , batch: 403 , training loss: 4.232778\n",
      "[INFO] Epoch: 62 , batch: 404 , training loss: 4.403299\n",
      "[INFO] Epoch: 62 , batch: 405 , training loss: 4.448065\n",
      "[INFO] Epoch: 62 , batch: 406 , training loss: 4.368242\n",
      "[INFO] Epoch: 62 , batch: 407 , training loss: 4.375948\n",
      "[INFO] Epoch: 62 , batch: 408 , training loss: 4.366819\n",
      "[INFO] Epoch: 62 , batch: 409 , training loss: 4.370502\n",
      "[INFO] Epoch: 62 , batch: 410 , training loss: 4.421147\n",
      "[INFO] Epoch: 62 , batch: 411 , training loss: 4.607456\n",
      "[INFO] Epoch: 62 , batch: 412 , training loss: 4.434599\n",
      "[INFO] Epoch: 62 , batch: 413 , training loss: 4.286474\n",
      "[INFO] Epoch: 62 , batch: 414 , training loss: 4.332049\n",
      "[INFO] Epoch: 62 , batch: 415 , training loss: 4.400276\n",
      "[INFO] Epoch: 62 , batch: 416 , training loss: 4.449933\n",
      "[INFO] Epoch: 62 , batch: 417 , training loss: 4.378388\n",
      "[INFO] Epoch: 62 , batch: 418 , training loss: 4.440257\n",
      "[INFO] Epoch: 62 , batch: 419 , training loss: 4.377775\n",
      "[INFO] Epoch: 62 , batch: 420 , training loss: 4.361564\n",
      "[INFO] Epoch: 62 , batch: 421 , training loss: 4.361332\n",
      "[INFO] Epoch: 62 , batch: 422 , training loss: 4.186488\n",
      "[INFO] Epoch: 62 , batch: 423 , training loss: 4.417811\n",
      "[INFO] Epoch: 62 , batch: 424 , training loss: 4.586507\n",
      "[INFO] Epoch: 62 , batch: 425 , training loss: 4.453767\n",
      "[INFO] Epoch: 62 , batch: 426 , training loss: 4.195583\n",
      "[INFO] Epoch: 62 , batch: 427 , training loss: 4.410146\n",
      "[INFO] Epoch: 62 , batch: 428 , training loss: 4.293291\n",
      "[INFO] Epoch: 62 , batch: 429 , training loss: 4.211279\n",
      "[INFO] Epoch: 62 , batch: 430 , training loss: 4.429120\n",
      "[INFO] Epoch: 62 , batch: 431 , training loss: 4.038932\n",
      "[INFO] Epoch: 62 , batch: 432 , training loss: 4.095026\n",
      "[INFO] Epoch: 62 , batch: 433 , training loss: 4.146184\n",
      "[INFO] Epoch: 62 , batch: 434 , training loss: 4.018345\n",
      "[INFO] Epoch: 62 , batch: 435 , training loss: 4.392520\n",
      "[INFO] Epoch: 62 , batch: 436 , training loss: 4.408373\n",
      "[INFO] Epoch: 62 , batch: 437 , training loss: 4.205504\n",
      "[INFO] Epoch: 62 , batch: 438 , training loss: 4.076540\n",
      "[INFO] Epoch: 62 , batch: 439 , training loss: 4.285509\n",
      "[INFO] Epoch: 62 , batch: 440 , training loss: 4.420092\n",
      "[INFO] Epoch: 62 , batch: 441 , training loss: 4.508719\n",
      "[INFO] Epoch: 62 , batch: 442 , training loss: 4.263030\n",
      "[INFO] Epoch: 62 , batch: 443 , training loss: 4.429981\n",
      "[INFO] Epoch: 62 , batch: 444 , training loss: 4.064334\n",
      "[INFO] Epoch: 62 , batch: 445 , training loss: 3.963168\n",
      "[INFO] Epoch: 62 , batch: 446 , training loss: 3.900961\n",
      "[INFO] Epoch: 62 , batch: 447 , training loss: 4.096502\n",
      "[INFO] Epoch: 62 , batch: 448 , training loss: 4.193229\n",
      "[INFO] Epoch: 62 , batch: 449 , training loss: 4.602790\n",
      "[INFO] Epoch: 62 , batch: 450 , training loss: 4.663342\n",
      "[INFO] Epoch: 62 , batch: 451 , training loss: 4.555713\n",
      "[INFO] Epoch: 62 , batch: 452 , training loss: 4.376366\n",
      "[INFO] Epoch: 62 , batch: 453 , training loss: 4.145693\n",
      "[INFO] Epoch: 62 , batch: 454 , training loss: 4.292501\n",
      "[INFO] Epoch: 62 , batch: 455 , training loss: 4.315311\n",
      "[INFO] Epoch: 62 , batch: 456 , training loss: 4.350122\n",
      "[INFO] Epoch: 62 , batch: 457 , training loss: 4.417585\n",
      "[INFO] Epoch: 62 , batch: 458 , training loss: 4.155556\n",
      "[INFO] Epoch: 62 , batch: 459 , training loss: 4.125473\n",
      "[INFO] Epoch: 62 , batch: 460 , training loss: 4.273117\n",
      "[INFO] Epoch: 62 , batch: 461 , training loss: 4.192951\n",
      "[INFO] Epoch: 62 , batch: 462 , training loss: 4.285357\n",
      "[INFO] Epoch: 62 , batch: 463 , training loss: 4.187886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 62 , batch: 464 , training loss: 4.357711\n",
      "[INFO] Epoch: 62 , batch: 465 , training loss: 4.309161\n",
      "[INFO] Epoch: 62 , batch: 466 , training loss: 4.387811\n",
      "[INFO] Epoch: 62 , batch: 467 , training loss: 4.346859\n",
      "[INFO] Epoch: 62 , batch: 468 , training loss: 4.329672\n",
      "[INFO] Epoch: 62 , batch: 469 , training loss: 4.361426\n",
      "[INFO] Epoch: 62 , batch: 470 , training loss: 4.176261\n",
      "[INFO] Epoch: 62 , batch: 471 , training loss: 4.272550\n",
      "[INFO] Epoch: 62 , batch: 472 , training loss: 4.316370\n",
      "[INFO] Epoch: 62 , batch: 473 , training loss: 4.256949\n",
      "[INFO] Epoch: 62 , batch: 474 , training loss: 4.025813\n",
      "[INFO] Epoch: 62 , batch: 475 , training loss: 3.918483\n",
      "[INFO] Epoch: 62 , batch: 476 , training loss: 4.294663\n",
      "[INFO] Epoch: 62 , batch: 477 , training loss: 4.426898\n",
      "[INFO] Epoch: 62 , batch: 478 , training loss: 4.418830\n",
      "[INFO] Epoch: 62 , batch: 479 , training loss: 4.406772\n",
      "[INFO] Epoch: 62 , batch: 480 , training loss: 4.539468\n",
      "[INFO] Epoch: 62 , batch: 481 , training loss: 4.411068\n",
      "[INFO] Epoch: 62 , batch: 482 , training loss: 4.516628\n",
      "[INFO] Epoch: 62 , batch: 483 , training loss: 4.344469\n",
      "[INFO] Epoch: 62 , batch: 484 , training loss: 4.155401\n",
      "[INFO] Epoch: 62 , batch: 485 , training loss: 4.262222\n",
      "[INFO] Epoch: 62 , batch: 486 , training loss: 4.156963\n",
      "[INFO] Epoch: 62 , batch: 487 , training loss: 4.150541\n",
      "[INFO] Epoch: 62 , batch: 488 , training loss: 4.314009\n",
      "[INFO] Epoch: 62 , batch: 489 , training loss: 4.223330\n",
      "[INFO] Epoch: 62 , batch: 490 , training loss: 4.283935\n",
      "[INFO] Epoch: 62 , batch: 491 , training loss: 4.202369\n",
      "[INFO] Epoch: 62 , batch: 492 , training loss: 4.189747\n",
      "[INFO] Epoch: 62 , batch: 493 , training loss: 4.334117\n",
      "[INFO] Epoch: 62 , batch: 494 , training loss: 4.257941\n",
      "[INFO] Epoch: 62 , batch: 495 , training loss: 4.424140\n",
      "[INFO] Epoch: 62 , batch: 496 , training loss: 4.301263\n",
      "[INFO] Epoch: 62 , batch: 497 , training loss: 4.327571\n",
      "[INFO] Epoch: 62 , batch: 498 , training loss: 4.312997\n",
      "[INFO] Epoch: 62 , batch: 499 , training loss: 4.368207\n",
      "[INFO] Epoch: 62 , batch: 500 , training loss: 4.516271\n",
      "[INFO] Epoch: 62 , batch: 501 , training loss: 4.841781\n",
      "[INFO] Epoch: 62 , batch: 502 , training loss: 4.864770\n",
      "[INFO] Epoch: 62 , batch: 503 , training loss: 4.478486\n",
      "[INFO] Epoch: 62 , batch: 504 , training loss: 4.626386\n",
      "[INFO] Epoch: 62 , batch: 505 , training loss: 4.601409\n",
      "[INFO] Epoch: 62 , batch: 506 , training loss: 4.590424\n",
      "[INFO] Epoch: 62 , batch: 507 , training loss: 4.642888\n",
      "[INFO] Epoch: 62 , batch: 508 , training loss: 4.553142\n",
      "[INFO] Epoch: 62 , batch: 509 , training loss: 4.390102\n",
      "[INFO] Epoch: 62 , batch: 510 , training loss: 4.460489\n",
      "[INFO] Epoch: 62 , batch: 511 , training loss: 4.381076\n",
      "[INFO] Epoch: 62 , batch: 512 , training loss: 4.482244\n",
      "[INFO] Epoch: 62 , batch: 513 , training loss: 4.735081\n",
      "[INFO] Epoch: 62 , batch: 514 , training loss: 4.362602\n",
      "[INFO] Epoch: 62 , batch: 515 , training loss: 4.637224\n",
      "[INFO] Epoch: 62 , batch: 516 , training loss: 4.418952\n",
      "[INFO] Epoch: 62 , batch: 517 , training loss: 4.402376\n",
      "[INFO] Epoch: 62 , batch: 518 , training loss: 4.364928\n",
      "[INFO] Epoch: 62 , batch: 519 , training loss: 4.174297\n",
      "[INFO] Epoch: 62 , batch: 520 , training loss: 4.437486\n",
      "[INFO] Epoch: 62 , batch: 521 , training loss: 4.412283\n",
      "[INFO] Epoch: 62 , batch: 522 , training loss: 4.489731\n",
      "[INFO] Epoch: 62 , batch: 523 , training loss: 4.425957\n",
      "[INFO] Epoch: 62 , batch: 524 , training loss: 4.723638\n",
      "[INFO] Epoch: 62 , batch: 525 , training loss: 4.591857\n",
      "[INFO] Epoch: 62 , batch: 526 , training loss: 4.370955\n",
      "[INFO] Epoch: 62 , batch: 527 , training loss: 4.412333\n",
      "[INFO] Epoch: 62 , batch: 528 , training loss: 4.420240\n",
      "[INFO] Epoch: 62 , batch: 529 , training loss: 4.388135\n",
      "[INFO] Epoch: 62 , batch: 530 , training loss: 4.266916\n",
      "[INFO] Epoch: 62 , batch: 531 , training loss: 4.394201\n",
      "[INFO] Epoch: 62 , batch: 532 , training loss: 4.322040\n",
      "[INFO] Epoch: 62 , batch: 533 , training loss: 4.429403\n",
      "[INFO] Epoch: 62 , batch: 534 , training loss: 4.446236\n",
      "[INFO] Epoch: 62 , batch: 535 , training loss: 4.454356\n",
      "[INFO] Epoch: 62 , batch: 536 , training loss: 4.312144\n",
      "[INFO] Epoch: 62 , batch: 537 , training loss: 4.294457\n",
      "[INFO] Epoch: 62 , batch: 538 , training loss: 4.382610\n",
      "[INFO] Epoch: 62 , batch: 539 , training loss: 4.471524\n",
      "[INFO] Epoch: 62 , batch: 540 , training loss: 4.968243\n",
      "[INFO] Epoch: 62 , batch: 541 , training loss: 4.822794\n",
      "[INFO] Epoch: 62 , batch: 542 , training loss: 4.706775\n",
      "[INFO] Epoch: 63 , batch: 0 , training loss: 3.514070\n",
      "[INFO] Epoch: 63 , batch: 1 , training loss: 3.485364\n",
      "[INFO] Epoch: 63 , batch: 2 , training loss: 3.720442\n",
      "[INFO] Epoch: 63 , batch: 3 , training loss: 3.575766\n",
      "[INFO] Epoch: 63 , batch: 4 , training loss: 3.947142\n",
      "[INFO] Epoch: 63 , batch: 5 , training loss: 3.583683\n",
      "[INFO] Epoch: 63 , batch: 6 , training loss: 4.007391\n",
      "[INFO] Epoch: 63 , batch: 7 , training loss: 3.932836\n",
      "[INFO] Epoch: 63 , batch: 8 , training loss: 3.571810\n",
      "[INFO] Epoch: 63 , batch: 9 , training loss: 3.825016\n",
      "[INFO] Epoch: 63 , batch: 10 , training loss: 3.839918\n",
      "[INFO] Epoch: 63 , batch: 11 , training loss: 3.760905\n",
      "[INFO] Epoch: 63 , batch: 12 , training loss: 3.659204\n",
      "[INFO] Epoch: 63 , batch: 13 , training loss: 3.658673\n",
      "[INFO] Epoch: 63 , batch: 14 , training loss: 3.601012\n",
      "[INFO] Epoch: 63 , batch: 15 , training loss: 3.794515\n",
      "[INFO] Epoch: 63 , batch: 16 , training loss: 3.594063\n",
      "[INFO] Epoch: 63 , batch: 17 , training loss: 3.740723\n",
      "[INFO] Epoch: 63 , batch: 18 , training loss: 3.730000\n",
      "[INFO] Epoch: 63 , batch: 19 , training loss: 3.501195\n",
      "[INFO] Epoch: 63 , batch: 20 , training loss: 3.451912\n",
      "[INFO] Epoch: 63 , batch: 21 , training loss: 3.584564\n",
      "[INFO] Epoch: 63 , batch: 22 , training loss: 3.447380\n",
      "[INFO] Epoch: 63 , batch: 23 , training loss: 3.681942\n",
      "[INFO] Epoch: 63 , batch: 24 , training loss: 3.524023\n",
      "[INFO] Epoch: 63 , batch: 25 , training loss: 3.642390\n",
      "[INFO] Epoch: 63 , batch: 26 , training loss: 3.478423\n",
      "[INFO] Epoch: 63 , batch: 27 , training loss: 3.490112\n",
      "[INFO] Epoch: 63 , batch: 28 , training loss: 3.669037\n",
      "[INFO] Epoch: 63 , batch: 29 , training loss: 3.468078\n",
      "[INFO] Epoch: 63 , batch: 30 , training loss: 3.518483\n",
      "[INFO] Epoch: 63 , batch: 31 , training loss: 3.589050\n",
      "[INFO] Epoch: 63 , batch: 32 , training loss: 3.566697\n",
      "[INFO] Epoch: 63 , batch: 33 , training loss: 3.609286\n",
      "[INFO] Epoch: 63 , batch: 34 , training loss: 3.577717\n",
      "[INFO] Epoch: 63 , batch: 35 , training loss: 3.543900\n",
      "[INFO] Epoch: 63 , batch: 36 , training loss: 3.661311\n",
      "[INFO] Epoch: 63 , batch: 37 , training loss: 3.528475\n",
      "[INFO] Epoch: 63 , batch: 38 , training loss: 3.566930\n",
      "[INFO] Epoch: 63 , batch: 39 , training loss: 3.386997\n",
      "[INFO] Epoch: 63 , batch: 40 , training loss: 3.618724\n",
      "[INFO] Epoch: 63 , batch: 41 , training loss: 3.547616\n",
      "[INFO] Epoch: 63 , batch: 42 , training loss: 4.028016\n",
      "[INFO] Epoch: 63 , batch: 43 , training loss: 3.758851\n",
      "[INFO] Epoch: 63 , batch: 44 , training loss: 4.124895\n",
      "[INFO] Epoch: 63 , batch: 45 , training loss: 4.059659\n",
      "[INFO] Epoch: 63 , batch: 46 , training loss: 4.032631\n",
      "[INFO] Epoch: 63 , batch: 47 , training loss: 3.601939\n",
      "[INFO] Epoch: 63 , batch: 48 , training loss: 3.646540\n",
      "[INFO] Epoch: 63 , batch: 49 , training loss: 3.821495\n",
      "[INFO] Epoch: 63 , batch: 50 , training loss: 3.614059\n",
      "[INFO] Epoch: 63 , batch: 51 , training loss: 3.797367\n",
      "[INFO] Epoch: 63 , batch: 52 , training loss: 3.581483\n",
      "[INFO] Epoch: 63 , batch: 53 , training loss: 3.723410\n",
      "[INFO] Epoch: 63 , batch: 54 , training loss: 3.751777\n",
      "[INFO] Epoch: 63 , batch: 55 , training loss: 3.793476\n",
      "[INFO] Epoch: 63 , batch: 56 , training loss: 3.628875\n",
      "[INFO] Epoch: 63 , batch: 57 , training loss: 3.586488\n",
      "[INFO] Epoch: 63 , batch: 58 , training loss: 3.627374\n",
      "[INFO] Epoch: 63 , batch: 59 , training loss: 3.672912\n",
      "[INFO] Epoch: 63 , batch: 60 , training loss: 3.636621\n",
      "[INFO] Epoch: 63 , batch: 61 , training loss: 3.737474\n",
      "[INFO] Epoch: 63 , batch: 62 , training loss: 3.609603\n",
      "[INFO] Epoch: 63 , batch: 63 , training loss: 3.829156\n",
      "[INFO] Epoch: 63 , batch: 64 , training loss: 4.009832\n",
      "[INFO] Epoch: 63 , batch: 65 , training loss: 3.743536\n",
      "[INFO] Epoch: 63 , batch: 66 , training loss: 3.584244\n",
      "[INFO] Epoch: 63 , batch: 67 , training loss: 3.603680\n",
      "[INFO] Epoch: 63 , batch: 68 , training loss: 3.767584\n",
      "[INFO] Epoch: 63 , batch: 69 , training loss: 3.696115\n",
      "[INFO] Epoch: 63 , batch: 70 , training loss: 3.908330\n",
      "[INFO] Epoch: 63 , batch: 71 , training loss: 3.764953\n",
      "[INFO] Epoch: 63 , batch: 72 , training loss: 3.800604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 63 , batch: 73 , training loss: 3.768852\n",
      "[INFO] Epoch: 63 , batch: 74 , training loss: 3.850164\n",
      "[INFO] Epoch: 63 , batch: 75 , training loss: 3.733792\n",
      "[INFO] Epoch: 63 , batch: 76 , training loss: 3.834553\n",
      "[INFO] Epoch: 63 , batch: 77 , training loss: 3.771659\n",
      "[INFO] Epoch: 63 , batch: 78 , training loss: 3.908058\n",
      "[INFO] Epoch: 63 , batch: 79 , training loss: 3.733030\n",
      "[INFO] Epoch: 63 , batch: 80 , training loss: 3.919429\n",
      "[INFO] Epoch: 63 , batch: 81 , training loss: 3.842747\n",
      "[INFO] Epoch: 63 , batch: 82 , training loss: 3.831426\n",
      "[INFO] Epoch: 63 , batch: 83 , training loss: 3.871109\n",
      "[INFO] Epoch: 63 , batch: 84 , training loss: 3.899328\n",
      "[INFO] Epoch: 63 , batch: 85 , training loss: 3.945241\n",
      "[INFO] Epoch: 63 , batch: 86 , training loss: 3.900095\n",
      "[INFO] Epoch: 63 , batch: 87 , training loss: 3.828053\n",
      "[INFO] Epoch: 63 , batch: 88 , training loss: 4.006461\n",
      "[INFO] Epoch: 63 , batch: 89 , training loss: 3.762490\n",
      "[INFO] Epoch: 63 , batch: 90 , training loss: 3.849939\n",
      "[INFO] Epoch: 63 , batch: 91 , training loss: 3.798671\n",
      "[INFO] Epoch: 63 , batch: 92 , training loss: 3.814839\n",
      "[INFO] Epoch: 63 , batch: 93 , training loss: 3.945769\n",
      "[INFO] Epoch: 63 , batch: 94 , training loss: 4.038285\n",
      "[INFO] Epoch: 63 , batch: 95 , training loss: 3.821638\n",
      "[INFO] Epoch: 63 , batch: 96 , training loss: 3.812775\n",
      "[INFO] Epoch: 63 , batch: 97 , training loss: 3.731523\n",
      "[INFO] Epoch: 63 , batch: 98 , training loss: 3.703465\n",
      "[INFO] Epoch: 63 , batch: 99 , training loss: 3.856803\n",
      "[INFO] Epoch: 63 , batch: 100 , training loss: 3.760199\n",
      "[INFO] Epoch: 63 , batch: 101 , training loss: 3.737794\n",
      "[INFO] Epoch: 63 , batch: 102 , training loss: 3.896018\n",
      "[INFO] Epoch: 63 , batch: 103 , training loss: 3.673536\n",
      "[INFO] Epoch: 63 , batch: 104 , training loss: 3.660533\n",
      "[INFO] Epoch: 63 , batch: 105 , training loss: 3.900640\n",
      "[INFO] Epoch: 63 , batch: 106 , training loss: 3.935117\n",
      "[INFO] Epoch: 63 , batch: 107 , training loss: 3.793458\n",
      "[INFO] Epoch: 63 , batch: 108 , training loss: 3.707663\n",
      "[INFO] Epoch: 63 , batch: 109 , training loss: 3.625351\n",
      "[INFO] Epoch: 63 , batch: 110 , training loss: 3.788390\n",
      "[INFO] Epoch: 63 , batch: 111 , training loss: 3.879489\n",
      "[INFO] Epoch: 63 , batch: 112 , training loss: 3.794565\n",
      "[INFO] Epoch: 63 , batch: 113 , training loss: 3.780105\n",
      "[INFO] Epoch: 63 , batch: 114 , training loss: 3.778543\n",
      "[INFO] Epoch: 63 , batch: 115 , training loss: 3.828324\n",
      "[INFO] Epoch: 63 , batch: 116 , training loss: 3.683290\n",
      "[INFO] Epoch: 63 , batch: 117 , training loss: 3.912306\n",
      "[INFO] Epoch: 63 , batch: 118 , training loss: 3.895616\n",
      "[INFO] Epoch: 63 , batch: 119 , training loss: 3.997172\n",
      "[INFO] Epoch: 63 , batch: 120 , training loss: 3.972996\n",
      "[INFO] Epoch: 63 , batch: 121 , training loss: 3.889219\n",
      "[INFO] Epoch: 63 , batch: 122 , training loss: 3.774316\n",
      "[INFO] Epoch: 63 , batch: 123 , training loss: 3.747044\n",
      "[INFO] Epoch: 63 , batch: 124 , training loss: 3.895536\n",
      "[INFO] Epoch: 63 , batch: 125 , training loss: 3.700018\n",
      "[INFO] Epoch: 63 , batch: 126 , training loss: 3.705047\n",
      "[INFO] Epoch: 63 , batch: 127 , training loss: 3.700651\n",
      "[INFO] Epoch: 63 , batch: 128 , training loss: 3.823523\n",
      "[INFO] Epoch: 63 , batch: 129 , training loss: 3.781443\n",
      "[INFO] Epoch: 63 , batch: 130 , training loss: 3.794022\n",
      "[INFO] Epoch: 63 , batch: 131 , training loss: 3.812807\n",
      "[INFO] Epoch: 63 , batch: 132 , training loss: 3.811305\n",
      "[INFO] Epoch: 63 , batch: 133 , training loss: 3.804901\n",
      "[INFO] Epoch: 63 , batch: 134 , training loss: 3.584402\n",
      "[INFO] Epoch: 63 , batch: 135 , training loss: 3.644293\n",
      "[INFO] Epoch: 63 , batch: 136 , training loss: 3.903824\n",
      "[INFO] Epoch: 63 , batch: 137 , training loss: 3.835053\n",
      "[INFO] Epoch: 63 , batch: 138 , training loss: 3.846273\n",
      "[INFO] Epoch: 63 , batch: 139 , training loss: 4.412728\n",
      "[INFO] Epoch: 63 , batch: 140 , training loss: 4.206199\n",
      "[INFO] Epoch: 63 , batch: 141 , training loss: 3.981616\n",
      "[INFO] Epoch: 63 , batch: 142 , training loss: 3.726233\n",
      "[INFO] Epoch: 63 , batch: 143 , training loss: 3.884447\n",
      "[INFO] Epoch: 63 , batch: 144 , training loss: 3.706783\n",
      "[INFO] Epoch: 63 , batch: 145 , training loss: 3.773169\n",
      "[INFO] Epoch: 63 , batch: 146 , training loss: 3.986346\n",
      "[INFO] Epoch: 63 , batch: 147 , training loss: 3.649718\n",
      "[INFO] Epoch: 63 , batch: 148 , training loss: 3.595969\n",
      "[INFO] Epoch: 63 , batch: 149 , training loss: 3.714806\n",
      "[INFO] Epoch: 63 , batch: 150 , training loss: 3.942969\n",
      "[INFO] Epoch: 63 , batch: 151 , training loss: 3.827621\n",
      "[INFO] Epoch: 63 , batch: 152 , training loss: 3.831376\n",
      "[INFO] Epoch: 63 , batch: 153 , training loss: 3.847832\n",
      "[INFO] Epoch: 63 , batch: 154 , training loss: 3.943564\n",
      "[INFO] Epoch: 63 , batch: 155 , training loss: 4.140528\n",
      "[INFO] Epoch: 63 , batch: 156 , training loss: 3.866056\n",
      "[INFO] Epoch: 63 , batch: 157 , training loss: 3.852846\n",
      "[INFO] Epoch: 63 , batch: 158 , training loss: 3.958172\n",
      "[INFO] Epoch: 63 , batch: 159 , training loss: 3.879300\n",
      "[INFO] Epoch: 63 , batch: 160 , training loss: 4.081499\n",
      "[INFO] Epoch: 63 , batch: 161 , training loss: 4.154294\n",
      "[INFO] Epoch: 63 , batch: 162 , training loss: 4.148841\n",
      "[INFO] Epoch: 63 , batch: 163 , training loss: 4.316748\n",
      "[INFO] Epoch: 63 , batch: 164 , training loss: 4.267611\n",
      "[INFO] Epoch: 63 , batch: 165 , training loss: 4.163738\n",
      "[INFO] Epoch: 63 , batch: 166 , training loss: 4.126695\n",
      "[INFO] Epoch: 63 , batch: 167 , training loss: 4.120167\n",
      "[INFO] Epoch: 63 , batch: 168 , training loss: 3.793050\n",
      "[INFO] Epoch: 63 , batch: 169 , training loss: 3.742546\n",
      "[INFO] Epoch: 63 , batch: 170 , training loss: 3.944934\n",
      "[INFO] Epoch: 63 , batch: 171 , training loss: 3.461942\n",
      "[INFO] Epoch: 63 , batch: 172 , training loss: 3.658851\n",
      "[INFO] Epoch: 63 , batch: 173 , training loss: 3.983232\n",
      "[INFO] Epoch: 63 , batch: 174 , training loss: 4.438024\n",
      "[INFO] Epoch: 63 , batch: 175 , training loss: 4.684912\n",
      "[INFO] Epoch: 63 , batch: 176 , training loss: 4.340082\n",
      "[INFO] Epoch: 63 , batch: 177 , training loss: 3.932715\n",
      "[INFO] Epoch: 63 , batch: 178 , training loss: 3.986351\n",
      "[INFO] Epoch: 63 , batch: 179 , training loss: 4.064603\n",
      "[INFO] Epoch: 63 , batch: 180 , training loss: 4.013785\n",
      "[INFO] Epoch: 63 , batch: 181 , training loss: 4.289674\n",
      "[INFO] Epoch: 63 , batch: 182 , training loss: 4.230345\n",
      "[INFO] Epoch: 63 , batch: 183 , training loss: 4.222200\n",
      "[INFO] Epoch: 63 , batch: 184 , training loss: 4.106138\n",
      "[INFO] Epoch: 63 , batch: 185 , training loss: 4.063305\n",
      "[INFO] Epoch: 63 , batch: 186 , training loss: 4.212039\n",
      "[INFO] Epoch: 63 , batch: 187 , training loss: 4.317160\n",
      "[INFO] Epoch: 63 , batch: 188 , training loss: 4.283329\n",
      "[INFO] Epoch: 63 , batch: 189 , training loss: 4.212216\n",
      "[INFO] Epoch: 63 , batch: 190 , training loss: 4.247890\n",
      "[INFO] Epoch: 63 , batch: 191 , training loss: 4.345725\n",
      "[INFO] Epoch: 63 , batch: 192 , training loss: 4.205963\n",
      "[INFO] Epoch: 63 , batch: 193 , training loss: 4.298040\n",
      "[INFO] Epoch: 63 , batch: 194 , training loss: 4.239764\n",
      "[INFO] Epoch: 63 , batch: 195 , training loss: 4.176339\n",
      "[INFO] Epoch: 63 , batch: 196 , training loss: 4.028181\n",
      "[INFO] Epoch: 63 , batch: 197 , training loss: 4.109948\n",
      "[INFO] Epoch: 63 , batch: 198 , training loss: 4.041195\n",
      "[INFO] Epoch: 63 , batch: 199 , training loss: 4.177378\n",
      "[INFO] Epoch: 63 , batch: 200 , training loss: 4.089244\n",
      "[INFO] Epoch: 63 , batch: 201 , training loss: 3.974720\n",
      "[INFO] Epoch: 63 , batch: 202 , training loss: 3.964916\n",
      "[INFO] Epoch: 63 , batch: 203 , training loss: 4.119510\n",
      "[INFO] Epoch: 63 , batch: 204 , training loss: 4.190117\n",
      "[INFO] Epoch: 63 , batch: 205 , training loss: 3.799777\n",
      "[INFO] Epoch: 63 , batch: 206 , training loss: 3.751919\n",
      "[INFO] Epoch: 63 , batch: 207 , training loss: 3.718984\n",
      "[INFO] Epoch: 63 , batch: 208 , training loss: 4.036797\n",
      "[INFO] Epoch: 63 , batch: 209 , training loss: 4.030203\n",
      "[INFO] Epoch: 63 , batch: 210 , training loss: 4.032462\n",
      "[INFO] Epoch: 63 , batch: 211 , training loss: 4.044429\n",
      "[INFO] Epoch: 63 , batch: 212 , training loss: 4.140207\n",
      "[INFO] Epoch: 63 , batch: 213 , training loss: 4.084778\n",
      "[INFO] Epoch: 63 , batch: 214 , training loss: 4.151585\n",
      "[INFO] Epoch: 63 , batch: 215 , training loss: 4.346083\n",
      "[INFO] Epoch: 63 , batch: 216 , training loss: 4.065249\n",
      "[INFO] Epoch: 63 , batch: 217 , training loss: 4.023430\n",
      "[INFO] Epoch: 63 , batch: 218 , training loss: 3.998237\n",
      "[INFO] Epoch: 63 , batch: 219 , training loss: 4.113894\n",
      "[INFO] Epoch: 63 , batch: 220 , training loss: 3.933499\n",
      "[INFO] Epoch: 63 , batch: 221 , training loss: 3.955886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 63 , batch: 222 , training loss: 4.057414\n",
      "[INFO] Epoch: 63 , batch: 223 , training loss: 4.227007\n",
      "[INFO] Epoch: 63 , batch: 224 , training loss: 4.234476\n",
      "[INFO] Epoch: 63 , batch: 225 , training loss: 4.122777\n",
      "[INFO] Epoch: 63 , batch: 226 , training loss: 4.246512\n",
      "[INFO] Epoch: 63 , batch: 227 , training loss: 4.235405\n",
      "[INFO] Epoch: 63 , batch: 228 , training loss: 4.260986\n",
      "[INFO] Epoch: 63 , batch: 229 , training loss: 4.111170\n",
      "[INFO] Epoch: 63 , batch: 230 , training loss: 3.965608\n",
      "[INFO] Epoch: 63 , batch: 231 , training loss: 3.833910\n",
      "[INFO] Epoch: 63 , batch: 232 , training loss: 3.987355\n",
      "[INFO] Epoch: 63 , batch: 233 , training loss: 4.015912\n",
      "[INFO] Epoch: 63 , batch: 234 , training loss: 3.696075\n",
      "[INFO] Epoch: 63 , batch: 235 , training loss: 3.815020\n",
      "[INFO] Epoch: 63 , batch: 236 , training loss: 3.889088\n",
      "[INFO] Epoch: 63 , batch: 237 , training loss: 4.115660\n",
      "[INFO] Epoch: 63 , batch: 238 , training loss: 3.903353\n",
      "[INFO] Epoch: 63 , batch: 239 , training loss: 3.926066\n",
      "[INFO] Epoch: 63 , batch: 240 , training loss: 3.995091\n",
      "[INFO] Epoch: 63 , batch: 241 , training loss: 3.800857\n",
      "[INFO] Epoch: 63 , batch: 242 , training loss: 3.824301\n",
      "[INFO] Epoch: 63 , batch: 243 , training loss: 4.109278\n",
      "[INFO] Epoch: 63 , batch: 244 , training loss: 4.058371\n",
      "[INFO] Epoch: 63 , batch: 245 , training loss: 3.979600\n",
      "[INFO] Epoch: 63 , batch: 246 , training loss: 3.731103\n",
      "[INFO] Epoch: 63 , batch: 247 , training loss: 3.914990\n",
      "[INFO] Epoch: 63 , batch: 248 , training loss: 3.973661\n",
      "[INFO] Epoch: 63 , batch: 249 , training loss: 3.939231\n",
      "[INFO] Epoch: 63 , batch: 250 , training loss: 3.749274\n",
      "[INFO] Epoch: 63 , batch: 251 , training loss: 4.196713\n",
      "[INFO] Epoch: 63 , batch: 252 , training loss: 3.918252\n",
      "[INFO] Epoch: 63 , batch: 253 , training loss: 3.813132\n",
      "[INFO] Epoch: 63 , batch: 254 , training loss: 4.078755\n",
      "[INFO] Epoch: 63 , batch: 255 , training loss: 4.051487\n",
      "[INFO] Epoch: 63 , batch: 256 , training loss: 4.028968\n",
      "[INFO] Epoch: 63 , batch: 257 , training loss: 4.242241\n",
      "[INFO] Epoch: 63 , batch: 258 , training loss: 4.213707\n",
      "[INFO] Epoch: 63 , batch: 259 , training loss: 4.241551\n",
      "[INFO] Epoch: 63 , batch: 260 , training loss: 4.018335\n",
      "[INFO] Epoch: 63 , batch: 261 , training loss: 4.218047\n",
      "[INFO] Epoch: 63 , batch: 262 , training loss: 4.328334\n",
      "[INFO] Epoch: 63 , batch: 263 , training loss: 4.492293\n",
      "[INFO] Epoch: 63 , batch: 264 , training loss: 3.872617\n",
      "[INFO] Epoch: 63 , batch: 265 , training loss: 3.974448\n",
      "[INFO] Epoch: 63 , batch: 266 , training loss: 4.360909\n",
      "[INFO] Epoch: 63 , batch: 267 , training loss: 4.141963\n",
      "[INFO] Epoch: 63 , batch: 268 , training loss: 4.062060\n",
      "[INFO] Epoch: 63 , batch: 269 , training loss: 4.030775\n",
      "[INFO] Epoch: 63 , batch: 270 , training loss: 4.059688\n",
      "[INFO] Epoch: 63 , batch: 271 , training loss: 4.068536\n",
      "[INFO] Epoch: 63 , batch: 272 , training loss: 4.076546\n",
      "[INFO] Epoch: 63 , batch: 273 , training loss: 4.092108\n",
      "[INFO] Epoch: 63 , batch: 274 , training loss: 4.183700\n",
      "[INFO] Epoch: 63 , batch: 275 , training loss: 4.040112\n",
      "[INFO] Epoch: 63 , batch: 276 , training loss: 4.097425\n",
      "[INFO] Epoch: 63 , batch: 277 , training loss: 4.260288\n",
      "[INFO] Epoch: 63 , batch: 278 , training loss: 3.956403\n",
      "[INFO] Epoch: 63 , batch: 279 , training loss: 3.957629\n",
      "[INFO] Epoch: 63 , batch: 280 , training loss: 3.965563\n",
      "[INFO] Epoch: 63 , batch: 281 , training loss: 4.054350\n",
      "[INFO] Epoch: 63 , batch: 282 , training loss: 4.009916\n",
      "[INFO] Epoch: 63 , batch: 283 , training loss: 3.988773\n",
      "[INFO] Epoch: 63 , batch: 284 , training loss: 4.006316\n",
      "[INFO] Epoch: 63 , batch: 285 , training loss: 3.947678\n",
      "[INFO] Epoch: 63 , batch: 286 , training loss: 3.956543\n",
      "[INFO] Epoch: 63 , batch: 287 , training loss: 3.919189\n",
      "[INFO] Epoch: 63 , batch: 288 , training loss: 3.859051\n",
      "[INFO] Epoch: 63 , batch: 289 , training loss: 3.945227\n",
      "[INFO] Epoch: 63 , batch: 290 , training loss: 3.726987\n",
      "[INFO] Epoch: 63 , batch: 291 , training loss: 3.717348\n",
      "[INFO] Epoch: 63 , batch: 292 , training loss: 3.821491\n",
      "[INFO] Epoch: 63 , batch: 293 , training loss: 3.750876\n",
      "[INFO] Epoch: 63 , batch: 294 , training loss: 4.388958\n",
      "[INFO] Epoch: 63 , batch: 295 , training loss: 4.168211\n",
      "[INFO] Epoch: 63 , batch: 296 , training loss: 4.101471\n",
      "[INFO] Epoch: 63 , batch: 297 , training loss: 4.077730\n",
      "[INFO] Epoch: 63 , batch: 298 , training loss: 3.905729\n",
      "[INFO] Epoch: 63 , batch: 299 , training loss: 3.946631\n",
      "[INFO] Epoch: 63 , batch: 300 , training loss: 3.936650\n",
      "[INFO] Epoch: 63 , batch: 301 , training loss: 3.853902\n",
      "[INFO] Epoch: 63 , batch: 302 , training loss: 4.027561\n",
      "[INFO] Epoch: 63 , batch: 303 , training loss: 4.037307\n",
      "[INFO] Epoch: 63 , batch: 304 , training loss: 4.168965\n",
      "[INFO] Epoch: 63 , batch: 305 , training loss: 4.009771\n",
      "[INFO] Epoch: 63 , batch: 306 , training loss: 4.116544\n",
      "[INFO] Epoch: 63 , batch: 307 , training loss: 4.131689\n",
      "[INFO] Epoch: 63 , batch: 308 , training loss: 3.953178\n",
      "[INFO] Epoch: 63 , batch: 309 , training loss: 3.935212\n",
      "[INFO] Epoch: 63 , batch: 310 , training loss: 3.886273\n",
      "[INFO] Epoch: 63 , batch: 311 , training loss: 3.870408\n",
      "[INFO] Epoch: 63 , batch: 312 , training loss: 3.775049\n",
      "[INFO] Epoch: 63 , batch: 313 , training loss: 3.852999\n",
      "[INFO] Epoch: 63 , batch: 314 , training loss: 3.935306\n",
      "[INFO] Epoch: 63 , batch: 315 , training loss: 4.032124\n",
      "[INFO] Epoch: 63 , batch: 316 , training loss: 4.257128\n",
      "[INFO] Epoch: 63 , batch: 317 , training loss: 4.589014\n",
      "[INFO] Epoch: 63 , batch: 318 , training loss: 4.705957\n",
      "[INFO] Epoch: 63 , batch: 319 , training loss: 4.398780\n",
      "[INFO] Epoch: 63 , batch: 320 , training loss: 3.986147\n",
      "[INFO] Epoch: 63 , batch: 321 , training loss: 3.805866\n",
      "[INFO] Epoch: 63 , batch: 322 , training loss: 3.904908\n",
      "[INFO] Epoch: 63 , batch: 323 , training loss: 3.932370\n",
      "[INFO] Epoch: 63 , batch: 324 , training loss: 3.915663\n",
      "[INFO] Epoch: 63 , batch: 325 , training loss: 4.020761\n",
      "[INFO] Epoch: 63 , batch: 326 , training loss: 4.085301\n",
      "[INFO] Epoch: 63 , batch: 327 , training loss: 4.016172\n",
      "[INFO] Epoch: 63 , batch: 328 , training loss: 4.023508\n",
      "[INFO] Epoch: 63 , batch: 329 , training loss: 3.934915\n",
      "[INFO] Epoch: 63 , batch: 330 , training loss: 3.920768\n",
      "[INFO] Epoch: 63 , batch: 331 , training loss: 4.059838\n",
      "[INFO] Epoch: 63 , batch: 332 , training loss: 3.926378\n",
      "[INFO] Epoch: 63 , batch: 333 , training loss: 3.906927\n",
      "[INFO] Epoch: 63 , batch: 334 , training loss: 3.918743\n",
      "[INFO] Epoch: 63 , batch: 335 , training loss: 4.028389\n",
      "[INFO] Epoch: 63 , batch: 336 , training loss: 4.052290\n",
      "[INFO] Epoch: 63 , batch: 337 , training loss: 4.097889\n",
      "[INFO] Epoch: 63 , batch: 338 , training loss: 4.304426\n",
      "[INFO] Epoch: 63 , batch: 339 , training loss: 4.106134\n",
      "[INFO] Epoch: 63 , batch: 340 , training loss: 4.302017\n",
      "[INFO] Epoch: 63 , batch: 341 , training loss: 4.070930\n",
      "[INFO] Epoch: 63 , batch: 342 , training loss: 3.850726\n",
      "[INFO] Epoch: 63 , batch: 343 , training loss: 3.937117\n",
      "[INFO] Epoch: 63 , batch: 344 , training loss: 3.791430\n",
      "[INFO] Epoch: 63 , batch: 345 , training loss: 3.898489\n",
      "[INFO] Epoch: 63 , batch: 346 , training loss: 3.975886\n",
      "[INFO] Epoch: 63 , batch: 347 , training loss: 3.876090\n",
      "[INFO] Epoch: 63 , batch: 348 , training loss: 3.972860\n",
      "[INFO] Epoch: 63 , batch: 349 , training loss: 4.058504\n",
      "[INFO] Epoch: 63 , batch: 350 , training loss: 3.940267\n",
      "[INFO] Epoch: 63 , batch: 351 , training loss: 4.010194\n",
      "[INFO] Epoch: 63 , batch: 352 , training loss: 4.028255\n",
      "[INFO] Epoch: 63 , batch: 353 , training loss: 4.013137\n",
      "[INFO] Epoch: 63 , batch: 354 , training loss: 4.070359\n",
      "[INFO] Epoch: 63 , batch: 355 , training loss: 4.107315\n",
      "[INFO] Epoch: 63 , batch: 356 , training loss: 3.943374\n",
      "[INFO] Epoch: 63 , batch: 357 , training loss: 4.011328\n",
      "[INFO] Epoch: 63 , batch: 358 , training loss: 3.940127\n",
      "[INFO] Epoch: 63 , batch: 359 , training loss: 3.943659\n",
      "[INFO] Epoch: 63 , batch: 360 , training loss: 4.043921\n",
      "[INFO] Epoch: 63 , batch: 361 , training loss: 4.003827\n",
      "[INFO] Epoch: 63 , batch: 362 , training loss: 4.138525\n",
      "[INFO] Epoch: 63 , batch: 363 , training loss: 4.007890\n",
      "[INFO] Epoch: 63 , batch: 364 , training loss: 4.076241\n",
      "[INFO] Epoch: 63 , batch: 365 , training loss: 3.982570\n",
      "[INFO] Epoch: 63 , batch: 366 , training loss: 4.069158\n",
      "[INFO] Epoch: 63 , batch: 367 , training loss: 4.116567\n",
      "[INFO] Epoch: 63 , batch: 368 , training loss: 4.517219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 63 , batch: 369 , training loss: 4.199583\n",
      "[INFO] Epoch: 63 , batch: 370 , training loss: 3.968898\n",
      "[INFO] Epoch: 63 , batch: 371 , training loss: 4.393861\n",
      "[INFO] Epoch: 63 , batch: 372 , training loss: 4.602390\n",
      "[INFO] Epoch: 63 , batch: 373 , training loss: 4.662091\n",
      "[INFO] Epoch: 63 , batch: 374 , training loss: 4.772297\n",
      "[INFO] Epoch: 63 , batch: 375 , training loss: 4.789063\n",
      "[INFO] Epoch: 63 , batch: 376 , training loss: 4.644927\n",
      "[INFO] Epoch: 63 , batch: 377 , training loss: 4.410787\n",
      "[INFO] Epoch: 63 , batch: 378 , training loss: 4.531847\n",
      "[INFO] Epoch: 63 , batch: 379 , training loss: 4.496705\n",
      "[INFO] Epoch: 63 , batch: 380 , training loss: 4.670377\n",
      "[INFO] Epoch: 63 , batch: 381 , training loss: 4.348974\n",
      "[INFO] Epoch: 63 , batch: 382 , training loss: 4.582613\n",
      "[INFO] Epoch: 63 , batch: 383 , training loss: 4.648160\n",
      "[INFO] Epoch: 63 , batch: 384 , training loss: 4.606086\n",
      "[INFO] Epoch: 63 , batch: 385 , training loss: 4.299221\n",
      "[INFO] Epoch: 63 , batch: 386 , training loss: 4.528901\n",
      "[INFO] Epoch: 63 , batch: 387 , training loss: 4.479494\n",
      "[INFO] Epoch: 63 , batch: 388 , training loss: 4.325918\n",
      "[INFO] Epoch: 63 , batch: 389 , training loss: 4.141464\n",
      "[INFO] Epoch: 63 , batch: 390 , training loss: 4.164087\n",
      "[INFO] Epoch: 63 , batch: 391 , training loss: 4.183167\n",
      "[INFO] Epoch: 63 , batch: 392 , training loss: 4.582509\n",
      "[INFO] Epoch: 63 , batch: 393 , training loss: 4.444290\n",
      "[INFO] Epoch: 63 , batch: 394 , training loss: 4.559009\n",
      "[INFO] Epoch: 63 , batch: 395 , training loss: 4.371314\n",
      "[INFO] Epoch: 63 , batch: 396 , training loss: 4.173573\n",
      "[INFO] Epoch: 63 , batch: 397 , training loss: 4.325228\n",
      "[INFO] Epoch: 63 , batch: 398 , training loss: 4.189297\n",
      "[INFO] Epoch: 63 , batch: 399 , training loss: 4.249364\n",
      "[INFO] Epoch: 63 , batch: 400 , training loss: 4.230227\n",
      "[INFO] Epoch: 63 , batch: 401 , training loss: 4.653257\n",
      "[INFO] Epoch: 63 , batch: 402 , training loss: 4.377316\n",
      "[INFO] Epoch: 63 , batch: 403 , training loss: 4.227555\n",
      "[INFO] Epoch: 63 , batch: 404 , training loss: 4.393043\n",
      "[INFO] Epoch: 63 , batch: 405 , training loss: 4.436177\n",
      "[INFO] Epoch: 63 , batch: 406 , training loss: 4.368966\n",
      "[INFO] Epoch: 63 , batch: 407 , training loss: 4.379040\n",
      "[INFO] Epoch: 63 , batch: 408 , training loss: 4.354834\n",
      "[INFO] Epoch: 63 , batch: 409 , training loss: 4.380002\n",
      "[INFO] Epoch: 63 , batch: 410 , training loss: 4.427744\n",
      "[INFO] Epoch: 63 , batch: 411 , training loss: 4.587088\n",
      "[INFO] Epoch: 63 , batch: 412 , training loss: 4.415866\n",
      "[INFO] Epoch: 63 , batch: 413 , training loss: 4.292280\n",
      "[INFO] Epoch: 63 , batch: 414 , training loss: 4.322125\n",
      "[INFO] Epoch: 63 , batch: 415 , training loss: 4.400841\n",
      "[INFO] Epoch: 63 , batch: 416 , training loss: 4.442858\n",
      "[INFO] Epoch: 63 , batch: 417 , training loss: 4.364146\n",
      "[INFO] Epoch: 63 , batch: 418 , training loss: 4.415701\n",
      "[INFO] Epoch: 63 , batch: 419 , training loss: 4.362784\n",
      "[INFO] Epoch: 63 , batch: 420 , training loss: 4.356300\n",
      "[INFO] Epoch: 63 , batch: 421 , training loss: 4.343363\n",
      "[INFO] Epoch: 63 , batch: 422 , training loss: 4.190187\n",
      "[INFO] Epoch: 63 , batch: 423 , training loss: 4.417211\n",
      "[INFO] Epoch: 63 , batch: 424 , training loss: 4.565977\n",
      "[INFO] Epoch: 63 , batch: 425 , training loss: 4.454500\n",
      "[INFO] Epoch: 63 , batch: 426 , training loss: 4.189347\n",
      "[INFO] Epoch: 63 , batch: 427 , training loss: 4.406102\n",
      "[INFO] Epoch: 63 , batch: 428 , training loss: 4.276564\n",
      "[INFO] Epoch: 63 , batch: 429 , training loss: 4.197686\n",
      "[INFO] Epoch: 63 , batch: 430 , training loss: 4.424872\n",
      "[INFO] Epoch: 63 , batch: 431 , training loss: 4.039997\n",
      "[INFO] Epoch: 63 , batch: 432 , training loss: 4.087947\n",
      "[INFO] Epoch: 63 , batch: 433 , training loss: 4.140322\n",
      "[INFO] Epoch: 63 , batch: 434 , training loss: 4.009584\n",
      "[INFO] Epoch: 63 , batch: 435 , training loss: 4.366430\n",
      "[INFO] Epoch: 63 , batch: 436 , training loss: 4.406262\n",
      "[INFO] Epoch: 63 , batch: 437 , training loss: 4.203895\n",
      "[INFO] Epoch: 63 , batch: 438 , training loss: 4.052177\n",
      "[INFO] Epoch: 63 , batch: 439 , training loss: 4.276850\n",
      "[INFO] Epoch: 63 , batch: 440 , training loss: 4.399556\n",
      "[INFO] Epoch: 63 , batch: 441 , training loss: 4.503157\n",
      "[INFO] Epoch: 63 , batch: 442 , training loss: 4.247768\n",
      "[INFO] Epoch: 63 , batch: 443 , training loss: 4.420586\n",
      "[INFO] Epoch: 63 , batch: 444 , training loss: 4.044811\n",
      "[INFO] Epoch: 63 , batch: 445 , training loss: 3.951411\n",
      "[INFO] Epoch: 63 , batch: 446 , training loss: 3.897426\n",
      "[INFO] Epoch: 63 , batch: 447 , training loss: 4.078873\n",
      "[INFO] Epoch: 63 , batch: 448 , training loss: 4.182471\n",
      "[INFO] Epoch: 63 , batch: 449 , training loss: 4.577049\n",
      "[INFO] Epoch: 63 , batch: 450 , training loss: 4.660476\n",
      "[INFO] Epoch: 63 , batch: 451 , training loss: 4.542533\n",
      "[INFO] Epoch: 63 , batch: 452 , training loss: 4.359935\n",
      "[INFO] Epoch: 63 , batch: 453 , training loss: 4.127431\n",
      "[INFO] Epoch: 63 , batch: 454 , training loss: 4.280907\n",
      "[INFO] Epoch: 63 , batch: 455 , training loss: 4.317526\n",
      "[INFO] Epoch: 63 , batch: 456 , training loss: 4.334910\n",
      "[INFO] Epoch: 63 , batch: 457 , training loss: 4.404399\n",
      "[INFO] Epoch: 63 , batch: 458 , training loss: 4.145336\n",
      "[INFO] Epoch: 63 , batch: 459 , training loss: 4.113841\n",
      "[INFO] Epoch: 63 , batch: 460 , training loss: 4.250803\n",
      "[INFO] Epoch: 63 , batch: 461 , training loss: 4.182495\n",
      "[INFO] Epoch: 63 , batch: 462 , training loss: 4.262825\n",
      "[INFO] Epoch: 63 , batch: 463 , training loss: 4.172019\n",
      "[INFO] Epoch: 63 , batch: 464 , training loss: 4.341624\n",
      "[INFO] Epoch: 63 , batch: 465 , training loss: 4.308708\n",
      "[INFO] Epoch: 63 , batch: 466 , training loss: 4.376836\n",
      "[INFO] Epoch: 63 , batch: 467 , training loss: 4.341649\n",
      "[INFO] Epoch: 63 , batch: 468 , training loss: 4.298374\n",
      "[INFO] Epoch: 63 , batch: 469 , training loss: 4.352080\n",
      "[INFO] Epoch: 63 , batch: 470 , training loss: 4.167441\n",
      "[INFO] Epoch: 63 , batch: 471 , training loss: 4.271806\n",
      "[INFO] Epoch: 63 , batch: 472 , training loss: 4.313374\n",
      "[INFO] Epoch: 63 , batch: 473 , training loss: 4.225481\n",
      "[INFO] Epoch: 63 , batch: 474 , training loss: 4.012462\n",
      "[INFO] Epoch: 63 , batch: 475 , training loss: 3.905992\n",
      "[INFO] Epoch: 63 , batch: 476 , training loss: 4.287847\n",
      "[INFO] Epoch: 63 , batch: 477 , training loss: 4.411720\n",
      "[INFO] Epoch: 63 , batch: 478 , training loss: 4.417427\n",
      "[INFO] Epoch: 63 , batch: 479 , training loss: 4.418792\n",
      "[INFO] Epoch: 63 , batch: 480 , training loss: 4.527218\n",
      "[INFO] Epoch: 63 , batch: 481 , training loss: 4.394056\n",
      "[INFO] Epoch: 63 , batch: 482 , training loss: 4.515314\n",
      "[INFO] Epoch: 63 , batch: 483 , training loss: 4.328525\n",
      "[INFO] Epoch: 63 , batch: 484 , training loss: 4.143987\n",
      "[INFO] Epoch: 63 , batch: 485 , training loss: 4.264397\n",
      "[INFO] Epoch: 63 , batch: 486 , training loss: 4.147490\n",
      "[INFO] Epoch: 63 , batch: 487 , training loss: 4.138758\n",
      "[INFO] Epoch: 63 , batch: 488 , training loss: 4.316156\n",
      "[INFO] Epoch: 63 , batch: 489 , training loss: 4.223378\n",
      "[INFO] Epoch: 63 , batch: 490 , training loss: 4.273998\n",
      "[INFO] Epoch: 63 , batch: 491 , training loss: 4.183369\n",
      "[INFO] Epoch: 63 , batch: 492 , training loss: 4.179543\n",
      "[INFO] Epoch: 63 , batch: 493 , training loss: 4.332119\n",
      "[INFO] Epoch: 63 , batch: 494 , training loss: 4.236527\n",
      "[INFO] Epoch: 63 , batch: 495 , training loss: 4.421031\n",
      "[INFO] Epoch: 63 , batch: 496 , training loss: 4.291076\n",
      "[INFO] Epoch: 63 , batch: 497 , training loss: 4.319268\n",
      "[INFO] Epoch: 63 , batch: 498 , training loss: 4.306754\n",
      "[INFO] Epoch: 63 , batch: 499 , training loss: 4.369621\n",
      "[INFO] Epoch: 63 , batch: 500 , training loss: 4.503359\n",
      "[INFO] Epoch: 63 , batch: 501 , training loss: 4.815377\n",
      "[INFO] Epoch: 63 , batch: 502 , training loss: 4.844231\n",
      "[INFO] Epoch: 63 , batch: 503 , training loss: 4.457332\n",
      "[INFO] Epoch: 63 , batch: 504 , training loss: 4.616023\n",
      "[INFO] Epoch: 63 , batch: 505 , training loss: 4.583806\n",
      "[INFO] Epoch: 63 , batch: 506 , training loss: 4.577444\n",
      "[INFO] Epoch: 63 , batch: 507 , training loss: 4.629603\n",
      "[INFO] Epoch: 63 , batch: 508 , training loss: 4.542338\n",
      "[INFO] Epoch: 63 , batch: 509 , training loss: 4.372366\n",
      "[INFO] Epoch: 63 , batch: 510 , training loss: 4.448689\n",
      "[INFO] Epoch: 63 , batch: 511 , training loss: 4.372066\n",
      "[INFO] Epoch: 63 , batch: 512 , training loss: 4.470246\n",
      "[INFO] Epoch: 63 , batch: 513 , training loss: 4.736591\n",
      "[INFO] Epoch: 63 , batch: 514 , training loss: 4.362483\n",
      "[INFO] Epoch: 63 , batch: 515 , training loss: 4.632158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 63 , batch: 516 , training loss: 4.413769\n",
      "[INFO] Epoch: 63 , batch: 517 , training loss: 4.386617\n",
      "[INFO] Epoch: 63 , batch: 518 , training loss: 4.350120\n",
      "[INFO] Epoch: 63 , batch: 519 , training loss: 4.171449\n",
      "[INFO] Epoch: 63 , batch: 520 , training loss: 4.420227\n",
      "[INFO] Epoch: 63 , batch: 521 , training loss: 4.395198\n",
      "[INFO] Epoch: 63 , batch: 522 , training loss: 4.487419\n",
      "[INFO] Epoch: 63 , batch: 523 , training loss: 4.402660\n",
      "[INFO] Epoch: 63 , batch: 524 , training loss: 4.698980\n",
      "[INFO] Epoch: 63 , batch: 525 , training loss: 4.587838\n",
      "[INFO] Epoch: 63 , batch: 526 , training loss: 4.365833\n",
      "[INFO] Epoch: 63 , batch: 527 , training loss: 4.390340\n",
      "[INFO] Epoch: 63 , batch: 528 , training loss: 4.421345\n",
      "[INFO] Epoch: 63 , batch: 529 , training loss: 4.390178\n",
      "[INFO] Epoch: 63 , batch: 530 , training loss: 4.240204\n",
      "[INFO] Epoch: 63 , batch: 531 , training loss: 4.406256\n",
      "[INFO] Epoch: 63 , batch: 532 , training loss: 4.310553\n",
      "[INFO] Epoch: 63 , batch: 533 , training loss: 4.416889\n",
      "[INFO] Epoch: 63 , batch: 534 , training loss: 4.432056\n",
      "[INFO] Epoch: 63 , batch: 535 , training loss: 4.434690\n",
      "[INFO] Epoch: 63 , batch: 536 , training loss: 4.295866\n",
      "[INFO] Epoch: 63 , batch: 537 , training loss: 4.279928\n",
      "[INFO] Epoch: 63 , batch: 538 , training loss: 4.362557\n",
      "[INFO] Epoch: 63 , batch: 539 , training loss: 4.431195\n",
      "[INFO] Epoch: 63 , batch: 540 , training loss: 4.935662\n",
      "[INFO] Epoch: 63 , batch: 541 , training loss: 4.783738\n",
      "[INFO] Epoch: 63 , batch: 542 , training loss: 4.676325\n",
      "[INFO] Epoch: 64 , batch: 0 , training loss: 3.523092\n",
      "[INFO] Epoch: 64 , batch: 1 , training loss: 3.459845\n",
      "[INFO] Epoch: 64 , batch: 2 , training loss: 3.669756\n",
      "[INFO] Epoch: 64 , batch: 3 , training loss: 3.548153\n",
      "[INFO] Epoch: 64 , batch: 4 , training loss: 3.919183\n",
      "[INFO] Epoch: 64 , batch: 5 , training loss: 3.592402\n",
      "[INFO] Epoch: 64 , batch: 6 , training loss: 3.892234\n",
      "[INFO] Epoch: 64 , batch: 7 , training loss: 3.888916\n",
      "[INFO] Epoch: 64 , batch: 8 , training loss: 3.551178\n",
      "[INFO] Epoch: 64 , batch: 9 , training loss: 3.828306\n",
      "[INFO] Epoch: 64 , batch: 10 , training loss: 3.806508\n",
      "[INFO] Epoch: 64 , batch: 11 , training loss: 3.739067\n",
      "[INFO] Epoch: 64 , batch: 12 , training loss: 3.649014\n",
      "[INFO] Epoch: 64 , batch: 13 , training loss: 3.613683\n",
      "[INFO] Epoch: 64 , batch: 14 , training loss: 3.567744\n",
      "[INFO] Epoch: 64 , batch: 15 , training loss: 3.786801\n",
      "[INFO] Epoch: 64 , batch: 16 , training loss: 3.594620\n",
      "[INFO] Epoch: 64 , batch: 17 , training loss: 3.711388\n",
      "[INFO] Epoch: 64 , batch: 18 , training loss: 3.668370\n",
      "[INFO] Epoch: 64 , batch: 19 , training loss: 3.443252\n",
      "[INFO] Epoch: 64 , batch: 20 , training loss: 3.444034\n",
      "[INFO] Epoch: 64 , batch: 21 , training loss: 3.551766\n",
      "[INFO] Epoch: 64 , batch: 22 , training loss: 3.409692\n",
      "[INFO] Epoch: 64 , batch: 23 , training loss: 3.676429\n",
      "[INFO] Epoch: 64 , batch: 24 , training loss: 3.483483\n",
      "[INFO] Epoch: 64 , batch: 25 , training loss: 3.616320\n",
      "[INFO] Epoch: 64 , batch: 26 , training loss: 3.469095\n",
      "[INFO] Epoch: 64 , batch: 27 , training loss: 3.473833\n",
      "[INFO] Epoch: 64 , batch: 28 , training loss: 3.641276\n",
      "[INFO] Epoch: 64 , batch: 29 , training loss: 3.460116\n",
      "[INFO] Epoch: 64 , batch: 30 , training loss: 3.494743\n",
      "[INFO] Epoch: 64 , batch: 31 , training loss: 3.571212\n",
      "[INFO] Epoch: 64 , batch: 32 , training loss: 3.542460\n",
      "[INFO] Epoch: 64 , batch: 33 , training loss: 3.569700\n",
      "[INFO] Epoch: 64 , batch: 34 , training loss: 3.544236\n",
      "[INFO] Epoch: 64 , batch: 35 , training loss: 3.512355\n",
      "[INFO] Epoch: 64 , batch: 36 , training loss: 3.622142\n",
      "[INFO] Epoch: 64 , batch: 37 , training loss: 3.477117\n",
      "[INFO] Epoch: 64 , batch: 38 , training loss: 3.532224\n",
      "[INFO] Epoch: 64 , batch: 39 , training loss: 3.371272\n",
      "[INFO] Epoch: 64 , batch: 40 , training loss: 3.573368\n",
      "[INFO] Epoch: 64 , batch: 41 , training loss: 3.546182\n",
      "[INFO] Epoch: 64 , batch: 42 , training loss: 4.002880\n",
      "[INFO] Epoch: 64 , batch: 43 , training loss: 3.763825\n",
      "[INFO] Epoch: 64 , batch: 44 , training loss: 4.090759\n",
      "[INFO] Epoch: 64 , batch: 45 , training loss: 4.045627\n",
      "[INFO] Epoch: 64 , batch: 46 , training loss: 3.975604\n",
      "[INFO] Epoch: 64 , batch: 47 , training loss: 3.573166\n",
      "[INFO] Epoch: 64 , batch: 48 , training loss: 3.579246\n",
      "[INFO] Epoch: 64 , batch: 49 , training loss: 3.799461\n",
      "[INFO] Epoch: 64 , batch: 50 , training loss: 3.585317\n",
      "[INFO] Epoch: 64 , batch: 51 , training loss: 3.794262\n",
      "[INFO] Epoch: 64 , batch: 52 , training loss: 3.578883\n",
      "[INFO] Epoch: 64 , batch: 53 , training loss: 3.718667\n",
      "[INFO] Epoch: 64 , batch: 54 , training loss: 3.718482\n",
      "[INFO] Epoch: 64 , batch: 55 , training loss: 3.775388\n",
      "[INFO] Epoch: 64 , batch: 56 , training loss: 3.650282\n",
      "[INFO] Epoch: 64 , batch: 57 , training loss: 3.583478\n",
      "[INFO] Epoch: 64 , batch: 58 , training loss: 3.612307\n",
      "[INFO] Epoch: 64 , batch: 59 , training loss: 3.688884\n",
      "[INFO] Epoch: 64 , batch: 60 , training loss: 3.620146\n",
      "[INFO] Epoch: 64 , batch: 61 , training loss: 3.753462\n",
      "[INFO] Epoch: 64 , batch: 62 , training loss: 3.604810\n",
      "[INFO] Epoch: 64 , batch: 63 , training loss: 3.801172\n",
      "[INFO] Epoch: 64 , batch: 64 , training loss: 3.995346\n",
      "[INFO] Epoch: 64 , batch: 65 , training loss: 3.716520\n",
      "[INFO] Epoch: 64 , batch: 66 , training loss: 3.554256\n",
      "[INFO] Epoch: 64 , batch: 67 , training loss: 3.602719\n",
      "[INFO] Epoch: 64 , batch: 68 , training loss: 3.738027\n",
      "[INFO] Epoch: 64 , batch: 69 , training loss: 3.677547\n",
      "[INFO] Epoch: 64 , batch: 70 , training loss: 3.904078\n",
      "[INFO] Epoch: 64 , batch: 71 , training loss: 3.749628\n",
      "[INFO] Epoch: 64 , batch: 72 , training loss: 3.807943\n",
      "[INFO] Epoch: 64 , batch: 73 , training loss: 3.755023\n",
      "[INFO] Epoch: 64 , batch: 74 , training loss: 3.854655\n",
      "[INFO] Epoch: 64 , batch: 75 , training loss: 3.742160\n",
      "[INFO] Epoch: 64 , batch: 76 , training loss: 3.836433\n",
      "[INFO] Epoch: 64 , batch: 77 , training loss: 3.775104\n",
      "[INFO] Epoch: 64 , batch: 78 , training loss: 3.866247\n",
      "[INFO] Epoch: 64 , batch: 79 , training loss: 3.748039\n",
      "[INFO] Epoch: 64 , batch: 80 , training loss: 3.922848\n",
      "[INFO] Epoch: 64 , batch: 81 , training loss: 3.832986\n",
      "[INFO] Epoch: 64 , batch: 82 , training loss: 3.803935\n",
      "[INFO] Epoch: 64 , batch: 83 , training loss: 3.892169\n",
      "[INFO] Epoch: 64 , batch: 84 , training loss: 3.869025\n",
      "[INFO] Epoch: 64 , batch: 85 , training loss: 3.940927\n",
      "[INFO] Epoch: 64 , batch: 86 , training loss: 3.856369\n",
      "[INFO] Epoch: 64 , batch: 87 , training loss: 3.815169\n",
      "[INFO] Epoch: 64 , batch: 88 , training loss: 3.982305\n",
      "[INFO] Epoch: 64 , batch: 89 , training loss: 3.775003\n",
      "[INFO] Epoch: 64 , batch: 90 , training loss: 3.837411\n",
      "[INFO] Epoch: 64 , batch: 91 , training loss: 3.790636\n",
      "[INFO] Epoch: 64 , batch: 92 , training loss: 3.817815\n",
      "[INFO] Epoch: 64 , batch: 93 , training loss: 3.932352\n",
      "[INFO] Epoch: 64 , batch: 94 , training loss: 4.013563\n",
      "[INFO] Epoch: 64 , batch: 95 , training loss: 3.809972\n",
      "[INFO] Epoch: 64 , batch: 96 , training loss: 3.820886\n",
      "[INFO] Epoch: 64 , batch: 97 , training loss: 3.727110\n",
      "[INFO] Epoch: 64 , batch: 98 , training loss: 3.690360\n",
      "[INFO] Epoch: 64 , batch: 99 , training loss: 3.843119\n",
      "[INFO] Epoch: 64 , batch: 100 , training loss: 3.740696\n",
      "[INFO] Epoch: 64 , batch: 101 , training loss: 3.761113\n",
      "[INFO] Epoch: 64 , batch: 102 , training loss: 3.877020\n",
      "[INFO] Epoch: 64 , batch: 103 , training loss: 3.686519\n",
      "[INFO] Epoch: 64 , batch: 104 , training loss: 3.646129\n",
      "[INFO] Epoch: 64 , batch: 105 , training loss: 3.879050\n",
      "[INFO] Epoch: 64 , batch: 106 , training loss: 3.907960\n",
      "[INFO] Epoch: 64 , batch: 107 , training loss: 3.746435\n",
      "[INFO] Epoch: 64 , batch: 108 , training loss: 3.709982\n",
      "[INFO] Epoch: 64 , batch: 109 , training loss: 3.632970\n",
      "[INFO] Epoch: 64 , batch: 110 , training loss: 3.783482\n",
      "[INFO] Epoch: 64 , batch: 111 , training loss: 3.881255\n",
      "[INFO] Epoch: 64 , batch: 112 , training loss: 3.773222\n",
      "[INFO] Epoch: 64 , batch: 113 , training loss: 3.750530\n",
      "[INFO] Epoch: 64 , batch: 114 , training loss: 3.771704\n",
      "[INFO] Epoch: 64 , batch: 115 , training loss: 3.798638\n",
      "[INFO] Epoch: 64 , batch: 116 , training loss: 3.690924\n",
      "[INFO] Epoch: 64 , batch: 117 , training loss: 3.887256\n",
      "[INFO] Epoch: 64 , batch: 118 , training loss: 3.871258\n",
      "[INFO] Epoch: 64 , batch: 119 , training loss: 3.987103\n",
      "[INFO] Epoch: 64 , batch: 120 , training loss: 3.985082\n",
      "[INFO] Epoch: 64 , batch: 121 , training loss: 3.896326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 64 , batch: 122 , training loss: 3.774542\n",
      "[INFO] Epoch: 64 , batch: 123 , training loss: 3.776642\n",
      "[INFO] Epoch: 64 , batch: 124 , training loss: 3.882340\n",
      "[INFO] Epoch: 64 , batch: 125 , training loss: 3.683806\n",
      "[INFO] Epoch: 64 , batch: 126 , training loss: 3.684330\n",
      "[INFO] Epoch: 64 , batch: 127 , training loss: 3.696824\n",
      "[INFO] Epoch: 64 , batch: 128 , training loss: 3.822997\n",
      "[INFO] Epoch: 64 , batch: 129 , training loss: 3.782823\n",
      "[INFO] Epoch: 64 , batch: 130 , training loss: 3.786084\n",
      "[INFO] Epoch: 64 , batch: 131 , training loss: 3.818274\n",
      "[INFO] Epoch: 64 , batch: 132 , training loss: 3.809822\n",
      "[INFO] Epoch: 64 , batch: 133 , training loss: 3.783657\n",
      "[INFO] Epoch: 64 , batch: 134 , training loss: 3.566031\n",
      "[INFO] Epoch: 64 , batch: 135 , training loss: 3.617432\n",
      "[INFO] Epoch: 64 , batch: 136 , training loss: 3.903960\n",
      "[INFO] Epoch: 64 , batch: 137 , training loss: 3.836272\n",
      "[INFO] Epoch: 64 , batch: 138 , training loss: 3.832530\n",
      "[INFO] Epoch: 64 , batch: 139 , training loss: 4.413328\n",
      "[INFO] Epoch: 64 , batch: 140 , training loss: 4.196443\n",
      "[INFO] Epoch: 64 , batch: 141 , training loss: 3.991009\n",
      "[INFO] Epoch: 64 , batch: 142 , training loss: 3.710738\n",
      "[INFO] Epoch: 64 , batch: 143 , training loss: 3.870954\n",
      "[INFO] Epoch: 64 , batch: 144 , training loss: 3.703357\n",
      "[INFO] Epoch: 64 , batch: 145 , training loss: 3.771922\n",
      "[INFO] Epoch: 64 , batch: 146 , training loss: 3.976718\n",
      "[INFO] Epoch: 64 , batch: 147 , training loss: 3.638343\n",
      "[INFO] Epoch: 64 , batch: 148 , training loss: 3.601309\n",
      "[INFO] Epoch: 64 , batch: 149 , training loss: 3.677964\n",
      "[INFO] Epoch: 64 , batch: 150 , training loss: 3.942634\n",
      "[INFO] Epoch: 64 , batch: 151 , training loss: 3.800351\n",
      "[INFO] Epoch: 64 , batch: 152 , training loss: 3.830483\n",
      "[INFO] Epoch: 64 , batch: 153 , training loss: 3.831611\n",
      "[INFO] Epoch: 64 , batch: 154 , training loss: 3.946236\n",
      "[INFO] Epoch: 64 , batch: 155 , training loss: 4.105175\n",
      "[INFO] Epoch: 64 , batch: 156 , training loss: 3.875986\n",
      "[INFO] Epoch: 64 , batch: 157 , training loss: 3.836536\n",
      "[INFO] Epoch: 64 , batch: 158 , training loss: 3.937150\n",
      "[INFO] Epoch: 64 , batch: 159 , training loss: 3.847861\n",
      "[INFO] Epoch: 64 , batch: 160 , training loss: 4.055883\n",
      "[INFO] Epoch: 64 , batch: 161 , training loss: 4.144538\n",
      "[INFO] Epoch: 64 , batch: 162 , training loss: 4.128463\n",
      "[INFO] Epoch: 64 , batch: 163 , training loss: 4.320153\n",
      "[INFO] Epoch: 64 , batch: 164 , training loss: 4.270810\n",
      "[INFO] Epoch: 64 , batch: 165 , training loss: 4.154688\n",
      "[INFO] Epoch: 64 , batch: 166 , training loss: 4.088638\n",
      "[INFO] Epoch: 64 , batch: 167 , training loss: 4.039364\n",
      "[INFO] Epoch: 64 , batch: 168 , training loss: 3.742410\n",
      "[INFO] Epoch: 64 , batch: 169 , training loss: 3.735484\n",
      "[INFO] Epoch: 64 , batch: 170 , training loss: 3.947811\n",
      "[INFO] Epoch: 64 , batch: 171 , training loss: 3.398556\n",
      "[INFO] Epoch: 64 , batch: 172 , training loss: 3.647556\n",
      "[INFO] Epoch: 64 , batch: 173 , training loss: 3.963801\n",
      "[INFO] Epoch: 64 , batch: 174 , training loss: 4.421105\n",
      "[INFO] Epoch: 64 , batch: 175 , training loss: 4.698630\n",
      "[INFO] Epoch: 64 , batch: 176 , training loss: 4.367818\n",
      "[INFO] Epoch: 64 , batch: 177 , training loss: 3.929207\n",
      "[INFO] Epoch: 64 , batch: 178 , training loss: 3.960736\n",
      "[INFO] Epoch: 64 , batch: 179 , training loss: 4.007631\n",
      "[INFO] Epoch: 64 , batch: 180 , training loss: 3.966265\n",
      "[INFO] Epoch: 64 , batch: 181 , training loss: 4.293518\n",
      "[INFO] Epoch: 64 , batch: 182 , training loss: 4.237964\n",
      "[INFO] Epoch: 64 , batch: 183 , training loss: 4.216470\n",
      "[INFO] Epoch: 64 , batch: 184 , training loss: 4.081865\n",
      "[INFO] Epoch: 64 , batch: 185 , training loss: 4.044588\n",
      "[INFO] Epoch: 64 , batch: 186 , training loss: 4.200903\n",
      "[INFO] Epoch: 64 , batch: 187 , training loss: 4.305695\n",
      "[INFO] Epoch: 64 , batch: 188 , training loss: 4.283530\n",
      "[INFO] Epoch: 64 , batch: 189 , training loss: 4.204960\n",
      "[INFO] Epoch: 64 , batch: 190 , training loss: 4.244764\n",
      "[INFO] Epoch: 64 , batch: 191 , training loss: 4.345086\n",
      "[INFO] Epoch: 64 , batch: 192 , training loss: 4.224622\n",
      "[INFO] Epoch: 64 , batch: 193 , training loss: 4.286608\n",
      "[INFO] Epoch: 64 , batch: 194 , training loss: 4.229895\n",
      "[INFO] Epoch: 64 , batch: 195 , training loss: 4.161666\n",
      "[INFO] Epoch: 64 , batch: 196 , training loss: 4.008722\n",
      "[INFO] Epoch: 64 , batch: 197 , training loss: 4.102604\n",
      "[INFO] Epoch: 64 , batch: 198 , training loss: 4.018521\n",
      "[INFO] Epoch: 64 , batch: 199 , training loss: 4.157713\n",
      "[INFO] Epoch: 64 , batch: 200 , training loss: 4.070027\n",
      "[INFO] Epoch: 64 , batch: 201 , training loss: 3.951038\n",
      "[INFO] Epoch: 64 , batch: 202 , training loss: 3.955561\n",
      "[INFO] Epoch: 64 , batch: 203 , training loss: 4.105795\n",
      "[INFO] Epoch: 64 , batch: 204 , training loss: 4.156361\n",
      "[INFO] Epoch: 64 , batch: 205 , training loss: 3.797530\n",
      "[INFO] Epoch: 64 , batch: 206 , training loss: 3.734930\n",
      "[INFO] Epoch: 64 , batch: 207 , training loss: 3.693137\n",
      "[INFO] Epoch: 64 , batch: 208 , training loss: 4.030202\n",
      "[INFO] Epoch: 64 , batch: 209 , training loss: 4.020563\n",
      "[INFO] Epoch: 64 , batch: 210 , training loss: 4.016082\n",
      "[INFO] Epoch: 64 , batch: 211 , training loss: 4.014298\n",
      "[INFO] Epoch: 64 , batch: 212 , training loss: 4.114117\n",
      "[INFO] Epoch: 64 , batch: 213 , training loss: 4.040402\n",
      "[INFO] Epoch: 64 , batch: 214 , training loss: 4.144178\n",
      "[INFO] Epoch: 64 , batch: 215 , training loss: 4.324412\n",
      "[INFO] Epoch: 64 , batch: 216 , training loss: 4.026943\n",
      "[INFO] Epoch: 64 , batch: 217 , training loss: 3.999155\n",
      "[INFO] Epoch: 64 , batch: 218 , training loss: 3.986060\n",
      "[INFO] Epoch: 64 , batch: 219 , training loss: 4.084942\n",
      "[INFO] Epoch: 64 , batch: 220 , training loss: 3.935462\n",
      "[INFO] Epoch: 64 , batch: 221 , training loss: 3.936006\n",
      "[INFO] Epoch: 64 , batch: 222 , training loss: 4.053504\n",
      "[INFO] Epoch: 64 , batch: 223 , training loss: 4.195023\n",
      "[INFO] Epoch: 64 , batch: 224 , training loss: 4.232383\n",
      "[INFO] Epoch: 64 , batch: 225 , training loss: 4.112506\n",
      "[INFO] Epoch: 64 , batch: 226 , training loss: 4.253431\n",
      "[INFO] Epoch: 64 , batch: 227 , training loss: 4.228834\n",
      "[INFO] Epoch: 64 , batch: 228 , training loss: 4.250409\n",
      "[INFO] Epoch: 64 , batch: 229 , training loss: 4.099839\n",
      "[INFO] Epoch: 64 , batch: 230 , training loss: 3.959546\n",
      "[INFO] Epoch: 64 , batch: 231 , training loss: 3.831014\n",
      "[INFO] Epoch: 64 , batch: 232 , training loss: 3.974774\n",
      "[INFO] Epoch: 64 , batch: 233 , training loss: 3.992537\n",
      "[INFO] Epoch: 64 , batch: 234 , training loss: 3.690186\n",
      "[INFO] Epoch: 64 , batch: 235 , training loss: 3.794474\n",
      "[INFO] Epoch: 64 , batch: 236 , training loss: 3.875292\n",
      "[INFO] Epoch: 64 , batch: 237 , training loss: 4.119680\n",
      "[INFO] Epoch: 64 , batch: 238 , training loss: 3.902318\n",
      "[INFO] Epoch: 64 , batch: 239 , training loss: 3.923347\n",
      "[INFO] Epoch: 64 , batch: 240 , training loss: 3.984556\n",
      "[INFO] Epoch: 64 , batch: 241 , training loss: 3.795888\n",
      "[INFO] Epoch: 64 , batch: 242 , training loss: 3.801306\n",
      "[INFO] Epoch: 64 , batch: 243 , training loss: 4.090458\n",
      "[INFO] Epoch: 64 , batch: 244 , training loss: 4.062686\n",
      "[INFO] Epoch: 64 , batch: 245 , training loss: 3.987333\n",
      "[INFO] Epoch: 64 , batch: 246 , training loss: 3.712800\n",
      "[INFO] Epoch: 64 , batch: 247 , training loss: 3.896338\n",
      "[INFO] Epoch: 64 , batch: 248 , training loss: 3.954986\n",
      "[INFO] Epoch: 64 , batch: 249 , training loss: 3.928204\n",
      "[INFO] Epoch: 64 , batch: 250 , training loss: 3.752992\n",
      "[INFO] Epoch: 64 , batch: 251 , training loss: 4.181191\n",
      "[INFO] Epoch: 64 , batch: 252 , training loss: 3.905351\n",
      "[INFO] Epoch: 64 , batch: 253 , training loss: 3.804617\n",
      "[INFO] Epoch: 64 , batch: 254 , training loss: 4.075731\n",
      "[INFO] Epoch: 64 , batch: 255 , training loss: 4.054263\n",
      "[INFO] Epoch: 64 , batch: 256 , training loss: 4.020328\n",
      "[INFO] Epoch: 64 , batch: 257 , training loss: 4.215908\n",
      "[INFO] Epoch: 64 , batch: 258 , training loss: 4.174050\n",
      "[INFO] Epoch: 64 , batch: 259 , training loss: 4.239045\n",
      "[INFO] Epoch: 64 , batch: 260 , training loss: 4.022449\n",
      "[INFO] Epoch: 64 , batch: 261 , training loss: 4.194159\n",
      "[INFO] Epoch: 64 , batch: 262 , training loss: 4.306749\n",
      "[INFO] Epoch: 64 , batch: 263 , training loss: 4.488609\n",
      "[INFO] Epoch: 64 , batch: 264 , training loss: 3.857116\n",
      "[INFO] Epoch: 64 , batch: 265 , training loss: 3.956105\n",
      "[INFO] Epoch: 64 , batch: 266 , training loss: 4.360279\n",
      "[INFO] Epoch: 64 , batch: 267 , training loss: 4.126484\n",
      "[INFO] Epoch: 64 , batch: 268 , training loss: 4.045343\n",
      "[INFO] Epoch: 64 , batch: 269 , training loss: 4.023066\n",
      "[INFO] Epoch: 64 , batch: 270 , training loss: 4.048255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 64 , batch: 271 , training loss: 4.067023\n",
      "[INFO] Epoch: 64 , batch: 272 , training loss: 4.074800\n",
      "[INFO] Epoch: 64 , batch: 273 , training loss: 4.090087\n",
      "[INFO] Epoch: 64 , batch: 274 , training loss: 4.173438\n",
      "[INFO] Epoch: 64 , batch: 275 , training loss: 4.040918\n",
      "[INFO] Epoch: 64 , batch: 276 , training loss: 4.111918\n",
      "[INFO] Epoch: 64 , batch: 277 , training loss: 4.259678\n",
      "[INFO] Epoch: 64 , batch: 278 , training loss: 3.954717\n",
      "[INFO] Epoch: 64 , batch: 279 , training loss: 3.943404\n",
      "[INFO] Epoch: 64 , batch: 280 , training loss: 3.941531\n",
      "[INFO] Epoch: 64 , batch: 281 , training loss: 4.058141\n",
      "[INFO] Epoch: 64 , batch: 282 , training loss: 4.003560\n",
      "[INFO] Epoch: 64 , batch: 283 , training loss: 3.987971\n",
      "[INFO] Epoch: 64 , batch: 284 , training loss: 4.006811\n",
      "[INFO] Epoch: 64 , batch: 285 , training loss: 3.946216\n",
      "[INFO] Epoch: 64 , batch: 286 , training loss: 3.952211\n",
      "[INFO] Epoch: 64 , batch: 287 , training loss: 3.904211\n",
      "[INFO] Epoch: 64 , batch: 288 , training loss: 3.854680\n",
      "[INFO] Epoch: 64 , batch: 289 , training loss: 3.948908\n",
      "[INFO] Epoch: 64 , batch: 290 , training loss: 3.734254\n",
      "[INFO] Epoch: 64 , batch: 291 , training loss: 3.715943\n",
      "[INFO] Epoch: 64 , batch: 292 , training loss: 3.825255\n",
      "[INFO] Epoch: 64 , batch: 293 , training loss: 3.745589\n",
      "[INFO] Epoch: 64 , batch: 294 , training loss: 4.368025\n",
      "[INFO] Epoch: 64 , batch: 295 , training loss: 4.163450\n",
      "[INFO] Epoch: 64 , batch: 296 , training loss: 4.097262\n",
      "[INFO] Epoch: 64 , batch: 297 , training loss: 4.075030\n",
      "[INFO] Epoch: 64 , batch: 298 , training loss: 3.891654\n",
      "[INFO] Epoch: 64 , batch: 299 , training loss: 3.942772\n",
      "[INFO] Epoch: 64 , batch: 300 , training loss: 3.928730\n",
      "[INFO] Epoch: 64 , batch: 301 , training loss: 3.850087\n",
      "[INFO] Epoch: 64 , batch: 302 , training loss: 4.018037\n",
      "[INFO] Epoch: 64 , batch: 303 , training loss: 4.021070\n",
      "[INFO] Epoch: 64 , batch: 304 , training loss: 4.163363\n",
      "[INFO] Epoch: 64 , batch: 305 , training loss: 4.026382\n",
      "[INFO] Epoch: 64 , batch: 306 , training loss: 4.107097\n",
      "[INFO] Epoch: 64 , batch: 307 , training loss: 4.133565\n",
      "[INFO] Epoch: 64 , batch: 308 , training loss: 3.933240\n",
      "[INFO] Epoch: 64 , batch: 309 , training loss: 3.937357\n",
      "[INFO] Epoch: 64 , batch: 310 , training loss: 3.875940\n",
      "[INFO] Epoch: 64 , batch: 311 , training loss: 3.866863\n",
      "[INFO] Epoch: 64 , batch: 312 , training loss: 3.767314\n",
      "[INFO] Epoch: 64 , batch: 313 , training loss: 3.870055\n",
      "[INFO] Epoch: 64 , batch: 314 , training loss: 3.933926\n",
      "[INFO] Epoch: 64 , batch: 315 , training loss: 4.036251\n",
      "[INFO] Epoch: 64 , batch: 316 , training loss: 4.243073\n",
      "[INFO] Epoch: 64 , batch: 317 , training loss: 4.571508\n",
      "[INFO] Epoch: 64 , batch: 318 , training loss: 4.688890\n",
      "[INFO] Epoch: 64 , batch: 319 , training loss: 4.395130\n",
      "[INFO] Epoch: 64 , batch: 320 , training loss: 3.977782\n",
      "[INFO] Epoch: 64 , batch: 321 , training loss: 3.809406\n",
      "[INFO] Epoch: 64 , batch: 322 , training loss: 3.905845\n",
      "[INFO] Epoch: 64 , batch: 323 , training loss: 3.939236\n",
      "[INFO] Epoch: 64 , batch: 324 , training loss: 3.912397\n",
      "[INFO] Epoch: 64 , batch: 325 , training loss: 4.007530\n",
      "[INFO] Epoch: 64 , batch: 326 , training loss: 4.068841\n",
      "[INFO] Epoch: 64 , batch: 327 , training loss: 4.014208\n",
      "[INFO] Epoch: 64 , batch: 328 , training loss: 4.028017\n",
      "[INFO] Epoch: 64 , batch: 329 , training loss: 3.941876\n",
      "[INFO] Epoch: 64 , batch: 330 , training loss: 3.908616\n",
      "[INFO] Epoch: 64 , batch: 331 , training loss: 4.057879\n",
      "[INFO] Epoch: 64 , batch: 332 , training loss: 3.919088\n",
      "[INFO] Epoch: 64 , batch: 333 , training loss: 3.906662\n",
      "[INFO] Epoch: 64 , batch: 334 , training loss: 3.927239\n",
      "[INFO] Epoch: 64 , batch: 335 , training loss: 4.026027\n",
      "[INFO] Epoch: 64 , batch: 336 , training loss: 4.043115\n",
      "[INFO] Epoch: 64 , batch: 337 , training loss: 4.097264\n",
      "[INFO] Epoch: 64 , batch: 338 , training loss: 4.303986\n",
      "[INFO] Epoch: 64 , batch: 339 , training loss: 4.121059\n",
      "[INFO] Epoch: 64 , batch: 340 , training loss: 4.291641\n",
      "[INFO] Epoch: 64 , batch: 341 , training loss: 4.072759\n",
      "[INFO] Epoch: 64 , batch: 342 , training loss: 3.852880\n",
      "[INFO] Epoch: 64 , batch: 343 , training loss: 3.928970\n",
      "[INFO] Epoch: 64 , batch: 344 , training loss: 3.779144\n",
      "[INFO] Epoch: 64 , batch: 345 , training loss: 3.905931\n",
      "[INFO] Epoch: 64 , batch: 346 , training loss: 3.957026\n",
      "[INFO] Epoch: 64 , batch: 347 , training loss: 3.867008\n",
      "[INFO] Epoch: 64 , batch: 348 , training loss: 3.967271\n",
      "[INFO] Epoch: 64 , batch: 349 , training loss: 4.053959\n",
      "[INFO] Epoch: 64 , batch: 350 , training loss: 3.938675\n",
      "[INFO] Epoch: 64 , batch: 351 , training loss: 4.010574\n",
      "[INFO] Epoch: 64 , batch: 352 , training loss: 4.025568\n",
      "[INFO] Epoch: 64 , batch: 353 , training loss: 4.011121\n",
      "[INFO] Epoch: 64 , batch: 354 , training loss: 4.074890\n",
      "[INFO] Epoch: 64 , batch: 355 , training loss: 4.095010\n",
      "[INFO] Epoch: 64 , batch: 356 , training loss: 3.955482\n",
      "[INFO] Epoch: 64 , batch: 357 , training loss: 4.005705\n",
      "[INFO] Epoch: 64 , batch: 358 , training loss: 3.927317\n",
      "[INFO] Epoch: 64 , batch: 359 , training loss: 3.938484\n",
      "[INFO] Epoch: 64 , batch: 360 , training loss: 4.029693\n",
      "[INFO] Epoch: 64 , batch: 361 , training loss: 3.991779\n",
      "[INFO] Epoch: 64 , batch: 362 , training loss: 4.125171\n",
      "[INFO] Epoch: 64 , batch: 363 , training loss: 3.996728\n",
      "[INFO] Epoch: 64 , batch: 364 , training loss: 4.071098\n",
      "[INFO] Epoch: 64 , batch: 365 , training loss: 3.984160\n",
      "[INFO] Epoch: 64 , batch: 366 , training loss: 4.078368\n",
      "[INFO] Epoch: 64 , batch: 367 , training loss: 4.112895\n",
      "[INFO] Epoch: 64 , batch: 368 , training loss: 4.525692\n",
      "[INFO] Epoch: 64 , batch: 369 , training loss: 4.183579\n",
      "[INFO] Epoch: 64 , batch: 370 , training loss: 3.961888\n",
      "[INFO] Epoch: 64 , batch: 371 , training loss: 4.355094\n",
      "[INFO] Epoch: 64 , batch: 372 , training loss: 4.628673\n",
      "[INFO] Epoch: 64 , batch: 373 , training loss: 4.652792\n",
      "[INFO] Epoch: 64 , batch: 374 , training loss: 4.807934\n",
      "[INFO] Epoch: 64 , batch: 375 , training loss: 4.828175\n",
      "[INFO] Epoch: 64 , batch: 376 , training loss: 4.710105\n",
      "[INFO] Epoch: 64 , batch: 377 , training loss: 4.473750\n",
      "[INFO] Epoch: 64 , batch: 378 , training loss: 4.574077\n",
      "[INFO] Epoch: 64 , batch: 379 , training loss: 4.575989\n",
      "[INFO] Epoch: 64 , batch: 380 , training loss: 4.731267\n",
      "[INFO] Epoch: 64 , batch: 381 , training loss: 4.431960\n",
      "[INFO] Epoch: 64 , batch: 382 , training loss: 4.667084\n",
      "[INFO] Epoch: 64 , batch: 383 , training loss: 4.786872\n",
      "[INFO] Epoch: 64 , batch: 384 , training loss: 5.007461\n",
      "[INFO] Epoch: 64 , batch: 385 , training loss: 4.605052\n",
      "[INFO] Epoch: 64 , batch: 386 , training loss: 4.790479\n",
      "[INFO] Epoch: 64 , batch: 387 , training loss: 4.606169\n",
      "[INFO] Epoch: 64 , batch: 388 , training loss: 4.426313\n",
      "[INFO] Epoch: 64 , batch: 389 , training loss: 4.246236\n",
      "[INFO] Epoch: 64 , batch: 390 , training loss: 4.255582\n",
      "[INFO] Epoch: 64 , batch: 391 , training loss: 4.290062\n",
      "[INFO] Epoch: 64 , batch: 392 , training loss: 4.652129\n",
      "[INFO] Epoch: 64 , batch: 393 , training loss: 4.527846\n",
      "[INFO] Epoch: 64 , batch: 394 , training loss: 4.644313\n",
      "[INFO] Epoch: 64 , batch: 395 , training loss: 4.440228\n",
      "[INFO] Epoch: 64 , batch: 396 , training loss: 4.244883\n",
      "[INFO] Epoch: 64 , batch: 397 , training loss: 4.425319\n",
      "[INFO] Epoch: 64 , batch: 398 , training loss: 4.288096\n",
      "[INFO] Epoch: 64 , batch: 399 , training loss: 4.354386\n",
      "[INFO] Epoch: 64 , batch: 400 , training loss: 4.320356\n",
      "[INFO] Epoch: 64 , batch: 401 , training loss: 4.725726\n",
      "[INFO] Epoch: 64 , batch: 402 , training loss: 4.476357\n",
      "[INFO] Epoch: 64 , batch: 403 , training loss: 4.309675\n",
      "[INFO] Epoch: 64 , batch: 404 , training loss: 4.486824\n",
      "[INFO] Epoch: 64 , batch: 405 , training loss: 4.551445\n",
      "[INFO] Epoch: 64 , batch: 406 , training loss: 4.445961\n",
      "[INFO] Epoch: 64 , batch: 407 , training loss: 4.434453\n",
      "[INFO] Epoch: 64 , batch: 408 , training loss: 4.438658\n",
      "[INFO] Epoch: 64 , batch: 409 , training loss: 4.450927\n",
      "[INFO] Epoch: 64 , batch: 410 , training loss: 4.491198\n",
      "[INFO] Epoch: 64 , batch: 411 , training loss: 4.682104\n",
      "[INFO] Epoch: 64 , batch: 412 , training loss: 4.521235\n",
      "[INFO] Epoch: 64 , batch: 413 , training loss: 4.376215\n",
      "[INFO] Epoch: 64 , batch: 414 , training loss: 4.415676\n",
      "[INFO] Epoch: 64 , batch: 415 , training loss: 4.466540\n",
      "[INFO] Epoch: 64 , batch: 416 , training loss: 4.503527\n",
      "[INFO] Epoch: 64 , batch: 417 , training loss: 4.444743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 64 , batch: 418 , training loss: 4.486291\n",
      "[INFO] Epoch: 64 , batch: 419 , training loss: 4.452069\n",
      "[INFO] Epoch: 64 , batch: 420 , training loss: 4.411467\n",
      "[INFO] Epoch: 64 , batch: 421 , training loss: 4.411982\n",
      "[INFO] Epoch: 64 , batch: 422 , training loss: 4.272037\n",
      "[INFO] Epoch: 64 , batch: 423 , training loss: 4.472154\n",
      "[INFO] Epoch: 64 , batch: 424 , training loss: 4.631788\n",
      "[INFO] Epoch: 64 , batch: 425 , training loss: 4.526348\n",
      "[INFO] Epoch: 64 , batch: 426 , training loss: 4.240634\n",
      "[INFO] Epoch: 64 , batch: 427 , training loss: 4.485672\n",
      "[INFO] Epoch: 64 , batch: 428 , training loss: 4.339180\n",
      "[INFO] Epoch: 64 , batch: 429 , training loss: 4.227875\n",
      "[INFO] Epoch: 64 , batch: 430 , training loss: 4.467064\n",
      "[INFO] Epoch: 64 , batch: 431 , training loss: 4.105424\n",
      "[INFO] Epoch: 64 , batch: 432 , training loss: 4.151383\n",
      "[INFO] Epoch: 64 , batch: 433 , training loss: 4.190038\n",
      "[INFO] Epoch: 64 , batch: 434 , training loss: 4.073260\n",
      "[INFO] Epoch: 64 , batch: 435 , training loss: 4.427135\n",
      "[INFO] Epoch: 64 , batch: 436 , training loss: 4.459926\n",
      "[INFO] Epoch: 64 , batch: 437 , training loss: 4.257349\n",
      "[INFO] Epoch: 64 , batch: 438 , training loss: 4.116652\n",
      "[INFO] Epoch: 64 , batch: 439 , training loss: 4.329918\n",
      "[INFO] Epoch: 64 , batch: 440 , training loss: 4.452662\n",
      "[INFO] Epoch: 64 , batch: 441 , training loss: 4.552677\n",
      "[INFO] Epoch: 64 , batch: 442 , training loss: 4.309618\n",
      "[INFO] Epoch: 64 , batch: 443 , training loss: 4.495178\n",
      "[INFO] Epoch: 64 , batch: 444 , training loss: 4.122230\n",
      "[INFO] Epoch: 64 , batch: 445 , training loss: 3.992073\n",
      "[INFO] Epoch: 64 , batch: 446 , training loss: 3.941443\n",
      "[INFO] Epoch: 64 , batch: 447 , training loss: 4.139210\n",
      "[INFO] Epoch: 64 , batch: 448 , training loss: 4.248272\n",
      "[INFO] Epoch: 64 , batch: 449 , training loss: 4.628801\n",
      "[INFO] Epoch: 64 , batch: 450 , training loss: 4.708224\n",
      "[INFO] Epoch: 64 , batch: 451 , training loss: 4.600395\n",
      "[INFO] Epoch: 64 , batch: 452 , training loss: 4.408517\n",
      "[INFO] Epoch: 64 , batch: 453 , training loss: 4.175939\n",
      "[INFO] Epoch: 64 , batch: 454 , training loss: 4.328107\n",
      "[INFO] Epoch: 64 , batch: 455 , training loss: 4.369436\n",
      "[INFO] Epoch: 64 , batch: 456 , training loss: 4.374761\n",
      "[INFO] Epoch: 64 , batch: 457 , training loss: 4.455245\n",
      "[INFO] Epoch: 64 , batch: 458 , training loss: 4.185571\n",
      "[INFO] Epoch: 64 , batch: 459 , training loss: 4.152160\n",
      "[INFO] Epoch: 64 , batch: 460 , training loss: 4.297821\n",
      "[INFO] Epoch: 64 , batch: 461 , training loss: 4.240457\n",
      "[INFO] Epoch: 64 , batch: 462 , training loss: 4.315840\n",
      "[INFO] Epoch: 64 , batch: 463 , training loss: 4.227511\n",
      "[INFO] Epoch: 64 , batch: 464 , training loss: 4.393027\n",
      "[INFO] Epoch: 64 , batch: 465 , training loss: 4.347854\n",
      "[INFO] Epoch: 64 , batch: 466 , training loss: 4.425867\n",
      "[INFO] Epoch: 64 , batch: 467 , training loss: 4.396831\n",
      "[INFO] Epoch: 64 , batch: 468 , training loss: 4.352740\n",
      "[INFO] Epoch: 64 , batch: 469 , training loss: 4.392743\n",
      "[INFO] Epoch: 64 , batch: 470 , training loss: 4.206076\n",
      "[INFO] Epoch: 64 , batch: 471 , training loss: 4.311971\n",
      "[INFO] Epoch: 64 , batch: 472 , training loss: 4.356770\n",
      "[INFO] Epoch: 64 , batch: 473 , training loss: 4.281844\n",
      "[INFO] Epoch: 64 , batch: 474 , training loss: 4.078028\n",
      "[INFO] Epoch: 64 , batch: 475 , training loss: 3.950478\n",
      "[INFO] Epoch: 64 , batch: 476 , training loss: 4.328804\n",
      "[INFO] Epoch: 64 , batch: 477 , training loss: 4.451293\n",
      "[INFO] Epoch: 64 , batch: 478 , training loss: 4.459271\n",
      "[INFO] Epoch: 64 , batch: 479 , training loss: 4.458758\n",
      "[INFO] Epoch: 64 , batch: 480 , training loss: 4.567577\n",
      "[INFO] Epoch: 64 , batch: 481 , training loss: 4.423793\n",
      "[INFO] Epoch: 64 , batch: 482 , training loss: 4.559476\n",
      "[INFO] Epoch: 64 , batch: 483 , training loss: 4.367807\n",
      "[INFO] Epoch: 64 , batch: 484 , training loss: 4.181257\n",
      "[INFO] Epoch: 64 , batch: 485 , training loss: 4.283984\n",
      "[INFO] Epoch: 64 , batch: 486 , training loss: 4.192485\n",
      "[INFO] Epoch: 64 , batch: 487 , training loss: 4.180083\n",
      "[INFO] Epoch: 64 , batch: 488 , training loss: 4.349366\n",
      "[INFO] Epoch: 64 , batch: 489 , training loss: 4.267071\n",
      "[INFO] Epoch: 64 , batch: 490 , training loss: 4.319805\n",
      "[INFO] Epoch: 64 , batch: 491 , training loss: 4.225698\n",
      "[INFO] Epoch: 64 , batch: 492 , training loss: 4.221809\n",
      "[INFO] Epoch: 64 , batch: 493 , training loss: 4.368110\n",
      "[INFO] Epoch: 64 , batch: 494 , training loss: 4.268877\n",
      "[INFO] Epoch: 64 , batch: 495 , training loss: 4.468155\n",
      "[INFO] Epoch: 64 , batch: 496 , training loss: 4.319278\n",
      "[INFO] Epoch: 64 , batch: 497 , training loss: 4.360977\n",
      "[INFO] Epoch: 64 , batch: 498 , training loss: 4.347761\n",
      "[INFO] Epoch: 64 , batch: 499 , training loss: 4.397070\n",
      "[INFO] Epoch: 64 , batch: 500 , training loss: 4.577573\n",
      "[INFO] Epoch: 64 , batch: 501 , training loss: 4.892734\n",
      "[INFO] Epoch: 64 , batch: 502 , training loss: 4.885817\n",
      "[INFO] Epoch: 64 , batch: 503 , training loss: 4.532369\n",
      "[INFO] Epoch: 64 , batch: 504 , training loss: 4.654840\n",
      "[INFO] Epoch: 64 , batch: 505 , training loss: 4.638103\n",
      "[INFO] Epoch: 64 , batch: 506 , training loss: 4.621789\n",
      "[INFO] Epoch: 64 , batch: 507 , training loss: 4.671754\n",
      "[INFO] Epoch: 64 , batch: 508 , training loss: 4.590921\n",
      "[INFO] Epoch: 64 , batch: 509 , training loss: 4.416728\n",
      "[INFO] Epoch: 64 , batch: 510 , training loss: 4.493430\n",
      "[INFO] Epoch: 64 , batch: 511 , training loss: 4.403973\n",
      "[INFO] Epoch: 64 , batch: 512 , training loss: 4.501108\n",
      "[INFO] Epoch: 64 , batch: 513 , training loss: 4.771517\n",
      "[INFO] Epoch: 64 , batch: 514 , training loss: 4.392940\n",
      "[INFO] Epoch: 64 , batch: 515 , training loss: 4.658161\n",
      "[INFO] Epoch: 64 , batch: 516 , training loss: 4.458972\n",
      "[INFO] Epoch: 64 , batch: 517 , training loss: 4.406267\n",
      "[INFO] Epoch: 64 , batch: 518 , training loss: 4.388683\n",
      "[INFO] Epoch: 64 , batch: 519 , training loss: 4.208132\n",
      "[INFO] Epoch: 64 , batch: 520 , training loss: 4.458368\n",
      "[INFO] Epoch: 64 , batch: 521 , training loss: 4.436666\n",
      "[INFO] Epoch: 64 , batch: 522 , training loss: 4.512825\n",
      "[INFO] Epoch: 64 , batch: 523 , training loss: 4.438916\n",
      "[INFO] Epoch: 64 , batch: 524 , training loss: 4.750845\n",
      "[INFO] Epoch: 64 , batch: 525 , training loss: 4.621796\n",
      "[INFO] Epoch: 64 , batch: 526 , training loss: 4.397933\n",
      "[INFO] Epoch: 64 , batch: 527 , training loss: 4.437104\n",
      "[INFO] Epoch: 64 , batch: 528 , training loss: 4.445514\n",
      "[INFO] Epoch: 64 , batch: 529 , training loss: 4.408890\n",
      "[INFO] Epoch: 64 , batch: 530 , training loss: 4.286508\n",
      "[INFO] Epoch: 64 , batch: 531 , training loss: 4.434735\n",
      "[INFO] Epoch: 64 , batch: 532 , training loss: 4.331639\n",
      "[INFO] Epoch: 64 , batch: 533 , training loss: 4.454154\n",
      "[INFO] Epoch: 64 , batch: 534 , training loss: 4.457809\n",
      "[INFO] Epoch: 64 , batch: 535 , training loss: 4.472516\n",
      "[INFO] Epoch: 64 , batch: 536 , training loss: 4.320487\n",
      "[INFO] Epoch: 64 , batch: 537 , training loss: 4.308819\n",
      "[INFO] Epoch: 64 , batch: 538 , training loss: 4.377706\n",
      "[INFO] Epoch: 64 , batch: 539 , training loss: 4.473994\n",
      "[INFO] Epoch: 64 , batch: 540 , training loss: 4.986783\n",
      "[INFO] Epoch: 64 , batch: 541 , training loss: 4.850551\n",
      "[INFO] Epoch: 64 , batch: 542 , training loss: 4.724082\n",
      "[INFO] Epoch: 65 , batch: 0 , training loss: 3.641453\n",
      "[INFO] Epoch: 65 , batch: 1 , training loss: 3.586580\n",
      "[INFO] Epoch: 65 , batch: 2 , training loss: 3.740190\n",
      "[INFO] Epoch: 65 , batch: 3 , training loss: 3.603928\n",
      "[INFO] Epoch: 65 , batch: 4 , training loss: 4.002581\n",
      "[INFO] Epoch: 65 , batch: 5 , training loss: 3.620286\n",
      "[INFO] Epoch: 65 , batch: 6 , training loss: 4.075562\n",
      "[INFO] Epoch: 65 , batch: 7 , training loss: 3.947682\n",
      "[INFO] Epoch: 65 , batch: 8 , training loss: 3.599956\n",
      "[INFO] Epoch: 65 , batch: 9 , training loss: 3.860531\n",
      "[INFO] Epoch: 65 , batch: 10 , training loss: 3.840203\n",
      "[INFO] Epoch: 65 , batch: 11 , training loss: 3.749025\n",
      "[INFO] Epoch: 65 , batch: 12 , training loss: 3.690262\n",
      "[INFO] Epoch: 65 , batch: 13 , training loss: 3.663980\n",
      "[INFO] Epoch: 65 , batch: 14 , training loss: 3.632466\n",
      "[INFO] Epoch: 65 , batch: 15 , training loss: 3.834929\n",
      "[INFO] Epoch: 65 , batch: 16 , training loss: 3.644515\n",
      "[INFO] Epoch: 65 , batch: 17 , training loss: 3.768971\n",
      "[INFO] Epoch: 65 , batch: 18 , training loss: 3.738351\n",
      "[INFO] Epoch: 65 , batch: 19 , training loss: 3.501837\n",
      "[INFO] Epoch: 65 , batch: 20 , training loss: 3.492233\n",
      "[INFO] Epoch: 65 , batch: 21 , training loss: 3.608859\n",
      "[INFO] Epoch: 65 , batch: 22 , training loss: 3.476774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 65 , batch: 23 , training loss: 3.706802\n",
      "[INFO] Epoch: 65 , batch: 24 , training loss: 3.545231\n",
      "[INFO] Epoch: 65 , batch: 25 , training loss: 3.649420\n",
      "[INFO] Epoch: 65 , batch: 26 , training loss: 3.512174\n",
      "[INFO] Epoch: 65 , batch: 27 , training loss: 3.523804\n",
      "[INFO] Epoch: 65 , batch: 28 , training loss: 3.699956\n",
      "[INFO] Epoch: 65 , batch: 29 , training loss: 3.504311\n",
      "[INFO] Epoch: 65 , batch: 30 , training loss: 3.530856\n",
      "[INFO] Epoch: 65 , batch: 31 , training loss: 3.620005\n",
      "[INFO] Epoch: 65 , batch: 32 , training loss: 3.585384\n",
      "[INFO] Epoch: 65 , batch: 33 , training loss: 3.629422\n",
      "[INFO] Epoch: 65 , batch: 34 , training loss: 3.595856\n",
      "[INFO] Epoch: 65 , batch: 35 , training loss: 3.561199\n",
      "[INFO] Epoch: 65 , batch: 36 , training loss: 3.681216\n",
      "[INFO] Epoch: 65 , batch: 37 , training loss: 3.496113\n",
      "[INFO] Epoch: 65 , batch: 38 , training loss: 3.595492\n",
      "[INFO] Epoch: 65 , batch: 39 , training loss: 3.426853\n",
      "[INFO] Epoch: 65 , batch: 40 , training loss: 3.630183\n",
      "[INFO] Epoch: 65 , batch: 41 , training loss: 3.601648\n",
      "[INFO] Epoch: 65 , batch: 42 , training loss: 4.088416\n",
      "[INFO] Epoch: 65 , batch: 43 , training loss: 3.791709\n",
      "[INFO] Epoch: 65 , batch: 44 , training loss: 4.112828\n",
      "[INFO] Epoch: 65 , batch: 45 , training loss: 4.136994\n",
      "[INFO] Epoch: 65 , batch: 46 , training loss: 4.080830\n",
      "[INFO] Epoch: 65 , batch: 47 , training loss: 3.680505\n",
      "[INFO] Epoch: 65 , batch: 48 , training loss: 3.682898\n",
      "[INFO] Epoch: 65 , batch: 49 , training loss: 3.900266\n",
      "[INFO] Epoch: 65 , batch: 50 , training loss: 3.623514\n",
      "[INFO] Epoch: 65 , batch: 51 , training loss: 3.798998\n",
      "[INFO] Epoch: 65 , batch: 52 , training loss: 3.607752\n",
      "[INFO] Epoch: 65 , batch: 53 , training loss: 3.749308\n",
      "[INFO] Epoch: 65 , batch: 54 , training loss: 3.742556\n",
      "[INFO] Epoch: 65 , batch: 55 , training loss: 3.808634\n",
      "[INFO] Epoch: 65 , batch: 56 , training loss: 3.687494\n",
      "[INFO] Epoch: 65 , batch: 57 , training loss: 3.609854\n",
      "[INFO] Epoch: 65 , batch: 58 , training loss: 3.654629\n",
      "[INFO] Epoch: 65 , batch: 59 , training loss: 3.721073\n",
      "[INFO] Epoch: 65 , batch: 60 , training loss: 3.660189\n",
      "[INFO] Epoch: 65 , batch: 61 , training loss: 3.744937\n",
      "[INFO] Epoch: 65 , batch: 62 , training loss: 3.640441\n",
      "[INFO] Epoch: 65 , batch: 63 , training loss: 3.840035\n",
      "[INFO] Epoch: 65 , batch: 64 , training loss: 3.994335\n",
      "[INFO] Epoch: 65 , batch: 65 , training loss: 3.751840\n",
      "[INFO] Epoch: 65 , batch: 66 , training loss: 3.590356\n",
      "[INFO] Epoch: 65 , batch: 67 , training loss: 3.606612\n",
      "[INFO] Epoch: 65 , batch: 68 , training loss: 3.771529\n",
      "[INFO] Epoch: 65 , batch: 69 , training loss: 3.719484\n",
      "[INFO] Epoch: 65 , batch: 70 , training loss: 3.926212\n",
      "[INFO] Epoch: 65 , batch: 71 , training loss: 3.773589\n",
      "[INFO] Epoch: 65 , batch: 72 , training loss: 3.802467\n",
      "[INFO] Epoch: 65 , batch: 73 , training loss: 3.773279\n",
      "[INFO] Epoch: 65 , batch: 74 , training loss: 3.875741\n",
      "[INFO] Epoch: 65 , batch: 75 , training loss: 3.758379\n",
      "[INFO] Epoch: 65 , batch: 76 , training loss: 3.860544\n",
      "[INFO] Epoch: 65 , batch: 77 , training loss: 3.816716\n",
      "[INFO] Epoch: 65 , batch: 78 , training loss: 3.889330\n",
      "[INFO] Epoch: 65 , batch: 79 , training loss: 3.769135\n",
      "[INFO] Epoch: 65 , batch: 80 , training loss: 3.942061\n",
      "[INFO] Epoch: 65 , batch: 81 , training loss: 3.866214\n",
      "[INFO] Epoch: 65 , batch: 82 , training loss: 3.841182\n",
      "[INFO] Epoch: 65 , batch: 83 , training loss: 3.925500\n",
      "[INFO] Epoch: 65 , batch: 84 , training loss: 3.931726\n",
      "[INFO] Epoch: 65 , batch: 85 , training loss: 3.979352\n",
      "[INFO] Epoch: 65 , batch: 86 , training loss: 3.939813\n",
      "[INFO] Epoch: 65 , batch: 87 , training loss: 3.861706\n",
      "[INFO] Epoch: 65 , batch: 88 , training loss: 4.047550\n",
      "[INFO] Epoch: 65 , batch: 89 , training loss: 3.778132\n",
      "[INFO] Epoch: 65 , batch: 90 , training loss: 3.887883\n",
      "[INFO] Epoch: 65 , batch: 91 , training loss: 3.831265\n",
      "[INFO] Epoch: 65 , batch: 92 , training loss: 3.851101\n",
      "[INFO] Epoch: 65 , batch: 93 , training loss: 3.954069\n",
      "[INFO] Epoch: 65 , batch: 94 , training loss: 4.045159\n",
      "[INFO] Epoch: 65 , batch: 95 , training loss: 3.835643\n",
      "[INFO] Epoch: 65 , batch: 96 , training loss: 3.871133\n",
      "[INFO] Epoch: 65 , batch: 97 , training loss: 3.804612\n",
      "[INFO] Epoch: 65 , batch: 98 , training loss: 3.742222\n",
      "[INFO] Epoch: 65 , batch: 99 , training loss: 3.897316\n",
      "[INFO] Epoch: 65 , batch: 100 , training loss: 3.779442\n",
      "[INFO] Epoch: 65 , batch: 101 , training loss: 3.805380\n",
      "[INFO] Epoch: 65 , batch: 102 , training loss: 3.925783\n",
      "[INFO] Epoch: 65 , batch: 103 , training loss: 3.720304\n",
      "[INFO] Epoch: 65 , batch: 104 , training loss: 3.719351\n",
      "[INFO] Epoch: 65 , batch: 105 , training loss: 3.956769\n",
      "[INFO] Epoch: 65 , batch: 106 , training loss: 3.964230\n",
      "[INFO] Epoch: 65 , batch: 107 , training loss: 3.803594\n",
      "[INFO] Epoch: 65 , batch: 108 , training loss: 3.758076\n",
      "[INFO] Epoch: 65 , batch: 109 , training loss: 3.656874\n",
      "[INFO] Epoch: 65 , batch: 110 , training loss: 3.858708\n",
      "[INFO] Epoch: 65 , batch: 111 , training loss: 3.901201\n",
      "[INFO] Epoch: 65 , batch: 112 , training loss: 3.802064\n",
      "[INFO] Epoch: 65 , batch: 113 , training loss: 3.803050\n",
      "[INFO] Epoch: 65 , batch: 114 , training loss: 3.821803\n",
      "[INFO] Epoch: 65 , batch: 115 , training loss: 3.839323\n",
      "[INFO] Epoch: 65 , batch: 116 , training loss: 3.738214\n",
      "[INFO] Epoch: 65 , batch: 117 , training loss: 3.946266\n",
      "[INFO] Epoch: 65 , batch: 118 , training loss: 3.912288\n",
      "[INFO] Epoch: 65 , batch: 119 , training loss: 4.051355\n",
      "[INFO] Epoch: 65 , batch: 120 , training loss: 4.031303\n",
      "[INFO] Epoch: 65 , batch: 121 , training loss: 3.918339\n",
      "[INFO] Epoch: 65 , batch: 122 , training loss: 3.797509\n",
      "[INFO] Epoch: 65 , batch: 123 , training loss: 3.800299\n",
      "[INFO] Epoch: 65 , batch: 124 , training loss: 3.949255\n",
      "[INFO] Epoch: 65 , batch: 125 , training loss: 3.743187\n",
      "[INFO] Epoch: 65 , batch: 126 , training loss: 3.743215\n",
      "[INFO] Epoch: 65 , batch: 127 , training loss: 3.727602\n",
      "[INFO] Epoch: 65 , batch: 128 , training loss: 3.877832\n",
      "[INFO] Epoch: 65 , batch: 129 , training loss: 3.818875\n",
      "[INFO] Epoch: 65 , batch: 130 , training loss: 3.854116\n",
      "[INFO] Epoch: 65 , batch: 131 , training loss: 3.853441\n",
      "[INFO] Epoch: 65 , batch: 132 , training loss: 3.868756\n",
      "[INFO] Epoch: 65 , batch: 133 , training loss: 3.793323\n",
      "[INFO] Epoch: 65 , batch: 134 , training loss: 3.619069\n",
      "[INFO] Epoch: 65 , batch: 135 , training loss: 3.664695\n",
      "[INFO] Epoch: 65 , batch: 136 , training loss: 3.916230\n",
      "[INFO] Epoch: 65 , batch: 137 , training loss: 3.877964\n",
      "[INFO] Epoch: 65 , batch: 138 , training loss: 3.923359\n",
      "[INFO] Epoch: 65 , batch: 139 , training loss: 4.489381\n",
      "[INFO] Epoch: 65 , batch: 140 , training loss: 4.256427\n",
      "[INFO] Epoch: 65 , batch: 141 , training loss: 4.053378\n",
      "[INFO] Epoch: 65 , batch: 142 , training loss: 3.778102\n",
      "[INFO] Epoch: 65 , batch: 143 , training loss: 3.914818\n",
      "[INFO] Epoch: 65 , batch: 144 , training loss: 3.748585\n",
      "[INFO] Epoch: 65 , batch: 145 , training loss: 3.785481\n",
      "[INFO] Epoch: 65 , batch: 146 , training loss: 4.018867\n",
      "[INFO] Epoch: 65 , batch: 147 , training loss: 3.690230\n",
      "[INFO] Epoch: 65 , batch: 148 , training loss: 3.651392\n",
      "[INFO] Epoch: 65 , batch: 149 , training loss: 3.743910\n",
      "[INFO] Epoch: 65 , batch: 150 , training loss: 3.967404\n",
      "[INFO] Epoch: 65 , batch: 151 , training loss: 3.822089\n",
      "[INFO] Epoch: 65 , batch: 152 , training loss: 3.843653\n",
      "[INFO] Epoch: 65 , batch: 153 , training loss: 3.911803\n",
      "[INFO] Epoch: 65 , batch: 154 , training loss: 3.989054\n",
      "[INFO] Epoch: 65 , batch: 155 , training loss: 4.148435\n",
      "[INFO] Epoch: 65 , batch: 156 , training loss: 3.914876\n",
      "[INFO] Epoch: 65 , batch: 157 , training loss: 3.863187\n",
      "[INFO] Epoch: 65 , batch: 158 , training loss: 4.001804\n",
      "[INFO] Epoch: 65 , batch: 159 , training loss: 3.889856\n",
      "[INFO] Epoch: 65 , batch: 160 , training loss: 4.126291\n",
      "[INFO] Epoch: 65 , batch: 161 , training loss: 4.169468\n",
      "[INFO] Epoch: 65 , batch: 162 , training loss: 4.140782\n",
      "[INFO] Epoch: 65 , batch: 163 , training loss: 4.328366\n",
      "[INFO] Epoch: 65 , batch: 164 , training loss: 4.281339\n",
      "[INFO] Epoch: 65 , batch: 165 , training loss: 4.174811\n",
      "[INFO] Epoch: 65 , batch: 166 , training loss: 4.150626\n",
      "[INFO] Epoch: 65 , batch: 167 , training loss: 4.130375\n",
      "[INFO] Epoch: 65 , batch: 168 , training loss: 3.904465\n",
      "[INFO] Epoch: 65 , batch: 169 , training loss: 3.858389\n",
      "[INFO] Epoch: 65 , batch: 170 , training loss: 4.030982\n",
      "[INFO] Epoch: 65 , batch: 171 , training loss: 3.537246\n",
      "[INFO] Epoch: 65 , batch: 172 , training loss: 3.740885\n",
      "[INFO] Epoch: 65 , batch: 173 , training loss: 4.013285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 65 , batch: 174 , training loss: 4.450930\n",
      "[INFO] Epoch: 65 , batch: 175 , training loss: 4.738050\n",
      "[INFO] Epoch: 65 , batch: 176 , training loss: 4.402318\n",
      "[INFO] Epoch: 65 , batch: 177 , training loss: 4.000750\n",
      "[INFO] Epoch: 65 , batch: 178 , training loss: 4.023017\n",
      "[INFO] Epoch: 65 , batch: 179 , training loss: 4.072572\n",
      "[INFO] Epoch: 65 , batch: 180 , training loss: 4.016655\n",
      "[INFO] Epoch: 65 , batch: 181 , training loss: 4.319777\n",
      "[INFO] Epoch: 65 , batch: 182 , training loss: 4.254972\n",
      "[INFO] Epoch: 65 , batch: 183 , training loss: 4.254114\n",
      "[INFO] Epoch: 65 , batch: 184 , training loss: 4.123281\n",
      "[INFO] Epoch: 65 , batch: 185 , training loss: 4.088009\n",
      "[INFO] Epoch: 65 , batch: 186 , training loss: 4.237432\n",
      "[INFO] Epoch: 65 , batch: 187 , training loss: 4.347785\n",
      "[INFO] Epoch: 65 , batch: 188 , training loss: 4.293925\n",
      "[INFO] Epoch: 65 , batch: 189 , training loss: 4.249212\n",
      "[INFO] Epoch: 65 , batch: 190 , training loss: 4.270271\n",
      "[INFO] Epoch: 65 , batch: 191 , training loss: 4.375386\n",
      "[INFO] Epoch: 65 , batch: 192 , training loss: 4.223223\n",
      "[INFO] Epoch: 65 , batch: 193 , training loss: 4.310476\n",
      "[INFO] Epoch: 65 , batch: 194 , training loss: 4.250523\n",
      "[INFO] Epoch: 65 , batch: 195 , training loss: 4.176403\n",
      "[INFO] Epoch: 65 , batch: 196 , training loss: 4.040370\n",
      "[INFO] Epoch: 65 , batch: 197 , training loss: 4.127745\n",
      "[INFO] Epoch: 65 , batch: 198 , training loss: 4.064521\n",
      "[INFO] Epoch: 65 , batch: 199 , training loss: 4.200210\n",
      "[INFO] Epoch: 65 , batch: 200 , training loss: 4.092603\n",
      "[INFO] Epoch: 65 , batch: 201 , training loss: 3.986881\n",
      "[INFO] Epoch: 65 , batch: 202 , training loss: 3.985302\n",
      "[INFO] Epoch: 65 , batch: 203 , training loss: 4.141154\n",
      "[INFO] Epoch: 65 , batch: 204 , training loss: 4.198133\n",
      "[INFO] Epoch: 65 , batch: 205 , training loss: 3.823020\n",
      "[INFO] Epoch: 65 , batch: 206 , training loss: 3.741473\n",
      "[INFO] Epoch: 65 , batch: 207 , training loss: 3.715703\n",
      "[INFO] Epoch: 65 , batch: 208 , training loss: 4.048380\n",
      "[INFO] Epoch: 65 , batch: 209 , training loss: 4.046697\n",
      "[INFO] Epoch: 65 , batch: 210 , training loss: 4.039141\n",
      "[INFO] Epoch: 65 , batch: 211 , training loss: 4.038074\n",
      "[INFO] Epoch: 65 , batch: 212 , training loss: 4.124457\n",
      "[INFO] Epoch: 65 , batch: 213 , training loss: 4.065950\n",
      "[INFO] Epoch: 65 , batch: 214 , training loss: 4.146620\n",
      "[INFO] Epoch: 65 , batch: 215 , training loss: 4.369254\n",
      "[INFO] Epoch: 65 , batch: 216 , training loss: 4.050537\n",
      "[INFO] Epoch: 65 , batch: 217 , training loss: 4.008009\n",
      "[INFO] Epoch: 65 , batch: 218 , training loss: 4.009104\n",
      "[INFO] Epoch: 65 , batch: 219 , training loss: 4.124565\n",
      "[INFO] Epoch: 65 , batch: 220 , training loss: 3.963771\n",
      "[INFO] Epoch: 65 , batch: 221 , training loss: 3.954203\n",
      "[INFO] Epoch: 65 , batch: 222 , training loss: 4.086322\n",
      "[INFO] Epoch: 65 , batch: 223 , training loss: 4.216043\n",
      "[INFO] Epoch: 65 , batch: 224 , training loss: 4.248901\n",
      "[INFO] Epoch: 65 , batch: 225 , training loss: 4.137621\n",
      "[INFO] Epoch: 65 , batch: 226 , training loss: 4.266037\n",
      "[INFO] Epoch: 65 , batch: 227 , training loss: 4.235220\n",
      "[INFO] Epoch: 65 , batch: 228 , training loss: 4.247329\n",
      "[INFO] Epoch: 65 , batch: 229 , training loss: 4.124777\n",
      "[INFO] Epoch: 65 , batch: 230 , training loss: 3.977361\n",
      "[INFO] Epoch: 65 , batch: 231 , training loss: 3.844427\n",
      "[INFO] Epoch: 65 , batch: 232 , training loss: 3.981396\n",
      "[INFO] Epoch: 65 , batch: 233 , training loss: 4.020056\n",
      "[INFO] Epoch: 65 , batch: 234 , training loss: 3.700658\n",
      "[INFO] Epoch: 65 , batch: 235 , training loss: 3.815799\n",
      "[INFO] Epoch: 65 , batch: 236 , training loss: 3.889294\n",
      "[INFO] Epoch: 65 , batch: 237 , training loss: 4.127160\n",
      "[INFO] Epoch: 65 , batch: 238 , training loss: 3.924986\n",
      "[INFO] Epoch: 65 , batch: 239 , training loss: 3.936633\n",
      "[INFO] Epoch: 65 , batch: 240 , training loss: 4.000935\n",
      "[INFO] Epoch: 65 , batch: 241 , training loss: 3.787733\n",
      "[INFO] Epoch: 65 , batch: 242 , training loss: 3.828524\n",
      "[INFO] Epoch: 65 , batch: 243 , training loss: 4.117598\n",
      "[INFO] Epoch: 65 , batch: 244 , training loss: 4.067442\n",
      "[INFO] Epoch: 65 , batch: 245 , training loss: 4.018474\n",
      "[INFO] Epoch: 65 , batch: 246 , training loss: 3.726703\n",
      "[INFO] Epoch: 65 , batch: 247 , training loss: 3.916607\n",
      "[INFO] Epoch: 65 , batch: 248 , training loss: 3.975000\n",
      "[INFO] Epoch: 65 , batch: 249 , training loss: 3.938026\n",
      "[INFO] Epoch: 65 , batch: 250 , training loss: 3.759458\n",
      "[INFO] Epoch: 65 , batch: 251 , training loss: 4.202413\n",
      "[INFO] Epoch: 65 , batch: 252 , training loss: 3.932288\n",
      "[INFO] Epoch: 65 , batch: 253 , training loss: 3.800505\n",
      "[INFO] Epoch: 65 , batch: 254 , training loss: 4.106968\n",
      "[INFO] Epoch: 65 , batch: 255 , training loss: 4.081892\n",
      "[INFO] Epoch: 65 , batch: 256 , training loss: 4.034218\n",
      "[INFO] Epoch: 65 , batch: 257 , training loss: 4.226486\n",
      "[INFO] Epoch: 65 , batch: 258 , training loss: 4.202284\n",
      "[INFO] Epoch: 65 , batch: 259 , training loss: 4.244264\n",
      "[INFO] Epoch: 65 , batch: 260 , training loss: 4.043950\n",
      "[INFO] Epoch: 65 , batch: 261 , training loss: 4.199555\n",
      "[INFO] Epoch: 65 , batch: 262 , training loss: 4.340458\n",
      "[INFO] Epoch: 65 , batch: 263 , training loss: 4.511282\n",
      "[INFO] Epoch: 65 , batch: 264 , training loss: 3.877672\n",
      "[INFO] Epoch: 65 , batch: 265 , training loss: 3.990347\n",
      "[INFO] Epoch: 65 , batch: 266 , training loss: 4.372584\n",
      "[INFO] Epoch: 65 , batch: 267 , training loss: 4.153172\n",
      "[INFO] Epoch: 65 , batch: 268 , training loss: 4.066951\n",
      "[INFO] Epoch: 65 , batch: 269 , training loss: 4.014067\n",
      "[INFO] Epoch: 65 , batch: 270 , training loss: 4.060666\n",
      "[INFO] Epoch: 65 , batch: 271 , training loss: 4.079589\n",
      "[INFO] Epoch: 65 , batch: 272 , training loss: 4.068577\n",
      "[INFO] Epoch: 65 , batch: 273 , training loss: 4.110090\n",
      "[INFO] Epoch: 65 , batch: 274 , training loss: 4.197918\n",
      "[INFO] Epoch: 65 , batch: 275 , training loss: 4.057796\n",
      "[INFO] Epoch: 65 , batch: 276 , training loss: 4.115294\n",
      "[INFO] Epoch: 65 , batch: 277 , training loss: 4.263042\n",
      "[INFO] Epoch: 65 , batch: 278 , training loss: 3.953877\n",
      "[INFO] Epoch: 65 , batch: 279 , training loss: 3.960074\n",
      "[INFO] Epoch: 65 , batch: 280 , training loss: 3.949855\n",
      "[INFO] Epoch: 65 , batch: 281 , training loss: 4.071943\n",
      "[INFO] Epoch: 65 , batch: 282 , training loss: 4.018137\n",
      "[INFO] Epoch: 65 , batch: 283 , training loss: 4.005945\n",
      "[INFO] Epoch: 65 , batch: 284 , training loss: 4.016314\n",
      "[INFO] Epoch: 65 , batch: 285 , training loss: 3.964042\n",
      "[INFO] Epoch: 65 , batch: 286 , training loss: 3.971414\n",
      "[INFO] Epoch: 65 , batch: 287 , training loss: 3.919931\n",
      "[INFO] Epoch: 65 , batch: 288 , training loss: 3.850771\n",
      "[INFO] Epoch: 65 , batch: 289 , training loss: 3.960202\n",
      "[INFO] Epoch: 65 , batch: 290 , training loss: 3.742492\n",
      "[INFO] Epoch: 65 , batch: 291 , training loss: 3.724764\n",
      "[INFO] Epoch: 65 , batch: 292 , training loss: 3.828343\n",
      "[INFO] Epoch: 65 , batch: 293 , training loss: 3.750489\n",
      "[INFO] Epoch: 65 , batch: 294 , training loss: 4.394567\n",
      "[INFO] Epoch: 65 , batch: 295 , training loss: 4.177675\n",
      "[INFO] Epoch: 65 , batch: 296 , training loss: 4.110175\n",
      "[INFO] Epoch: 65 , batch: 297 , training loss: 4.091989\n",
      "[INFO] Epoch: 65 , batch: 298 , training loss: 3.918136\n",
      "[INFO] Epoch: 65 , batch: 299 , training loss: 3.956630\n",
      "[INFO] Epoch: 65 , batch: 300 , training loss: 3.929860\n",
      "[INFO] Epoch: 65 , batch: 301 , training loss: 3.868587\n",
      "[INFO] Epoch: 65 , batch: 302 , training loss: 4.029493\n",
      "[INFO] Epoch: 65 , batch: 303 , training loss: 4.057074\n",
      "[INFO] Epoch: 65 , batch: 304 , training loss: 4.183132\n",
      "[INFO] Epoch: 65 , batch: 305 , training loss: 4.032772\n",
      "[INFO] Epoch: 65 , batch: 306 , training loss: 4.119345\n",
      "[INFO] Epoch: 65 , batch: 307 , training loss: 4.162508\n",
      "[INFO] Epoch: 65 , batch: 308 , training loss: 3.965869\n",
      "[INFO] Epoch: 65 , batch: 309 , training loss: 3.936539\n",
      "[INFO] Epoch: 65 , batch: 310 , training loss: 3.883051\n",
      "[INFO] Epoch: 65 , batch: 311 , training loss: 3.876939\n",
      "[INFO] Epoch: 65 , batch: 312 , training loss: 3.767707\n",
      "[INFO] Epoch: 65 , batch: 313 , training loss: 3.883138\n",
      "[INFO] Epoch: 65 , batch: 314 , training loss: 3.958561\n",
      "[INFO] Epoch: 65 , batch: 315 , training loss: 4.045382\n",
      "[INFO] Epoch: 65 , batch: 316 , training loss: 4.272509\n",
      "[INFO] Epoch: 65 , batch: 317 , training loss: 4.611221\n",
      "[INFO] Epoch: 65 , batch: 318 , training loss: 4.734998\n",
      "[INFO] Epoch: 65 , batch: 319 , training loss: 4.416206\n",
      "[INFO] Epoch: 65 , batch: 320 , training loss: 3.994585\n",
      "[INFO] Epoch: 65 , batch: 321 , training loss: 3.819506\n",
      "[INFO] Epoch: 65 , batch: 322 , training loss: 3.908131\n",
      "[INFO] Epoch: 65 , batch: 323 , training loss: 3.959862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 65 , batch: 324 , training loss: 3.925011\n",
      "[INFO] Epoch: 65 , batch: 325 , training loss: 4.052569\n",
      "[INFO] Epoch: 65 , batch: 326 , training loss: 4.077784\n",
      "[INFO] Epoch: 65 , batch: 327 , training loss: 4.046756\n",
      "[INFO] Epoch: 65 , batch: 328 , training loss: 4.052092\n",
      "[INFO] Epoch: 65 , batch: 329 , training loss: 3.960781\n",
      "[INFO] Epoch: 65 , batch: 330 , training loss: 3.935408\n",
      "[INFO] Epoch: 65 , batch: 331 , training loss: 4.085133\n",
      "[INFO] Epoch: 65 , batch: 332 , training loss: 3.927797\n",
      "[INFO] Epoch: 65 , batch: 333 , training loss: 3.913927\n",
      "[INFO] Epoch: 65 , batch: 334 , training loss: 3.936083\n",
      "[INFO] Epoch: 65 , batch: 335 , training loss: 4.058908\n",
      "[INFO] Epoch: 65 , batch: 336 , training loss: 4.069242\n",
      "[INFO] Epoch: 65 , batch: 337 , training loss: 4.120296\n",
      "[INFO] Epoch: 65 , batch: 338 , training loss: 4.323312\n",
      "[INFO] Epoch: 65 , batch: 339 , training loss: 4.142982\n",
      "[INFO] Epoch: 65 , batch: 340 , training loss: 4.318347\n",
      "[INFO] Epoch: 65 , batch: 341 , training loss: 4.088481\n",
      "[INFO] Epoch: 65 , batch: 342 , training loss: 3.870795\n",
      "[INFO] Epoch: 65 , batch: 343 , training loss: 3.927921\n",
      "[INFO] Epoch: 65 , batch: 344 , training loss: 3.794641\n",
      "[INFO] Epoch: 65 , batch: 345 , training loss: 3.927536\n",
      "[INFO] Epoch: 65 , batch: 346 , training loss: 3.980479\n",
      "[INFO] Epoch: 65 , batch: 347 , training loss: 3.891293\n",
      "[INFO] Epoch: 65 , batch: 348 , training loss: 3.985751\n",
      "[INFO] Epoch: 65 , batch: 349 , training loss: 4.071727\n",
      "[INFO] Epoch: 65 , batch: 350 , training loss: 3.948725\n",
      "[INFO] Epoch: 65 , batch: 351 , training loss: 4.028751\n",
      "[INFO] Epoch: 65 , batch: 352 , training loss: 4.041794\n",
      "[INFO] Epoch: 65 , batch: 353 , training loss: 4.031439\n",
      "[INFO] Epoch: 65 , batch: 354 , training loss: 4.079397\n",
      "[INFO] Epoch: 65 , batch: 355 , training loss: 4.097775\n",
      "[INFO] Epoch: 65 , batch: 356 , training loss: 3.964706\n",
      "[INFO] Epoch: 65 , batch: 357 , training loss: 4.013450\n",
      "[INFO] Epoch: 65 , batch: 358 , training loss: 3.943028\n",
      "[INFO] Epoch: 65 , batch: 359 , training loss: 3.944364\n",
      "[INFO] Epoch: 65 , batch: 360 , training loss: 4.046844\n",
      "[INFO] Epoch: 65 , batch: 361 , training loss: 4.020782\n",
      "[INFO] Epoch: 65 , batch: 362 , training loss: 4.143960\n",
      "[INFO] Epoch: 65 , batch: 363 , training loss: 4.030977\n",
      "[INFO] Epoch: 65 , batch: 364 , training loss: 4.072484\n",
      "[INFO] Epoch: 65 , batch: 365 , training loss: 4.000104\n",
      "[INFO] Epoch: 65 , batch: 366 , training loss: 4.084575\n",
      "[INFO] Epoch: 65 , batch: 367 , training loss: 4.119966\n",
      "[INFO] Epoch: 65 , batch: 368 , training loss: 4.536218\n",
      "[INFO] Epoch: 65 , batch: 369 , training loss: 4.202501\n",
      "[INFO] Epoch: 65 , batch: 370 , training loss: 3.979866\n",
      "[INFO] Epoch: 65 , batch: 371 , training loss: 4.375148\n",
      "[INFO] Epoch: 65 , batch: 372 , training loss: 4.641384\n",
      "[INFO] Epoch: 65 , batch: 373 , training loss: 4.685120\n",
      "[INFO] Epoch: 65 , batch: 374 , training loss: 4.799310\n",
      "[INFO] Epoch: 65 , batch: 375 , training loss: 4.808352\n",
      "[INFO] Epoch: 65 , batch: 376 , training loss: 4.653772\n",
      "[INFO] Epoch: 65 , batch: 377 , training loss: 4.426877\n",
      "[INFO] Epoch: 65 , batch: 378 , training loss: 4.510488\n",
      "[INFO] Epoch: 65 , batch: 379 , training loss: 4.494528\n",
      "[INFO] Epoch: 65 , batch: 380 , training loss: 4.695626\n",
      "[INFO] Epoch: 65 , batch: 381 , training loss: 4.358807\n",
      "[INFO] Epoch: 65 , batch: 382 , training loss: 4.606101\n",
      "[INFO] Epoch: 65 , batch: 383 , training loss: 4.672414\n",
      "[INFO] Epoch: 65 , batch: 384 , training loss: 4.646815\n",
      "[INFO] Epoch: 65 , batch: 385 , training loss: 4.328857\n",
      "[INFO] Epoch: 65 , batch: 386 , training loss: 4.563346\n",
      "[INFO] Epoch: 65 , batch: 387 , training loss: 4.498005\n",
      "[INFO] Epoch: 65 , batch: 388 , training loss: 4.331942\n",
      "[INFO] Epoch: 65 , batch: 389 , training loss: 4.148643\n",
      "[INFO] Epoch: 65 , batch: 390 , training loss: 4.194466\n",
      "[INFO] Epoch: 65 , batch: 391 , training loss: 4.223067\n",
      "[INFO] Epoch: 65 , batch: 392 , training loss: 4.586138\n",
      "[INFO] Epoch: 65 , batch: 393 , training loss: 4.483716\n",
      "[INFO] Epoch: 65 , batch: 394 , training loss: 4.576600\n",
      "[INFO] Epoch: 65 , batch: 395 , training loss: 4.391706\n",
      "[INFO] Epoch: 65 , batch: 396 , training loss: 4.195369\n",
      "[INFO] Epoch: 65 , batch: 397 , training loss: 4.385812\n",
      "[INFO] Epoch: 65 , batch: 398 , training loss: 4.220861\n",
      "[INFO] Epoch: 65 , batch: 399 , training loss: 4.301701\n",
      "[INFO] Epoch: 65 , batch: 400 , training loss: 4.299734\n",
      "[INFO] Epoch: 65 , batch: 401 , training loss: 4.696199\n",
      "[INFO] Epoch: 65 , batch: 402 , training loss: 4.449813\n",
      "[INFO] Epoch: 65 , batch: 403 , training loss: 4.275628\n",
      "[INFO] Epoch: 65 , batch: 404 , training loss: 4.450813\n",
      "[INFO] Epoch: 65 , batch: 405 , training loss: 4.483652\n",
      "[INFO] Epoch: 65 , batch: 406 , training loss: 4.429469\n",
      "[INFO] Epoch: 65 , batch: 407 , training loss: 4.435574\n",
      "[INFO] Epoch: 65 , batch: 408 , training loss: 4.407942\n",
      "[INFO] Epoch: 65 , batch: 409 , training loss: 4.441509\n",
      "[INFO] Epoch: 65 , batch: 410 , training loss: 4.482450\n",
      "[INFO] Epoch: 65 , batch: 411 , training loss: 4.676429\n",
      "[INFO] Epoch: 65 , batch: 412 , training loss: 4.483472\n",
      "[INFO] Epoch: 65 , batch: 413 , training loss: 4.341921\n",
      "[INFO] Epoch: 65 , batch: 414 , training loss: 4.379212\n",
      "[INFO] Epoch: 65 , batch: 415 , training loss: 4.447005\n",
      "[INFO] Epoch: 65 , batch: 416 , training loss: 4.486762\n",
      "[INFO] Epoch: 65 , batch: 417 , training loss: 4.425591\n",
      "[INFO] Epoch: 65 , batch: 418 , training loss: 4.482965\n",
      "[INFO] Epoch: 65 , batch: 419 , training loss: 4.424264\n",
      "[INFO] Epoch: 65 , batch: 420 , training loss: 4.403352\n",
      "[INFO] Epoch: 65 , batch: 421 , training loss: 4.396020\n",
      "[INFO] Epoch: 65 , batch: 422 , training loss: 4.236398\n",
      "[INFO] Epoch: 65 , batch: 423 , training loss: 4.469848\n",
      "[INFO] Epoch: 65 , batch: 424 , training loss: 4.624100\n",
      "[INFO] Epoch: 65 , batch: 425 , training loss: 4.501406\n",
      "[INFO] Epoch: 65 , batch: 426 , training loss: 4.227546\n",
      "[INFO] Epoch: 65 , batch: 427 , training loss: 4.453645\n",
      "[INFO] Epoch: 65 , batch: 428 , training loss: 4.331759\n",
      "[INFO] Epoch: 65 , batch: 429 , training loss: 4.233696\n",
      "[INFO] Epoch: 65 , batch: 430 , training loss: 4.467874\n",
      "[INFO] Epoch: 65 , batch: 431 , training loss: 4.082482\n",
      "[INFO] Epoch: 65 , batch: 432 , training loss: 4.136601\n",
      "[INFO] Epoch: 65 , batch: 433 , training loss: 4.187782\n",
      "[INFO] Epoch: 65 , batch: 434 , training loss: 4.061119\n",
      "[INFO] Epoch: 65 , batch: 435 , training loss: 4.413235\n",
      "[INFO] Epoch: 65 , batch: 436 , training loss: 4.446906\n",
      "[INFO] Epoch: 65 , batch: 437 , training loss: 4.251065\n",
      "[INFO] Epoch: 65 , batch: 438 , training loss: 4.109302\n",
      "[INFO] Epoch: 65 , batch: 439 , training loss: 4.329906\n",
      "[INFO] Epoch: 65 , batch: 440 , training loss: 4.448754\n",
      "[INFO] Epoch: 65 , batch: 441 , training loss: 4.553497\n",
      "[INFO] Epoch: 65 , batch: 442 , training loss: 4.295654\n",
      "[INFO] Epoch: 65 , batch: 443 , training loss: 4.461691\n",
      "[INFO] Epoch: 65 , batch: 444 , training loss: 4.106161\n",
      "[INFO] Epoch: 65 , batch: 445 , training loss: 3.973499\n",
      "[INFO] Epoch: 65 , batch: 446 , training loss: 3.926106\n",
      "[INFO] Epoch: 65 , batch: 447 , training loss: 4.125295\n",
      "[INFO] Epoch: 65 , batch: 448 , training loss: 4.246863\n",
      "[INFO] Epoch: 65 , batch: 449 , training loss: 4.638966\n",
      "[INFO] Epoch: 65 , batch: 450 , training loss: 4.683423\n",
      "[INFO] Epoch: 65 , batch: 451 , training loss: 4.589299\n",
      "[INFO] Epoch: 65 , batch: 452 , training loss: 4.409317\n",
      "[INFO] Epoch: 65 , batch: 453 , training loss: 4.154222\n",
      "[INFO] Epoch: 65 , batch: 454 , training loss: 4.315969\n",
      "[INFO] Epoch: 65 , batch: 455 , training loss: 4.353177\n",
      "[INFO] Epoch: 65 , batch: 456 , training loss: 4.364306\n",
      "[INFO] Epoch: 65 , batch: 457 , training loss: 4.451916\n",
      "[INFO] Epoch: 65 , batch: 458 , training loss: 4.179102\n",
      "[INFO] Epoch: 65 , batch: 459 , training loss: 4.144946\n",
      "[INFO] Epoch: 65 , batch: 460 , training loss: 4.283823\n",
      "[INFO] Epoch: 65 , batch: 461 , training loss: 4.217165\n",
      "[INFO] Epoch: 65 , batch: 462 , training loss: 4.306812\n",
      "[INFO] Epoch: 65 , batch: 463 , training loss: 4.210552\n",
      "[INFO] Epoch: 65 , batch: 464 , training loss: 4.390302\n",
      "[INFO] Epoch: 65 , batch: 465 , training loss: 4.347456\n",
      "[INFO] Epoch: 65 , batch: 466 , training loss: 4.414445\n",
      "[INFO] Epoch: 65 , batch: 467 , training loss: 4.381855\n",
      "[INFO] Epoch: 65 , batch: 468 , training loss: 4.349593\n",
      "[INFO] Epoch: 65 , batch: 469 , training loss: 4.377277\n",
      "[INFO] Epoch: 65 , batch: 470 , training loss: 4.191908\n",
      "[INFO] Epoch: 65 , batch: 471 , training loss: 4.306118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 65 , batch: 472 , training loss: 4.345187\n",
      "[INFO] Epoch: 65 , batch: 473 , training loss: 4.270350\n",
      "[INFO] Epoch: 65 , batch: 474 , training loss: 4.066106\n",
      "[INFO] Epoch: 65 , batch: 475 , training loss: 3.942265\n",
      "[INFO] Epoch: 65 , batch: 476 , training loss: 4.307053\n",
      "[INFO] Epoch: 65 , batch: 477 , training loss: 4.456346\n",
      "[INFO] Epoch: 65 , batch: 478 , training loss: 4.450177\n",
      "[INFO] Epoch: 65 , batch: 479 , training loss: 4.457984\n",
      "[INFO] Epoch: 65 , batch: 480 , training loss: 4.543813\n",
      "[INFO] Epoch: 65 , batch: 481 , training loss: 4.433136\n",
      "[INFO] Epoch: 65 , batch: 482 , training loss: 4.550266\n",
      "[INFO] Epoch: 65 , batch: 483 , training loss: 4.387207\n",
      "[INFO] Epoch: 65 , batch: 484 , training loss: 4.162668\n",
      "[INFO] Epoch: 65 , batch: 485 , training loss: 4.283257\n",
      "[INFO] Epoch: 65 , batch: 486 , training loss: 4.193319\n",
      "[INFO] Epoch: 65 , batch: 487 , training loss: 4.171178\n",
      "[INFO] Epoch: 65 , batch: 488 , training loss: 4.353408\n",
      "[INFO] Epoch: 65 , batch: 489 , training loss: 4.263564\n",
      "[INFO] Epoch: 65 , batch: 490 , training loss: 4.308136\n",
      "[INFO] Epoch: 65 , batch: 491 , training loss: 4.218520\n",
      "[INFO] Epoch: 65 , batch: 492 , training loss: 4.208705\n",
      "[INFO] Epoch: 65 , batch: 493 , training loss: 4.369913\n",
      "[INFO] Epoch: 65 , batch: 494 , training loss: 4.276487\n",
      "[INFO] Epoch: 65 , batch: 495 , training loss: 4.442529\n",
      "[INFO] Epoch: 65 , batch: 496 , training loss: 4.305789\n",
      "[INFO] Epoch: 65 , batch: 497 , training loss: 4.348022\n",
      "[INFO] Epoch: 65 , batch: 498 , training loss: 4.329659\n",
      "[INFO] Epoch: 65 , batch: 499 , training loss: 4.390767\n",
      "[INFO] Epoch: 65 , batch: 500 , training loss: 4.553494\n",
      "[INFO] Epoch: 65 , batch: 501 , training loss: 4.858099\n",
      "[INFO] Epoch: 65 , batch: 502 , training loss: 4.878374\n",
      "[INFO] Epoch: 65 , batch: 503 , training loss: 4.505445\n",
      "[INFO] Epoch: 65 , batch: 504 , training loss: 4.667398\n",
      "[INFO] Epoch: 65 , batch: 505 , training loss: 4.620538\n",
      "[INFO] Epoch: 65 , batch: 506 , training loss: 4.637725\n",
      "[INFO] Epoch: 65 , batch: 507 , training loss: 4.660472\n",
      "[INFO] Epoch: 65 , batch: 508 , training loss: 4.578871\n",
      "[INFO] Epoch: 65 , batch: 509 , training loss: 4.385109\n",
      "[INFO] Epoch: 65 , batch: 510 , training loss: 4.473284\n",
      "[INFO] Epoch: 65 , batch: 511 , training loss: 4.385408\n",
      "[INFO] Epoch: 65 , batch: 512 , training loss: 4.498204\n",
      "[INFO] Epoch: 65 , batch: 513 , training loss: 4.753932\n",
      "[INFO] Epoch: 65 , batch: 514 , training loss: 4.383722\n",
      "[INFO] Epoch: 65 , batch: 515 , training loss: 4.654239\n",
      "[INFO] Epoch: 65 , batch: 516 , training loss: 4.444610\n",
      "[INFO] Epoch: 65 , batch: 517 , training loss: 4.421130\n",
      "[INFO] Epoch: 65 , batch: 518 , training loss: 4.370408\n",
      "[INFO] Epoch: 65 , batch: 519 , training loss: 4.199135\n",
      "[INFO] Epoch: 65 , batch: 520 , training loss: 4.466739\n",
      "[INFO] Epoch: 65 , batch: 521 , training loss: 4.421115\n",
      "[INFO] Epoch: 65 , batch: 522 , training loss: 4.513420\n",
      "[INFO] Epoch: 65 , batch: 523 , training loss: 4.431885\n",
      "[INFO] Epoch: 65 , batch: 524 , training loss: 4.718720\n",
      "[INFO] Epoch: 65 , batch: 525 , training loss: 4.618398\n",
      "[INFO] Epoch: 65 , batch: 526 , training loss: 4.380095\n",
      "[INFO] Epoch: 65 , batch: 527 , training loss: 4.417403\n",
      "[INFO] Epoch: 65 , batch: 528 , training loss: 4.435777\n",
      "[INFO] Epoch: 65 , batch: 529 , training loss: 4.401361\n",
      "[INFO] Epoch: 65 , batch: 530 , training loss: 4.264447\n",
      "[INFO] Epoch: 65 , batch: 531 , training loss: 4.410688\n",
      "[INFO] Epoch: 65 , batch: 532 , training loss: 4.332395\n",
      "[INFO] Epoch: 65 , batch: 533 , training loss: 4.444721\n",
      "[INFO] Epoch: 65 , batch: 534 , training loss: 4.459648\n",
      "[INFO] Epoch: 65 , batch: 535 , training loss: 4.464801\n",
      "[INFO] Epoch: 65 , batch: 536 , training loss: 4.313013\n",
      "[INFO] Epoch: 65 , batch: 537 , training loss: 4.293199\n",
      "[INFO] Epoch: 65 , batch: 538 , training loss: 4.378064\n",
      "[INFO] Epoch: 65 , batch: 539 , training loss: 4.482338\n",
      "[INFO] Epoch: 65 , batch: 540 , training loss: 4.973023\n",
      "[INFO] Epoch: 65 , batch: 541 , training loss: 4.823613\n",
      "[INFO] Epoch: 65 , batch: 542 , training loss: 4.705110\n",
      "[INFO] Epoch: 66 , batch: 0 , training loss: 3.590780\n",
      "[INFO] Epoch: 66 , batch: 1 , training loss: 3.522545\n",
      "[INFO] Epoch: 66 , batch: 2 , training loss: 3.709197\n",
      "[INFO] Epoch: 66 , batch: 3 , training loss: 3.596912\n",
      "[INFO] Epoch: 66 , batch: 4 , training loss: 3.948894\n",
      "[INFO] Epoch: 66 , batch: 5 , training loss: 3.585753\n",
      "[INFO] Epoch: 66 , batch: 6 , training loss: 4.007036\n",
      "[INFO] Epoch: 66 , batch: 7 , training loss: 3.919427\n",
      "[INFO] Epoch: 66 , batch: 8 , training loss: 3.587045\n",
      "[INFO] Epoch: 66 , batch: 9 , training loss: 3.858028\n",
      "[INFO] Epoch: 66 , batch: 10 , training loss: 3.832506\n",
      "[INFO] Epoch: 66 , batch: 11 , training loss: 3.751532\n",
      "[INFO] Epoch: 66 , batch: 12 , training loss: 3.692316\n",
      "[INFO] Epoch: 66 , batch: 13 , training loss: 3.664392\n",
      "[INFO] Epoch: 66 , batch: 14 , training loss: 3.605520\n",
      "[INFO] Epoch: 66 , batch: 15 , training loss: 3.830903\n",
      "[INFO] Epoch: 66 , batch: 16 , training loss: 3.611534\n",
      "[INFO] Epoch: 66 , batch: 17 , training loss: 3.728368\n",
      "[INFO] Epoch: 66 , batch: 18 , training loss: 3.712453\n",
      "[INFO] Epoch: 66 , batch: 19 , training loss: 3.470236\n",
      "[INFO] Epoch: 66 , batch: 20 , training loss: 3.460874\n",
      "[INFO] Epoch: 66 , batch: 21 , training loss: 3.579283\n",
      "[INFO] Epoch: 66 , batch: 22 , training loss: 3.445992\n",
      "[INFO] Epoch: 66 , batch: 23 , training loss: 3.687710\n",
      "[INFO] Epoch: 66 , batch: 24 , training loss: 3.542503\n",
      "[INFO] Epoch: 66 , batch: 25 , training loss: 3.646814\n",
      "[INFO] Epoch: 66 , batch: 26 , training loss: 3.503754\n",
      "[INFO] Epoch: 66 , batch: 27 , training loss: 3.528858\n",
      "[INFO] Epoch: 66 , batch: 28 , training loss: 3.670214\n",
      "[INFO] Epoch: 66 , batch: 29 , training loss: 3.516290\n",
      "[INFO] Epoch: 66 , batch: 30 , training loss: 3.524600\n",
      "[INFO] Epoch: 66 , batch: 31 , training loss: 3.574552\n",
      "[INFO] Epoch: 66 , batch: 32 , training loss: 3.567372\n",
      "[INFO] Epoch: 66 , batch: 33 , training loss: 3.606961\n",
      "[INFO] Epoch: 66 , batch: 34 , training loss: 3.553447\n",
      "[INFO] Epoch: 66 , batch: 35 , training loss: 3.557310\n",
      "[INFO] Epoch: 66 , batch: 36 , training loss: 3.647372\n",
      "[INFO] Epoch: 66 , batch: 37 , training loss: 3.500149\n",
      "[INFO] Epoch: 66 , batch: 38 , training loss: 3.568233\n",
      "[INFO] Epoch: 66 , batch: 39 , training loss: 3.402476\n",
      "[INFO] Epoch: 66 , batch: 40 , training loss: 3.615198\n",
      "[INFO] Epoch: 66 , batch: 41 , training loss: 3.588708\n",
      "[INFO] Epoch: 66 , batch: 42 , training loss: 4.032895\n",
      "[INFO] Epoch: 66 , batch: 43 , training loss: 3.793005\n",
      "[INFO] Epoch: 66 , batch: 44 , training loss: 4.159971\n",
      "[INFO] Epoch: 66 , batch: 45 , training loss: 4.079880\n",
      "[INFO] Epoch: 66 , batch: 46 , training loss: 4.018837\n",
      "[INFO] Epoch: 66 , batch: 47 , training loss: 3.646373\n",
      "[INFO] Epoch: 66 , batch: 48 , training loss: 3.652565\n",
      "[INFO] Epoch: 66 , batch: 49 , training loss: 3.858901\n",
      "[INFO] Epoch: 66 , batch: 50 , training loss: 3.588295\n",
      "[INFO] Epoch: 66 , batch: 51 , training loss: 3.812225\n",
      "[INFO] Epoch: 66 , batch: 52 , training loss: 3.607328\n",
      "[INFO] Epoch: 66 , batch: 53 , training loss: 3.725776\n",
      "[INFO] Epoch: 66 , batch: 54 , training loss: 3.718621\n",
      "[INFO] Epoch: 66 , batch: 55 , training loss: 3.801628\n",
      "[INFO] Epoch: 66 , batch: 56 , training loss: 3.640952\n",
      "[INFO] Epoch: 66 , batch: 57 , training loss: 3.587971\n",
      "[INFO] Epoch: 66 , batch: 58 , training loss: 3.634416\n",
      "[INFO] Epoch: 66 , batch: 59 , training loss: 3.713865\n",
      "[INFO] Epoch: 66 , batch: 60 , training loss: 3.655512\n",
      "[INFO] Epoch: 66 , batch: 61 , training loss: 3.743440\n",
      "[INFO] Epoch: 66 , batch: 62 , training loss: 3.623766\n",
      "[INFO] Epoch: 66 , batch: 63 , training loss: 3.832884\n",
      "[INFO] Epoch: 66 , batch: 64 , training loss: 4.016955\n",
      "[INFO] Epoch: 66 , batch: 65 , training loss: 3.729206\n",
      "[INFO] Epoch: 66 , batch: 66 , training loss: 3.576690\n",
      "[INFO] Epoch: 66 , batch: 67 , training loss: 3.610533\n",
      "[INFO] Epoch: 66 , batch: 68 , training loss: 3.773034\n",
      "[INFO] Epoch: 66 , batch: 69 , training loss: 3.714976\n",
      "[INFO] Epoch: 66 , batch: 70 , training loss: 3.905763\n",
      "[INFO] Epoch: 66 , batch: 71 , training loss: 3.748829\n",
      "[INFO] Epoch: 66 , batch: 72 , training loss: 3.804689\n",
      "[INFO] Epoch: 66 , batch: 73 , training loss: 3.744020\n",
      "[INFO] Epoch: 66 , batch: 74 , training loss: 3.855495\n",
      "[INFO] Epoch: 66 , batch: 75 , training loss: 3.738363\n",
      "[INFO] Epoch: 66 , batch: 76 , training loss: 3.868264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 66 , batch: 77 , training loss: 3.785568\n",
      "[INFO] Epoch: 66 , batch: 78 , training loss: 3.889005\n",
      "[INFO] Epoch: 66 , batch: 79 , training loss: 3.754303\n",
      "[INFO] Epoch: 66 , batch: 80 , training loss: 3.958237\n",
      "[INFO] Epoch: 66 , batch: 81 , training loss: 3.847394\n",
      "[INFO] Epoch: 66 , batch: 82 , training loss: 3.822067\n",
      "[INFO] Epoch: 66 , batch: 83 , training loss: 3.927917\n",
      "[INFO] Epoch: 66 , batch: 84 , training loss: 3.891274\n",
      "[INFO] Epoch: 66 , batch: 85 , training loss: 3.963529\n",
      "[INFO] Epoch: 66 , batch: 86 , training loss: 3.904275\n",
      "[INFO] Epoch: 66 , batch: 87 , training loss: 3.831820\n",
      "[INFO] Epoch: 66 , batch: 88 , training loss: 4.025936\n",
      "[INFO] Epoch: 66 , batch: 89 , training loss: 3.766534\n",
      "[INFO] Epoch: 66 , batch: 90 , training loss: 3.885742\n",
      "[INFO] Epoch: 66 , batch: 91 , training loss: 3.809427\n",
      "[INFO] Epoch: 66 , batch: 92 , training loss: 3.836327\n",
      "[INFO] Epoch: 66 , batch: 93 , training loss: 3.946747\n",
      "[INFO] Epoch: 66 , batch: 94 , training loss: 4.017019\n",
      "[INFO] Epoch: 66 , batch: 95 , training loss: 3.841153\n",
      "[INFO] Epoch: 66 , batch: 96 , training loss: 3.871359\n",
      "[INFO] Epoch: 66 , batch: 97 , training loss: 3.759818\n",
      "[INFO] Epoch: 66 , batch: 98 , training loss: 3.695581\n",
      "[INFO] Epoch: 66 , batch: 99 , training loss: 3.859730\n",
      "[INFO] Epoch: 66 , batch: 100 , training loss: 3.743963\n",
      "[INFO] Epoch: 66 , batch: 101 , training loss: 3.763790\n",
      "[INFO] Epoch: 66 , batch: 102 , training loss: 3.903296\n",
      "[INFO] Epoch: 66 , batch: 103 , training loss: 3.711092\n",
      "[INFO] Epoch: 66 , batch: 104 , training loss: 3.668877\n",
      "[INFO] Epoch: 66 , batch: 105 , training loss: 3.924987\n",
      "[INFO] Epoch: 66 , batch: 106 , training loss: 3.918816\n",
      "[INFO] Epoch: 66 , batch: 107 , training loss: 3.773736\n",
      "[INFO] Epoch: 66 , batch: 108 , training loss: 3.731660\n",
      "[INFO] Epoch: 66 , batch: 109 , training loss: 3.622330\n",
      "[INFO] Epoch: 66 , batch: 110 , training loss: 3.817442\n",
      "[INFO] Epoch: 66 , batch: 111 , training loss: 3.910841\n",
      "[INFO] Epoch: 66 , batch: 112 , training loss: 3.824511\n",
      "[INFO] Epoch: 66 , batch: 113 , training loss: 3.802130\n",
      "[INFO] Epoch: 66 , batch: 114 , training loss: 3.814615\n",
      "[INFO] Epoch: 66 , batch: 115 , training loss: 3.815053\n",
      "[INFO] Epoch: 66 , batch: 116 , training loss: 3.704329\n",
      "[INFO] Epoch: 66 , batch: 117 , training loss: 3.967814\n",
      "[INFO] Epoch: 66 , batch: 118 , training loss: 3.905333\n",
      "[INFO] Epoch: 66 , batch: 119 , training loss: 4.010101\n",
      "[INFO] Epoch: 66 , batch: 120 , training loss: 4.028022\n",
      "[INFO] Epoch: 66 , batch: 121 , training loss: 3.908018\n",
      "[INFO] Epoch: 66 , batch: 122 , training loss: 3.795771\n",
      "[INFO] Epoch: 66 , batch: 123 , training loss: 3.793271\n",
      "[INFO] Epoch: 66 , batch: 124 , training loss: 3.925751\n",
      "[INFO] Epoch: 66 , batch: 125 , training loss: 3.708107\n",
      "[INFO] Epoch: 66 , batch: 126 , training loss: 3.718230\n",
      "[INFO] Epoch: 66 , batch: 127 , training loss: 3.708874\n",
      "[INFO] Epoch: 66 , batch: 128 , training loss: 3.857436\n",
      "[INFO] Epoch: 66 , batch: 129 , training loss: 3.824335\n",
      "[INFO] Epoch: 66 , batch: 130 , training loss: 3.815618\n",
      "[INFO] Epoch: 66 , batch: 131 , training loss: 3.835004\n",
      "[INFO] Epoch: 66 , batch: 132 , training loss: 3.835174\n",
      "[INFO] Epoch: 66 , batch: 133 , training loss: 3.782054\n",
      "[INFO] Epoch: 66 , batch: 134 , training loss: 3.602979\n",
      "[INFO] Epoch: 66 , batch: 135 , training loss: 3.642226\n",
      "[INFO] Epoch: 66 , batch: 136 , training loss: 3.891011\n",
      "[INFO] Epoch: 66 , batch: 137 , training loss: 3.864057\n",
      "[INFO] Epoch: 66 , batch: 138 , training loss: 3.915771\n",
      "[INFO] Epoch: 66 , batch: 139 , training loss: 4.458286\n",
      "[INFO] Epoch: 66 , batch: 140 , training loss: 4.238866\n",
      "[INFO] Epoch: 66 , batch: 141 , training loss: 4.039902\n",
      "[INFO] Epoch: 66 , batch: 142 , training loss: 3.759876\n",
      "[INFO] Epoch: 66 , batch: 143 , training loss: 3.878197\n",
      "[INFO] Epoch: 66 , batch: 144 , training loss: 3.743589\n",
      "[INFO] Epoch: 66 , batch: 145 , training loss: 3.790854\n",
      "[INFO] Epoch: 66 , batch: 146 , training loss: 3.998462\n",
      "[INFO] Epoch: 66 , batch: 147 , training loss: 3.691754\n",
      "[INFO] Epoch: 66 , batch: 148 , training loss: 3.642213\n",
      "[INFO] Epoch: 66 , batch: 149 , training loss: 3.730167\n",
      "[INFO] Epoch: 66 , batch: 150 , training loss: 3.980238\n",
      "[INFO] Epoch: 66 , batch: 151 , training loss: 3.830906\n",
      "[INFO] Epoch: 66 , batch: 152 , training loss: 3.830527\n",
      "[INFO] Epoch: 66 , batch: 153 , training loss: 3.852234\n",
      "[INFO] Epoch: 66 , batch: 154 , training loss: 3.950908\n",
      "[INFO] Epoch: 66 , batch: 155 , training loss: 4.136156\n",
      "[INFO] Epoch: 66 , batch: 156 , training loss: 3.883931\n",
      "[INFO] Epoch: 66 , batch: 157 , training loss: 3.860006\n",
      "[INFO] Epoch: 66 , batch: 158 , training loss: 3.965629\n",
      "[INFO] Epoch: 66 , batch: 159 , training loss: 3.879865\n",
      "[INFO] Epoch: 66 , batch: 160 , training loss: 4.077689\n",
      "[INFO] Epoch: 66 , batch: 161 , training loss: 4.157647\n",
      "[INFO] Epoch: 66 , batch: 162 , training loss: 4.143999\n",
      "[INFO] Epoch: 66 , batch: 163 , training loss: 4.313740\n",
      "[INFO] Epoch: 66 , batch: 164 , training loss: 4.288891\n",
      "[INFO] Epoch: 66 , batch: 165 , training loss: 4.199025\n",
      "[INFO] Epoch: 66 , batch: 166 , training loss: 4.077370\n",
      "[INFO] Epoch: 66 , batch: 167 , training loss: 4.089682\n",
      "[INFO] Epoch: 66 , batch: 168 , training loss: 3.785440\n",
      "[INFO] Epoch: 66 , batch: 169 , training loss: 3.793402\n",
      "[INFO] Epoch: 66 , batch: 170 , training loss: 3.968108\n",
      "[INFO] Epoch: 66 , batch: 171 , training loss: 3.441792\n",
      "[INFO] Epoch: 66 , batch: 172 , training loss: 3.711644\n",
      "[INFO] Epoch: 66 , batch: 173 , training loss: 3.971870\n",
      "[INFO] Epoch: 66 , batch: 174 , training loss: 4.437805\n",
      "[INFO] Epoch: 66 , batch: 175 , training loss: 4.719050\n",
      "[INFO] Epoch: 66 , batch: 176 , training loss: 4.358432\n",
      "[INFO] Epoch: 66 , batch: 177 , training loss: 3.954653\n",
      "[INFO] Epoch: 66 , batch: 178 , training loss: 4.003130\n",
      "[INFO] Epoch: 66 , batch: 179 , training loss: 4.048374\n",
      "[INFO] Epoch: 66 , batch: 180 , training loss: 3.974993\n",
      "[INFO] Epoch: 66 , batch: 181 , training loss: 4.273861\n",
      "[INFO] Epoch: 66 , batch: 182 , training loss: 4.219609\n",
      "[INFO] Epoch: 66 , batch: 183 , training loss: 4.217978\n",
      "[INFO] Epoch: 66 , batch: 184 , training loss: 4.123356\n",
      "[INFO] Epoch: 66 , batch: 185 , training loss: 4.057879\n",
      "[INFO] Epoch: 66 , batch: 186 , training loss: 4.184794\n",
      "[INFO] Epoch: 66 , batch: 187 , training loss: 4.296602\n",
      "[INFO] Epoch: 66 , batch: 188 , training loss: 4.298364\n",
      "[INFO] Epoch: 66 , batch: 189 , training loss: 4.226081\n",
      "[INFO] Epoch: 66 , batch: 190 , training loss: 4.261660\n",
      "[INFO] Epoch: 66 , batch: 191 , training loss: 4.356456\n",
      "[INFO] Epoch: 66 , batch: 192 , training loss: 4.202607\n",
      "[INFO] Epoch: 66 , batch: 193 , training loss: 4.299356\n",
      "[INFO] Epoch: 66 , batch: 194 , training loss: 4.234442\n",
      "[INFO] Epoch: 66 , batch: 195 , training loss: 4.177989\n",
      "[INFO] Epoch: 66 , batch: 196 , training loss: 4.025345\n",
      "[INFO] Epoch: 66 , batch: 197 , training loss: 4.128357\n",
      "[INFO] Epoch: 66 , batch: 198 , training loss: 4.050339\n",
      "[INFO] Epoch: 66 , batch: 199 , training loss: 4.177233\n",
      "[INFO] Epoch: 66 , batch: 200 , training loss: 4.095894\n",
      "[INFO] Epoch: 66 , batch: 201 , training loss: 3.971180\n",
      "[INFO] Epoch: 66 , batch: 202 , training loss: 3.972474\n",
      "[INFO] Epoch: 66 , batch: 203 , training loss: 4.125469\n",
      "[INFO] Epoch: 66 , batch: 204 , training loss: 4.170307\n",
      "[INFO] Epoch: 66 , batch: 205 , training loss: 3.787315\n",
      "[INFO] Epoch: 66 , batch: 206 , training loss: 3.743420\n",
      "[INFO] Epoch: 66 , batch: 207 , training loss: 3.718492\n",
      "[INFO] Epoch: 66 , batch: 208 , training loss: 4.029826\n",
      "[INFO] Epoch: 66 , batch: 209 , training loss: 4.034570\n",
      "[INFO] Epoch: 66 , batch: 210 , training loss: 4.034202\n",
      "[INFO] Epoch: 66 , batch: 211 , training loss: 4.025311\n",
      "[INFO] Epoch: 66 , batch: 212 , training loss: 4.129735\n",
      "[INFO] Epoch: 66 , batch: 213 , training loss: 4.055002\n",
      "[INFO] Epoch: 66 , batch: 214 , training loss: 4.140156\n",
      "[INFO] Epoch: 66 , batch: 215 , training loss: 4.344604\n",
      "[INFO] Epoch: 66 , batch: 216 , training loss: 4.032490\n",
      "[INFO] Epoch: 66 , batch: 217 , training loss: 3.997414\n",
      "[INFO] Epoch: 66 , batch: 218 , training loss: 3.973690\n",
      "[INFO] Epoch: 66 , batch: 219 , training loss: 4.087161\n",
      "[INFO] Epoch: 66 , batch: 220 , training loss: 3.934048\n",
      "[INFO] Epoch: 66 , batch: 221 , training loss: 3.950011\n",
      "[INFO] Epoch: 66 , batch: 222 , training loss: 4.081872\n",
      "[INFO] Epoch: 66 , batch: 223 , training loss: 4.203066\n",
      "[INFO] Epoch: 66 , batch: 224 , training loss: 4.220934\n",
      "[INFO] Epoch: 66 , batch: 225 , training loss: 4.119325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 66 , batch: 226 , training loss: 4.236635\n",
      "[INFO] Epoch: 66 , batch: 227 , training loss: 4.247248\n",
      "[INFO] Epoch: 66 , batch: 228 , training loss: 4.237437\n",
      "[INFO] Epoch: 66 , batch: 229 , training loss: 4.091811\n",
      "[INFO] Epoch: 66 , batch: 230 , training loss: 3.970147\n",
      "[INFO] Epoch: 66 , batch: 231 , training loss: 3.842331\n",
      "[INFO] Epoch: 66 , batch: 232 , training loss: 3.977106\n",
      "[INFO] Epoch: 66 , batch: 233 , training loss: 4.018054\n",
      "[INFO] Epoch: 66 , batch: 234 , training loss: 3.683188\n",
      "[INFO] Epoch: 66 , batch: 235 , training loss: 3.795611\n",
      "[INFO] Epoch: 66 , batch: 236 , training loss: 3.878651\n",
      "[INFO] Epoch: 66 , batch: 237 , training loss: 4.114943\n",
      "[INFO] Epoch: 66 , batch: 238 , training loss: 3.922149\n",
      "[INFO] Epoch: 66 , batch: 239 , training loss: 3.929513\n",
      "[INFO] Epoch: 66 , batch: 240 , training loss: 3.998213\n",
      "[INFO] Epoch: 66 , batch: 241 , training loss: 3.787964\n",
      "[INFO] Epoch: 66 , batch: 242 , training loss: 3.822835\n",
      "[INFO] Epoch: 66 , batch: 243 , training loss: 4.103985\n",
      "[INFO] Epoch: 66 , batch: 244 , training loss: 4.052632\n",
      "[INFO] Epoch: 66 , batch: 245 , training loss: 3.998058\n",
      "[INFO] Epoch: 66 , batch: 246 , training loss: 3.716865\n",
      "[INFO] Epoch: 66 , batch: 247 , training loss: 3.893226\n",
      "[INFO] Epoch: 66 , batch: 248 , training loss: 3.966811\n",
      "[INFO] Epoch: 66 , batch: 249 , training loss: 3.936774\n",
      "[INFO] Epoch: 66 , batch: 250 , training loss: 3.741495\n",
      "[INFO] Epoch: 66 , batch: 251 , training loss: 4.192212\n",
      "[INFO] Epoch: 66 , batch: 252 , training loss: 3.906904\n",
      "[INFO] Epoch: 66 , batch: 253 , training loss: 3.794260\n",
      "[INFO] Epoch: 66 , batch: 254 , training loss: 4.103053\n",
      "[INFO] Epoch: 66 , batch: 255 , training loss: 4.053208\n",
      "[INFO] Epoch: 66 , batch: 256 , training loss: 4.027203\n",
      "[INFO] Epoch: 66 , batch: 257 , training loss: 4.210876\n",
      "[INFO] Epoch: 66 , batch: 258 , training loss: 4.196786\n",
      "[INFO] Epoch: 66 , batch: 259 , training loss: 4.226599\n",
      "[INFO] Epoch: 66 , batch: 260 , training loss: 4.017934\n",
      "[INFO] Epoch: 66 , batch: 261 , training loss: 4.200050\n",
      "[INFO] Epoch: 66 , batch: 262 , training loss: 4.322730\n",
      "[INFO] Epoch: 66 , batch: 263 , training loss: 4.490037\n",
      "[INFO] Epoch: 66 , batch: 264 , training loss: 3.868271\n",
      "[INFO] Epoch: 66 , batch: 265 , training loss: 3.971179\n",
      "[INFO] Epoch: 66 , batch: 266 , training loss: 4.366110\n",
      "[INFO] Epoch: 66 , batch: 267 , training loss: 4.136969\n",
      "[INFO] Epoch: 66 , batch: 268 , training loss: 4.051449\n",
      "[INFO] Epoch: 66 , batch: 269 , training loss: 4.011194\n",
      "[INFO] Epoch: 66 , batch: 270 , training loss: 4.047861\n",
      "[INFO] Epoch: 66 , batch: 271 , training loss: 4.064764\n",
      "[INFO] Epoch: 66 , batch: 272 , training loss: 4.080468\n",
      "[INFO] Epoch: 66 , batch: 273 , training loss: 4.100694\n",
      "[INFO] Epoch: 66 , batch: 274 , training loss: 4.177158\n",
      "[INFO] Epoch: 66 , batch: 275 , training loss: 4.052708\n",
      "[INFO] Epoch: 66 , batch: 276 , training loss: 4.109563\n",
      "[INFO] Epoch: 66 , batch: 277 , training loss: 4.257868\n",
      "[INFO] Epoch: 66 , batch: 278 , training loss: 3.950964\n",
      "[INFO] Epoch: 66 , batch: 279 , training loss: 3.952449\n",
      "[INFO] Epoch: 66 , batch: 280 , training loss: 3.949226\n",
      "[INFO] Epoch: 66 , batch: 281 , training loss: 4.062964\n",
      "[INFO] Epoch: 66 , batch: 282 , training loss: 4.008530\n",
      "[INFO] Epoch: 66 , batch: 283 , training loss: 3.991448\n",
      "[INFO] Epoch: 66 , batch: 284 , training loss: 4.004948\n",
      "[INFO] Epoch: 66 , batch: 285 , training loss: 3.952348\n",
      "[INFO] Epoch: 66 , batch: 286 , training loss: 3.965929\n",
      "[INFO] Epoch: 66 , batch: 287 , training loss: 3.908070\n",
      "[INFO] Epoch: 66 , batch: 288 , training loss: 3.860178\n",
      "[INFO] Epoch: 66 , batch: 289 , training loss: 3.944899\n",
      "[INFO] Epoch: 66 , batch: 290 , training loss: 3.737728\n",
      "[INFO] Epoch: 66 , batch: 291 , training loss: 3.716248\n",
      "[INFO] Epoch: 66 , batch: 292 , training loss: 3.815118\n",
      "[INFO] Epoch: 66 , batch: 293 , training loss: 3.743998\n",
      "[INFO] Epoch: 66 , batch: 294 , training loss: 4.398316\n",
      "[INFO] Epoch: 66 , batch: 295 , training loss: 4.173859\n",
      "[INFO] Epoch: 66 , batch: 296 , training loss: 4.096698\n",
      "[INFO] Epoch: 66 , batch: 297 , training loss: 4.077112\n",
      "[INFO] Epoch: 66 , batch: 298 , training loss: 3.923376\n",
      "[INFO] Epoch: 66 , batch: 299 , training loss: 3.951290\n",
      "[INFO] Epoch: 66 , batch: 300 , training loss: 3.931222\n",
      "[INFO] Epoch: 66 , batch: 301 , training loss: 3.851939\n",
      "[INFO] Epoch: 66 , batch: 302 , training loss: 4.028276\n",
      "[INFO] Epoch: 66 , batch: 303 , training loss: 4.044587\n",
      "[INFO] Epoch: 66 , batch: 304 , training loss: 4.167531\n",
      "[INFO] Epoch: 66 , batch: 305 , training loss: 4.031753\n",
      "[INFO] Epoch: 66 , batch: 306 , training loss: 4.162055\n",
      "[INFO] Epoch: 66 , batch: 307 , training loss: 4.173100\n",
      "[INFO] Epoch: 66 , batch: 308 , training loss: 3.992769\n",
      "[INFO] Epoch: 66 , batch: 309 , training loss: 3.983911\n",
      "[INFO] Epoch: 66 , batch: 310 , training loss: 3.901729\n",
      "[INFO] Epoch: 66 , batch: 311 , training loss: 3.917931\n",
      "[INFO] Epoch: 66 , batch: 312 , training loss: 3.794127\n",
      "[INFO] Epoch: 66 , batch: 313 , training loss: 3.931962\n",
      "[INFO] Epoch: 66 , batch: 314 , training loss: 3.998632\n",
      "[INFO] Epoch: 66 , batch: 315 , training loss: 4.086798\n",
      "[INFO] Epoch: 66 , batch: 316 , training loss: 4.329974\n",
      "[INFO] Epoch: 66 , batch: 317 , training loss: 4.678391\n",
      "[INFO] Epoch: 66 , batch: 318 , training loss: 4.782689\n",
      "[INFO] Epoch: 66 , batch: 319 , training loss: 4.482620\n",
      "[INFO] Epoch: 66 , batch: 320 , training loss: 4.047407\n",
      "[INFO] Epoch: 66 , batch: 321 , training loss: 3.874782\n",
      "[INFO] Epoch: 66 , batch: 322 , training loss: 3.949618\n",
      "[INFO] Epoch: 66 , batch: 323 , training loss: 3.993569\n",
      "[INFO] Epoch: 66 , batch: 324 , training loss: 3.968753\n",
      "[INFO] Epoch: 66 , batch: 325 , training loss: 4.106616\n",
      "[INFO] Epoch: 66 , batch: 326 , training loss: 4.150709\n",
      "[INFO] Epoch: 66 , batch: 327 , training loss: 4.106078\n",
      "[INFO] Epoch: 66 , batch: 328 , training loss: 4.103323\n",
      "[INFO] Epoch: 66 , batch: 329 , training loss: 4.013144\n",
      "[INFO] Epoch: 66 , batch: 330 , training loss: 3.964603\n",
      "[INFO] Epoch: 66 , batch: 331 , training loss: 4.135274\n",
      "[INFO] Epoch: 66 , batch: 332 , training loss: 3.969265\n",
      "[INFO] Epoch: 66 , batch: 333 , training loss: 3.951080\n",
      "[INFO] Epoch: 66 , batch: 334 , training loss: 3.990700\n",
      "[INFO] Epoch: 66 , batch: 335 , training loss: 4.108254\n",
      "[INFO] Epoch: 66 , batch: 336 , training loss: 4.098927\n",
      "[INFO] Epoch: 66 , batch: 337 , training loss: 4.156271\n",
      "[INFO] Epoch: 66 , batch: 338 , training loss: 4.366426\n",
      "[INFO] Epoch: 66 , batch: 339 , training loss: 4.191941\n",
      "[INFO] Epoch: 66 , batch: 340 , training loss: 4.365893\n",
      "[INFO] Epoch: 66 , batch: 341 , training loss: 4.134257\n",
      "[INFO] Epoch: 66 , batch: 342 , training loss: 3.916340\n",
      "[INFO] Epoch: 66 , batch: 343 , training loss: 3.967382\n",
      "[INFO] Epoch: 66 , batch: 344 , training loss: 3.826508\n",
      "[INFO] Epoch: 66 , batch: 345 , training loss: 3.981162\n",
      "[INFO] Epoch: 66 , batch: 346 , training loss: 4.038393\n",
      "[INFO] Epoch: 66 , batch: 347 , training loss: 3.928002\n",
      "[INFO] Epoch: 66 , batch: 348 , training loss: 4.032104\n",
      "[INFO] Epoch: 66 , batch: 349 , training loss: 4.129169\n",
      "[INFO] Epoch: 66 , batch: 350 , training loss: 3.999449\n",
      "[INFO] Epoch: 66 , batch: 351 , training loss: 4.065878\n",
      "[INFO] Epoch: 66 , batch: 352 , training loss: 4.067695\n",
      "[INFO] Epoch: 66 , batch: 353 , training loss: 4.048863\n",
      "[INFO] Epoch: 66 , batch: 354 , training loss: 4.141512\n",
      "[INFO] Epoch: 66 , batch: 355 , training loss: 4.154519\n",
      "[INFO] Epoch: 66 , batch: 356 , training loss: 4.005286\n",
      "[INFO] Epoch: 66 , batch: 357 , training loss: 4.062906\n",
      "[INFO] Epoch: 66 , batch: 358 , training loss: 3.991806\n",
      "[INFO] Epoch: 66 , batch: 359 , training loss: 3.990547\n",
      "[INFO] Epoch: 66 , batch: 360 , training loss: 4.103746\n",
      "[INFO] Epoch: 66 , batch: 361 , training loss: 4.060515\n",
      "[INFO] Epoch: 66 , batch: 362 , training loss: 4.180770\n",
      "[INFO] Epoch: 66 , batch: 363 , training loss: 4.040498\n",
      "[INFO] Epoch: 66 , batch: 364 , training loss: 4.097679\n",
      "[INFO] Epoch: 66 , batch: 365 , training loss: 4.020089\n",
      "[INFO] Epoch: 66 , batch: 366 , training loss: 4.107679\n",
      "[INFO] Epoch: 66 , batch: 367 , training loss: 4.162085\n",
      "[INFO] Epoch: 66 , batch: 368 , training loss: 4.597761\n",
      "[INFO] Epoch: 66 , batch: 369 , training loss: 4.257696\n",
      "[INFO] Epoch: 66 , batch: 370 , training loss: 4.016050\n",
      "[INFO] Epoch: 66 , batch: 371 , training loss: 4.421956\n",
      "[INFO] Epoch: 66 , batch: 372 , training loss: 4.765586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 66 , batch: 373 , training loss: 4.770918\n",
      "[INFO] Epoch: 66 , batch: 374 , training loss: 4.879443\n",
      "[INFO] Epoch: 66 , batch: 375 , training loss: 4.862253\n",
      "[INFO] Epoch: 66 , batch: 376 , training loss: 4.705419\n",
      "[INFO] Epoch: 66 , batch: 377 , training loss: 4.446153\n",
      "[INFO] Epoch: 66 , batch: 378 , training loss: 4.570342\n",
      "[INFO] Epoch: 66 , batch: 379 , training loss: 4.563205\n",
      "[INFO] Epoch: 66 , batch: 380 , training loss: 4.746308\n",
      "[INFO] Epoch: 66 , batch: 381 , training loss: 4.414752\n",
      "[INFO] Epoch: 66 , batch: 382 , training loss: 4.636860\n",
      "[INFO] Epoch: 66 , batch: 383 , training loss: 4.721159\n",
      "[INFO] Epoch: 66 , batch: 384 , training loss: 4.699354\n",
      "[INFO] Epoch: 66 , batch: 385 , training loss: 4.401651\n",
      "[INFO] Epoch: 66 , batch: 386 , training loss: 4.647179\n",
      "[INFO] Epoch: 66 , batch: 387 , training loss: 4.586607\n",
      "[INFO] Epoch: 66 , batch: 388 , training loss: 4.405650\n",
      "[INFO] Epoch: 66 , batch: 389 , training loss: 4.220661\n",
      "[INFO] Epoch: 66 , batch: 390 , training loss: 4.231728\n",
      "[INFO] Epoch: 66 , batch: 391 , training loss: 4.284591\n",
      "[INFO] Epoch: 66 , batch: 392 , training loss: 4.651746\n",
      "[INFO] Epoch: 66 , batch: 393 , training loss: 4.526089\n",
      "[INFO] Epoch: 66 , batch: 394 , training loss: 4.627927\n",
      "[INFO] Epoch: 66 , batch: 395 , training loss: 4.434005\n",
      "[INFO] Epoch: 66 , batch: 396 , training loss: 4.272436\n",
      "[INFO] Epoch: 66 , batch: 397 , training loss: 4.419441\n",
      "[INFO] Epoch: 66 , batch: 398 , training loss: 4.250109\n",
      "[INFO] Epoch: 66 , batch: 399 , training loss: 4.321541\n",
      "[INFO] Epoch: 66 , batch: 400 , training loss: 4.321325\n",
      "[INFO] Epoch: 66 , batch: 401 , training loss: 4.733065\n",
      "[INFO] Epoch: 66 , batch: 402 , training loss: 4.468510\n",
      "[INFO] Epoch: 66 , batch: 403 , training loss: 4.310134\n",
      "[INFO] Epoch: 66 , batch: 404 , training loss: 4.454966\n",
      "[INFO] Epoch: 66 , batch: 405 , training loss: 4.526002\n",
      "[INFO] Epoch: 66 , batch: 406 , training loss: 4.428203\n",
      "[INFO] Epoch: 66 , batch: 407 , training loss: 4.441134\n",
      "[INFO] Epoch: 66 , batch: 408 , training loss: 4.429767\n",
      "[INFO] Epoch: 66 , batch: 409 , training loss: 4.452596\n",
      "[INFO] Epoch: 66 , batch: 410 , training loss: 4.477707\n",
      "[INFO] Epoch: 66 , batch: 411 , training loss: 4.676771\n",
      "[INFO] Epoch: 66 , batch: 412 , training loss: 4.496900\n",
      "[INFO] Epoch: 66 , batch: 413 , training loss: 4.349695\n",
      "[INFO] Epoch: 66 , batch: 414 , training loss: 4.394650\n",
      "[INFO] Epoch: 66 , batch: 415 , training loss: 4.461390\n",
      "[INFO] Epoch: 66 , batch: 416 , training loss: 4.504348\n",
      "[INFO] Epoch: 66 , batch: 417 , training loss: 4.419665\n",
      "[INFO] Epoch: 66 , batch: 418 , training loss: 4.485466\n",
      "[INFO] Epoch: 66 , batch: 419 , training loss: 4.435191\n",
      "[INFO] Epoch: 66 , batch: 420 , training loss: 4.403186\n",
      "[INFO] Epoch: 66 , batch: 421 , training loss: 4.410696\n",
      "[INFO] Epoch: 66 , batch: 422 , training loss: 4.245759\n",
      "[INFO] Epoch: 66 , batch: 423 , training loss: 4.481147\n",
      "[INFO] Epoch: 66 , batch: 424 , training loss: 4.625099\n",
      "[INFO] Epoch: 66 , batch: 425 , training loss: 4.525872\n",
      "[INFO] Epoch: 66 , batch: 426 , training loss: 4.245457\n",
      "[INFO] Epoch: 66 , batch: 427 , training loss: 4.489415\n",
      "[INFO] Epoch: 66 , batch: 428 , training loss: 4.337885\n",
      "[INFO] Epoch: 66 , batch: 429 , training loss: 4.248765\n",
      "[INFO] Epoch: 66 , batch: 430 , training loss: 4.492800\n",
      "[INFO] Epoch: 66 , batch: 431 , training loss: 4.100827\n",
      "[INFO] Epoch: 66 , batch: 432 , training loss: 4.150529\n",
      "[INFO] Epoch: 66 , batch: 433 , training loss: 4.185860\n",
      "[INFO] Epoch: 66 , batch: 434 , training loss: 4.067560\n",
      "[INFO] Epoch: 66 , batch: 435 , training loss: 4.436610\n",
      "[INFO] Epoch: 66 , batch: 436 , training loss: 4.465692\n",
      "[INFO] Epoch: 66 , batch: 437 , training loss: 4.253182\n",
      "[INFO] Epoch: 66 , batch: 438 , training loss: 4.109067\n",
      "[INFO] Epoch: 66 , batch: 439 , training loss: 4.328564\n",
      "[INFO] Epoch: 66 , batch: 440 , training loss: 4.458039\n",
      "[INFO] Epoch: 66 , batch: 441 , training loss: 4.562305\n",
      "[INFO] Epoch: 66 , batch: 442 , training loss: 4.306829\n",
      "[INFO] Epoch: 66 , batch: 443 , training loss: 4.485047\n",
      "[INFO] Epoch: 66 , batch: 444 , training loss: 4.112360\n",
      "[INFO] Epoch: 66 , batch: 445 , training loss: 3.989748\n",
      "[INFO] Epoch: 66 , batch: 446 , training loss: 3.941026\n",
      "[INFO] Epoch: 66 , batch: 447 , training loss: 4.129441\n",
      "[INFO] Epoch: 66 , batch: 448 , training loss: 4.262028\n",
      "[INFO] Epoch: 66 , batch: 449 , training loss: 4.635528\n",
      "[INFO] Epoch: 66 , batch: 450 , training loss: 4.732872\n",
      "[INFO] Epoch: 66 , batch: 451 , training loss: 4.617234\n",
      "[INFO] Epoch: 66 , batch: 452 , training loss: 4.417391\n",
      "[INFO] Epoch: 66 , batch: 453 , training loss: 4.166320\n",
      "[INFO] Epoch: 66 , batch: 454 , training loss: 4.326628\n",
      "[INFO] Epoch: 66 , batch: 455 , training loss: 4.368301\n",
      "[INFO] Epoch: 66 , batch: 456 , training loss: 4.377294\n",
      "[INFO] Epoch: 66 , batch: 457 , training loss: 4.442722\n",
      "[INFO] Epoch: 66 , batch: 458 , training loss: 4.209944\n",
      "[INFO] Epoch: 66 , batch: 459 , training loss: 4.154076\n",
      "[INFO] Epoch: 66 , batch: 460 , training loss: 4.307374\n",
      "[INFO] Epoch: 66 , batch: 461 , training loss: 4.250244\n",
      "[INFO] Epoch: 66 , batch: 462 , training loss: 4.311193\n",
      "[INFO] Epoch: 66 , batch: 463 , training loss: 4.208108\n",
      "[INFO] Epoch: 66 , batch: 464 , training loss: 4.410423\n",
      "[INFO] Epoch: 66 , batch: 465 , training loss: 4.360306\n",
      "[INFO] Epoch: 66 , batch: 466 , training loss: 4.432883\n",
      "[INFO] Epoch: 66 , batch: 467 , training loss: 4.413933\n",
      "[INFO] Epoch: 66 , batch: 468 , training loss: 4.366928\n",
      "[INFO] Epoch: 66 , batch: 469 , training loss: 4.393291\n",
      "[INFO] Epoch: 66 , batch: 470 , training loss: 4.219387\n",
      "[INFO] Epoch: 66 , batch: 471 , training loss: 4.333370\n",
      "[INFO] Epoch: 66 , batch: 472 , training loss: 4.349111\n",
      "[INFO] Epoch: 66 , batch: 473 , training loss: 4.298065\n",
      "[INFO] Epoch: 66 , batch: 474 , training loss: 4.064842\n",
      "[INFO] Epoch: 66 , batch: 475 , training loss: 3.954016\n",
      "[INFO] Epoch: 66 , batch: 476 , training loss: 4.331182\n",
      "[INFO] Epoch: 66 , batch: 477 , training loss: 4.461889\n",
      "[INFO] Epoch: 66 , batch: 478 , training loss: 4.460540\n",
      "[INFO] Epoch: 66 , batch: 479 , training loss: 4.474163\n",
      "[INFO] Epoch: 66 , batch: 480 , training loss: 4.594004\n",
      "[INFO] Epoch: 66 , batch: 481 , training loss: 4.452968\n",
      "[INFO] Epoch: 66 , batch: 482 , training loss: 4.574727\n",
      "[INFO] Epoch: 66 , batch: 483 , training loss: 4.396800\n",
      "[INFO] Epoch: 66 , batch: 484 , training loss: 4.193713\n",
      "[INFO] Epoch: 66 , batch: 485 , training loss: 4.308510\n",
      "[INFO] Epoch: 66 , batch: 486 , training loss: 4.206604\n",
      "[INFO] Epoch: 66 , batch: 487 , training loss: 4.180859\n",
      "[INFO] Epoch: 66 , batch: 488 , training loss: 4.365090\n",
      "[INFO] Epoch: 66 , batch: 489 , training loss: 4.286836\n",
      "[INFO] Epoch: 66 , batch: 490 , training loss: 4.331728\n",
      "[INFO] Epoch: 66 , batch: 491 , training loss: 4.256504\n",
      "[INFO] Epoch: 66 , batch: 492 , training loss: 4.233583\n",
      "[INFO] Epoch: 66 , batch: 493 , training loss: 4.395371\n",
      "[INFO] Epoch: 66 , batch: 494 , training loss: 4.286993\n",
      "[INFO] Epoch: 66 , batch: 495 , training loss: 4.480010\n",
      "[INFO] Epoch: 66 , batch: 496 , training loss: 4.328362\n",
      "[INFO] Epoch: 66 , batch: 497 , training loss: 4.375250\n",
      "[INFO] Epoch: 66 , batch: 498 , training loss: 4.357442\n",
      "[INFO] Epoch: 66 , batch: 499 , training loss: 4.445402\n",
      "[INFO] Epoch: 66 , batch: 500 , training loss: 4.581334\n",
      "[INFO] Epoch: 66 , batch: 501 , training loss: 4.921884\n",
      "[INFO] Epoch: 66 , batch: 502 , training loss: 4.933910\n",
      "[INFO] Epoch: 66 , batch: 503 , training loss: 4.533481\n",
      "[INFO] Epoch: 66 , batch: 504 , training loss: 4.716582\n",
      "[INFO] Epoch: 66 , batch: 505 , training loss: 4.674176\n",
      "[INFO] Epoch: 66 , batch: 506 , training loss: 4.655525\n",
      "[INFO] Epoch: 66 , batch: 507 , training loss: 4.685222\n",
      "[INFO] Epoch: 66 , batch: 508 , training loss: 4.603708\n",
      "[INFO] Epoch: 66 , batch: 509 , training loss: 4.426076\n",
      "[INFO] Epoch: 66 , batch: 510 , training loss: 4.506487\n",
      "[INFO] Epoch: 66 , batch: 511 , training loss: 4.420674\n",
      "[INFO] Epoch: 66 , batch: 512 , training loss: 4.524601\n",
      "[INFO] Epoch: 66 , batch: 513 , training loss: 4.815050\n",
      "[INFO] Epoch: 66 , batch: 514 , training loss: 4.418481\n",
      "[INFO] Epoch: 66 , batch: 515 , training loss: 4.697589\n",
      "[INFO] Epoch: 66 , batch: 516 , training loss: 4.475945\n",
      "[INFO] Epoch: 66 , batch: 517 , training loss: 4.429150\n",
      "[INFO] Epoch: 66 , batch: 518 , training loss: 4.398635\n",
      "[INFO] Epoch: 66 , batch: 519 , training loss: 4.224986\n",
      "[INFO] Epoch: 66 , batch: 520 , training loss: 4.490748\n",
      "[INFO] Epoch: 66 , batch: 521 , training loss: 4.451940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 66 , batch: 522 , training loss: 4.535753\n",
      "[INFO] Epoch: 66 , batch: 523 , training loss: 4.465285\n",
      "[INFO] Epoch: 66 , batch: 524 , training loss: 4.741029\n",
      "[INFO] Epoch: 66 , batch: 525 , training loss: 4.643828\n",
      "[INFO] Epoch: 66 , batch: 526 , training loss: 4.409123\n",
      "[INFO] Epoch: 66 , batch: 527 , training loss: 4.464881\n",
      "[INFO] Epoch: 66 , batch: 528 , training loss: 4.464798\n",
      "[INFO] Epoch: 66 , batch: 529 , training loss: 4.431538\n",
      "[INFO] Epoch: 66 , batch: 530 , training loss: 4.290251\n",
      "[INFO] Epoch: 66 , batch: 531 , training loss: 4.449624\n",
      "[INFO] Epoch: 66 , batch: 532 , training loss: 4.349138\n",
      "[INFO] Epoch: 66 , batch: 533 , training loss: 4.448774\n",
      "[INFO] Epoch: 66 , batch: 534 , training loss: 4.486604\n",
      "[INFO] Epoch: 66 , batch: 535 , training loss: 4.484489\n",
      "[INFO] Epoch: 66 , batch: 536 , training loss: 4.330801\n",
      "[INFO] Epoch: 66 , batch: 537 , training loss: 4.324830\n",
      "[INFO] Epoch: 66 , batch: 538 , training loss: 4.400353\n",
      "[INFO] Epoch: 66 , batch: 539 , training loss: 4.509928\n",
      "[INFO] Epoch: 66 , batch: 540 , training loss: 5.017509\n",
      "[INFO] Epoch: 66 , batch: 541 , training loss: 4.879116\n",
      "[INFO] Epoch: 66 , batch: 542 , training loss: 4.729649\n",
      "[INFO] Epoch: 67 , batch: 0 , training loss: 3.688773\n",
      "[INFO] Epoch: 67 , batch: 1 , training loss: 3.643627\n",
      "[INFO] Epoch: 67 , batch: 2 , training loss: 3.756233\n",
      "[INFO] Epoch: 67 , batch: 3 , training loss: 3.650597\n",
      "[INFO] Epoch: 67 , batch: 4 , training loss: 4.018466\n",
      "[INFO] Epoch: 67 , batch: 5 , training loss: 3.651231\n",
      "[INFO] Epoch: 67 , batch: 6 , training loss: 4.057018\n",
      "[INFO] Epoch: 67 , batch: 7 , training loss: 3.974287\n",
      "[INFO] Epoch: 67 , batch: 8 , training loss: 3.639738\n",
      "[INFO] Epoch: 67 , batch: 9 , training loss: 3.905371\n",
      "[INFO] Epoch: 67 , batch: 10 , training loss: 3.881815\n",
      "[INFO] Epoch: 67 , batch: 11 , training loss: 3.779903\n",
      "[INFO] Epoch: 67 , batch: 12 , training loss: 3.696509\n",
      "[INFO] Epoch: 67 , batch: 13 , training loss: 3.685108\n",
      "[INFO] Epoch: 67 , batch: 14 , training loss: 3.639513\n",
      "[INFO] Epoch: 67 , batch: 15 , training loss: 3.861959\n",
      "[INFO] Epoch: 67 , batch: 16 , training loss: 3.659637\n",
      "[INFO] Epoch: 67 , batch: 17 , training loss: 3.811215\n",
      "[INFO] Epoch: 67 , batch: 18 , training loss: 3.789805\n",
      "[INFO] Epoch: 67 , batch: 19 , training loss: 3.519775\n",
      "[INFO] Epoch: 67 , batch: 20 , training loss: 3.545141\n",
      "[INFO] Epoch: 67 , batch: 21 , training loss: 3.637493\n",
      "[INFO] Epoch: 67 , batch: 22 , training loss: 3.554761\n",
      "[INFO] Epoch: 67 , batch: 23 , training loss: 3.733746\n",
      "[INFO] Epoch: 67 , batch: 24 , training loss: 3.596457\n",
      "[INFO] Epoch: 67 , batch: 25 , training loss: 3.695961\n",
      "[INFO] Epoch: 67 , batch: 26 , training loss: 3.562479\n",
      "[INFO] Epoch: 67 , batch: 27 , training loss: 3.566807\n",
      "[INFO] Epoch: 67 , batch: 28 , training loss: 3.739115\n",
      "[INFO] Epoch: 67 , batch: 29 , training loss: 3.558224\n",
      "[INFO] Epoch: 67 , batch: 30 , training loss: 3.585373\n",
      "[INFO] Epoch: 67 , batch: 31 , training loss: 3.634452\n",
      "[INFO] Epoch: 67 , batch: 32 , training loss: 3.642774\n",
      "[INFO] Epoch: 67 , batch: 33 , training loss: 3.673395\n",
      "[INFO] Epoch: 67 , batch: 34 , training loss: 3.650588\n",
      "[INFO] Epoch: 67 , batch: 35 , training loss: 3.611717\n",
      "[INFO] Epoch: 67 , batch: 36 , training loss: 3.691937\n",
      "[INFO] Epoch: 67 , batch: 37 , training loss: 3.553361\n",
      "[INFO] Epoch: 67 , batch: 38 , training loss: 3.625331\n",
      "[INFO] Epoch: 67 , batch: 39 , training loss: 3.463951\n",
      "[INFO] Epoch: 67 , batch: 40 , training loss: 3.650978\n",
      "[INFO] Epoch: 67 , batch: 41 , training loss: 3.630538\n",
      "[INFO] Epoch: 67 , batch: 42 , training loss: 4.126009\n",
      "[INFO] Epoch: 67 , batch: 43 , training loss: 3.867794\n",
      "[INFO] Epoch: 67 , batch: 44 , training loss: 4.185675\n",
      "[INFO] Epoch: 67 , batch: 45 , training loss: 4.146424\n",
      "[INFO] Epoch: 67 , batch: 46 , training loss: 4.085811\n",
      "[INFO] Epoch: 67 , batch: 47 , training loss: 3.725411\n",
      "[INFO] Epoch: 67 , batch: 48 , training loss: 3.731680\n",
      "[INFO] Epoch: 67 , batch: 49 , training loss: 3.902350\n",
      "[INFO] Epoch: 67 , batch: 50 , training loss: 3.657133\n",
      "[INFO] Epoch: 67 , batch: 51 , training loss: 3.865312\n",
      "[INFO] Epoch: 67 , batch: 52 , training loss: 3.647586\n",
      "[INFO] Epoch: 67 , batch: 53 , training loss: 3.779502\n",
      "[INFO] Epoch: 67 , batch: 54 , training loss: 3.787948\n",
      "[INFO] Epoch: 67 , batch: 55 , training loss: 3.840427\n",
      "[INFO] Epoch: 67 , batch: 56 , training loss: 3.691737\n",
      "[INFO] Epoch: 67 , batch: 57 , training loss: 3.653032\n",
      "[INFO] Epoch: 67 , batch: 58 , training loss: 3.685540\n",
      "[INFO] Epoch: 67 , batch: 59 , training loss: 3.748349\n",
      "[INFO] Epoch: 67 , batch: 60 , training loss: 3.706301\n",
      "[INFO] Epoch: 67 , batch: 61 , training loss: 3.791604\n",
      "[INFO] Epoch: 67 , batch: 62 , training loss: 3.674790\n",
      "[INFO] Epoch: 67 , batch: 63 , training loss: 3.883641\n",
      "[INFO] Epoch: 67 , batch: 64 , training loss: 4.051205\n",
      "[INFO] Epoch: 67 , batch: 65 , training loss: 3.786242\n",
      "[INFO] Epoch: 67 , batch: 66 , training loss: 3.605221\n",
      "[INFO] Epoch: 67 , batch: 67 , training loss: 3.651761\n",
      "[INFO] Epoch: 67 , batch: 68 , training loss: 3.806646\n",
      "[INFO] Epoch: 67 , batch: 69 , training loss: 3.736534\n",
      "[INFO] Epoch: 67 , batch: 70 , training loss: 3.957659\n",
      "[INFO] Epoch: 67 , batch: 71 , training loss: 3.791184\n",
      "[INFO] Epoch: 67 , batch: 72 , training loss: 3.846424\n",
      "[INFO] Epoch: 67 , batch: 73 , training loss: 3.794341\n",
      "[INFO] Epoch: 67 , batch: 74 , training loss: 3.892610\n",
      "[INFO] Epoch: 67 , batch: 75 , training loss: 3.767627\n",
      "[INFO] Epoch: 67 , batch: 76 , training loss: 3.904565\n",
      "[INFO] Epoch: 67 , batch: 77 , training loss: 3.834559\n",
      "[INFO] Epoch: 67 , batch: 78 , training loss: 3.912742\n",
      "[INFO] Epoch: 67 , batch: 79 , training loss: 3.751880\n",
      "[INFO] Epoch: 67 , batch: 80 , training loss: 3.966755\n",
      "[INFO] Epoch: 67 , batch: 81 , training loss: 3.877477\n",
      "[INFO] Epoch: 67 , batch: 82 , training loss: 3.858034\n",
      "[INFO] Epoch: 67 , batch: 83 , training loss: 3.940600\n",
      "[INFO] Epoch: 67 , batch: 84 , training loss: 3.907448\n",
      "[INFO] Epoch: 67 , batch: 85 , training loss: 3.998598\n",
      "[INFO] Epoch: 67 , batch: 86 , training loss: 3.920307\n",
      "[INFO] Epoch: 67 , batch: 87 , training loss: 3.891399\n",
      "[INFO] Epoch: 67 , batch: 88 , training loss: 4.040013\n",
      "[INFO] Epoch: 67 , batch: 89 , training loss: 3.819414\n",
      "[INFO] Epoch: 67 , batch: 90 , training loss: 3.897338\n",
      "[INFO] Epoch: 67 , batch: 91 , training loss: 3.838546\n",
      "[INFO] Epoch: 67 , batch: 92 , training loss: 3.869895\n",
      "[INFO] Epoch: 67 , batch: 93 , training loss: 3.959340\n",
      "[INFO] Epoch: 67 , batch: 94 , training loss: 4.088605\n",
      "[INFO] Epoch: 67 , batch: 95 , training loss: 3.860292\n",
      "[INFO] Epoch: 67 , batch: 96 , training loss: 3.882118\n",
      "[INFO] Epoch: 67 , batch: 97 , training loss: 3.762794\n",
      "[INFO] Epoch: 67 , batch: 98 , training loss: 3.717687\n",
      "[INFO] Epoch: 67 , batch: 99 , training loss: 3.901916\n",
      "[INFO] Epoch: 67 , batch: 100 , training loss: 3.769478\n",
      "[INFO] Epoch: 67 , batch: 101 , training loss: 3.786646\n",
      "[INFO] Epoch: 67 , batch: 102 , training loss: 3.926750\n",
      "[INFO] Epoch: 67 , batch: 103 , training loss: 3.715189\n",
      "[INFO] Epoch: 67 , batch: 104 , training loss: 3.686551\n",
      "[INFO] Epoch: 67 , batch: 105 , training loss: 3.964480\n",
      "[INFO] Epoch: 67 , batch: 106 , training loss: 3.954646\n",
      "[INFO] Epoch: 67 , batch: 107 , training loss: 3.806912\n",
      "[INFO] Epoch: 67 , batch: 108 , training loss: 3.748487\n",
      "[INFO] Epoch: 67 , batch: 109 , training loss: 3.670460\n",
      "[INFO] Epoch: 67 , batch: 110 , training loss: 3.845595\n",
      "[INFO] Epoch: 67 , batch: 111 , training loss: 3.913506\n",
      "[INFO] Epoch: 67 , batch: 112 , training loss: 3.864502\n",
      "[INFO] Epoch: 67 , batch: 113 , training loss: 3.826049\n",
      "[INFO] Epoch: 67 , batch: 114 , training loss: 3.823981\n",
      "[INFO] Epoch: 67 , batch: 115 , training loss: 3.848967\n",
      "[INFO] Epoch: 67 , batch: 116 , training loss: 3.724783\n",
      "[INFO] Epoch: 67 , batch: 117 , training loss: 3.958735\n",
      "[INFO] Epoch: 67 , batch: 118 , training loss: 3.948350\n",
      "[INFO] Epoch: 67 , batch: 119 , training loss: 4.081599\n",
      "[INFO] Epoch: 67 , batch: 120 , training loss: 4.056917\n",
      "[INFO] Epoch: 67 , batch: 121 , training loss: 3.925909\n",
      "[INFO] Epoch: 67 , batch: 122 , training loss: 3.812369\n",
      "[INFO] Epoch: 67 , batch: 123 , training loss: 3.796703\n",
      "[INFO] Epoch: 67 , batch: 124 , training loss: 3.939554\n",
      "[INFO] Epoch: 67 , batch: 125 , training loss: 3.738428\n",
      "[INFO] Epoch: 67 , batch: 126 , training loss: 3.733236\n",
      "[INFO] Epoch: 67 , batch: 127 , training loss: 3.759367\n",
      "[INFO] Epoch: 67 , batch: 128 , training loss: 3.883944\n",
      "[INFO] Epoch: 67 , batch: 129 , training loss: 3.822303\n",
      "[INFO] Epoch: 67 , batch: 130 , training loss: 3.841481\n",
      "[INFO] Epoch: 67 , batch: 131 , training loss: 3.871081\n",
      "[INFO] Epoch: 67 , batch: 132 , training loss: 3.868107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 67 , batch: 133 , training loss: 3.814185\n",
      "[INFO] Epoch: 67 , batch: 134 , training loss: 3.611897\n",
      "[INFO] Epoch: 67 , batch: 135 , training loss: 3.673431\n",
      "[INFO] Epoch: 67 , batch: 136 , training loss: 3.960747\n",
      "[INFO] Epoch: 67 , batch: 137 , training loss: 3.891539\n",
      "[INFO] Epoch: 67 , batch: 138 , training loss: 3.916682\n",
      "[INFO] Epoch: 67 , batch: 139 , training loss: 4.505634\n",
      "[INFO] Epoch: 67 , batch: 140 , training loss: 4.258419\n",
      "[INFO] Epoch: 67 , batch: 141 , training loss: 4.031064\n",
      "[INFO] Epoch: 67 , batch: 142 , training loss: 3.780301\n",
      "[INFO] Epoch: 67 , batch: 143 , training loss: 3.902293\n",
      "[INFO] Epoch: 67 , batch: 144 , training loss: 3.750590\n",
      "[INFO] Epoch: 67 , batch: 145 , training loss: 3.810873\n",
      "[INFO] Epoch: 67 , batch: 146 , training loss: 4.018497\n",
      "[INFO] Epoch: 67 , batch: 147 , training loss: 3.698000\n",
      "[INFO] Epoch: 67 , batch: 148 , training loss: 3.649080\n",
      "[INFO] Epoch: 67 , batch: 149 , training loss: 3.729870\n",
      "[INFO] Epoch: 67 , batch: 150 , training loss: 3.996146\n",
      "[INFO] Epoch: 67 , batch: 151 , training loss: 3.840166\n",
      "[INFO] Epoch: 67 , batch: 152 , training loss: 3.837819\n",
      "[INFO] Epoch: 67 , batch: 153 , training loss: 3.897366\n",
      "[INFO] Epoch: 67 , batch: 154 , training loss: 3.988559\n",
      "[INFO] Epoch: 67 , batch: 155 , training loss: 4.166107\n",
      "[INFO] Epoch: 67 , batch: 156 , training loss: 3.941934\n",
      "[INFO] Epoch: 67 , batch: 157 , training loss: 3.877610\n",
      "[INFO] Epoch: 67 , batch: 158 , training loss: 4.005369\n",
      "[INFO] Epoch: 67 , batch: 159 , training loss: 3.914016\n",
      "[INFO] Epoch: 67 , batch: 160 , training loss: 4.138632\n",
      "[INFO] Epoch: 67 , batch: 161 , training loss: 4.166055\n",
      "[INFO] Epoch: 67 , batch: 162 , training loss: 4.145525\n",
      "[INFO] Epoch: 67 , batch: 163 , training loss: 4.344588\n",
      "[INFO] Epoch: 67 , batch: 164 , training loss: 4.279613\n",
      "[INFO] Epoch: 67 , batch: 165 , training loss: 4.191634\n",
      "[INFO] Epoch: 67 , batch: 166 , training loss: 4.119704\n",
      "[INFO] Epoch: 67 , batch: 167 , training loss: 4.148306\n",
      "[INFO] Epoch: 67 , batch: 168 , training loss: 3.880893\n",
      "[INFO] Epoch: 67 , batch: 169 , training loss: 3.880610\n",
      "[INFO] Epoch: 67 , batch: 170 , training loss: 4.048564\n",
      "[INFO] Epoch: 67 , batch: 171 , training loss: 3.568502\n",
      "[INFO] Epoch: 67 , batch: 172 , training loss: 3.787174\n",
      "[INFO] Epoch: 67 , batch: 173 , training loss: 4.072531\n",
      "[INFO] Epoch: 67 , batch: 174 , training loss: 4.492876\n",
      "[INFO] Epoch: 67 , batch: 175 , training loss: 4.715572\n",
      "[INFO] Epoch: 67 , batch: 176 , training loss: 4.386951\n",
      "[INFO] Epoch: 67 , batch: 177 , training loss: 4.004240\n",
      "[INFO] Epoch: 67 , batch: 178 , training loss: 4.040050\n",
      "[INFO] Epoch: 67 , batch: 179 , training loss: 4.104004\n",
      "[INFO] Epoch: 67 , batch: 180 , training loss: 4.061682\n",
      "[INFO] Epoch: 67 , batch: 181 , training loss: 4.361995\n",
      "[INFO] Epoch: 67 , batch: 182 , training loss: 4.254327\n",
      "[INFO] Epoch: 67 , batch: 183 , training loss: 4.266611\n",
      "[INFO] Epoch: 67 , batch: 184 , training loss: 4.141805\n",
      "[INFO] Epoch: 67 , batch: 185 , training loss: 4.111432\n",
      "[INFO] Epoch: 67 , batch: 186 , training loss: 4.247367\n",
      "[INFO] Epoch: 67 , batch: 187 , training loss: 4.356642\n",
      "[INFO] Epoch: 67 , batch: 188 , training loss: 4.333445\n",
      "[INFO] Epoch: 67 , batch: 189 , training loss: 4.258730\n",
      "[INFO] Epoch: 67 , batch: 190 , training loss: 4.311565\n",
      "[INFO] Epoch: 67 , batch: 191 , training loss: 4.396643\n",
      "[INFO] Epoch: 67 , batch: 192 , training loss: 4.235677\n",
      "[INFO] Epoch: 67 , batch: 193 , training loss: 4.340597\n",
      "[INFO] Epoch: 67 , batch: 194 , training loss: 4.294559\n",
      "[INFO] Epoch: 67 , batch: 195 , training loss: 4.224290\n",
      "[INFO] Epoch: 67 , batch: 196 , training loss: 4.082273\n",
      "[INFO] Epoch: 67 , batch: 197 , training loss: 4.151975\n",
      "[INFO] Epoch: 67 , batch: 198 , training loss: 4.082493\n",
      "[INFO] Epoch: 67 , batch: 199 , training loss: 4.214013\n",
      "[INFO] Epoch: 67 , batch: 200 , training loss: 4.112919\n",
      "[INFO] Epoch: 67 , batch: 201 , training loss: 4.016577\n",
      "[INFO] Epoch: 67 , batch: 202 , training loss: 4.021056\n",
      "[INFO] Epoch: 67 , batch: 203 , training loss: 4.155006\n",
      "[INFO] Epoch: 67 , batch: 204 , training loss: 4.202829\n",
      "[INFO] Epoch: 67 , batch: 205 , training loss: 3.818644\n",
      "[INFO] Epoch: 67 , batch: 206 , training loss: 3.768564\n",
      "[INFO] Epoch: 67 , batch: 207 , training loss: 3.735546\n",
      "[INFO] Epoch: 67 , batch: 208 , training loss: 4.080091\n",
      "[INFO] Epoch: 67 , batch: 209 , training loss: 4.052614\n",
      "[INFO] Epoch: 67 , batch: 210 , training loss: 4.065088\n",
      "[INFO] Epoch: 67 , batch: 211 , training loss: 4.063139\n",
      "[INFO] Epoch: 67 , batch: 212 , training loss: 4.166775\n",
      "[INFO] Epoch: 67 , batch: 213 , training loss: 4.103168\n",
      "[INFO] Epoch: 67 , batch: 214 , training loss: 4.163215\n",
      "[INFO] Epoch: 67 , batch: 215 , training loss: 4.366984\n",
      "[INFO] Epoch: 67 , batch: 216 , training loss: 4.073542\n",
      "[INFO] Epoch: 67 , batch: 217 , training loss: 4.039728\n",
      "[INFO] Epoch: 67 , batch: 218 , training loss: 4.020652\n",
      "[INFO] Epoch: 67 , batch: 219 , training loss: 4.146477\n",
      "[INFO] Epoch: 67 , batch: 220 , training loss: 3.958027\n",
      "[INFO] Epoch: 67 , batch: 221 , training loss: 3.964965\n",
      "[INFO] Epoch: 67 , batch: 222 , training loss: 4.118879\n",
      "[INFO] Epoch: 67 , batch: 223 , training loss: 4.240910\n",
      "[INFO] Epoch: 67 , batch: 224 , training loss: 4.267421\n",
      "[INFO] Epoch: 67 , batch: 225 , training loss: 4.136150\n",
      "[INFO] Epoch: 67 , batch: 226 , training loss: 4.275898\n",
      "[INFO] Epoch: 67 , batch: 227 , training loss: 4.263314\n",
      "[INFO] Epoch: 67 , batch: 228 , training loss: 4.276858\n",
      "[INFO] Epoch: 67 , batch: 229 , training loss: 4.124721\n",
      "[INFO] Epoch: 67 , batch: 230 , training loss: 3.995724\n",
      "[INFO] Epoch: 67 , batch: 231 , training loss: 3.850906\n",
      "[INFO] Epoch: 67 , batch: 232 , training loss: 3.993080\n",
      "[INFO] Epoch: 67 , batch: 233 , training loss: 4.036967\n",
      "[INFO] Epoch: 67 , batch: 234 , training loss: 3.714563\n",
      "[INFO] Epoch: 67 , batch: 235 , training loss: 3.833021\n",
      "[INFO] Epoch: 67 , batch: 236 , training loss: 3.936615\n",
      "[INFO] Epoch: 67 , batch: 237 , training loss: 4.152042\n",
      "[INFO] Epoch: 67 , batch: 238 , training loss: 3.953350\n",
      "[INFO] Epoch: 67 , batch: 239 , training loss: 3.977117\n",
      "[INFO] Epoch: 67 , batch: 240 , training loss: 4.011395\n",
      "[INFO] Epoch: 67 , batch: 241 , training loss: 3.806989\n",
      "[INFO] Epoch: 67 , batch: 242 , training loss: 3.845652\n",
      "[INFO] Epoch: 67 , batch: 243 , training loss: 4.134367\n",
      "[INFO] Epoch: 67 , batch: 244 , training loss: 4.063478\n",
      "[INFO] Epoch: 67 , batch: 245 , training loss: 4.019646\n",
      "[INFO] Epoch: 67 , batch: 246 , training loss: 3.727170\n",
      "[INFO] Epoch: 67 , batch: 247 , training loss: 3.927246\n",
      "[INFO] Epoch: 67 , batch: 248 , training loss: 3.981668\n",
      "[INFO] Epoch: 67 , batch: 249 , training loss: 3.959983\n",
      "[INFO] Epoch: 67 , batch: 250 , training loss: 3.782346\n",
      "[INFO] Epoch: 67 , batch: 251 , training loss: 4.223294\n",
      "[INFO] Epoch: 67 , batch: 252 , training loss: 3.922275\n",
      "[INFO] Epoch: 67 , batch: 253 , training loss: 3.816389\n",
      "[INFO] Epoch: 67 , batch: 254 , training loss: 4.139596\n",
      "[INFO] Epoch: 67 , batch: 255 , training loss: 4.107515\n",
      "[INFO] Epoch: 67 , batch: 256 , training loss: 4.084782\n",
      "[INFO] Epoch: 67 , batch: 257 , training loss: 4.255692\n",
      "[INFO] Epoch: 67 , batch: 258 , training loss: 4.238275\n",
      "[INFO] Epoch: 67 , batch: 259 , training loss: 4.290610\n",
      "[INFO] Epoch: 67 , batch: 260 , training loss: 4.058495\n",
      "[INFO] Epoch: 67 , batch: 261 , training loss: 4.233900\n",
      "[INFO] Epoch: 67 , batch: 262 , training loss: 4.368522\n",
      "[INFO] Epoch: 67 , batch: 263 , training loss: 4.531996\n",
      "[INFO] Epoch: 67 , batch: 264 , training loss: 3.890541\n",
      "[INFO] Epoch: 67 , batch: 265 , training loss: 4.002999\n",
      "[INFO] Epoch: 67 , batch: 266 , training loss: 4.414919\n",
      "[INFO] Epoch: 67 , batch: 267 , training loss: 4.185025\n",
      "[INFO] Epoch: 67 , batch: 268 , training loss: 4.089863\n",
      "[INFO] Epoch: 67 , batch: 269 , training loss: 4.045101\n",
      "[INFO] Epoch: 67 , batch: 270 , training loss: 4.088182\n",
      "[INFO] Epoch: 67 , batch: 271 , training loss: 4.109525\n",
      "[INFO] Epoch: 67 , batch: 272 , training loss: 4.116849\n",
      "[INFO] Epoch: 67 , batch: 273 , training loss: 4.144550\n",
      "[INFO] Epoch: 67 , batch: 274 , training loss: 4.217830\n",
      "[INFO] Epoch: 67 , batch: 275 , training loss: 4.079998\n",
      "[INFO] Epoch: 67 , batch: 276 , training loss: 4.129216\n",
      "[INFO] Epoch: 67 , batch: 277 , training loss: 4.287486\n",
      "[INFO] Epoch: 67 , batch: 278 , training loss: 3.985945\n",
      "[INFO] Epoch: 67 , batch: 279 , training loss: 3.967192\n",
      "[INFO] Epoch: 67 , batch: 280 , training loss: 3.963171\n",
      "[INFO] Epoch: 67 , batch: 281 , training loss: 4.087293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 67 , batch: 282 , training loss: 4.026241\n",
      "[INFO] Epoch: 67 , batch: 283 , training loss: 4.020268\n",
      "[INFO] Epoch: 67 , batch: 284 , training loss: 4.040576\n",
      "[INFO] Epoch: 67 , batch: 285 , training loss: 3.983639\n",
      "[INFO] Epoch: 67 , batch: 286 , training loss: 3.978784\n",
      "[INFO] Epoch: 67 , batch: 287 , training loss: 3.930163\n",
      "[INFO] Epoch: 67 , batch: 288 , training loss: 3.891465\n",
      "[INFO] Epoch: 67 , batch: 289 , training loss: 3.979147\n",
      "[INFO] Epoch: 67 , batch: 290 , training loss: 3.778947\n",
      "[INFO] Epoch: 67 , batch: 291 , training loss: 3.734157\n",
      "[INFO] Epoch: 67 , batch: 292 , training loss: 3.855909\n",
      "[INFO] Epoch: 67 , batch: 293 , training loss: 3.776498\n",
      "[INFO] Epoch: 67 , batch: 294 , training loss: 4.435353\n",
      "[INFO] Epoch: 67 , batch: 295 , training loss: 4.176951\n",
      "[INFO] Epoch: 67 , batch: 296 , training loss: 4.148318\n",
      "[INFO] Epoch: 67 , batch: 297 , training loss: 4.089418\n",
      "[INFO] Epoch: 67 , batch: 298 , training loss: 3.930863\n",
      "[INFO] Epoch: 67 , batch: 299 , training loss: 3.984321\n",
      "[INFO] Epoch: 67 , batch: 300 , training loss: 3.942807\n",
      "[INFO] Epoch: 67 , batch: 301 , training loss: 3.871479\n",
      "[INFO] Epoch: 67 , batch: 302 , training loss: 4.065139\n",
      "[INFO] Epoch: 67 , batch: 303 , training loss: 4.075157\n",
      "[INFO] Epoch: 67 , batch: 304 , training loss: 4.197148\n",
      "[INFO] Epoch: 67 , batch: 305 , training loss: 4.028185\n",
      "[INFO] Epoch: 67 , batch: 306 , training loss: 4.138893\n",
      "[INFO] Epoch: 67 , batch: 307 , training loss: 4.164829\n",
      "[INFO] Epoch: 67 , batch: 308 , training loss: 3.990148\n",
      "[INFO] Epoch: 67 , batch: 309 , training loss: 3.969506\n",
      "[INFO] Epoch: 67 , batch: 310 , training loss: 3.903329\n",
      "[INFO] Epoch: 67 , batch: 311 , training loss: 3.885654\n",
      "[INFO] Epoch: 67 , batch: 312 , training loss: 3.785296\n",
      "[INFO] Epoch: 67 , batch: 313 , training loss: 3.897335\n",
      "[INFO] Epoch: 67 , batch: 314 , training loss: 3.973084\n",
      "[INFO] Epoch: 67 , batch: 315 , training loss: 4.071893\n",
      "[INFO] Epoch: 67 , batch: 316 , training loss: 4.286314\n",
      "[INFO] Epoch: 67 , batch: 317 , training loss: 4.647096\n",
      "[INFO] Epoch: 67 , batch: 318 , training loss: 4.755263\n",
      "[INFO] Epoch: 67 , batch: 319 , training loss: 4.461030\n",
      "[INFO] Epoch: 67 , batch: 320 , training loss: 4.014785\n",
      "[INFO] Epoch: 67 , batch: 321 , training loss: 3.832509\n",
      "[INFO] Epoch: 67 , batch: 322 , training loss: 3.936038\n",
      "[INFO] Epoch: 67 , batch: 323 , training loss: 3.960192\n",
      "[INFO] Epoch: 67 , batch: 324 , training loss: 3.939030\n",
      "[INFO] Epoch: 67 , batch: 325 , training loss: 4.065850\n",
      "[INFO] Epoch: 67 , batch: 326 , training loss: 4.102096\n",
      "[INFO] Epoch: 67 , batch: 327 , training loss: 4.066495\n",
      "[INFO] Epoch: 67 , batch: 328 , training loss: 4.058273\n",
      "[INFO] Epoch: 67 , batch: 329 , training loss: 3.959683\n",
      "[INFO] Epoch: 67 , batch: 330 , training loss: 3.942347\n",
      "[INFO] Epoch: 67 , batch: 331 , training loss: 4.108822\n",
      "[INFO] Epoch: 67 , batch: 332 , training loss: 3.961545\n",
      "[INFO] Epoch: 67 , batch: 333 , training loss: 3.942105\n",
      "[INFO] Epoch: 67 , batch: 334 , training loss: 3.958369\n",
      "[INFO] Epoch: 67 , batch: 335 , training loss: 4.062753\n",
      "[INFO] Epoch: 67 , batch: 336 , training loss: 4.075687\n",
      "[INFO] Epoch: 67 , batch: 337 , training loss: 4.137569\n",
      "[INFO] Epoch: 67 , batch: 338 , training loss: 4.334637\n",
      "[INFO] Epoch: 67 , batch: 339 , training loss: 4.152061\n",
      "[INFO] Epoch: 67 , batch: 340 , training loss: 4.332693\n",
      "[INFO] Epoch: 67 , batch: 341 , training loss: 4.091403\n",
      "[INFO] Epoch: 67 , batch: 342 , training loss: 3.888569\n",
      "[INFO] Epoch: 67 , batch: 343 , training loss: 3.953939\n",
      "[INFO] Epoch: 67 , batch: 344 , training loss: 3.811973\n",
      "[INFO] Epoch: 67 , batch: 345 , training loss: 3.943839\n",
      "[INFO] Epoch: 67 , batch: 346 , training loss: 4.004788\n",
      "[INFO] Epoch: 67 , batch: 347 , training loss: 3.904372\n",
      "[INFO] Epoch: 67 , batch: 348 , training loss: 4.002721\n",
      "[INFO] Epoch: 67 , batch: 349 , training loss: 4.084641\n",
      "[INFO] Epoch: 67 , batch: 350 , training loss: 3.963528\n",
      "[INFO] Epoch: 67 , batch: 351 , training loss: 4.024726\n",
      "[INFO] Epoch: 67 , batch: 352 , training loss: 4.041136\n",
      "[INFO] Epoch: 67 , batch: 353 , training loss: 4.024903\n",
      "[INFO] Epoch: 67 , batch: 354 , training loss: 4.126982\n",
      "[INFO] Epoch: 67 , batch: 355 , training loss: 4.106059\n",
      "[INFO] Epoch: 67 , batch: 356 , training loss: 3.987092\n",
      "[INFO] Epoch: 67 , batch: 357 , training loss: 4.048088\n",
      "[INFO] Epoch: 67 , batch: 358 , training loss: 3.967598\n",
      "[INFO] Epoch: 67 , batch: 359 , training loss: 3.975278\n",
      "[INFO] Epoch: 67 , batch: 360 , training loss: 4.079661\n",
      "[INFO] Epoch: 67 , batch: 361 , training loss: 4.020422\n",
      "[INFO] Epoch: 67 , batch: 362 , training loss: 4.154781\n",
      "[INFO] Epoch: 67 , batch: 363 , training loss: 4.009411\n",
      "[INFO] Epoch: 67 , batch: 364 , training loss: 4.089872\n",
      "[INFO] Epoch: 67 , batch: 365 , training loss: 4.005747\n",
      "[INFO] Epoch: 67 , batch: 366 , training loss: 4.101160\n",
      "[INFO] Epoch: 67 , batch: 367 , training loss: 4.153203\n",
      "[INFO] Epoch: 67 , batch: 368 , training loss: 4.565711\n",
      "[INFO] Epoch: 67 , batch: 369 , training loss: 4.241334\n",
      "[INFO] Epoch: 67 , batch: 370 , training loss: 4.010835\n",
      "[INFO] Epoch: 67 , batch: 371 , training loss: 4.416181\n",
      "[INFO] Epoch: 67 , batch: 372 , training loss: 4.689436\n",
      "[INFO] Epoch: 67 , batch: 373 , training loss: 4.721531\n",
      "[INFO] Epoch: 67 , batch: 374 , training loss: 4.870435\n",
      "[INFO] Epoch: 67 , batch: 375 , training loss: 4.817700\n",
      "[INFO] Epoch: 67 , batch: 376 , training loss: 4.677993\n",
      "[INFO] Epoch: 67 , batch: 377 , training loss: 4.455734\n",
      "[INFO] Epoch: 67 , batch: 378 , training loss: 4.542675\n",
      "[INFO] Epoch: 67 , batch: 379 , training loss: 4.537772\n",
      "[INFO] Epoch: 67 , batch: 380 , training loss: 4.720467\n",
      "[INFO] Epoch: 67 , batch: 381 , training loss: 4.369967\n",
      "[INFO] Epoch: 67 , batch: 382 , training loss: 4.627674\n",
      "[INFO] Epoch: 67 , batch: 383 , training loss: 4.719527\n",
      "[INFO] Epoch: 67 , batch: 384 , training loss: 4.692735\n",
      "[INFO] Epoch: 67 , batch: 385 , training loss: 4.359856\n",
      "[INFO] Epoch: 67 , batch: 386 , training loss: 4.670727\n",
      "[INFO] Epoch: 67 , batch: 387 , training loss: 4.556077\n",
      "[INFO] Epoch: 67 , batch: 388 , training loss: 4.363747\n",
      "[INFO] Epoch: 67 , batch: 389 , training loss: 4.188558\n",
      "[INFO] Epoch: 67 , batch: 390 , training loss: 4.219488\n",
      "[INFO] Epoch: 67 , batch: 391 , training loss: 4.263204\n",
      "[INFO] Epoch: 67 , batch: 392 , training loss: 4.614558\n",
      "[INFO] Epoch: 67 , batch: 393 , training loss: 4.507177\n",
      "[INFO] Epoch: 67 , batch: 394 , training loss: 4.633018\n",
      "[INFO] Epoch: 67 , batch: 395 , training loss: 4.413526\n",
      "[INFO] Epoch: 67 , batch: 396 , training loss: 4.253066\n",
      "[INFO] Epoch: 67 , batch: 397 , training loss: 4.387457\n",
      "[INFO] Epoch: 67 , batch: 398 , training loss: 4.241776\n",
      "[INFO] Epoch: 67 , batch: 399 , training loss: 4.314574\n",
      "[INFO] Epoch: 67 , batch: 400 , training loss: 4.299097\n",
      "[INFO] Epoch: 67 , batch: 401 , training loss: 4.708763\n",
      "[INFO] Epoch: 67 , batch: 402 , training loss: 4.444530\n",
      "[INFO] Epoch: 67 , batch: 403 , training loss: 4.280982\n",
      "[INFO] Epoch: 67 , batch: 404 , training loss: 4.466148\n",
      "[INFO] Epoch: 67 , batch: 405 , training loss: 4.478475\n",
      "[INFO] Epoch: 67 , batch: 406 , training loss: 4.413934\n",
      "[INFO] Epoch: 67 , batch: 407 , training loss: 4.430413\n",
      "[INFO] Epoch: 67 , batch: 408 , training loss: 4.416328\n",
      "[INFO] Epoch: 67 , batch: 409 , training loss: 4.429324\n",
      "[INFO] Epoch: 67 , batch: 410 , training loss: 4.479931\n",
      "[INFO] Epoch: 67 , batch: 411 , training loss: 4.644581\n",
      "[INFO] Epoch: 67 , batch: 412 , training loss: 4.484004\n",
      "[INFO] Epoch: 67 , batch: 413 , training loss: 4.345162\n",
      "[INFO] Epoch: 67 , batch: 414 , training loss: 4.390492\n",
      "[INFO] Epoch: 67 , batch: 415 , training loss: 4.431107\n",
      "[INFO] Epoch: 67 , batch: 416 , training loss: 4.501634\n",
      "[INFO] Epoch: 67 , batch: 417 , training loss: 4.412174\n",
      "[INFO] Epoch: 67 , batch: 418 , training loss: 4.458372\n",
      "[INFO] Epoch: 67 , batch: 419 , training loss: 4.441072\n",
      "[INFO] Epoch: 67 , batch: 420 , training loss: 4.406730\n",
      "[INFO] Epoch: 67 , batch: 421 , training loss: 4.389249\n",
      "[INFO] Epoch: 67 , batch: 422 , training loss: 4.242392\n",
      "[INFO] Epoch: 67 , batch: 423 , training loss: 4.460563\n",
      "[INFO] Epoch: 67 , batch: 424 , training loss: 4.623601\n",
      "[INFO] Epoch: 67 , batch: 425 , training loss: 4.503365\n",
      "[INFO] Epoch: 67 , batch: 426 , training loss: 4.226510\n",
      "[INFO] Epoch: 67 , batch: 427 , training loss: 4.465064\n",
      "[INFO] Epoch: 67 , batch: 428 , training loss: 4.349244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 67 , batch: 429 , training loss: 4.232678\n",
      "[INFO] Epoch: 67 , batch: 430 , training loss: 4.479813\n",
      "[INFO] Epoch: 67 , batch: 431 , training loss: 4.098118\n",
      "[INFO] Epoch: 67 , batch: 432 , training loss: 4.136372\n",
      "[INFO] Epoch: 67 , batch: 433 , training loss: 4.168867\n",
      "[INFO] Epoch: 67 , batch: 434 , training loss: 4.071635\n",
      "[INFO] Epoch: 67 , batch: 435 , training loss: 4.398790\n",
      "[INFO] Epoch: 67 , batch: 436 , training loss: 4.464430\n",
      "[INFO] Epoch: 67 , batch: 437 , training loss: 4.257416\n",
      "[INFO] Epoch: 67 , batch: 438 , training loss: 4.103260\n",
      "[INFO] Epoch: 67 , batch: 439 , training loss: 4.328461\n",
      "[INFO] Epoch: 67 , batch: 440 , training loss: 4.435379\n",
      "[INFO] Epoch: 67 , batch: 441 , training loss: 4.568100\n",
      "[INFO] Epoch: 67 , batch: 442 , training loss: 4.295494\n",
      "[INFO] Epoch: 67 , batch: 443 , training loss: 4.466596\n",
      "[INFO] Epoch: 67 , batch: 444 , training loss: 4.103615\n",
      "[INFO] Epoch: 67 , batch: 445 , training loss: 3.985967\n",
      "[INFO] Epoch: 67 , batch: 446 , training loss: 3.923166\n",
      "[INFO] Epoch: 67 , batch: 447 , training loss: 4.131280\n",
      "[INFO] Epoch: 67 , batch: 448 , training loss: 4.237958\n",
      "[INFO] Epoch: 67 , batch: 449 , training loss: 4.619652\n",
      "[INFO] Epoch: 67 , batch: 450 , training loss: 4.701853\n",
      "[INFO] Epoch: 67 , batch: 451 , training loss: 4.589379\n",
      "[INFO] Epoch: 67 , batch: 452 , training loss: 4.414570\n",
      "[INFO] Epoch: 67 , batch: 453 , training loss: 4.164087\n",
      "[INFO] Epoch: 67 , batch: 454 , training loss: 4.318371\n",
      "[INFO] Epoch: 67 , batch: 455 , training loss: 4.366328\n",
      "[INFO] Epoch: 67 , batch: 456 , training loss: 4.362653\n",
      "[INFO] Epoch: 67 , batch: 457 , training loss: 4.448783\n",
      "[INFO] Epoch: 67 , batch: 458 , training loss: 4.175841\n",
      "[INFO] Epoch: 67 , batch: 459 , training loss: 4.155253\n",
      "[INFO] Epoch: 67 , batch: 460 , training loss: 4.287739\n",
      "[INFO] Epoch: 67 , batch: 461 , training loss: 4.230549\n",
      "[INFO] Epoch: 67 , batch: 462 , training loss: 4.292813\n",
      "[INFO] Epoch: 67 , batch: 463 , training loss: 4.215948\n",
      "[INFO] Epoch: 67 , batch: 464 , training loss: 4.393041\n",
      "[INFO] Epoch: 67 , batch: 465 , training loss: 4.348526\n",
      "[INFO] Epoch: 67 , batch: 466 , training loss: 4.427407\n",
      "[INFO] Epoch: 67 , batch: 467 , training loss: 4.388872\n",
      "[INFO] Epoch: 67 , batch: 468 , training loss: 4.355519\n",
      "[INFO] Epoch: 67 , batch: 469 , training loss: 4.395638\n",
      "[INFO] Epoch: 67 , batch: 470 , training loss: 4.204152\n",
      "[INFO] Epoch: 67 , batch: 471 , training loss: 4.318359\n",
      "[INFO] Epoch: 67 , batch: 472 , training loss: 4.356911\n",
      "[INFO] Epoch: 67 , batch: 473 , training loss: 4.285955\n",
      "[INFO] Epoch: 67 , batch: 474 , training loss: 4.070535\n",
      "[INFO] Epoch: 67 , batch: 475 , training loss: 3.954417\n",
      "[INFO] Epoch: 67 , batch: 476 , training loss: 4.329809\n",
      "[INFO] Epoch: 67 , batch: 477 , training loss: 4.456848\n",
      "[INFO] Epoch: 67 , batch: 478 , training loss: 4.466266\n",
      "[INFO] Epoch: 67 , batch: 479 , training loss: 4.459216\n",
      "[INFO] Epoch: 67 , batch: 480 , training loss: 4.566579\n",
      "[INFO] Epoch: 67 , batch: 481 , training loss: 4.445179\n",
      "[INFO] Epoch: 67 , batch: 482 , training loss: 4.563020\n",
      "[INFO] Epoch: 67 , batch: 483 , training loss: 4.389068\n",
      "[INFO] Epoch: 67 , batch: 484 , training loss: 4.178626\n",
      "[INFO] Epoch: 67 , batch: 485 , training loss: 4.309070\n",
      "[INFO] Epoch: 67 , batch: 486 , training loss: 4.190527\n",
      "[INFO] Epoch: 67 , batch: 487 , training loss: 4.165652\n",
      "[INFO] Epoch: 67 , batch: 488 , training loss: 4.353847\n",
      "[INFO] Epoch: 67 , batch: 489 , training loss: 4.283068\n",
      "[INFO] Epoch: 67 , batch: 490 , training loss: 4.322794\n",
      "[INFO] Epoch: 67 , batch: 491 , training loss: 4.245055\n",
      "[INFO] Epoch: 67 , batch: 492 , training loss: 4.226835\n",
      "[INFO] Epoch: 67 , batch: 493 , training loss: 4.395225\n",
      "[INFO] Epoch: 67 , batch: 494 , training loss: 4.303168\n",
      "[INFO] Epoch: 67 , batch: 495 , training loss: 4.468037\n",
      "[INFO] Epoch: 67 , batch: 496 , training loss: 4.316547\n",
      "[INFO] Epoch: 67 , batch: 497 , training loss: 4.357234\n",
      "[INFO] Epoch: 67 , batch: 498 , training loss: 4.336269\n",
      "[INFO] Epoch: 67 , batch: 499 , training loss: 4.410371\n",
      "[INFO] Epoch: 67 , batch: 500 , training loss: 4.576910\n",
      "[INFO] Epoch: 67 , batch: 501 , training loss: 4.879237\n",
      "[INFO] Epoch: 67 , batch: 502 , training loss: 4.907012\n",
      "[INFO] Epoch: 67 , batch: 503 , training loss: 4.524063\n",
      "[INFO] Epoch: 67 , batch: 504 , training loss: 4.659139\n",
      "[INFO] Epoch: 67 , batch: 505 , training loss: 4.631956\n",
      "[INFO] Epoch: 67 , batch: 506 , training loss: 4.634867\n",
      "[INFO] Epoch: 67 , batch: 507 , training loss: 4.678616\n",
      "[INFO] Epoch: 67 , batch: 508 , training loss: 4.578320\n",
      "[INFO] Epoch: 67 , batch: 509 , training loss: 4.403113\n",
      "[INFO] Epoch: 67 , batch: 510 , training loss: 4.486907\n",
      "[INFO] Epoch: 67 , batch: 511 , training loss: 4.400201\n",
      "[INFO] Epoch: 67 , batch: 512 , training loss: 4.507987\n",
      "[INFO] Epoch: 67 , batch: 513 , training loss: 4.772616\n",
      "[INFO] Epoch: 67 , batch: 514 , training loss: 4.398347\n",
      "[INFO] Epoch: 67 , batch: 515 , training loss: 4.669846\n",
      "[INFO] Epoch: 67 , batch: 516 , training loss: 4.473797\n",
      "[INFO] Epoch: 67 , batch: 517 , training loss: 4.429332\n",
      "[INFO] Epoch: 67 , batch: 518 , training loss: 4.398761\n",
      "[INFO] Epoch: 67 , batch: 519 , training loss: 4.218609\n",
      "[INFO] Epoch: 67 , batch: 520 , training loss: 4.478689\n",
      "[INFO] Epoch: 67 , batch: 521 , training loss: 4.429909\n",
      "[INFO] Epoch: 67 , batch: 522 , training loss: 4.532656\n",
      "[INFO] Epoch: 67 , batch: 523 , training loss: 4.462428\n",
      "[INFO] Epoch: 67 , batch: 524 , training loss: 4.739794\n",
      "[INFO] Epoch: 67 , batch: 525 , training loss: 4.628079\n",
      "[INFO] Epoch: 67 , batch: 526 , training loss: 4.410141\n",
      "[INFO] Epoch: 67 , batch: 527 , training loss: 4.441917\n",
      "[INFO] Epoch: 67 , batch: 528 , training loss: 4.455648\n",
      "[INFO] Epoch: 67 , batch: 529 , training loss: 4.440334\n",
      "[INFO] Epoch: 67 , batch: 530 , training loss: 4.288608\n",
      "[INFO] Epoch: 67 , batch: 531 , training loss: 4.434076\n",
      "[INFO] Epoch: 67 , batch: 532 , training loss: 4.344203\n",
      "[INFO] Epoch: 67 , batch: 533 , training loss: 4.456963\n",
      "[INFO] Epoch: 67 , batch: 534 , training loss: 4.475742\n",
      "[INFO] Epoch: 67 , batch: 535 , training loss: 4.475861\n",
      "[INFO] Epoch: 67 , batch: 536 , training loss: 4.324625\n",
      "[INFO] Epoch: 67 , batch: 537 , training loss: 4.321160\n",
      "[INFO] Epoch: 67 , batch: 538 , training loss: 4.406355\n",
      "[INFO] Epoch: 67 , batch: 539 , training loss: 4.496706\n",
      "[INFO] Epoch: 67 , batch: 540 , training loss: 5.045886\n",
      "[INFO] Epoch: 67 , batch: 541 , training loss: 4.874099\n",
      "[INFO] Epoch: 67 , batch: 542 , training loss: 4.732964\n",
      "[INFO] Epoch: 68 , batch: 0 , training loss: 3.734346\n",
      "[INFO] Epoch: 68 , batch: 1 , training loss: 3.594348\n",
      "[INFO] Epoch: 68 , batch: 2 , training loss: 3.732423\n",
      "[INFO] Epoch: 68 , batch: 3 , training loss: 3.627004\n",
      "[INFO] Epoch: 68 , batch: 4 , training loss: 4.010489\n",
      "[INFO] Epoch: 68 , batch: 5 , training loss: 3.645818\n",
      "[INFO] Epoch: 68 , batch: 6 , training loss: 4.109894\n",
      "[INFO] Epoch: 68 , batch: 7 , training loss: 3.968884\n",
      "[INFO] Epoch: 68 , batch: 8 , training loss: 3.636114\n",
      "[INFO] Epoch: 68 , batch: 9 , training loss: 3.912080\n",
      "[INFO] Epoch: 68 , batch: 10 , training loss: 3.876093\n",
      "[INFO] Epoch: 68 , batch: 11 , training loss: 3.770667\n",
      "[INFO] Epoch: 68 , batch: 12 , training loss: 3.725091\n",
      "[INFO] Epoch: 68 , batch: 13 , training loss: 3.702109\n",
      "[INFO] Epoch: 68 , batch: 14 , training loss: 3.655283\n",
      "[INFO] Epoch: 68 , batch: 15 , training loss: 3.883596\n",
      "[INFO] Epoch: 68 , batch: 16 , training loss: 3.667949\n",
      "[INFO] Epoch: 68 , batch: 17 , training loss: 3.798287\n",
      "[INFO] Epoch: 68 , batch: 18 , training loss: 3.743871\n",
      "[INFO] Epoch: 68 , batch: 19 , training loss: 3.514211\n",
      "[INFO] Epoch: 68 , batch: 20 , training loss: 3.557701\n",
      "[INFO] Epoch: 68 , batch: 21 , training loss: 3.651270\n",
      "[INFO] Epoch: 68 , batch: 22 , training loss: 3.547663\n",
      "[INFO] Epoch: 68 , batch: 23 , training loss: 3.737057\n",
      "[INFO] Epoch: 68 , batch: 24 , training loss: 3.587499\n",
      "[INFO] Epoch: 68 , batch: 25 , training loss: 3.728682\n",
      "[INFO] Epoch: 68 , batch: 26 , training loss: 3.584160\n",
      "[INFO] Epoch: 68 , batch: 27 , training loss: 3.593865\n",
      "[INFO] Epoch: 68 , batch: 28 , training loss: 3.730793\n",
      "[INFO] Epoch: 68 , batch: 29 , training loss: 3.558196\n",
      "[INFO] Epoch: 68 , batch: 30 , training loss: 3.648245\n",
      "[INFO] Epoch: 68 , batch: 31 , training loss: 3.659843\n",
      "[INFO] Epoch: 68 , batch: 32 , training loss: 3.619778\n",
      "[INFO] Epoch: 68 , batch: 33 , training loss: 3.661861\n",
      "[INFO] Epoch: 68 , batch: 34 , training loss: 3.696630\n",
      "[INFO] Epoch: 68 , batch: 35 , training loss: 3.599440\n",
      "[INFO] Epoch: 68 , batch: 36 , training loss: 3.705764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 68 , batch: 37 , training loss: 3.548855\n",
      "[INFO] Epoch: 68 , batch: 38 , training loss: 3.632975\n",
      "[INFO] Epoch: 68 , batch: 39 , training loss: 3.461396\n",
      "[INFO] Epoch: 68 , batch: 40 , training loss: 3.659282\n",
      "[INFO] Epoch: 68 , batch: 41 , training loss: 3.652766\n",
      "[INFO] Epoch: 68 , batch: 42 , training loss: 4.125460\n",
      "[INFO] Epoch: 68 , batch: 43 , training loss: 3.902508\n",
      "[INFO] Epoch: 68 , batch: 44 , training loss: 4.193121\n",
      "[INFO] Epoch: 68 , batch: 45 , training loss: 4.173852\n",
      "[INFO] Epoch: 68 , batch: 46 , training loss: 4.131940\n",
      "[INFO] Epoch: 68 , batch: 47 , training loss: 3.690733\n",
      "[INFO] Epoch: 68 , batch: 48 , training loss: 3.744583\n",
      "[INFO] Epoch: 68 , batch: 49 , training loss: 3.904828\n",
      "[INFO] Epoch: 68 , batch: 50 , training loss: 3.700145\n",
      "[INFO] Epoch: 68 , batch: 51 , training loss: 3.867331\n",
      "[INFO] Epoch: 68 , batch: 52 , training loss: 3.679422\n",
      "[INFO] Epoch: 68 , batch: 53 , training loss: 3.816211\n",
      "[INFO] Epoch: 68 , batch: 54 , training loss: 3.844553\n",
      "[INFO] Epoch: 68 , batch: 55 , training loss: 3.864537\n",
      "[INFO] Epoch: 68 , batch: 56 , training loss: 3.688123\n",
      "[INFO] Epoch: 68 , batch: 57 , training loss: 3.653881\n",
      "[INFO] Epoch: 68 , batch: 58 , training loss: 3.690891\n",
      "[INFO] Epoch: 68 , batch: 59 , training loss: 3.737597\n",
      "[INFO] Epoch: 68 , batch: 60 , training loss: 3.673735\n",
      "[INFO] Epoch: 68 , batch: 61 , training loss: 3.814948\n",
      "[INFO] Epoch: 68 , batch: 62 , training loss: 3.670398\n",
      "[INFO] Epoch: 68 , batch: 63 , training loss: 3.875874\n",
      "[INFO] Epoch: 68 , batch: 64 , training loss: 4.024991\n",
      "[INFO] Epoch: 68 , batch: 65 , training loss: 3.756331\n",
      "[INFO] Epoch: 68 , batch: 66 , training loss: 3.602371\n",
      "[INFO] Epoch: 68 , batch: 67 , training loss: 3.637160\n",
      "[INFO] Epoch: 68 , batch: 68 , training loss: 3.819486\n",
      "[INFO] Epoch: 68 , batch: 69 , training loss: 3.762635\n",
      "[INFO] Epoch: 68 , batch: 70 , training loss: 3.946550\n",
      "[INFO] Epoch: 68 , batch: 71 , training loss: 3.798179\n",
      "[INFO] Epoch: 68 , batch: 72 , training loss: 3.853186\n",
      "[INFO] Epoch: 68 , batch: 73 , training loss: 3.816041\n",
      "[INFO] Epoch: 68 , batch: 74 , training loss: 3.912681\n",
      "[INFO] Epoch: 68 , batch: 75 , training loss: 3.746332\n",
      "[INFO] Epoch: 68 , batch: 76 , training loss: 3.888957\n",
      "[INFO] Epoch: 68 , batch: 77 , training loss: 3.803456\n",
      "[INFO] Epoch: 68 , batch: 78 , training loss: 3.924334\n",
      "[INFO] Epoch: 68 , batch: 79 , training loss: 3.748100\n",
      "[INFO] Epoch: 68 , batch: 80 , training loss: 3.991323\n",
      "[INFO] Epoch: 68 , batch: 81 , training loss: 3.872822\n",
      "[INFO] Epoch: 68 , batch: 82 , training loss: 3.857947\n",
      "[INFO] Epoch: 68 , batch: 83 , training loss: 3.973463\n",
      "[INFO] Epoch: 68 , batch: 84 , training loss: 3.921304\n",
      "[INFO] Epoch: 68 , batch: 85 , training loss: 3.992192\n",
      "[INFO] Epoch: 68 , batch: 86 , training loss: 3.954969\n",
      "[INFO] Epoch: 68 , batch: 87 , training loss: 3.878251\n",
      "[INFO] Epoch: 68 , batch: 88 , training loss: 4.064300\n",
      "[INFO] Epoch: 68 , batch: 89 , training loss: 3.787349\n",
      "[INFO] Epoch: 68 , batch: 90 , training loss: 3.925939\n",
      "[INFO] Epoch: 68 , batch: 91 , training loss: 3.851341\n",
      "[INFO] Epoch: 68 , batch: 92 , training loss: 3.897673\n",
      "[INFO] Epoch: 68 , batch: 93 , training loss: 3.968733\n",
      "[INFO] Epoch: 68 , batch: 94 , training loss: 4.060038\n",
      "[INFO] Epoch: 68 , batch: 95 , training loss: 3.893979\n",
      "[INFO] Epoch: 68 , batch: 96 , training loss: 3.879683\n",
      "[INFO] Epoch: 68 , batch: 97 , training loss: 3.798046\n",
      "[INFO] Epoch: 68 , batch: 98 , training loss: 3.741908\n",
      "[INFO] Epoch: 68 , batch: 99 , training loss: 3.911018\n",
      "[INFO] Epoch: 68 , batch: 100 , training loss: 3.757900\n",
      "[INFO] Epoch: 68 , batch: 101 , training loss: 3.791003\n",
      "[INFO] Epoch: 68 , batch: 102 , training loss: 3.948694\n",
      "[INFO] Epoch: 68 , batch: 103 , training loss: 3.716184\n",
      "[INFO] Epoch: 68 , batch: 104 , training loss: 3.688796\n",
      "[INFO] Epoch: 68 , batch: 105 , training loss: 3.939893\n",
      "[INFO] Epoch: 68 , batch: 106 , training loss: 3.995193\n",
      "[INFO] Epoch: 68 , batch: 107 , training loss: 3.837985\n",
      "[INFO] Epoch: 68 , batch: 108 , training loss: 3.762779\n",
      "[INFO] Epoch: 68 , batch: 109 , training loss: 3.665303\n",
      "[INFO] Epoch: 68 , batch: 110 , training loss: 3.850982\n",
      "[INFO] Epoch: 68 , batch: 111 , training loss: 3.937831\n",
      "[INFO] Epoch: 68 , batch: 112 , training loss: 3.856399\n",
      "[INFO] Epoch: 68 , batch: 113 , training loss: 3.828308\n",
      "[INFO] Epoch: 68 , batch: 114 , training loss: 3.817494\n",
      "[INFO] Epoch: 68 , batch: 115 , training loss: 3.826618\n",
      "[INFO] Epoch: 68 , batch: 116 , training loss: 3.753465\n",
      "[INFO] Epoch: 68 , batch: 117 , training loss: 3.957409\n",
      "[INFO] Epoch: 68 , batch: 118 , training loss: 3.941015\n",
      "[INFO] Epoch: 68 , batch: 119 , training loss: 4.081357\n",
      "[INFO] Epoch: 68 , batch: 120 , training loss: 4.053528\n",
      "[INFO] Epoch: 68 , batch: 121 , training loss: 3.921601\n",
      "[INFO] Epoch: 68 , batch: 122 , training loss: 3.826397\n",
      "[INFO] Epoch: 68 , batch: 123 , training loss: 3.799297\n",
      "[INFO] Epoch: 68 , batch: 124 , training loss: 3.947759\n",
      "[INFO] Epoch: 68 , batch: 125 , training loss: 3.758509\n",
      "[INFO] Epoch: 68 , batch: 126 , training loss: 3.745802\n",
      "[INFO] Epoch: 68 , batch: 127 , training loss: 3.765826\n",
      "[INFO] Epoch: 68 , batch: 128 , training loss: 3.906705\n",
      "[INFO] Epoch: 68 , batch: 129 , training loss: 3.832164\n",
      "[INFO] Epoch: 68 , batch: 130 , training loss: 3.858706\n",
      "[INFO] Epoch: 68 , batch: 131 , training loss: 3.860880\n",
      "[INFO] Epoch: 68 , batch: 132 , training loss: 3.871081\n",
      "[INFO] Epoch: 68 , batch: 133 , training loss: 3.812500\n",
      "[INFO] Epoch: 68 , batch: 134 , training loss: 3.612370\n",
      "[INFO] Epoch: 68 , batch: 135 , training loss: 3.673154\n",
      "[INFO] Epoch: 68 , batch: 136 , training loss: 3.957371\n",
      "[INFO] Epoch: 68 , batch: 137 , training loss: 3.908420\n",
      "[INFO] Epoch: 68 , batch: 138 , training loss: 3.927031\n",
      "[INFO] Epoch: 68 , batch: 139 , training loss: 4.551885\n",
      "[INFO] Epoch: 68 , batch: 140 , training loss: 4.279386\n",
      "[INFO] Epoch: 68 , batch: 141 , training loss: 4.065567\n",
      "[INFO] Epoch: 68 , batch: 142 , training loss: 3.787335\n",
      "[INFO] Epoch: 68 , batch: 143 , training loss: 3.913088\n",
      "[INFO] Epoch: 68 , batch: 144 , training loss: 3.772070\n",
      "[INFO] Epoch: 68 , batch: 145 , training loss: 3.815244\n",
      "[INFO] Epoch: 68 , batch: 146 , training loss: 4.063070\n",
      "[INFO] Epoch: 68 , batch: 147 , training loss: 3.695234\n",
      "[INFO] Epoch: 68 , batch: 148 , training loss: 3.661745\n",
      "[INFO] Epoch: 68 , batch: 149 , training loss: 3.754154\n",
      "[INFO] Epoch: 68 , batch: 150 , training loss: 4.009629\n",
      "[INFO] Epoch: 68 , batch: 151 , training loss: 3.848051\n",
      "[INFO] Epoch: 68 , batch: 152 , training loss: 3.856846\n",
      "[INFO] Epoch: 68 , batch: 153 , training loss: 3.898757\n",
      "[INFO] Epoch: 68 , batch: 154 , training loss: 3.996938\n",
      "[INFO] Epoch: 68 , batch: 155 , training loss: 4.238401\n",
      "[INFO] Epoch: 68 , batch: 156 , training loss: 3.925313\n",
      "[INFO] Epoch: 68 , batch: 157 , training loss: 3.875909\n",
      "[INFO] Epoch: 68 , batch: 158 , training loss: 4.017047\n",
      "[INFO] Epoch: 68 , batch: 159 , training loss: 3.951695\n",
      "[INFO] Epoch: 68 , batch: 160 , training loss: 4.124568\n",
      "[INFO] Epoch: 68 , batch: 161 , training loss: 4.190839\n",
      "[INFO] Epoch: 68 , batch: 162 , training loss: 4.182680\n",
      "[INFO] Epoch: 68 , batch: 163 , training loss: 4.328110\n",
      "[INFO] Epoch: 68 , batch: 164 , training loss: 4.283432\n",
      "[INFO] Epoch: 68 , batch: 165 , training loss: 4.224441\n",
      "[INFO] Epoch: 68 , batch: 166 , training loss: 4.134951\n",
      "[INFO] Epoch: 68 , batch: 167 , training loss: 4.175190\n",
      "[INFO] Epoch: 68 , batch: 168 , training loss: 3.924051\n",
      "[INFO] Epoch: 68 , batch: 169 , training loss: 3.926124\n",
      "[INFO] Epoch: 68 , batch: 170 , training loss: 4.074598\n",
      "[INFO] Epoch: 68 , batch: 171 , training loss: 3.574355\n",
      "[INFO] Epoch: 68 , batch: 172 , training loss: 3.771392\n",
      "[INFO] Epoch: 68 , batch: 173 , training loss: 4.069799\n",
      "[INFO] Epoch: 68 , batch: 174 , training loss: 4.505163\n",
      "[INFO] Epoch: 68 , batch: 175 , training loss: 4.730827\n",
      "[INFO] Epoch: 68 , batch: 176 , training loss: 4.414072\n",
      "[INFO] Epoch: 68 , batch: 177 , training loss: 4.009080\n",
      "[INFO] Epoch: 68 , batch: 178 , training loss: 4.028651\n",
      "[INFO] Epoch: 68 , batch: 179 , training loss: 4.066377\n",
      "[INFO] Epoch: 68 , batch: 180 , training loss: 4.032220\n",
      "[INFO] Epoch: 68 , batch: 181 , training loss: 4.343922\n",
      "[INFO] Epoch: 68 , batch: 182 , training loss: 4.251512\n",
      "[INFO] Epoch: 68 , batch: 183 , training loss: 4.262915\n",
      "[INFO] Epoch: 68 , batch: 184 , training loss: 4.140501\n",
      "[INFO] Epoch: 68 , batch: 185 , training loss: 4.104530\n",
      "[INFO] Epoch: 68 , batch: 186 , training loss: 4.247523\n",
      "[INFO] Epoch: 68 , batch: 187 , training loss: 4.340439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 68 , batch: 188 , training loss: 4.342269\n",
      "[INFO] Epoch: 68 , batch: 189 , training loss: 4.243617\n",
      "[INFO] Epoch: 68 , batch: 190 , training loss: 4.301697\n",
      "[INFO] Epoch: 68 , batch: 191 , training loss: 4.382640\n",
      "[INFO] Epoch: 68 , batch: 192 , training loss: 4.237031\n",
      "[INFO] Epoch: 68 , batch: 193 , training loss: 4.334488\n",
      "[INFO] Epoch: 68 , batch: 194 , training loss: 4.278983\n",
      "[INFO] Epoch: 68 , batch: 195 , training loss: 4.213048\n",
      "[INFO] Epoch: 68 , batch: 196 , training loss: 4.067143\n",
      "[INFO] Epoch: 68 , batch: 197 , training loss: 4.150609\n",
      "[INFO] Epoch: 68 , batch: 198 , training loss: 4.080446\n",
      "[INFO] Epoch: 68 , batch: 199 , training loss: 4.189634\n",
      "[INFO] Epoch: 68 , batch: 200 , training loss: 4.088896\n",
      "[INFO] Epoch: 68 , batch: 201 , training loss: 4.004460\n",
      "[INFO] Epoch: 68 , batch: 202 , training loss: 4.012346\n",
      "[INFO] Epoch: 68 , batch: 203 , training loss: 4.153096\n",
      "[INFO] Epoch: 68 , batch: 204 , training loss: 4.209075\n",
      "[INFO] Epoch: 68 , batch: 205 , training loss: 3.832230\n",
      "[INFO] Epoch: 68 , batch: 206 , training loss: 3.774210\n",
      "[INFO] Epoch: 68 , batch: 207 , training loss: 3.744492\n",
      "[INFO] Epoch: 68 , batch: 208 , training loss: 4.085432\n",
      "[INFO] Epoch: 68 , batch: 209 , training loss: 4.058991\n",
      "[INFO] Epoch: 68 , batch: 210 , training loss: 4.060526\n",
      "[INFO] Epoch: 68 , batch: 211 , training loss: 4.058819\n",
      "[INFO] Epoch: 68 , batch: 212 , training loss: 4.163621\n",
      "[INFO] Epoch: 68 , batch: 213 , training loss: 4.104578\n",
      "[INFO] Epoch: 68 , batch: 214 , training loss: 4.168038\n",
      "[INFO] Epoch: 68 , batch: 215 , training loss: 4.363129\n",
      "[INFO] Epoch: 68 , batch: 216 , training loss: 4.065971\n",
      "[INFO] Epoch: 68 , batch: 217 , training loss: 4.031524\n",
      "[INFO] Epoch: 68 , batch: 218 , training loss: 4.005997\n",
      "[INFO] Epoch: 68 , batch: 219 , training loss: 4.109774\n",
      "[INFO] Epoch: 68 , batch: 220 , training loss: 3.965636\n",
      "[INFO] Epoch: 68 , batch: 221 , training loss: 3.973357\n",
      "[INFO] Epoch: 68 , batch: 222 , training loss: 4.114473\n",
      "[INFO] Epoch: 68 , batch: 223 , training loss: 4.235442\n",
      "[INFO] Epoch: 68 , batch: 224 , training loss: 4.271004\n",
      "[INFO] Epoch: 68 , batch: 225 , training loss: 4.167012\n",
      "[INFO] Epoch: 68 , batch: 226 , training loss: 4.284739\n",
      "[INFO] Epoch: 68 , batch: 227 , training loss: 4.261649\n",
      "[INFO] Epoch: 68 , batch: 228 , training loss: 4.273312\n",
      "[INFO] Epoch: 68 , batch: 229 , training loss: 4.116547\n",
      "[INFO] Epoch: 68 , batch: 230 , training loss: 3.986780\n",
      "[INFO] Epoch: 68 , batch: 231 , training loss: 3.858164\n",
      "[INFO] Epoch: 68 , batch: 232 , training loss: 3.984994\n",
      "[INFO] Epoch: 68 , batch: 233 , training loss: 4.020997\n",
      "[INFO] Epoch: 68 , batch: 234 , training loss: 3.709351\n",
      "[INFO] Epoch: 68 , batch: 235 , training loss: 3.823387\n",
      "[INFO] Epoch: 68 , batch: 236 , training loss: 3.924879\n",
      "[INFO] Epoch: 68 , batch: 237 , training loss: 4.152300\n",
      "[INFO] Epoch: 68 , batch: 238 , training loss: 3.948569\n",
      "[INFO] Epoch: 68 , batch: 239 , training loss: 3.973532\n",
      "[INFO] Epoch: 68 , batch: 240 , training loss: 4.013867\n",
      "[INFO] Epoch: 68 , batch: 241 , training loss: 3.800641\n",
      "[INFO] Epoch: 68 , batch: 242 , training loss: 3.864021\n",
      "[INFO] Epoch: 68 , batch: 243 , training loss: 4.122522\n",
      "[INFO] Epoch: 68 , batch: 244 , training loss: 4.039729\n",
      "[INFO] Epoch: 68 , batch: 245 , training loss: 4.015754\n",
      "[INFO] Epoch: 68 , batch: 246 , training loss: 3.736372\n",
      "[INFO] Epoch: 68 , batch: 247 , training loss: 3.920732\n",
      "[INFO] Epoch: 68 , batch: 248 , training loss: 3.990260\n",
      "[INFO] Epoch: 68 , batch: 249 , training loss: 3.963942\n",
      "[INFO] Epoch: 68 , batch: 250 , training loss: 3.775441\n",
      "[INFO] Epoch: 68 , batch: 251 , training loss: 4.218416\n",
      "[INFO] Epoch: 68 , batch: 252 , training loss: 3.928456\n",
      "[INFO] Epoch: 68 , batch: 253 , training loss: 3.838794\n",
      "[INFO] Epoch: 68 , batch: 254 , training loss: 4.117238\n",
      "[INFO] Epoch: 68 , batch: 255 , training loss: 4.094829\n",
      "[INFO] Epoch: 68 , batch: 256 , training loss: 4.048089\n",
      "[INFO] Epoch: 68 , batch: 257 , training loss: 4.233812\n",
      "[INFO] Epoch: 68 , batch: 258 , training loss: 4.235387\n",
      "[INFO] Epoch: 68 , batch: 259 , training loss: 4.295833\n",
      "[INFO] Epoch: 68 , batch: 260 , training loss: 4.037340\n",
      "[INFO] Epoch: 68 , batch: 261 , training loss: 4.225310\n",
      "[INFO] Epoch: 68 , batch: 262 , training loss: 4.336224\n",
      "[INFO] Epoch: 68 , batch: 263 , training loss: 4.546321\n",
      "[INFO] Epoch: 68 , batch: 264 , training loss: 3.887835\n",
      "[INFO] Epoch: 68 , batch: 265 , training loss: 3.986006\n",
      "[INFO] Epoch: 68 , batch: 266 , training loss: 4.398180\n",
      "[INFO] Epoch: 68 , batch: 267 , training loss: 4.185104\n",
      "[INFO] Epoch: 68 , batch: 268 , training loss: 4.079517\n",
      "[INFO] Epoch: 68 , batch: 269 , training loss: 4.032833\n",
      "[INFO] Epoch: 68 , batch: 270 , training loss: 4.085463\n",
      "[INFO] Epoch: 68 , batch: 271 , training loss: 4.091010\n",
      "[INFO] Epoch: 68 , batch: 272 , training loss: 4.103573\n",
      "[INFO] Epoch: 68 , batch: 273 , training loss: 4.119748\n",
      "[INFO] Epoch: 68 , batch: 274 , training loss: 4.218878\n",
      "[INFO] Epoch: 68 , batch: 275 , training loss: 4.064132\n",
      "[INFO] Epoch: 68 , batch: 276 , training loss: 4.137384\n",
      "[INFO] Epoch: 68 , batch: 277 , training loss: 4.270056\n",
      "[INFO] Epoch: 68 , batch: 278 , training loss: 3.966040\n",
      "[INFO] Epoch: 68 , batch: 279 , training loss: 3.977257\n",
      "[INFO] Epoch: 68 , batch: 280 , training loss: 3.957828\n",
      "[INFO] Epoch: 68 , batch: 281 , training loss: 4.091368\n",
      "[INFO] Epoch: 68 , batch: 282 , training loss: 4.027491\n",
      "[INFO] Epoch: 68 , batch: 283 , training loss: 4.005260\n",
      "[INFO] Epoch: 68 , batch: 284 , training loss: 4.032449\n",
      "[INFO] Epoch: 68 , batch: 285 , training loss: 3.988762\n",
      "[INFO] Epoch: 68 , batch: 286 , training loss: 3.981154\n",
      "[INFO] Epoch: 68 , batch: 287 , training loss: 3.922312\n",
      "[INFO] Epoch: 68 , batch: 288 , training loss: 3.893183\n",
      "[INFO] Epoch: 68 , batch: 289 , training loss: 3.976537\n",
      "[INFO] Epoch: 68 , batch: 290 , training loss: 3.747427\n",
      "[INFO] Epoch: 68 , batch: 291 , training loss: 3.735453\n",
      "[INFO] Epoch: 68 , batch: 292 , training loss: 3.842425\n",
      "[INFO] Epoch: 68 , batch: 293 , training loss: 3.760732\n",
      "[INFO] Epoch: 68 , batch: 294 , training loss: 4.426933\n",
      "[INFO] Epoch: 68 , batch: 295 , training loss: 4.179443\n",
      "[INFO] Epoch: 68 , batch: 296 , training loss: 4.106881\n",
      "[INFO] Epoch: 68 , batch: 297 , training loss: 4.085229\n",
      "[INFO] Epoch: 68 , batch: 298 , training loss: 3.929673\n",
      "[INFO] Epoch: 68 , batch: 299 , training loss: 3.962862\n",
      "[INFO] Epoch: 68 , batch: 300 , training loss: 3.958448\n",
      "[INFO] Epoch: 68 , batch: 301 , training loss: 3.869916\n",
      "[INFO] Epoch: 68 , batch: 302 , training loss: 4.033937\n",
      "[INFO] Epoch: 68 , batch: 303 , training loss: 4.067080\n",
      "[INFO] Epoch: 68 , batch: 304 , training loss: 4.187940\n",
      "[INFO] Epoch: 68 , batch: 305 , training loss: 4.018936\n",
      "[INFO] Epoch: 68 , batch: 306 , training loss: 4.151350\n",
      "[INFO] Epoch: 68 , batch: 307 , training loss: 4.168198\n",
      "[INFO] Epoch: 68 , batch: 308 , training loss: 3.987517\n",
      "[INFO] Epoch: 68 , batch: 309 , training loss: 3.954405\n",
      "[INFO] Epoch: 68 , batch: 310 , training loss: 3.900225\n",
      "[INFO] Epoch: 68 , batch: 311 , training loss: 3.883389\n",
      "[INFO] Epoch: 68 , batch: 312 , training loss: 3.773488\n",
      "[INFO] Epoch: 68 , batch: 313 , training loss: 3.878906\n",
      "[INFO] Epoch: 68 , batch: 314 , training loss: 3.961207\n",
      "[INFO] Epoch: 68 , batch: 315 , training loss: 4.054681\n",
      "[INFO] Epoch: 68 , batch: 316 , training loss: 4.299846\n",
      "[INFO] Epoch: 68 , batch: 317 , training loss: 4.631862\n",
      "[INFO] Epoch: 68 , batch: 318 , training loss: 4.752710\n",
      "[INFO] Epoch: 68 , batch: 319 , training loss: 4.445554\n",
      "[INFO] Epoch: 68 , batch: 320 , training loss: 3.987134\n",
      "[INFO] Epoch: 68 , batch: 321 , training loss: 3.816905\n",
      "[INFO] Epoch: 68 , batch: 322 , training loss: 3.923836\n",
      "[INFO] Epoch: 68 , batch: 323 , training loss: 3.960799\n",
      "[INFO] Epoch: 68 , batch: 324 , training loss: 3.919585\n",
      "[INFO] Epoch: 68 , batch: 325 , training loss: 4.060219\n",
      "[INFO] Epoch: 68 , batch: 326 , training loss: 4.102288\n",
      "[INFO] Epoch: 68 , batch: 327 , training loss: 4.060247\n",
      "[INFO] Epoch: 68 , batch: 328 , training loss: 4.052525\n",
      "[INFO] Epoch: 68 , batch: 329 , training loss: 3.951099\n",
      "[INFO] Epoch: 68 , batch: 330 , training loss: 3.932632\n",
      "[INFO] Epoch: 68 , batch: 331 , training loss: 4.104385\n",
      "[INFO] Epoch: 68 , batch: 332 , training loss: 3.940717\n",
      "[INFO] Epoch: 68 , batch: 333 , training loss: 3.925054\n",
      "[INFO] Epoch: 68 , batch: 334 , training loss: 3.958050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 68 , batch: 335 , training loss: 4.073825\n",
      "[INFO] Epoch: 68 , batch: 336 , training loss: 4.068823\n",
      "[INFO] Epoch: 68 , batch: 337 , training loss: 4.129737\n",
      "[INFO] Epoch: 68 , batch: 338 , training loss: 4.335823\n",
      "[INFO] Epoch: 68 , batch: 339 , training loss: 4.134354\n",
      "[INFO] Epoch: 68 , batch: 340 , training loss: 4.316991\n",
      "[INFO] Epoch: 68 , batch: 341 , training loss: 4.098121\n",
      "[INFO] Epoch: 68 , batch: 342 , training loss: 3.871255\n",
      "[INFO] Epoch: 68 , batch: 343 , training loss: 3.944201\n",
      "[INFO] Epoch: 68 , batch: 344 , training loss: 3.822391\n",
      "[INFO] Epoch: 68 , batch: 345 , training loss: 3.930113\n",
      "[INFO] Epoch: 68 , batch: 346 , training loss: 3.996956\n",
      "[INFO] Epoch: 68 , batch: 347 , training loss: 3.903978\n",
      "[INFO] Epoch: 68 , batch: 348 , training loss: 3.994488\n",
      "[INFO] Epoch: 68 , batch: 349 , training loss: 4.081774\n",
      "[INFO] Epoch: 68 , batch: 350 , training loss: 3.966354\n",
      "[INFO] Epoch: 68 , batch: 351 , training loss: 4.029216\n",
      "[INFO] Epoch: 68 , batch: 352 , training loss: 4.038766\n",
      "[INFO] Epoch: 68 , batch: 353 , training loss: 4.034079\n",
      "[INFO] Epoch: 68 , batch: 354 , training loss: 4.118455\n",
      "[INFO] Epoch: 68 , batch: 355 , training loss: 4.105153\n",
      "[INFO] Epoch: 68 , batch: 356 , training loss: 3.986055\n",
      "[INFO] Epoch: 68 , batch: 357 , training loss: 4.037261\n",
      "[INFO] Epoch: 68 , batch: 358 , training loss: 3.983118\n",
      "[INFO] Epoch: 68 , batch: 359 , training loss: 3.960255\n",
      "[INFO] Epoch: 68 , batch: 360 , training loss: 4.067457\n",
      "[INFO] Epoch: 68 , batch: 361 , training loss: 4.022255\n",
      "[INFO] Epoch: 68 , batch: 362 , training loss: 4.159359\n",
      "[INFO] Epoch: 68 , batch: 363 , training loss: 4.012767\n",
      "[INFO] Epoch: 68 , batch: 364 , training loss: 4.087321\n",
      "[INFO] Epoch: 68 , batch: 365 , training loss: 4.003975\n",
      "[INFO] Epoch: 68 , batch: 366 , training loss: 4.092135\n",
      "[INFO] Epoch: 68 , batch: 367 , training loss: 4.136642\n",
      "[INFO] Epoch: 68 , batch: 368 , training loss: 4.561352\n",
      "[INFO] Epoch: 68 , batch: 369 , training loss: 4.219527\n",
      "[INFO] Epoch: 68 , batch: 370 , training loss: 3.983754\n",
      "[INFO] Epoch: 68 , batch: 371 , training loss: 4.400987\n",
      "[INFO] Epoch: 68 , batch: 372 , training loss: 4.662991\n",
      "[INFO] Epoch: 68 , batch: 373 , training loss: 4.707854\n",
      "[INFO] Epoch: 68 , batch: 374 , training loss: 4.860454\n",
      "[INFO] Epoch: 68 , batch: 375 , training loss: 4.805614\n",
      "[INFO] Epoch: 68 , batch: 376 , training loss: 4.681881\n",
      "[INFO] Epoch: 68 , batch: 377 , training loss: 4.435496\n",
      "[INFO] Epoch: 68 , batch: 378 , training loss: 4.517241\n",
      "[INFO] Epoch: 68 , batch: 379 , training loss: 4.530066\n",
      "[INFO] Epoch: 68 , batch: 380 , training loss: 4.716548\n",
      "[INFO] Epoch: 68 , batch: 381 , training loss: 4.381487\n",
      "[INFO] Epoch: 68 , batch: 382 , training loss: 4.597928\n",
      "[INFO] Epoch: 68 , batch: 383 , training loss: 4.694062\n",
      "[INFO] Epoch: 68 , batch: 384 , training loss: 4.620050\n",
      "[INFO] Epoch: 68 , batch: 385 , training loss: 4.325286\n",
      "[INFO] Epoch: 68 , batch: 386 , training loss: 4.588716\n",
      "[INFO] Epoch: 68 , batch: 387 , training loss: 4.507988\n",
      "[INFO] Epoch: 68 , batch: 388 , training loss: 4.343292\n",
      "[INFO] Epoch: 68 , batch: 389 , training loss: 4.160437\n",
      "[INFO] Epoch: 68 , batch: 390 , training loss: 4.201446\n",
      "[INFO] Epoch: 68 , batch: 391 , training loss: 4.223288\n",
      "[INFO] Epoch: 68 , batch: 392 , training loss: 4.588356\n",
      "[INFO] Epoch: 68 , batch: 393 , training loss: 4.473838\n",
      "[INFO] Epoch: 68 , batch: 394 , training loss: 4.581935\n",
      "[INFO] Epoch: 68 , batch: 395 , training loss: 4.376362\n",
      "[INFO] Epoch: 68 , batch: 396 , training loss: 4.210272\n",
      "[INFO] Epoch: 68 , batch: 397 , training loss: 4.349892\n",
      "[INFO] Epoch: 68 , batch: 398 , training loss: 4.220722\n",
      "[INFO] Epoch: 68 , batch: 399 , training loss: 4.278738\n",
      "[INFO] Epoch: 68 , batch: 400 , training loss: 4.264518\n",
      "[INFO] Epoch: 68 , batch: 401 , training loss: 4.681623\n",
      "[INFO] Epoch: 68 , batch: 402 , training loss: 4.416084\n",
      "[INFO] Epoch: 68 , batch: 403 , training loss: 4.259118\n",
      "[INFO] Epoch: 68 , batch: 404 , training loss: 4.417522\n",
      "[INFO] Epoch: 68 , batch: 405 , training loss: 4.470356\n",
      "[INFO] Epoch: 68 , batch: 406 , training loss: 4.379080\n",
      "[INFO] Epoch: 68 , batch: 407 , training loss: 4.401435\n",
      "[INFO] Epoch: 68 , batch: 408 , training loss: 4.386080\n",
      "[INFO] Epoch: 68 , batch: 409 , training loss: 4.416640\n",
      "[INFO] Epoch: 68 , batch: 410 , training loss: 4.447025\n",
      "[INFO] Epoch: 68 , batch: 411 , training loss: 4.628727\n",
      "[INFO] Epoch: 68 , batch: 412 , training loss: 4.449249\n",
      "[INFO] Epoch: 68 , batch: 413 , training loss: 4.300642\n",
      "[INFO] Epoch: 68 , batch: 414 , training loss: 4.374713\n",
      "[INFO] Epoch: 68 , batch: 415 , training loss: 4.402614\n",
      "[INFO] Epoch: 68 , batch: 416 , training loss: 4.466721\n",
      "[INFO] Epoch: 68 , batch: 417 , training loss: 4.389617\n",
      "[INFO] Epoch: 68 , batch: 418 , training loss: 4.448435\n",
      "[INFO] Epoch: 68 , batch: 419 , training loss: 4.394418\n",
      "[INFO] Epoch: 68 , batch: 420 , training loss: 4.370950\n",
      "[INFO] Epoch: 68 , batch: 421 , training loss: 4.370664\n",
      "[INFO] Epoch: 68 , batch: 422 , training loss: 4.205384\n",
      "[INFO] Epoch: 68 , batch: 423 , training loss: 4.429034\n",
      "[INFO] Epoch: 68 , batch: 424 , training loss: 4.593405\n",
      "[INFO] Epoch: 68 , batch: 425 , training loss: 4.476549\n",
      "[INFO] Epoch: 68 , batch: 426 , training loss: 4.224406\n",
      "[INFO] Epoch: 68 , batch: 427 , training loss: 4.456304\n",
      "[INFO] Epoch: 68 , batch: 428 , training loss: 4.319506\n",
      "[INFO] Epoch: 68 , batch: 429 , training loss: 4.200869\n",
      "[INFO] Epoch: 68 , batch: 430 , training loss: 4.435281\n",
      "[INFO] Epoch: 68 , batch: 431 , training loss: 4.060547\n",
      "[INFO] Epoch: 68 , batch: 432 , training loss: 4.110498\n",
      "[INFO] Epoch: 68 , batch: 433 , training loss: 4.155022\n",
      "[INFO] Epoch: 68 , batch: 434 , training loss: 4.032831\n",
      "[INFO] Epoch: 68 , batch: 435 , training loss: 4.382487\n",
      "[INFO] Epoch: 68 , batch: 436 , training loss: 4.431645\n",
      "[INFO] Epoch: 68 , batch: 437 , training loss: 4.211605\n",
      "[INFO] Epoch: 68 , batch: 438 , training loss: 4.088702\n",
      "[INFO] Epoch: 68 , batch: 439 , training loss: 4.312378\n",
      "[INFO] Epoch: 68 , batch: 440 , training loss: 4.415233\n",
      "[INFO] Epoch: 68 , batch: 441 , training loss: 4.519865\n",
      "[INFO] Epoch: 68 , batch: 442 , training loss: 4.278625\n",
      "[INFO] Epoch: 68 , batch: 443 , training loss: 4.467014\n",
      "[INFO] Epoch: 68 , batch: 444 , training loss: 4.083376\n",
      "[INFO] Epoch: 68 , batch: 445 , training loss: 3.963475\n",
      "[INFO] Epoch: 68 , batch: 446 , training loss: 3.906220\n",
      "[INFO] Epoch: 68 , batch: 447 , training loss: 4.111270\n",
      "[INFO] Epoch: 68 , batch: 448 , training loss: 4.231030\n",
      "[INFO] Epoch: 68 , batch: 449 , training loss: 4.606022\n",
      "[INFO] Epoch: 68 , batch: 450 , training loss: 4.697231\n",
      "[INFO] Epoch: 68 , batch: 451 , training loss: 4.572382\n",
      "[INFO] Epoch: 68 , batch: 452 , training loss: 4.389227\n",
      "[INFO] Epoch: 68 , batch: 453 , training loss: 4.149383\n",
      "[INFO] Epoch: 68 , batch: 454 , training loss: 4.300757\n",
      "[INFO] Epoch: 68 , batch: 455 , training loss: 4.357639\n",
      "[INFO] Epoch: 68 , batch: 456 , training loss: 4.349155\n",
      "[INFO] Epoch: 68 , batch: 457 , training loss: 4.428912\n",
      "[INFO] Epoch: 68 , batch: 458 , training loss: 4.163170\n",
      "[INFO] Epoch: 68 , batch: 459 , training loss: 4.138147\n",
      "[INFO] Epoch: 68 , batch: 460 , training loss: 4.260648\n",
      "[INFO] Epoch: 68 , batch: 461 , training loss: 4.217939\n",
      "[INFO] Epoch: 68 , batch: 462 , training loss: 4.289587\n",
      "[INFO] Epoch: 68 , batch: 463 , training loss: 4.210495\n",
      "[INFO] Epoch: 68 , batch: 464 , training loss: 4.368866\n",
      "[INFO] Epoch: 68 , batch: 465 , training loss: 4.329507\n",
      "[INFO] Epoch: 68 , batch: 466 , training loss: 4.416740\n",
      "[INFO] Epoch: 68 , batch: 467 , training loss: 4.377907\n",
      "[INFO] Epoch: 68 , batch: 468 , training loss: 4.329976\n",
      "[INFO] Epoch: 68 , batch: 469 , training loss: 4.363414\n",
      "[INFO] Epoch: 68 , batch: 470 , training loss: 4.181587\n",
      "[INFO] Epoch: 68 , batch: 471 , training loss: 4.302342\n",
      "[INFO] Epoch: 68 , batch: 472 , training loss: 4.330434\n",
      "[INFO] Epoch: 68 , batch: 473 , training loss: 4.266318\n",
      "[INFO] Epoch: 68 , batch: 474 , training loss: 4.050098\n",
      "[INFO] Epoch: 68 , batch: 475 , training loss: 3.935590\n",
      "[INFO] Epoch: 68 , batch: 476 , training loss: 4.304918\n",
      "[INFO] Epoch: 68 , batch: 477 , training loss: 4.447644\n",
      "[INFO] Epoch: 68 , batch: 478 , training loss: 4.433530\n",
      "[INFO] Epoch: 68 , batch: 479 , training loss: 4.431057\n",
      "[INFO] Epoch: 68 , batch: 480 , training loss: 4.541637\n",
      "[INFO] Epoch: 68 , batch: 481 , training loss: 4.425681\n",
      "[INFO] Epoch: 68 , batch: 482 , training loss: 4.525315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 68 , batch: 483 , training loss: 4.373194\n",
      "[INFO] Epoch: 68 , batch: 484 , training loss: 4.164587\n",
      "[INFO] Epoch: 68 , batch: 485 , training loss: 4.277777\n",
      "[INFO] Epoch: 68 , batch: 486 , training loss: 4.176401\n",
      "[INFO] Epoch: 68 , batch: 487 , training loss: 4.159907\n",
      "[INFO] Epoch: 68 , batch: 488 , training loss: 4.335625\n",
      "[INFO] Epoch: 68 , batch: 489 , training loss: 4.251782\n",
      "[INFO] Epoch: 68 , batch: 490 , training loss: 4.310020\n",
      "[INFO] Epoch: 68 , batch: 491 , training loss: 4.219444\n",
      "[INFO] Epoch: 68 , batch: 492 , training loss: 4.202963\n",
      "[INFO] Epoch: 68 , batch: 493 , training loss: 4.367462\n",
      "[INFO] Epoch: 68 , batch: 494 , training loss: 4.266728\n",
      "[INFO] Epoch: 68 , batch: 495 , training loss: 4.449666\n",
      "[INFO] Epoch: 68 , batch: 496 , training loss: 4.296016\n",
      "[INFO] Epoch: 68 , batch: 497 , training loss: 4.341583\n",
      "[INFO] Epoch: 68 , batch: 498 , training loss: 4.318723\n",
      "[INFO] Epoch: 68 , batch: 499 , training loss: 4.388242\n",
      "[INFO] Epoch: 68 , batch: 500 , training loss: 4.535351\n",
      "[INFO] Epoch: 68 , batch: 501 , training loss: 4.841448\n",
      "[INFO] Epoch: 68 , batch: 502 , training loss: 4.881610\n",
      "[INFO] Epoch: 68 , batch: 503 , training loss: 4.486959\n",
      "[INFO] Epoch: 68 , batch: 504 , training loss: 4.632421\n",
      "[INFO] Epoch: 68 , batch: 505 , training loss: 4.592995\n",
      "[INFO] Epoch: 68 , batch: 506 , training loss: 4.572503\n",
      "[INFO] Epoch: 68 , batch: 507 , training loss: 4.618535\n",
      "[INFO] Epoch: 68 , batch: 508 , training loss: 4.538040\n",
      "[INFO] Epoch: 68 , batch: 509 , training loss: 4.391041\n",
      "[INFO] Epoch: 68 , batch: 510 , training loss: 4.489507\n",
      "[INFO] Epoch: 68 , batch: 511 , training loss: 4.376038\n",
      "[INFO] Epoch: 68 , batch: 512 , training loss: 4.471664\n",
      "[INFO] Epoch: 68 , batch: 513 , training loss: 4.737151\n",
      "[INFO] Epoch: 68 , batch: 514 , training loss: 4.366969\n",
      "[INFO] Epoch: 68 , batch: 515 , training loss: 4.639746\n",
      "[INFO] Epoch: 68 , batch: 516 , training loss: 4.434652\n",
      "[INFO] Epoch: 68 , batch: 517 , training loss: 4.400194\n",
      "[INFO] Epoch: 68 , batch: 518 , training loss: 4.362453\n",
      "[INFO] Epoch: 68 , batch: 519 , training loss: 4.201876\n",
      "[INFO] Epoch: 68 , batch: 520 , training loss: 4.450143\n",
      "[INFO] Epoch: 68 , batch: 521 , training loss: 4.411738\n",
      "[INFO] Epoch: 68 , batch: 522 , training loss: 4.512311\n",
      "[INFO] Epoch: 68 , batch: 523 , training loss: 4.423582\n",
      "[INFO] Epoch: 68 , batch: 524 , training loss: 4.726772\n",
      "[INFO] Epoch: 68 , batch: 525 , training loss: 4.612964\n",
      "[INFO] Epoch: 68 , batch: 526 , training loss: 4.370385\n",
      "[INFO] Epoch: 68 , batch: 527 , training loss: 4.415676\n",
      "[INFO] Epoch: 68 , batch: 528 , training loss: 4.432702\n",
      "[INFO] Epoch: 68 , batch: 529 , training loss: 4.416964\n",
      "[INFO] Epoch: 68 , batch: 530 , training loss: 4.259501\n",
      "[INFO] Epoch: 68 , batch: 531 , training loss: 4.401894\n",
      "[INFO] Epoch: 68 , batch: 532 , training loss: 4.312221\n",
      "[INFO] Epoch: 68 , batch: 533 , training loss: 4.428402\n",
      "[INFO] Epoch: 68 , batch: 534 , training loss: 4.435241\n",
      "[INFO] Epoch: 68 , batch: 535 , training loss: 4.438524\n",
      "[INFO] Epoch: 68 , batch: 536 , training loss: 4.310608\n",
      "[INFO] Epoch: 68 , batch: 537 , training loss: 4.291466\n",
      "[INFO] Epoch: 68 , batch: 538 , training loss: 4.373738\n",
      "[INFO] Epoch: 68 , batch: 539 , training loss: 4.463510\n",
      "[INFO] Epoch: 68 , batch: 540 , training loss: 4.991760\n",
      "[INFO] Epoch: 68 , batch: 541 , training loss: 4.831398\n",
      "[INFO] Epoch: 68 , batch: 542 , training loss: 4.709495\n",
      "[INFO] Epoch: 69 , batch: 0 , training loss: 3.591544\n",
      "[INFO] Epoch: 69 , batch: 1 , training loss: 3.515444\n",
      "[INFO] Epoch: 69 , batch: 2 , training loss: 3.726496\n",
      "[INFO] Epoch: 69 , batch: 3 , training loss: 3.571635\n",
      "[INFO] Epoch: 69 , batch: 4 , training loss: 3.986853\n",
      "[INFO] Epoch: 69 , batch: 5 , training loss: 3.580298\n",
      "[INFO] Epoch: 69 , batch: 6 , training loss: 3.982850\n",
      "[INFO] Epoch: 69 , batch: 7 , training loss: 3.955901\n",
      "[INFO] Epoch: 69 , batch: 8 , training loss: 3.606164\n",
      "[INFO] Epoch: 69 , batch: 9 , training loss: 3.885088\n",
      "[INFO] Epoch: 69 , batch: 10 , training loss: 3.874177\n",
      "[INFO] Epoch: 69 , batch: 11 , training loss: 3.781974\n",
      "[INFO] Epoch: 69 , batch: 12 , training loss: 3.701468\n",
      "[INFO] Epoch: 69 , batch: 13 , training loss: 3.666512\n",
      "[INFO] Epoch: 69 , batch: 14 , training loss: 3.628548\n",
      "[INFO] Epoch: 69 , batch: 15 , training loss: 3.861406\n",
      "[INFO] Epoch: 69 , batch: 16 , training loss: 3.640226\n",
      "[INFO] Epoch: 69 , batch: 17 , training loss: 3.747496\n",
      "[INFO] Epoch: 69 , batch: 18 , training loss: 3.715508\n",
      "[INFO] Epoch: 69 , batch: 19 , training loss: 3.506850\n",
      "[INFO] Epoch: 69 , batch: 20 , training loss: 3.490164\n",
      "[INFO] Epoch: 69 , batch: 21 , training loss: 3.588878\n",
      "[INFO] Epoch: 69 , batch: 22 , training loss: 3.466433\n",
      "[INFO] Epoch: 69 , batch: 23 , training loss: 3.707483\n",
      "[INFO] Epoch: 69 , batch: 24 , training loss: 3.552494\n",
      "[INFO] Epoch: 69 , batch: 25 , training loss: 3.668167\n",
      "[INFO] Epoch: 69 , batch: 26 , training loss: 3.484982\n",
      "[INFO] Epoch: 69 , batch: 27 , training loss: 3.513659\n",
      "[INFO] Epoch: 69 , batch: 28 , training loss: 3.665611\n",
      "[INFO] Epoch: 69 , batch: 29 , training loss: 3.496700\n",
      "[INFO] Epoch: 69 , batch: 30 , training loss: 3.535954\n",
      "[INFO] Epoch: 69 , batch: 31 , training loss: 3.592818\n",
      "[INFO] Epoch: 69 , batch: 32 , training loss: 3.567241\n",
      "[INFO] Epoch: 69 , batch: 33 , training loss: 3.609613\n",
      "[INFO] Epoch: 69 , batch: 34 , training loss: 3.607928\n",
      "[INFO] Epoch: 69 , batch: 35 , training loss: 3.569403\n",
      "[INFO] Epoch: 69 , batch: 36 , training loss: 3.650220\n",
      "[INFO] Epoch: 69 , batch: 37 , training loss: 3.503684\n",
      "[INFO] Epoch: 69 , batch: 38 , training loss: 3.584704\n",
      "[INFO] Epoch: 69 , batch: 39 , training loss: 3.395990\n",
      "[INFO] Epoch: 69 , batch: 40 , training loss: 3.611104\n",
      "[INFO] Epoch: 69 , batch: 41 , training loss: 3.584053\n",
      "[INFO] Epoch: 69 , batch: 42 , training loss: 4.050380\n",
      "[INFO] Epoch: 69 , batch: 43 , training loss: 3.798566\n",
      "[INFO] Epoch: 69 , batch: 44 , training loss: 4.129601\n",
      "[INFO] Epoch: 69 , batch: 45 , training loss: 4.113995\n",
      "[INFO] Epoch: 69 , batch: 46 , training loss: 4.044692\n",
      "[INFO] Epoch: 69 , batch: 47 , training loss: 3.649003\n",
      "[INFO] Epoch: 69 , batch: 48 , training loss: 3.666764\n",
      "[INFO] Epoch: 69 , batch: 49 , training loss: 3.847799\n",
      "[INFO] Epoch: 69 , batch: 50 , training loss: 3.612286\n",
      "[INFO] Epoch: 69 , batch: 51 , training loss: 3.838496\n",
      "[INFO] Epoch: 69 , batch: 52 , training loss: 3.645946\n",
      "[INFO] Epoch: 69 , batch: 53 , training loss: 3.770103\n",
      "[INFO] Epoch: 69 , batch: 54 , training loss: 3.793501\n",
      "[INFO] Epoch: 69 , batch: 55 , training loss: 3.801920\n",
      "[INFO] Epoch: 69 , batch: 56 , training loss: 3.676200\n",
      "[INFO] Epoch: 69 , batch: 57 , training loss: 3.593851\n",
      "[INFO] Epoch: 69 , batch: 58 , training loss: 3.619066\n",
      "[INFO] Epoch: 69 , batch: 59 , training loss: 3.694968\n",
      "[INFO] Epoch: 69 , batch: 60 , training loss: 3.639027\n",
      "[INFO] Epoch: 69 , batch: 61 , training loss: 3.749450\n",
      "[INFO] Epoch: 69 , batch: 62 , training loss: 3.607026\n",
      "[INFO] Epoch: 69 , batch: 63 , training loss: 3.818914\n",
      "[INFO] Epoch: 69 , batch: 64 , training loss: 3.988112\n",
      "[INFO] Epoch: 69 , batch: 65 , training loss: 3.716791\n",
      "[INFO] Epoch: 69 , batch: 66 , training loss: 3.577213\n",
      "[INFO] Epoch: 69 , batch: 67 , training loss: 3.611380\n",
      "[INFO] Epoch: 69 , batch: 68 , training loss: 3.750071\n",
      "[INFO] Epoch: 69 , batch: 69 , training loss: 3.695014\n",
      "[INFO] Epoch: 69 , batch: 70 , training loss: 3.899803\n",
      "[INFO] Epoch: 69 , batch: 71 , training loss: 3.758548\n",
      "[INFO] Epoch: 69 , batch: 72 , training loss: 3.806218\n",
      "[INFO] Epoch: 69 , batch: 73 , training loss: 3.760514\n",
      "[INFO] Epoch: 69 , batch: 74 , training loss: 3.836056\n",
      "[INFO] Epoch: 69 , batch: 75 , training loss: 3.729916\n",
      "[INFO] Epoch: 69 , batch: 76 , training loss: 3.862752\n",
      "[INFO] Epoch: 69 , batch: 77 , training loss: 3.786044\n",
      "[INFO] Epoch: 69 , batch: 78 , training loss: 3.887060\n",
      "[INFO] Epoch: 69 , batch: 79 , training loss: 3.725985\n",
      "[INFO] Epoch: 69 , batch: 80 , training loss: 3.937217\n",
      "[INFO] Epoch: 69 , batch: 81 , training loss: 3.834637\n",
      "[INFO] Epoch: 69 , batch: 82 , training loss: 3.829247\n",
      "[INFO] Epoch: 69 , batch: 83 , training loss: 3.914199\n",
      "[INFO] Epoch: 69 , batch: 84 , training loss: 3.893947\n",
      "[INFO] Epoch: 69 , batch: 85 , training loss: 3.949234\n",
      "[INFO] Epoch: 69 , batch: 86 , training loss: 3.906474\n",
      "[INFO] Epoch: 69 , batch: 87 , training loss: 3.851767\n",
      "[INFO] Epoch: 69 , batch: 88 , training loss: 3.999327\n",
      "[INFO] Epoch: 69 , batch: 89 , training loss: 3.753126\n",
      "[INFO] Epoch: 69 , batch: 90 , training loss: 3.873220\n",
      "[INFO] Epoch: 69 , batch: 91 , training loss: 3.825557\n",
      "[INFO] Epoch: 69 , batch: 92 , training loss: 3.861576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 69 , batch: 93 , training loss: 3.935374\n",
      "[INFO] Epoch: 69 , batch: 94 , training loss: 4.034331\n",
      "[INFO] Epoch: 69 , batch: 95 , training loss: 3.845607\n",
      "[INFO] Epoch: 69 , batch: 96 , training loss: 3.822221\n",
      "[INFO] Epoch: 69 , batch: 97 , training loss: 3.756308\n",
      "[INFO] Epoch: 69 , batch: 98 , training loss: 3.705441\n",
      "[INFO] Epoch: 69 , batch: 99 , training loss: 3.837782\n",
      "[INFO] Epoch: 69 , batch: 100 , training loss: 3.738886\n",
      "[INFO] Epoch: 69 , batch: 101 , training loss: 3.738694\n",
      "[INFO] Epoch: 69 , batch: 102 , training loss: 3.912515\n",
      "[INFO] Epoch: 69 , batch: 103 , training loss: 3.694040\n",
      "[INFO] Epoch: 69 , batch: 104 , training loss: 3.656705\n",
      "[INFO] Epoch: 69 , batch: 105 , training loss: 3.925936\n",
      "[INFO] Epoch: 69 , batch: 106 , training loss: 3.917396\n",
      "[INFO] Epoch: 69 , batch: 107 , training loss: 3.779366\n",
      "[INFO] Epoch: 69 , batch: 108 , training loss: 3.692551\n",
      "[INFO] Epoch: 69 , batch: 109 , training loss: 3.632038\n",
      "[INFO] Epoch: 69 , batch: 110 , training loss: 3.822938\n",
      "[INFO] Epoch: 69 , batch: 111 , training loss: 3.909489\n",
      "[INFO] Epoch: 69 , batch: 112 , training loss: 3.824727\n",
      "[INFO] Epoch: 69 , batch: 113 , training loss: 3.786500\n",
      "[INFO] Epoch: 69 , batch: 114 , training loss: 3.781472\n",
      "[INFO] Epoch: 69 , batch: 115 , training loss: 3.772907\n",
      "[INFO] Epoch: 69 , batch: 116 , training loss: 3.709875\n",
      "[INFO] Epoch: 69 , batch: 117 , training loss: 3.926669\n",
      "[INFO] Epoch: 69 , batch: 118 , training loss: 3.909698\n",
      "[INFO] Epoch: 69 , batch: 119 , training loss: 4.018012\n",
      "[INFO] Epoch: 69 , batch: 120 , training loss: 4.012247\n",
      "[INFO] Epoch: 69 , batch: 121 , training loss: 3.884047\n",
      "[INFO] Epoch: 69 , batch: 122 , training loss: 3.790749\n",
      "[INFO] Epoch: 69 , batch: 123 , training loss: 3.778400\n",
      "[INFO] Epoch: 69 , batch: 124 , training loss: 3.898986\n",
      "[INFO] Epoch: 69 , batch: 125 , training loss: 3.738302\n",
      "[INFO] Epoch: 69 , batch: 126 , training loss: 3.693049\n",
      "[INFO] Epoch: 69 , batch: 127 , training loss: 3.727602\n",
      "[INFO] Epoch: 69 , batch: 128 , training loss: 3.860113\n",
      "[INFO] Epoch: 69 , batch: 129 , training loss: 3.800487\n",
      "[INFO] Epoch: 69 , batch: 130 , training loss: 3.842347\n",
      "[INFO] Epoch: 69 , batch: 131 , training loss: 3.823030\n",
      "[INFO] Epoch: 69 , batch: 132 , training loss: 3.838458\n",
      "[INFO] Epoch: 69 , batch: 133 , training loss: 3.774298\n",
      "[INFO] Epoch: 69 , batch: 134 , training loss: 3.581978\n",
      "[INFO] Epoch: 69 , batch: 135 , training loss: 3.641886\n",
      "[INFO] Epoch: 69 , batch: 136 , training loss: 3.926235\n",
      "[INFO] Epoch: 69 , batch: 137 , training loss: 3.871324\n",
      "[INFO] Epoch: 69 , batch: 138 , training loss: 3.896852\n",
      "[INFO] Epoch: 69 , batch: 139 , training loss: 4.426391\n",
      "[INFO] Epoch: 69 , batch: 140 , training loss: 4.224922\n",
      "[INFO] Epoch: 69 , batch: 141 , training loss: 4.011714\n",
      "[INFO] Epoch: 69 , batch: 142 , training loss: 3.743701\n",
      "[INFO] Epoch: 69 , batch: 143 , training loss: 3.875118\n",
      "[INFO] Epoch: 69 , batch: 144 , training loss: 3.737085\n",
      "[INFO] Epoch: 69 , batch: 145 , training loss: 3.761419\n",
      "[INFO] Epoch: 69 , batch: 146 , training loss: 3.995670\n",
      "[INFO] Epoch: 69 , batch: 147 , training loss: 3.633271\n",
      "[INFO] Epoch: 69 , batch: 148 , training loss: 3.628044\n",
      "[INFO] Epoch: 69 , batch: 149 , training loss: 3.707753\n",
      "[INFO] Epoch: 69 , batch: 150 , training loss: 3.946328\n",
      "[INFO] Epoch: 69 , batch: 151 , training loss: 3.799600\n",
      "[INFO] Epoch: 69 , batch: 152 , training loss: 3.807687\n",
      "[INFO] Epoch: 69 , batch: 153 , training loss: 3.862879\n",
      "[INFO] Epoch: 69 , batch: 154 , training loss: 3.955053\n",
      "[INFO] Epoch: 69 , batch: 155 , training loss: 4.108407\n",
      "[INFO] Epoch: 69 , batch: 156 , training loss: 3.902774\n",
      "[INFO] Epoch: 69 , batch: 157 , training loss: 3.827722\n",
      "[INFO] Epoch: 69 , batch: 158 , training loss: 3.962828\n",
      "[INFO] Epoch: 69 , batch: 159 , training loss: 3.877510\n",
      "[INFO] Epoch: 69 , batch: 160 , training loss: 4.034241\n",
      "[INFO] Epoch: 69 , batch: 161 , training loss: 4.139878\n",
      "[INFO] Epoch: 69 , batch: 162 , training loss: 4.126760\n",
      "[INFO] Epoch: 69 , batch: 163 , training loss: 4.325345\n",
      "[INFO] Epoch: 69 , batch: 164 , training loss: 4.255316\n",
      "[INFO] Epoch: 69 , batch: 165 , training loss: 4.201853\n",
      "[INFO] Epoch: 69 , batch: 166 , training loss: 4.076398\n",
      "[INFO] Epoch: 69 , batch: 167 , training loss: 4.071512\n",
      "[INFO] Epoch: 69 , batch: 168 , training loss: 3.814118\n",
      "[INFO] Epoch: 69 , batch: 169 , training loss: 3.762317\n",
      "[INFO] Epoch: 69 , batch: 170 , training loss: 3.931774\n",
      "[INFO] Epoch: 69 , batch: 171 , training loss: 3.463410\n",
      "[INFO] Epoch: 69 , batch: 172 , training loss: 3.678338\n",
      "[INFO] Epoch: 69 , batch: 173 , training loss: 3.978189\n",
      "[INFO] Epoch: 69 , batch: 174 , training loss: 4.462751\n",
      "[INFO] Epoch: 69 , batch: 175 , training loss: 4.688234\n",
      "[INFO] Epoch: 69 , batch: 176 , training loss: 4.314648\n",
      "[INFO] Epoch: 69 , batch: 177 , training loss: 3.949153\n",
      "[INFO] Epoch: 69 , batch: 178 , training loss: 3.973513\n",
      "[INFO] Epoch: 69 , batch: 179 , training loss: 4.011017\n",
      "[INFO] Epoch: 69 , batch: 180 , training loss: 3.965632\n",
      "[INFO] Epoch: 69 , batch: 181 , training loss: 4.273472\n",
      "[INFO] Epoch: 69 , batch: 182 , training loss: 4.206064\n",
      "[INFO] Epoch: 69 , batch: 183 , training loss: 4.209333\n",
      "[INFO] Epoch: 69 , batch: 184 , training loss: 4.073602\n",
      "[INFO] Epoch: 69 , batch: 185 , training loss: 4.044234\n",
      "[INFO] Epoch: 69 , batch: 186 , training loss: 4.184656\n",
      "[INFO] Epoch: 69 , batch: 187 , training loss: 4.297060\n",
      "[INFO] Epoch: 69 , batch: 188 , training loss: 4.284101\n",
      "[INFO] Epoch: 69 , batch: 189 , training loss: 4.194414\n",
      "[INFO] Epoch: 69 , batch: 190 , training loss: 4.250519\n",
      "[INFO] Epoch: 69 , batch: 191 , training loss: 4.352235\n",
      "[INFO] Epoch: 69 , batch: 192 , training loss: 4.173237\n",
      "[INFO] Epoch: 69 , batch: 193 , training loss: 4.279571\n",
      "[INFO] Epoch: 69 , batch: 194 , training loss: 4.239597\n",
      "[INFO] Epoch: 69 , batch: 195 , training loss: 4.165567\n",
      "[INFO] Epoch: 69 , batch: 196 , training loss: 4.029146\n",
      "[INFO] Epoch: 69 , batch: 197 , training loss: 4.076183\n",
      "[INFO] Epoch: 69 , batch: 198 , training loss: 4.025975\n",
      "[INFO] Epoch: 69 , batch: 199 , training loss: 4.153388\n",
      "[INFO] Epoch: 69 , batch: 200 , training loss: 4.044083\n",
      "[INFO] Epoch: 69 , batch: 201 , training loss: 3.947717\n",
      "[INFO] Epoch: 69 , batch: 202 , training loss: 3.955641\n",
      "[INFO] Epoch: 69 , batch: 203 , training loss: 4.112701\n",
      "[INFO] Epoch: 69 , batch: 204 , training loss: 4.157997\n",
      "[INFO] Epoch: 69 , batch: 205 , training loss: 3.784189\n",
      "[INFO] Epoch: 69 , batch: 206 , training loss: 3.740461\n",
      "[INFO] Epoch: 69 , batch: 207 , training loss: 3.703458\n",
      "[INFO] Epoch: 69 , batch: 208 , training loss: 4.036111\n",
      "[INFO] Epoch: 69 , batch: 209 , training loss: 4.020429\n",
      "[INFO] Epoch: 69 , batch: 210 , training loss: 4.013807\n",
      "[INFO] Epoch: 69 , batch: 211 , training loss: 4.008181\n",
      "[INFO] Epoch: 69 , batch: 212 , training loss: 4.121863\n",
      "[INFO] Epoch: 69 , batch: 213 , training loss: 4.056621\n",
      "[INFO] Epoch: 69 , batch: 214 , training loss: 4.151160\n",
      "[INFO] Epoch: 69 , batch: 215 , training loss: 4.325270\n",
      "[INFO] Epoch: 69 , batch: 216 , training loss: 4.038026\n",
      "[INFO] Epoch: 69 , batch: 217 , training loss: 4.005087\n",
      "[INFO] Epoch: 69 , batch: 218 , training loss: 3.969537\n",
      "[INFO] Epoch: 69 , batch: 219 , training loss: 4.084082\n",
      "[INFO] Epoch: 69 , batch: 220 , training loss: 3.935587\n",
      "[INFO] Epoch: 69 , batch: 221 , training loss: 3.929434\n",
      "[INFO] Epoch: 69 , batch: 222 , training loss: 4.068801\n",
      "[INFO] Epoch: 69 , batch: 223 , training loss: 4.198552\n",
      "[INFO] Epoch: 69 , batch: 224 , training loss: 4.241292\n",
      "[INFO] Epoch: 69 , batch: 225 , training loss: 4.118398\n",
      "[INFO] Epoch: 69 , batch: 226 , training loss: 4.238327\n",
      "[INFO] Epoch: 69 , batch: 227 , training loss: 4.209818\n",
      "[INFO] Epoch: 69 , batch: 228 , training loss: 4.230014\n",
      "[INFO] Epoch: 69 , batch: 229 , training loss: 4.088349\n",
      "[INFO] Epoch: 69 , batch: 230 , training loss: 3.947440\n",
      "[INFO] Epoch: 69 , batch: 231 , training loss: 3.833397\n",
      "[INFO] Epoch: 69 , batch: 232 , training loss: 3.969650\n",
      "[INFO] Epoch: 69 , batch: 233 , training loss: 3.996254\n",
      "[INFO] Epoch: 69 , batch: 234 , training loss: 3.683017\n",
      "[INFO] Epoch: 69 , batch: 235 , training loss: 3.781087\n",
      "[INFO] Epoch: 69 , batch: 236 , training loss: 3.876046\n",
      "[INFO] Epoch: 69 , batch: 237 , training loss: 4.106316\n",
      "[INFO] Epoch: 69 , batch: 238 , training loss: 3.912893\n",
      "[INFO] Epoch: 69 , batch: 239 , training loss: 3.921430\n",
      "[INFO] Epoch: 69 , batch: 240 , training loss: 3.981340\n",
      "[INFO] Epoch: 69 , batch: 241 , training loss: 3.777969\n",
      "[INFO] Epoch: 69 , batch: 242 , training loss: 3.819741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 69 , batch: 243 , training loss: 4.087204\n",
      "[INFO] Epoch: 69 , batch: 244 , training loss: 4.023658\n",
      "[INFO] Epoch: 69 , batch: 245 , training loss: 3.985897\n",
      "[INFO] Epoch: 69 , batch: 246 , training loss: 3.707755\n",
      "[INFO] Epoch: 69 , batch: 247 , training loss: 3.897299\n",
      "[INFO] Epoch: 69 , batch: 248 , training loss: 3.948486\n",
      "[INFO] Epoch: 69 , batch: 249 , training loss: 3.934453\n",
      "[INFO] Epoch: 69 , batch: 250 , training loss: 3.747179\n",
      "[INFO] Epoch: 69 , batch: 251 , training loss: 4.192703\n",
      "[INFO] Epoch: 69 , batch: 252 , training loss: 3.911950\n",
      "[INFO] Epoch: 69 , batch: 253 , training loss: 3.811225\n",
      "[INFO] Epoch: 69 , batch: 254 , training loss: 4.073194\n",
      "[INFO] Epoch: 69 , batch: 255 , training loss: 4.057159\n",
      "[INFO] Epoch: 69 , batch: 256 , training loss: 4.020039\n",
      "[INFO] Epoch: 69 , batch: 257 , training loss: 4.206551\n",
      "[INFO] Epoch: 69 , batch: 258 , training loss: 4.186555\n",
      "[INFO] Epoch: 69 , batch: 259 , training loss: 4.258572\n",
      "[INFO] Epoch: 69 , batch: 260 , training loss: 4.011458\n",
      "[INFO] Epoch: 69 , batch: 261 , training loss: 4.205896\n",
      "[INFO] Epoch: 69 , batch: 262 , training loss: 4.311958\n",
      "[INFO] Epoch: 69 , batch: 263 , training loss: 4.498239\n",
      "[INFO] Epoch: 69 , batch: 264 , training loss: 3.847707\n",
      "[INFO] Epoch: 69 , batch: 265 , training loss: 3.958422\n",
      "[INFO] Epoch: 69 , batch: 266 , training loss: 4.345190\n",
      "[INFO] Epoch: 69 , batch: 267 , training loss: 4.132637\n",
      "[INFO] Epoch: 69 , batch: 268 , training loss: 4.050451\n",
      "[INFO] Epoch: 69 , batch: 269 , training loss: 4.001703\n",
      "[INFO] Epoch: 69 , batch: 270 , training loss: 4.049824\n",
      "[INFO] Epoch: 69 , batch: 271 , training loss: 4.063087\n",
      "[INFO] Epoch: 69 , batch: 272 , training loss: 4.075384\n",
      "[INFO] Epoch: 69 , batch: 273 , training loss: 4.076895\n",
      "[INFO] Epoch: 69 , batch: 274 , training loss: 4.187372\n",
      "[INFO] Epoch: 69 , batch: 275 , training loss: 4.026855\n",
      "[INFO] Epoch: 69 , batch: 276 , training loss: 4.094585\n",
      "[INFO] Epoch: 69 , batch: 277 , training loss: 4.233080\n",
      "[INFO] Epoch: 69 , batch: 278 , training loss: 3.939012\n",
      "[INFO] Epoch: 69 , batch: 279 , training loss: 3.965370\n",
      "[INFO] Epoch: 69 , batch: 280 , training loss: 3.948484\n",
      "[INFO] Epoch: 69 , batch: 281 , training loss: 4.054812\n",
      "[INFO] Epoch: 69 , batch: 282 , training loss: 4.001026\n",
      "[INFO] Epoch: 69 , batch: 283 , training loss: 3.985989\n",
      "[INFO] Epoch: 69 , batch: 284 , training loss: 4.015975\n",
      "[INFO] Epoch: 69 , batch: 285 , training loss: 3.951403\n",
      "[INFO] Epoch: 69 , batch: 286 , training loss: 3.961302\n",
      "[INFO] Epoch: 69 , batch: 287 , training loss: 3.889806\n",
      "[INFO] Epoch: 69 , batch: 288 , training loss: 3.869071\n",
      "[INFO] Epoch: 69 , batch: 289 , training loss: 3.932644\n",
      "[INFO] Epoch: 69 , batch: 290 , training loss: 3.716683\n",
      "[INFO] Epoch: 69 , batch: 291 , training loss: 3.703820\n",
      "[INFO] Epoch: 69 , batch: 292 , training loss: 3.819658\n",
      "[INFO] Epoch: 69 , batch: 293 , training loss: 3.747686\n",
      "[INFO] Epoch: 69 , batch: 294 , training loss: 4.385268\n",
      "[INFO] Epoch: 69 , batch: 295 , training loss: 4.155241\n",
      "[INFO] Epoch: 69 , batch: 296 , training loss: 4.102736\n",
      "[INFO] Epoch: 69 , batch: 297 , training loss: 4.070765\n",
      "[INFO] Epoch: 69 , batch: 298 , training loss: 3.896238\n",
      "[INFO] Epoch: 69 , batch: 299 , training loss: 3.947675\n",
      "[INFO] Epoch: 69 , batch: 300 , training loss: 3.932603\n",
      "[INFO] Epoch: 69 , batch: 301 , training loss: 3.846362\n",
      "[INFO] Epoch: 69 , batch: 302 , training loss: 4.024395\n",
      "[INFO] Epoch: 69 , batch: 303 , training loss: 4.030174\n",
      "[INFO] Epoch: 69 , batch: 304 , training loss: 4.150279\n",
      "[INFO] Epoch: 69 , batch: 305 , training loss: 4.010376\n",
      "[INFO] Epoch: 69 , batch: 306 , training loss: 4.131982\n",
      "[INFO] Epoch: 69 , batch: 307 , training loss: 4.142651\n",
      "[INFO] Epoch: 69 , batch: 308 , training loss: 3.965418\n",
      "[INFO] Epoch: 69 , batch: 309 , training loss: 3.931636\n",
      "[INFO] Epoch: 69 , batch: 310 , training loss: 3.877612\n",
      "[INFO] Epoch: 69 , batch: 311 , training loss: 3.860561\n",
      "[INFO] Epoch: 69 , batch: 312 , training loss: 3.759409\n",
      "[INFO] Epoch: 69 , batch: 313 , training loss: 3.855431\n",
      "[INFO] Epoch: 69 , batch: 314 , training loss: 3.939833\n",
      "[INFO] Epoch: 69 , batch: 315 , training loss: 4.025698\n",
      "[INFO] Epoch: 69 , batch: 316 , training loss: 4.250972\n",
      "[INFO] Epoch: 69 , batch: 317 , training loss: 4.598367\n",
      "[INFO] Epoch: 69 , batch: 318 , training loss: 4.673738\n",
      "[INFO] Epoch: 69 , batch: 319 , training loss: 4.411917\n",
      "[INFO] Epoch: 69 , batch: 320 , training loss: 3.974797\n",
      "[INFO] Epoch: 69 , batch: 321 , training loss: 3.807528\n",
      "[INFO] Epoch: 69 , batch: 322 , training loss: 3.902392\n",
      "[INFO] Epoch: 69 , batch: 323 , training loss: 3.930356\n",
      "[INFO] Epoch: 69 , batch: 324 , training loss: 3.894750\n",
      "[INFO] Epoch: 69 , batch: 325 , training loss: 4.030042\n",
      "[INFO] Epoch: 69 , batch: 326 , training loss: 4.064281\n",
      "[INFO] Epoch: 69 , batch: 327 , training loss: 4.020894\n",
      "[INFO] Epoch: 69 , batch: 328 , training loss: 4.035716\n",
      "[INFO] Epoch: 69 , batch: 329 , training loss: 3.933363\n",
      "[INFO] Epoch: 69 , batch: 330 , training loss: 3.911965\n",
      "[INFO] Epoch: 69 , batch: 331 , training loss: 4.065630\n",
      "[INFO] Epoch: 69 , batch: 332 , training loss: 3.919287\n",
      "[INFO] Epoch: 69 , batch: 333 , training loss: 3.905715\n",
      "[INFO] Epoch: 69 , batch: 334 , training loss: 3.921306\n",
      "[INFO] Epoch: 69 , batch: 335 , training loss: 4.036070\n",
      "[INFO] Epoch: 69 , batch: 336 , training loss: 4.046960\n",
      "[INFO] Epoch: 69 , batch: 337 , training loss: 4.110072\n",
      "[INFO] Epoch: 69 , batch: 338 , training loss: 4.316093\n",
      "[INFO] Epoch: 69 , batch: 339 , training loss: 4.127193\n",
      "[INFO] Epoch: 69 , batch: 340 , training loss: 4.284860\n",
      "[INFO] Epoch: 69 , batch: 341 , training loss: 4.058319\n",
      "[INFO] Epoch: 69 , batch: 342 , training loss: 3.847569\n",
      "[INFO] Epoch: 69 , batch: 343 , training loss: 3.908337\n",
      "[INFO] Epoch: 69 , batch: 344 , training loss: 3.793746\n",
      "[INFO] Epoch: 69 , batch: 345 , training loss: 3.907975\n",
      "[INFO] Epoch: 69 , batch: 346 , training loss: 3.975893\n",
      "[INFO] Epoch: 69 , batch: 347 , training loss: 3.862623\n",
      "[INFO] Epoch: 69 , batch: 348 , training loss: 3.970118\n",
      "[INFO] Epoch: 69 , batch: 349 , training loss: 4.057919\n",
      "[INFO] Epoch: 69 , batch: 350 , training loss: 3.936789\n",
      "[INFO] Epoch: 69 , batch: 351 , training loss: 4.007705\n",
      "[INFO] Epoch: 69 , batch: 352 , training loss: 4.010717\n",
      "[INFO] Epoch: 69 , batch: 353 , training loss: 4.023226\n",
      "[INFO] Epoch: 69 , batch: 354 , training loss: 4.091114\n",
      "[INFO] Epoch: 69 , batch: 355 , training loss: 4.079961\n",
      "[INFO] Epoch: 69 , batch: 356 , training loss: 3.951092\n",
      "[INFO] Epoch: 69 , batch: 357 , training loss: 4.004700\n",
      "[INFO] Epoch: 69 , batch: 358 , training loss: 3.947592\n",
      "[INFO] Epoch: 69 , batch: 359 , training loss: 3.934545\n",
      "[INFO] Epoch: 69 , batch: 360 , training loss: 4.030751\n",
      "[INFO] Epoch: 69 , batch: 361 , training loss: 4.004913\n",
      "[INFO] Epoch: 69 , batch: 362 , training loss: 4.124942\n",
      "[INFO] Epoch: 69 , batch: 363 , training loss: 3.993174\n",
      "[INFO] Epoch: 69 , batch: 364 , training loss: 4.064174\n",
      "[INFO] Epoch: 69 , batch: 365 , training loss: 3.991194\n",
      "[INFO] Epoch: 69 , batch: 366 , training loss: 4.072569\n",
      "[INFO] Epoch: 69 , batch: 367 , training loss: 4.114648\n",
      "[INFO] Epoch: 69 , batch: 368 , training loss: 4.546309\n",
      "[INFO] Epoch: 69 , batch: 369 , training loss: 4.194907\n",
      "[INFO] Epoch: 69 , batch: 370 , training loss: 3.969262\n",
      "[INFO] Epoch: 69 , batch: 371 , training loss: 4.377410\n",
      "[INFO] Epoch: 69 , batch: 372 , training loss: 4.617296\n",
      "[INFO] Epoch: 69 , batch: 373 , training loss: 4.650671\n",
      "[INFO] Epoch: 69 , batch: 374 , training loss: 4.805964\n",
      "[INFO] Epoch: 69 , batch: 375 , training loss: 4.761897\n",
      "[INFO] Epoch: 69 , batch: 376 , training loss: 4.651175\n",
      "[INFO] Epoch: 69 , batch: 377 , training loss: 4.426922\n",
      "[INFO] Epoch: 69 , batch: 378 , training loss: 4.515250\n",
      "[INFO] Epoch: 69 , batch: 379 , training loss: 4.515431\n",
      "[INFO] Epoch: 69 , batch: 380 , training loss: 4.684186\n",
      "[INFO] Epoch: 69 , batch: 381 , training loss: 4.356312\n",
      "[INFO] Epoch: 69 , batch: 382 , training loss: 4.592746\n",
      "[INFO] Epoch: 69 , batch: 383 , training loss: 4.673005\n",
      "[INFO] Epoch: 69 , batch: 384 , training loss: 4.633888\n",
      "[INFO] Epoch: 69 , batch: 385 , training loss: 4.318120\n",
      "[INFO] Epoch: 69 , batch: 386 , training loss: 4.581549\n",
      "[INFO] Epoch: 69 , batch: 387 , training loss: 4.519618\n",
      "[INFO] Epoch: 69 , batch: 388 , training loss: 4.341316\n",
      "[INFO] Epoch: 69 , batch: 389 , training loss: 4.168670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 69 , batch: 390 , training loss: 4.192369\n",
      "[INFO] Epoch: 69 , batch: 391 , training loss: 4.229545\n",
      "[INFO] Epoch: 69 , batch: 392 , training loss: 4.589180\n",
      "[INFO] Epoch: 69 , batch: 393 , training loss: 4.475377\n",
      "[INFO] Epoch: 69 , batch: 394 , training loss: 4.588535\n",
      "[INFO] Epoch: 69 , batch: 395 , training loss: 4.378377\n",
      "[INFO] Epoch: 69 , batch: 396 , training loss: 4.213384\n",
      "[INFO] Epoch: 69 , batch: 397 , training loss: 4.355666\n",
      "[INFO] Epoch: 69 , batch: 398 , training loss: 4.204445\n",
      "[INFO] Epoch: 69 , batch: 399 , training loss: 4.280101\n",
      "[INFO] Epoch: 69 , batch: 400 , training loss: 4.269832\n",
      "[INFO] Epoch: 69 , batch: 401 , training loss: 4.667555\n",
      "[INFO] Epoch: 69 , batch: 402 , training loss: 4.423004\n",
      "[INFO] Epoch: 69 , batch: 403 , training loss: 4.237876\n",
      "[INFO] Epoch: 69 , batch: 404 , training loss: 4.421261\n",
      "[INFO] Epoch: 69 , batch: 405 , training loss: 4.455173\n",
      "[INFO] Epoch: 69 , batch: 406 , training loss: 4.378500\n",
      "[INFO] Epoch: 69 , batch: 407 , training loss: 4.404887\n",
      "[INFO] Epoch: 69 , batch: 408 , training loss: 4.385368\n",
      "[INFO] Epoch: 69 , batch: 409 , training loss: 4.396548\n",
      "[INFO] Epoch: 69 , batch: 410 , training loss: 4.443097\n",
      "[INFO] Epoch: 69 , batch: 411 , training loss: 4.603017\n",
      "[INFO] Epoch: 69 , batch: 412 , training loss: 4.432763\n",
      "[INFO] Epoch: 69 , batch: 413 , training loss: 4.288818\n",
      "[INFO] Epoch: 69 , batch: 414 , training loss: 4.363226\n",
      "[INFO] Epoch: 69 , batch: 415 , training loss: 4.405913\n",
      "[INFO] Epoch: 69 , batch: 416 , training loss: 4.452984\n",
      "[INFO] Epoch: 69 , batch: 417 , training loss: 4.370417\n",
      "[INFO] Epoch: 69 , batch: 418 , training loss: 4.442436\n",
      "[INFO] Epoch: 69 , batch: 419 , training loss: 4.378938\n",
      "[INFO] Epoch: 69 , batch: 420 , training loss: 4.358875\n",
      "[INFO] Epoch: 69 , batch: 421 , training loss: 4.355584\n",
      "[INFO] Epoch: 69 , batch: 422 , training loss: 4.199277\n",
      "[INFO] Epoch: 69 , batch: 423 , training loss: 4.418790\n",
      "[INFO] Epoch: 69 , batch: 424 , training loss: 4.586282\n",
      "[INFO] Epoch: 69 , batch: 425 , training loss: 4.460832\n",
      "[INFO] Epoch: 69 , batch: 426 , training loss: 4.212530\n",
      "[INFO] Epoch: 69 , batch: 427 , training loss: 4.443552\n",
      "[INFO] Epoch: 69 , batch: 428 , training loss: 4.294664\n",
      "[INFO] Epoch: 69 , batch: 429 , training loss: 4.201021\n",
      "[INFO] Epoch: 69 , batch: 430 , training loss: 4.433508\n",
      "[INFO] Epoch: 69 , batch: 431 , training loss: 4.043532\n",
      "[INFO] Epoch: 69 , batch: 432 , training loss: 4.103540\n",
      "[INFO] Epoch: 69 , batch: 433 , training loss: 4.136243\n",
      "[INFO] Epoch: 69 , batch: 434 , training loss: 4.029736\n",
      "[INFO] Epoch: 69 , batch: 435 , training loss: 4.379410\n",
      "[INFO] Epoch: 69 , batch: 436 , training loss: 4.426077\n",
      "[INFO] Epoch: 69 , batch: 437 , training loss: 4.207214\n",
      "[INFO] Epoch: 69 , batch: 438 , training loss: 4.082131\n",
      "[INFO] Epoch: 69 , batch: 439 , training loss: 4.303016\n",
      "[INFO] Epoch: 69 , batch: 440 , training loss: 4.387428\n",
      "[INFO] Epoch: 69 , batch: 441 , training loss: 4.515039\n",
      "[INFO] Epoch: 69 , batch: 442 , training loss: 4.266581\n",
      "[INFO] Epoch: 69 , batch: 443 , training loss: 4.440200\n",
      "[INFO] Epoch: 69 , batch: 444 , training loss: 4.065021\n",
      "[INFO] Epoch: 69 , batch: 445 , training loss: 3.953543\n",
      "[INFO] Epoch: 69 , batch: 446 , training loss: 3.906496\n",
      "[INFO] Epoch: 69 , batch: 447 , training loss: 4.090028\n",
      "[INFO] Epoch: 69 , batch: 448 , training loss: 4.210841\n",
      "[INFO] Epoch: 69 , batch: 449 , training loss: 4.596726\n",
      "[INFO] Epoch: 69 , batch: 450 , training loss: 4.662658\n",
      "[INFO] Epoch: 69 , batch: 451 , training loss: 4.572713\n",
      "[INFO] Epoch: 69 , batch: 452 , training loss: 4.372980\n",
      "[INFO] Epoch: 69 , batch: 453 , training loss: 4.127302\n",
      "[INFO] Epoch: 69 , batch: 454 , training loss: 4.279301\n",
      "[INFO] Epoch: 69 , batch: 455 , training loss: 4.335230\n",
      "[INFO] Epoch: 69 , batch: 456 , training loss: 4.332354\n",
      "[INFO] Epoch: 69 , batch: 457 , training loss: 4.419447\n",
      "[INFO] Epoch: 69 , batch: 458 , training loss: 4.148540\n",
      "[INFO] Epoch: 69 , batch: 459 , training loss: 4.127242\n",
      "[INFO] Epoch: 69 , batch: 460 , training loss: 4.244210\n",
      "[INFO] Epoch: 69 , batch: 461 , training loss: 4.209991\n",
      "[INFO] Epoch: 69 , batch: 462 , training loss: 4.273078\n",
      "[INFO] Epoch: 69 , batch: 463 , training loss: 4.200358\n",
      "[INFO] Epoch: 69 , batch: 464 , training loss: 4.364915\n",
      "[INFO] Epoch: 69 , batch: 465 , training loss: 4.325735\n",
      "[INFO] Epoch: 69 , batch: 466 , training loss: 4.403932\n",
      "[INFO] Epoch: 69 , batch: 467 , training loss: 4.370287\n",
      "[INFO] Epoch: 69 , batch: 468 , training loss: 4.305005\n",
      "[INFO] Epoch: 69 , batch: 469 , training loss: 4.343700\n",
      "[INFO] Epoch: 69 , batch: 470 , training loss: 4.188629\n",
      "[INFO] Epoch: 69 , batch: 471 , training loss: 4.283444\n",
      "[INFO] Epoch: 69 , batch: 472 , training loss: 4.327631\n",
      "[INFO] Epoch: 69 , batch: 473 , training loss: 4.255195\n",
      "[INFO] Epoch: 69 , batch: 474 , training loss: 4.032140\n",
      "[INFO] Epoch: 69 , batch: 475 , training loss: 3.916601\n",
      "[INFO] Epoch: 69 , batch: 476 , training loss: 4.298590\n",
      "[INFO] Epoch: 69 , batch: 477 , training loss: 4.432799\n",
      "[INFO] Epoch: 69 , batch: 478 , training loss: 4.418647\n",
      "[INFO] Epoch: 69 , batch: 479 , training loss: 4.415091\n",
      "[INFO] Epoch: 69 , batch: 480 , training loss: 4.535420\n",
      "[INFO] Epoch: 69 , batch: 481 , training loss: 4.415801\n",
      "[INFO] Epoch: 69 , batch: 482 , training loss: 4.523527\n",
      "[INFO] Epoch: 69 , batch: 483 , training loss: 4.355828\n",
      "[INFO] Epoch: 69 , batch: 484 , training loss: 4.147182\n",
      "[INFO] Epoch: 69 , batch: 485 , training loss: 4.263004\n",
      "[INFO] Epoch: 69 , batch: 486 , training loss: 4.159214\n",
      "[INFO] Epoch: 69 , batch: 487 , training loss: 4.146448\n",
      "[INFO] Epoch: 69 , batch: 488 , training loss: 4.336861\n",
      "[INFO] Epoch: 69 , batch: 489 , training loss: 4.232702\n",
      "[INFO] Epoch: 69 , batch: 490 , training loss: 4.270563\n",
      "[INFO] Epoch: 69 , batch: 491 , training loss: 4.192488\n",
      "[INFO] Epoch: 69 , batch: 492 , training loss: 4.186052\n",
      "[INFO] Epoch: 69 , batch: 493 , training loss: 4.353277\n",
      "[INFO] Epoch: 69 , batch: 494 , training loss: 4.256060\n",
      "[INFO] Epoch: 69 , batch: 495 , training loss: 4.431273\n",
      "[INFO] Epoch: 69 , batch: 496 , training loss: 4.284794\n",
      "[INFO] Epoch: 69 , batch: 497 , training loss: 4.338992\n",
      "[INFO] Epoch: 69 , batch: 498 , training loss: 4.310028\n",
      "[INFO] Epoch: 69 , batch: 499 , training loss: 4.373142\n",
      "[INFO] Epoch: 69 , batch: 500 , training loss: 4.528833\n",
      "[INFO] Epoch: 69 , batch: 501 , training loss: 4.829238\n",
      "[INFO] Epoch: 69 , batch: 502 , training loss: 4.870535\n",
      "[INFO] Epoch: 69 , batch: 503 , training loss: 4.473445\n",
      "[INFO] Epoch: 69 , batch: 504 , training loss: 4.608941\n",
      "[INFO] Epoch: 69 , batch: 505 , training loss: 4.585308\n",
      "[INFO] Epoch: 69 , batch: 506 , training loss: 4.560511\n",
      "[INFO] Epoch: 69 , batch: 507 , training loss: 4.617905\n",
      "[INFO] Epoch: 69 , batch: 508 , training loss: 4.528612\n",
      "[INFO] Epoch: 69 , batch: 509 , training loss: 4.375935\n",
      "[INFO] Epoch: 69 , batch: 510 , training loss: 4.452661\n",
      "[INFO] Epoch: 69 , batch: 511 , training loss: 4.374116\n",
      "[INFO] Epoch: 69 , batch: 512 , training loss: 4.453229\n",
      "[INFO] Epoch: 69 , batch: 513 , training loss: 4.717329\n",
      "[INFO] Epoch: 69 , batch: 514 , training loss: 4.356082\n",
      "[INFO] Epoch: 69 , batch: 515 , training loss: 4.627134\n",
      "[INFO] Epoch: 69 , batch: 516 , training loss: 4.440567\n",
      "[INFO] Epoch: 69 , batch: 517 , training loss: 4.396138\n",
      "[INFO] Epoch: 69 , batch: 518 , training loss: 4.354375\n",
      "[INFO] Epoch: 69 , batch: 519 , training loss: 4.186800\n",
      "[INFO] Epoch: 69 , batch: 520 , training loss: 4.436700\n",
      "[INFO] Epoch: 69 , batch: 521 , training loss: 4.403965\n",
      "[INFO] Epoch: 69 , batch: 522 , training loss: 4.500618\n",
      "[INFO] Epoch: 69 , batch: 523 , training loss: 4.420958\n",
      "[INFO] Epoch: 69 , batch: 524 , training loss: 4.719869\n",
      "[INFO] Epoch: 69 , batch: 525 , training loss: 4.590239\n",
      "[INFO] Epoch: 69 , batch: 526 , training loss: 4.362744\n",
      "[INFO] Epoch: 69 , batch: 527 , training loss: 4.411063\n",
      "[INFO] Epoch: 69 , batch: 528 , training loss: 4.427736\n",
      "[INFO] Epoch: 69 , batch: 529 , training loss: 4.401250\n",
      "[INFO] Epoch: 69 , batch: 530 , training loss: 4.269621\n",
      "[INFO] Epoch: 69 , batch: 531 , training loss: 4.402293\n",
      "[INFO] Epoch: 69 , batch: 532 , training loss: 4.312614\n",
      "[INFO] Epoch: 69 , batch: 533 , training loss: 4.429031\n",
      "[INFO] Epoch: 69 , batch: 534 , training loss: 4.434795\n",
      "[INFO] Epoch: 69 , batch: 535 , training loss: 4.439063\n",
      "[INFO] Epoch: 69 , batch: 536 , training loss: 4.301343\n",
      "[INFO] Epoch: 69 , batch: 537 , training loss: 4.281100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 69 , batch: 538 , training loss: 4.366045\n",
      "[INFO] Epoch: 69 , batch: 539 , training loss: 4.459099\n",
      "[INFO] Epoch: 69 , batch: 540 , training loss: 4.979544\n",
      "[INFO] Epoch: 69 , batch: 541 , training loss: 4.796803\n",
      "[INFO] Epoch: 69 , batch: 542 , training loss: 4.703868\n",
      "[INFO] Epoch: 70 , batch: 0 , training loss: 3.578553\n",
      "[INFO] Epoch: 70 , batch: 1 , training loss: 3.494299\n",
      "[INFO] Epoch: 70 , batch: 2 , training loss: 3.683210\n",
      "[INFO] Epoch: 70 , batch: 3 , training loss: 3.568437\n",
      "[INFO] Epoch: 70 , batch: 4 , training loss: 3.915625\n",
      "[INFO] Epoch: 70 , batch: 5 , training loss: 3.611974\n",
      "[INFO] Epoch: 70 , batch: 6 , training loss: 4.003702\n",
      "[INFO] Epoch: 70 , batch: 7 , training loss: 3.972223\n",
      "[INFO] Epoch: 70 , batch: 8 , training loss: 3.596498\n",
      "[INFO] Epoch: 70 , batch: 9 , training loss: 3.863126\n",
      "[INFO] Epoch: 70 , batch: 10 , training loss: 3.828467\n",
      "[INFO] Epoch: 70 , batch: 11 , training loss: 3.729814\n",
      "[INFO] Epoch: 70 , batch: 12 , training loss: 3.673842\n",
      "[INFO] Epoch: 70 , batch: 13 , training loss: 3.650907\n",
      "[INFO] Epoch: 70 , batch: 14 , training loss: 3.597157\n",
      "[INFO] Epoch: 70 , batch: 15 , training loss: 3.827393\n",
      "[INFO] Epoch: 70 , batch: 16 , training loss: 3.608786\n",
      "[INFO] Epoch: 70 , batch: 17 , training loss: 3.733478\n",
      "[INFO] Epoch: 70 , batch: 18 , training loss: 3.703309\n",
      "[INFO] Epoch: 70 , batch: 19 , training loss: 3.493861\n",
      "[INFO] Epoch: 70 , batch: 20 , training loss: 3.480611\n",
      "[INFO] Epoch: 70 , batch: 21 , training loss: 3.610652\n",
      "[INFO] Epoch: 70 , batch: 22 , training loss: 3.465159\n",
      "[INFO] Epoch: 70 , batch: 23 , training loss: 3.706290\n",
      "[INFO] Epoch: 70 , batch: 24 , training loss: 3.541854\n",
      "[INFO] Epoch: 70 , batch: 25 , training loss: 3.646324\n",
      "[INFO] Epoch: 70 , batch: 26 , training loss: 3.515977\n",
      "[INFO] Epoch: 70 , batch: 27 , training loss: 3.505228\n",
      "[INFO] Epoch: 70 , batch: 28 , training loss: 3.673686\n",
      "[INFO] Epoch: 70 , batch: 29 , training loss: 3.487116\n",
      "[INFO] Epoch: 70 , batch: 30 , training loss: 3.554777\n",
      "[INFO] Epoch: 70 , batch: 31 , training loss: 3.602661\n",
      "[INFO] Epoch: 70 , batch: 32 , training loss: 3.546420\n",
      "[INFO] Epoch: 70 , batch: 33 , training loss: 3.606474\n",
      "[INFO] Epoch: 70 , batch: 34 , training loss: 3.594712\n",
      "[INFO] Epoch: 70 , batch: 35 , training loss: 3.564039\n",
      "[INFO] Epoch: 70 , batch: 36 , training loss: 3.648413\n",
      "[INFO] Epoch: 70 , batch: 37 , training loss: 3.491640\n",
      "[INFO] Epoch: 70 , batch: 38 , training loss: 3.572442\n",
      "[INFO] Epoch: 70 , batch: 39 , training loss: 3.385538\n",
      "[INFO] Epoch: 70 , batch: 40 , training loss: 3.598518\n",
      "[INFO] Epoch: 70 , batch: 41 , training loss: 3.582042\n",
      "[INFO] Epoch: 70 , batch: 42 , training loss: 4.049198\n",
      "[INFO] Epoch: 70 , batch: 43 , training loss: 3.816188\n",
      "[INFO] Epoch: 70 , batch: 44 , training loss: 4.150981\n",
      "[INFO] Epoch: 70 , batch: 45 , training loss: 4.075528\n",
      "[INFO] Epoch: 70 , batch: 46 , training loss: 4.047697\n",
      "[INFO] Epoch: 70 , batch: 47 , training loss: 3.625449\n",
      "[INFO] Epoch: 70 , batch: 48 , training loss: 3.636884\n",
      "[INFO] Epoch: 70 , batch: 49 , training loss: 3.833387\n",
      "[INFO] Epoch: 70 , batch: 50 , training loss: 3.630569\n",
      "[INFO] Epoch: 70 , batch: 51 , training loss: 3.841182\n",
      "[INFO] Epoch: 70 , batch: 52 , training loss: 3.593473\n",
      "[INFO] Epoch: 70 , batch: 53 , training loss: 3.756634\n",
      "[INFO] Epoch: 70 , batch: 54 , training loss: 3.751444\n",
      "[INFO] Epoch: 70 , batch: 55 , training loss: 3.783183\n",
      "[INFO] Epoch: 70 , batch: 56 , training loss: 3.635854\n",
      "[INFO] Epoch: 70 , batch: 57 , training loss: 3.594954\n",
      "[INFO] Epoch: 70 , batch: 58 , training loss: 3.643626\n",
      "[INFO] Epoch: 70 , batch: 59 , training loss: 3.698059\n",
      "[INFO] Epoch: 70 , batch: 60 , training loss: 3.645863\n",
      "[INFO] Epoch: 70 , batch: 61 , training loss: 3.746649\n",
      "[INFO] Epoch: 70 , batch: 62 , training loss: 3.631469\n",
      "[INFO] Epoch: 70 , batch: 63 , training loss: 3.833176\n",
      "[INFO] Epoch: 70 , batch: 64 , training loss: 3.979095\n",
      "[INFO] Epoch: 70 , batch: 65 , training loss: 3.722273\n",
      "[INFO] Epoch: 70 , batch: 66 , training loss: 3.579700\n",
      "[INFO] Epoch: 70 , batch: 67 , training loss: 3.614434\n",
      "[INFO] Epoch: 70 , batch: 68 , training loss: 3.763590\n",
      "[INFO] Epoch: 70 , batch: 69 , training loss: 3.700353\n",
      "[INFO] Epoch: 70 , batch: 70 , training loss: 3.876902\n",
      "[INFO] Epoch: 70 , batch: 71 , training loss: 3.757921\n",
      "[INFO] Epoch: 70 , batch: 72 , training loss: 3.805134\n",
      "[INFO] Epoch: 70 , batch: 73 , training loss: 3.770489\n",
      "[INFO] Epoch: 70 , batch: 74 , training loss: 3.850847\n",
      "[INFO] Epoch: 70 , batch: 75 , training loss: 3.732215\n",
      "[INFO] Epoch: 70 , batch: 76 , training loss: 3.860433\n",
      "[INFO] Epoch: 70 , batch: 77 , training loss: 3.762795\n",
      "[INFO] Epoch: 70 , batch: 78 , training loss: 3.882667\n",
      "[INFO] Epoch: 70 , batch: 79 , training loss: 3.706323\n",
      "[INFO] Epoch: 70 , batch: 80 , training loss: 3.939238\n",
      "[INFO] Epoch: 70 , batch: 81 , training loss: 3.834418\n",
      "[INFO] Epoch: 70 , batch: 82 , training loss: 3.819084\n",
      "[INFO] Epoch: 70 , batch: 83 , training loss: 3.909499\n",
      "[INFO] Epoch: 70 , batch: 84 , training loss: 3.867157\n",
      "[INFO] Epoch: 70 , batch: 85 , training loss: 3.940252\n",
      "[INFO] Epoch: 70 , batch: 86 , training loss: 3.875858\n",
      "[INFO] Epoch: 70 , batch: 87 , training loss: 3.844058\n",
      "[INFO] Epoch: 70 , batch: 88 , training loss: 4.003967\n",
      "[INFO] Epoch: 70 , batch: 89 , training loss: 3.775059\n",
      "[INFO] Epoch: 70 , batch: 90 , training loss: 3.860831\n",
      "[INFO] Epoch: 70 , batch: 91 , training loss: 3.819143\n",
      "[INFO] Epoch: 70 , batch: 92 , training loss: 3.849514\n",
      "[INFO] Epoch: 70 , batch: 93 , training loss: 3.928961\n",
      "[INFO] Epoch: 70 , batch: 94 , training loss: 4.028909\n",
      "[INFO] Epoch: 70 , batch: 95 , training loss: 3.827054\n",
      "[INFO] Epoch: 70 , batch: 96 , training loss: 3.842188\n",
      "[INFO] Epoch: 70 , batch: 97 , training loss: 3.750865\n",
      "[INFO] Epoch: 70 , batch: 98 , training loss: 3.721223\n",
      "[INFO] Epoch: 70 , batch: 99 , training loss: 3.837612\n",
      "[INFO] Epoch: 70 , batch: 100 , training loss: 3.731139\n",
      "[INFO] Epoch: 70 , batch: 101 , training loss: 3.734852\n",
      "[INFO] Epoch: 70 , batch: 102 , training loss: 3.887221\n",
      "[INFO] Epoch: 70 , batch: 103 , training loss: 3.700809\n",
      "[INFO] Epoch: 70 , batch: 104 , training loss: 3.662134\n",
      "[INFO] Epoch: 70 , batch: 105 , training loss: 3.908344\n",
      "[INFO] Epoch: 70 , batch: 106 , training loss: 3.928087\n",
      "[INFO] Epoch: 70 , batch: 107 , training loss: 3.760402\n",
      "[INFO] Epoch: 70 , batch: 108 , training loss: 3.724746\n",
      "[INFO] Epoch: 70 , batch: 109 , training loss: 3.625101\n",
      "[INFO] Epoch: 70 , batch: 110 , training loss: 3.783600\n",
      "[INFO] Epoch: 70 , batch: 111 , training loss: 3.900724\n",
      "[INFO] Epoch: 70 , batch: 112 , training loss: 3.803779\n",
      "[INFO] Epoch: 70 , batch: 113 , training loss: 3.807553\n",
      "[INFO] Epoch: 70 , batch: 114 , training loss: 3.796956\n",
      "[INFO] Epoch: 70 , batch: 115 , training loss: 3.792295\n",
      "[INFO] Epoch: 70 , batch: 116 , training loss: 3.719857\n",
      "[INFO] Epoch: 70 , batch: 117 , training loss: 3.917102\n",
      "[INFO] Epoch: 70 , batch: 118 , training loss: 3.905950\n",
      "[INFO] Epoch: 70 , batch: 119 , training loss: 4.033844\n",
      "[INFO] Epoch: 70 , batch: 120 , training loss: 4.020296\n",
      "[INFO] Epoch: 70 , batch: 121 , training loss: 3.891725\n",
      "[INFO] Epoch: 70 , batch: 122 , training loss: 3.811221\n",
      "[INFO] Epoch: 70 , batch: 123 , training loss: 3.783989\n",
      "[INFO] Epoch: 70 , batch: 124 , training loss: 3.915095\n",
      "[INFO] Epoch: 70 , batch: 125 , training loss: 3.736743\n",
      "[INFO] Epoch: 70 , batch: 126 , training loss: 3.707648\n",
      "[INFO] Epoch: 70 , batch: 127 , training loss: 3.739520\n",
      "[INFO] Epoch: 70 , batch: 128 , training loss: 3.872491\n",
      "[INFO] Epoch: 70 , batch: 129 , training loss: 3.811292\n",
      "[INFO] Epoch: 70 , batch: 130 , training loss: 3.837386\n",
      "[INFO] Epoch: 70 , batch: 131 , training loss: 3.834471\n",
      "[INFO] Epoch: 70 , batch: 132 , training loss: 3.836817\n",
      "[INFO] Epoch: 70 , batch: 133 , training loss: 3.795691\n",
      "[INFO] Epoch: 70 , batch: 134 , training loss: 3.574386\n",
      "[INFO] Epoch: 70 , batch: 135 , training loss: 3.650311\n",
      "[INFO] Epoch: 70 , batch: 136 , training loss: 3.929622\n",
      "[INFO] Epoch: 70 , batch: 137 , training loss: 3.877178\n",
      "[INFO] Epoch: 70 , batch: 138 , training loss: 3.909721\n",
      "[INFO] Epoch: 70 , batch: 139 , training loss: 4.461733\n",
      "[INFO] Epoch: 70 , batch: 140 , training loss: 4.243096\n",
      "[INFO] Epoch: 70 , batch: 141 , training loss: 4.024696\n",
      "[INFO] Epoch: 70 , batch: 142 , training loss: 3.757833\n",
      "[INFO] Epoch: 70 , batch: 143 , training loss: 3.895532\n",
      "[INFO] Epoch: 70 , batch: 144 , training loss: 3.733106\n",
      "[INFO] Epoch: 70 , batch: 145 , training loss: 3.785130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 70 , batch: 146 , training loss: 4.000794\n",
      "[INFO] Epoch: 70 , batch: 147 , training loss: 3.658469\n",
      "[INFO] Epoch: 70 , batch: 148 , training loss: 3.642023\n",
      "[INFO] Epoch: 70 , batch: 149 , training loss: 3.713154\n",
      "[INFO] Epoch: 70 , batch: 150 , training loss: 3.957096\n",
      "[INFO] Epoch: 70 , batch: 151 , training loss: 3.824078\n",
      "[INFO] Epoch: 70 , batch: 152 , training loss: 3.841888\n",
      "[INFO] Epoch: 70 , batch: 153 , training loss: 3.874768\n",
      "[INFO] Epoch: 70 , batch: 154 , training loss: 3.975493\n",
      "[INFO] Epoch: 70 , batch: 155 , training loss: 4.190573\n",
      "[INFO] Epoch: 70 , batch: 156 , training loss: 3.901084\n",
      "[INFO] Epoch: 70 , batch: 157 , training loss: 3.851059\n",
      "[INFO] Epoch: 70 , batch: 158 , training loss: 3.976101\n",
      "[INFO] Epoch: 70 , batch: 159 , training loss: 3.901814\n",
      "[INFO] Epoch: 70 , batch: 160 , training loss: 4.102332\n",
      "[INFO] Epoch: 70 , batch: 161 , training loss: 4.153212\n",
      "[INFO] Epoch: 70 , batch: 162 , training loss: 4.143347\n",
      "[INFO] Epoch: 70 , batch: 163 , training loss: 4.336188\n",
      "[INFO] Epoch: 70 , batch: 164 , training loss: 4.250840\n",
      "[INFO] Epoch: 70 , batch: 165 , training loss: 4.209629\n",
      "[INFO] Epoch: 70 , batch: 166 , training loss: 4.113935\n",
      "[INFO] Epoch: 70 , batch: 167 , training loss: 4.069913\n",
      "[INFO] Epoch: 70 , batch: 168 , training loss: 3.830005\n",
      "[INFO] Epoch: 70 , batch: 169 , training loss: 3.797059\n",
      "[INFO] Epoch: 70 , batch: 170 , training loss: 3.975168\n",
      "[INFO] Epoch: 70 , batch: 171 , training loss: 3.448541\n",
      "[INFO] Epoch: 70 , batch: 172 , training loss: 3.719344\n",
      "[INFO] Epoch: 70 , batch: 173 , training loss: 3.993546\n",
      "[INFO] Epoch: 70 , batch: 174 , training loss: 4.477747\n",
      "[INFO] Epoch: 70 , batch: 175 , training loss: 4.713515\n",
      "[INFO] Epoch: 70 , batch: 176 , training loss: 4.348457\n",
      "[INFO] Epoch: 70 , batch: 177 , training loss: 3.968648\n",
      "[INFO] Epoch: 70 , batch: 178 , training loss: 3.995798\n",
      "[INFO] Epoch: 70 , batch: 179 , training loss: 4.064915\n",
      "[INFO] Epoch: 70 , batch: 180 , training loss: 3.994829\n",
      "[INFO] Epoch: 70 , batch: 181 , training loss: 4.313555\n",
      "[INFO] Epoch: 70 , batch: 182 , training loss: 4.240331\n",
      "[INFO] Epoch: 70 , batch: 183 , training loss: 4.245852\n",
      "[INFO] Epoch: 70 , batch: 184 , training loss: 4.118789\n",
      "[INFO] Epoch: 70 , batch: 185 , training loss: 4.099413\n",
      "[INFO] Epoch: 70 , batch: 186 , training loss: 4.235573\n",
      "[INFO] Epoch: 70 , batch: 187 , training loss: 4.358992\n",
      "[INFO] Epoch: 70 , batch: 188 , training loss: 4.313998\n",
      "[INFO] Epoch: 70 , batch: 189 , training loss: 4.226826\n",
      "[INFO] Epoch: 70 , batch: 190 , training loss: 4.306291\n",
      "[INFO] Epoch: 70 , batch: 191 , training loss: 4.383967\n",
      "[INFO] Epoch: 70 , batch: 192 , training loss: 4.251439\n",
      "[INFO] Epoch: 70 , batch: 193 , training loss: 4.323837\n",
      "[INFO] Epoch: 70 , batch: 194 , training loss: 4.287066\n",
      "[INFO] Epoch: 70 , batch: 195 , training loss: 4.202620\n",
      "[INFO] Epoch: 70 , batch: 196 , training loss: 4.047716\n",
      "[INFO] Epoch: 70 , batch: 197 , training loss: 4.128711\n",
      "[INFO] Epoch: 70 , batch: 198 , training loss: 4.089340\n",
      "[INFO] Epoch: 70 , batch: 199 , training loss: 4.202431\n",
      "[INFO] Epoch: 70 , batch: 200 , training loss: 4.117658\n",
      "[INFO] Epoch: 70 , batch: 201 , training loss: 4.013430\n",
      "[INFO] Epoch: 70 , batch: 202 , training loss: 3.990103\n",
      "[INFO] Epoch: 70 , batch: 203 , training loss: 4.144731\n",
      "[INFO] Epoch: 70 , batch: 204 , training loss: 4.202625\n",
      "[INFO] Epoch: 70 , batch: 205 , training loss: 3.833561\n",
      "[INFO] Epoch: 70 , batch: 206 , training loss: 3.776040\n",
      "[INFO] Epoch: 70 , batch: 207 , training loss: 3.761703\n",
      "[INFO] Epoch: 70 , batch: 208 , training loss: 4.083511\n",
      "[INFO] Epoch: 70 , batch: 209 , training loss: 4.050389\n",
      "[INFO] Epoch: 70 , batch: 210 , training loss: 4.045034\n",
      "[INFO] Epoch: 70 , batch: 211 , training loss: 4.050780\n",
      "[INFO] Epoch: 70 , batch: 212 , training loss: 4.153336\n",
      "[INFO] Epoch: 70 , batch: 213 , training loss: 4.082560\n",
      "[INFO] Epoch: 70 , batch: 214 , training loss: 4.175969\n",
      "[INFO] Epoch: 70 , batch: 215 , training loss: 4.382546\n",
      "[INFO] Epoch: 70 , batch: 216 , training loss: 4.082285\n",
      "[INFO] Epoch: 70 , batch: 217 , training loss: 4.037303\n",
      "[INFO] Epoch: 70 , batch: 218 , training loss: 4.005334\n",
      "[INFO] Epoch: 70 , batch: 219 , training loss: 4.114619\n",
      "[INFO] Epoch: 70 , batch: 220 , training loss: 3.973011\n",
      "[INFO] Epoch: 70 , batch: 221 , training loss: 3.958175\n",
      "[INFO] Epoch: 70 , batch: 222 , training loss: 4.105318\n",
      "[INFO] Epoch: 70 , batch: 223 , training loss: 4.238684\n",
      "[INFO] Epoch: 70 , batch: 224 , training loss: 4.268400\n",
      "[INFO] Epoch: 70 , batch: 225 , training loss: 4.149837\n",
      "[INFO] Epoch: 70 , batch: 226 , training loss: 4.253046\n",
      "[INFO] Epoch: 70 , batch: 227 , training loss: 4.266102\n",
      "[INFO] Epoch: 70 , batch: 228 , training loss: 4.256286\n",
      "[INFO] Epoch: 70 , batch: 229 , training loss: 4.130845\n",
      "[INFO] Epoch: 70 , batch: 230 , training loss: 3.976949\n",
      "[INFO] Epoch: 70 , batch: 231 , training loss: 3.842508\n",
      "[INFO] Epoch: 70 , batch: 232 , training loss: 4.003412\n",
      "[INFO] Epoch: 70 , batch: 233 , training loss: 4.028777\n",
      "[INFO] Epoch: 70 , batch: 234 , training loss: 3.726282\n",
      "[INFO] Epoch: 70 , batch: 235 , training loss: 3.813308\n",
      "[INFO] Epoch: 70 , batch: 236 , training loss: 3.902078\n",
      "[INFO] Epoch: 70 , batch: 237 , training loss: 4.145242\n",
      "[INFO] Epoch: 70 , batch: 238 , training loss: 3.934277\n",
      "[INFO] Epoch: 70 , batch: 239 , training loss: 3.968493\n",
      "[INFO] Epoch: 70 , batch: 240 , training loss: 4.003987\n",
      "[INFO] Epoch: 70 , batch: 241 , training loss: 3.804940\n",
      "[INFO] Epoch: 70 , batch: 242 , training loss: 3.847381\n",
      "[INFO] Epoch: 70 , batch: 243 , training loss: 4.114584\n",
      "[INFO] Epoch: 70 , batch: 244 , training loss: 4.060056\n",
      "[INFO] Epoch: 70 , batch: 245 , training loss: 4.012465\n",
      "[INFO] Epoch: 70 , batch: 246 , training loss: 3.739035\n",
      "[INFO] Epoch: 70 , batch: 247 , training loss: 3.925908\n",
      "[INFO] Epoch: 70 , batch: 248 , training loss: 3.973432\n",
      "[INFO] Epoch: 70 , batch: 249 , training loss: 3.955647\n",
      "[INFO] Epoch: 70 , batch: 250 , training loss: 3.769933\n",
      "[INFO] Epoch: 70 , batch: 251 , training loss: 4.218409\n",
      "[INFO] Epoch: 70 , batch: 252 , training loss: 3.925445\n",
      "[INFO] Epoch: 70 , batch: 253 , training loss: 3.826530\n",
      "[INFO] Epoch: 70 , batch: 254 , training loss: 4.102954\n",
      "[INFO] Epoch: 70 , batch: 255 , training loss: 4.115411\n",
      "[INFO] Epoch: 70 , batch: 256 , training loss: 4.043206\n",
      "[INFO] Epoch: 70 , batch: 257 , training loss: 4.235935\n",
      "[INFO] Epoch: 70 , batch: 258 , training loss: 4.229829\n",
      "[INFO] Epoch: 70 , batch: 259 , training loss: 4.294502\n",
      "[INFO] Epoch: 70 , batch: 260 , training loss: 4.045962\n",
      "[INFO] Epoch: 70 , batch: 261 , training loss: 4.212304\n",
      "[INFO] Epoch: 70 , batch: 262 , training loss: 4.335236\n",
      "[INFO] Epoch: 70 , batch: 263 , training loss: 4.519342\n",
      "[INFO] Epoch: 70 , batch: 264 , training loss: 3.880053\n",
      "[INFO] Epoch: 70 , batch: 265 , training loss: 3.983145\n",
      "[INFO] Epoch: 70 , batch: 266 , training loss: 4.387043\n",
      "[INFO] Epoch: 70 , batch: 267 , training loss: 4.152913\n",
      "[INFO] Epoch: 70 , batch: 268 , training loss: 4.072237\n",
      "[INFO] Epoch: 70 , batch: 269 , training loss: 4.034451\n",
      "[INFO] Epoch: 70 , batch: 270 , training loss: 4.078413\n",
      "[INFO] Epoch: 70 , batch: 271 , training loss: 4.098644\n",
      "[INFO] Epoch: 70 , batch: 272 , training loss: 4.115320\n",
      "[INFO] Epoch: 70 , batch: 273 , training loss: 4.116758\n",
      "[INFO] Epoch: 70 , batch: 274 , training loss: 4.186656\n",
      "[INFO] Epoch: 70 , batch: 275 , training loss: 4.044876\n",
      "[INFO] Epoch: 70 , batch: 276 , training loss: 4.130122\n",
      "[INFO] Epoch: 70 , batch: 277 , training loss: 4.263981\n",
      "[INFO] Epoch: 70 , batch: 278 , training loss: 3.960131\n",
      "[INFO] Epoch: 70 , batch: 279 , training loss: 3.975955\n",
      "[INFO] Epoch: 70 , batch: 280 , training loss: 3.969647\n",
      "[INFO] Epoch: 70 , batch: 281 , training loss: 4.064487\n",
      "[INFO] Epoch: 70 , batch: 282 , training loss: 4.017801\n",
      "[INFO] Epoch: 70 , batch: 283 , training loss: 3.991754\n",
      "[INFO] Epoch: 70 , batch: 284 , training loss: 4.036690\n",
      "[INFO] Epoch: 70 , batch: 285 , training loss: 3.961281\n",
      "[INFO] Epoch: 70 , batch: 286 , training loss: 3.972410\n",
      "[INFO] Epoch: 70 , batch: 287 , training loss: 3.906841\n",
      "[INFO] Epoch: 70 , batch: 288 , training loss: 3.880979\n",
      "[INFO] Epoch: 70 , batch: 289 , training loss: 3.953555\n",
      "[INFO] Epoch: 70 , batch: 290 , training loss: 3.746581\n",
      "[INFO] Epoch: 70 , batch: 291 , training loss: 3.724429\n",
      "[INFO] Epoch: 70 , batch: 292 , training loss: 3.843487\n",
      "[INFO] Epoch: 70 , batch: 293 , training loss: 3.759515\n",
      "[INFO] Epoch: 70 , batch: 294 , training loss: 4.412824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 70 , batch: 295 , training loss: 4.171139\n",
      "[INFO] Epoch: 70 , batch: 296 , training loss: 4.099365\n",
      "[INFO] Epoch: 70 , batch: 297 , training loss: 4.080554\n",
      "[INFO] Epoch: 70 , batch: 298 , training loss: 3.926401\n",
      "[INFO] Epoch: 70 , batch: 299 , training loss: 3.944996\n",
      "[INFO] Epoch: 70 , batch: 300 , training loss: 3.942158\n",
      "[INFO] Epoch: 70 , batch: 301 , training loss: 3.858130\n",
      "[INFO] Epoch: 70 , batch: 302 , training loss: 4.026115\n",
      "[INFO] Epoch: 70 , batch: 303 , training loss: 4.038955\n",
      "[INFO] Epoch: 70 , batch: 304 , training loss: 4.172995\n",
      "[INFO] Epoch: 70 , batch: 305 , training loss: 4.021776\n",
      "[INFO] Epoch: 70 , batch: 306 , training loss: 4.141798\n",
      "[INFO] Epoch: 70 , batch: 307 , training loss: 4.151932\n",
      "[INFO] Epoch: 70 , batch: 308 , training loss: 3.967978\n",
      "[INFO] Epoch: 70 , batch: 309 , training loss: 3.947356\n",
      "[INFO] Epoch: 70 , batch: 310 , training loss: 3.894505\n",
      "[INFO] Epoch: 70 , batch: 311 , training loss: 3.883110\n",
      "[INFO] Epoch: 70 , batch: 312 , training loss: 3.766950\n",
      "[INFO] Epoch: 70 , batch: 313 , training loss: 3.892475\n",
      "[INFO] Epoch: 70 , batch: 314 , training loss: 3.963670\n",
      "[INFO] Epoch: 70 , batch: 315 , training loss: 4.037954\n",
      "[INFO] Epoch: 70 , batch: 316 , training loss: 4.265350\n",
      "[INFO] Epoch: 70 , batch: 317 , training loss: 4.610373\n",
      "[INFO] Epoch: 70 , batch: 318 , training loss: 4.724686\n",
      "[INFO] Epoch: 70 , batch: 319 , training loss: 4.425895\n",
      "[INFO] Epoch: 70 , batch: 320 , training loss: 3.986780\n",
      "[INFO] Epoch: 70 , batch: 321 , training loss: 3.834956\n",
      "[INFO] Epoch: 70 , batch: 322 , training loss: 3.930805\n",
      "[INFO] Epoch: 70 , batch: 323 , training loss: 3.942083\n",
      "[INFO] Epoch: 70 , batch: 324 , training loss: 3.914699\n",
      "[INFO] Epoch: 70 , batch: 325 , training loss: 4.044264\n",
      "[INFO] Epoch: 70 , batch: 326 , training loss: 4.071168\n",
      "[INFO] Epoch: 70 , batch: 327 , training loss: 4.038248\n",
      "[INFO] Epoch: 70 , batch: 328 , training loss: 4.048357\n",
      "[INFO] Epoch: 70 , batch: 329 , training loss: 3.942810\n",
      "[INFO] Epoch: 70 , batch: 330 , training loss: 3.937107\n",
      "[INFO] Epoch: 70 , batch: 331 , training loss: 4.099438\n",
      "[INFO] Epoch: 70 , batch: 332 , training loss: 3.932416\n",
      "[INFO] Epoch: 70 , batch: 333 , training loss: 3.915527\n",
      "[INFO] Epoch: 70 , batch: 334 , training loss: 3.939892\n",
      "[INFO] Epoch: 70 , batch: 335 , training loss: 4.063225\n",
      "[INFO] Epoch: 70 , batch: 336 , training loss: 4.055023\n",
      "[INFO] Epoch: 70 , batch: 337 , training loss: 4.119184\n",
      "[INFO] Epoch: 70 , batch: 338 , training loss: 4.316063\n",
      "[INFO] Epoch: 70 , batch: 339 , training loss: 4.139992\n",
      "[INFO] Epoch: 70 , batch: 340 , training loss: 4.307640\n",
      "[INFO] Epoch: 70 , batch: 341 , training loss: 4.076740\n",
      "[INFO] Epoch: 70 , batch: 342 , training loss: 3.861938\n",
      "[INFO] Epoch: 70 , batch: 343 , training loss: 3.927112\n",
      "[INFO] Epoch: 70 , batch: 344 , training loss: 3.794667\n",
      "[INFO] Epoch: 70 , batch: 345 , training loss: 3.925238\n",
      "[INFO] Epoch: 70 , batch: 346 , training loss: 3.990606\n",
      "[INFO] Epoch: 70 , batch: 347 , training loss: 3.883382\n",
      "[INFO] Epoch: 70 , batch: 348 , training loss: 3.977067\n",
      "[INFO] Epoch: 70 , batch: 349 , training loss: 4.068528\n",
      "[INFO] Epoch: 70 , batch: 350 , training loss: 3.957326\n",
      "[INFO] Epoch: 70 , batch: 351 , training loss: 4.009324\n",
      "[INFO] Epoch: 70 , batch: 352 , training loss: 4.030325\n",
      "[INFO] Epoch: 70 , batch: 353 , training loss: 4.018399\n",
      "[INFO] Epoch: 70 , batch: 354 , training loss: 4.114624\n",
      "[INFO] Epoch: 70 , batch: 355 , training loss: 4.105337\n",
      "[INFO] Epoch: 70 , batch: 356 , training loss: 3.957908\n",
      "[INFO] Epoch: 70 , batch: 357 , training loss: 4.012717\n",
      "[INFO] Epoch: 70 , batch: 358 , training loss: 3.973071\n",
      "[INFO] Epoch: 70 , batch: 359 , training loss: 3.948100\n",
      "[INFO] Epoch: 70 , batch: 360 , training loss: 4.049780\n",
      "[INFO] Epoch: 70 , batch: 361 , training loss: 4.006737\n",
      "[INFO] Epoch: 70 , batch: 362 , training loss: 4.134956\n",
      "[INFO] Epoch: 70 , batch: 363 , training loss: 4.012479\n",
      "[INFO] Epoch: 70 , batch: 364 , training loss: 4.074095\n",
      "[INFO] Epoch: 70 , batch: 365 , training loss: 4.000015\n",
      "[INFO] Epoch: 70 , batch: 366 , training loss: 4.082717\n",
      "[INFO] Epoch: 70 , batch: 367 , training loss: 4.123181\n",
      "[INFO] Epoch: 70 , batch: 368 , training loss: 4.567619\n",
      "[INFO] Epoch: 70 , batch: 369 , training loss: 4.205829\n",
      "[INFO] Epoch: 70 , batch: 370 , training loss: 3.986102\n",
      "[INFO] Epoch: 70 , batch: 371 , training loss: 4.384287\n",
      "[INFO] Epoch: 70 , batch: 372 , training loss: 4.651413\n",
      "[INFO] Epoch: 70 , batch: 373 , training loss: 4.680518\n",
      "[INFO] Epoch: 70 , batch: 374 , training loss: 4.856902\n",
      "[INFO] Epoch: 70 , batch: 375 , training loss: 4.811917\n",
      "[INFO] Epoch: 70 , batch: 376 , training loss: 4.678226\n",
      "[INFO] Epoch: 70 , batch: 377 , training loss: 4.435606\n",
      "[INFO] Epoch: 70 , batch: 378 , training loss: 4.520206\n",
      "[INFO] Epoch: 70 , batch: 379 , training loss: 4.520692\n",
      "[INFO] Epoch: 70 , batch: 380 , training loss: 4.684587\n",
      "[INFO] Epoch: 70 , batch: 381 , training loss: 4.373914\n",
      "[INFO] Epoch: 70 , batch: 382 , training loss: 4.575372\n",
      "[INFO] Epoch: 70 , batch: 383 , training loss: 4.691576\n",
      "[INFO] Epoch: 70 , batch: 384 , training loss: 4.627749\n",
      "[INFO] Epoch: 70 , batch: 385 , training loss: 4.327741\n",
      "[INFO] Epoch: 70 , batch: 386 , training loss: 4.556996\n",
      "[INFO] Epoch: 70 , batch: 387 , training loss: 4.491682\n",
      "[INFO] Epoch: 70 , batch: 388 , training loss: 4.336841\n",
      "[INFO] Epoch: 70 , batch: 389 , training loss: 4.150434\n",
      "[INFO] Epoch: 70 , batch: 390 , training loss: 4.188634\n",
      "[INFO] Epoch: 70 , batch: 391 , training loss: 4.206225\n",
      "[INFO] Epoch: 70 , batch: 392 , training loss: 4.568298\n",
      "[INFO] Epoch: 70 , batch: 393 , training loss: 4.457922\n",
      "[INFO] Epoch: 70 , batch: 394 , training loss: 4.572829\n",
      "[INFO] Epoch: 70 , batch: 395 , training loss: 4.388854\n",
      "[INFO] Epoch: 70 , batch: 396 , training loss: 4.206048\n",
      "[INFO] Epoch: 70 , batch: 397 , training loss: 4.331397\n",
      "[INFO] Epoch: 70 , batch: 398 , training loss: 4.216931\n",
      "[INFO] Epoch: 70 , batch: 399 , training loss: 4.273998\n",
      "[INFO] Epoch: 70 , batch: 400 , training loss: 4.252900\n",
      "[INFO] Epoch: 70 , batch: 401 , training loss: 4.680180\n",
      "[INFO] Epoch: 70 , batch: 402 , training loss: 4.419765\n",
      "[INFO] Epoch: 70 , batch: 403 , training loss: 4.240500\n",
      "[INFO] Epoch: 70 , batch: 404 , training loss: 4.415846\n",
      "[INFO] Epoch: 70 , batch: 405 , training loss: 4.450310\n",
      "[INFO] Epoch: 70 , batch: 406 , training loss: 4.377639\n",
      "[INFO] Epoch: 70 , batch: 407 , training loss: 4.391464\n",
      "[INFO] Epoch: 70 , batch: 408 , training loss: 4.393113\n",
      "[INFO] Epoch: 70 , batch: 409 , training loss: 4.402905\n",
      "[INFO] Epoch: 70 , batch: 410 , training loss: 4.438695\n",
      "[INFO] Epoch: 70 , batch: 411 , training loss: 4.615510\n",
      "[INFO] Epoch: 70 , batch: 412 , training loss: 4.429199\n",
      "[INFO] Epoch: 70 , batch: 413 , training loss: 4.298773\n",
      "[INFO] Epoch: 70 , batch: 414 , training loss: 4.381162\n",
      "[INFO] Epoch: 70 , batch: 415 , training loss: 4.401103\n",
      "[INFO] Epoch: 70 , batch: 416 , training loss: 4.465669\n",
      "[INFO] Epoch: 70 , batch: 417 , training loss: 4.385637\n",
      "[INFO] Epoch: 70 , batch: 418 , training loss: 4.454966\n",
      "[INFO] Epoch: 70 , batch: 419 , training loss: 4.384732\n",
      "[INFO] Epoch: 70 , batch: 420 , training loss: 4.367378\n",
      "[INFO] Epoch: 70 , batch: 421 , training loss: 4.366716\n",
      "[INFO] Epoch: 70 , batch: 422 , training loss: 4.206481\n",
      "[INFO] Epoch: 70 , batch: 423 , training loss: 4.433544\n",
      "[INFO] Epoch: 70 , batch: 424 , training loss: 4.583065\n",
      "[INFO] Epoch: 70 , batch: 425 , training loss: 4.453084\n",
      "[INFO] Epoch: 70 , batch: 426 , training loss: 4.212158\n",
      "[INFO] Epoch: 70 , batch: 427 , training loss: 4.450024\n",
      "[INFO] Epoch: 70 , batch: 428 , training loss: 4.318861\n",
      "[INFO] Epoch: 70 , batch: 429 , training loss: 4.191805\n",
      "[INFO] Epoch: 70 , batch: 430 , training loss: 4.426127\n",
      "[INFO] Epoch: 70 , batch: 431 , training loss: 4.069026\n",
      "[INFO] Epoch: 70 , batch: 432 , training loss: 4.107086\n",
      "[INFO] Epoch: 70 , batch: 433 , training loss: 4.151616\n",
      "[INFO] Epoch: 70 , batch: 434 , training loss: 4.028954\n",
      "[INFO] Epoch: 70 , batch: 435 , training loss: 4.375873\n",
      "[INFO] Epoch: 70 , batch: 436 , training loss: 4.422793\n",
      "[INFO] Epoch: 70 , batch: 437 , training loss: 4.219856\n",
      "[INFO] Epoch: 70 , batch: 438 , training loss: 4.081850\n",
      "[INFO] Epoch: 70 , batch: 439 , training loss: 4.318229\n",
      "[INFO] Epoch: 70 , batch: 440 , training loss: 4.412678\n",
      "[INFO] Epoch: 70 , batch: 441 , training loss: 4.522448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 70 , batch: 442 , training loss: 4.284900\n",
      "[INFO] Epoch: 70 , batch: 443 , training loss: 4.434328\n",
      "[INFO] Epoch: 70 , batch: 444 , training loss: 4.078905\n",
      "[INFO] Epoch: 70 , batch: 445 , training loss: 3.961398\n",
      "[INFO] Epoch: 70 , batch: 446 , training loss: 3.899685\n",
      "[INFO] Epoch: 70 , batch: 447 , training loss: 4.103964\n",
      "[INFO] Epoch: 70 , batch: 448 , training loss: 4.217112\n",
      "[INFO] Epoch: 70 , batch: 449 , training loss: 4.599840\n",
      "[INFO] Epoch: 70 , batch: 450 , training loss: 4.678648\n",
      "[INFO] Epoch: 70 , batch: 451 , training loss: 4.580640\n",
      "[INFO] Epoch: 70 , batch: 452 , training loss: 4.400016\n",
      "[INFO] Epoch: 70 , batch: 453 , training loss: 4.127885\n",
      "[INFO] Epoch: 70 , batch: 454 , training loss: 4.313208\n",
      "[INFO] Epoch: 70 , batch: 455 , training loss: 4.348167\n",
      "[INFO] Epoch: 70 , batch: 456 , training loss: 4.349704\n",
      "[INFO] Epoch: 70 , batch: 457 , training loss: 4.442665\n",
      "[INFO] Epoch: 70 , batch: 458 , training loss: 4.183940\n",
      "[INFO] Epoch: 70 , batch: 459 , training loss: 4.148580\n",
      "[INFO] Epoch: 70 , batch: 460 , training loss: 4.280207\n",
      "[INFO] Epoch: 70 , batch: 461 , training loss: 4.230157\n",
      "[INFO] Epoch: 70 , batch: 462 , training loss: 4.303921\n",
      "[INFO] Epoch: 70 , batch: 463 , training loss: 4.221958\n",
      "[INFO] Epoch: 70 , batch: 464 , training loss: 4.378618\n",
      "[INFO] Epoch: 70 , batch: 465 , training loss: 4.338156\n",
      "[INFO] Epoch: 70 , batch: 466 , training loss: 4.423291\n",
      "[INFO] Epoch: 70 , batch: 467 , training loss: 4.396364\n",
      "[INFO] Epoch: 70 , batch: 468 , training loss: 4.342587\n",
      "[INFO] Epoch: 70 , batch: 469 , training loss: 4.376839\n",
      "[INFO] Epoch: 70 , batch: 470 , training loss: 4.189740\n",
      "[INFO] Epoch: 70 , batch: 471 , training loss: 4.295605\n",
      "[INFO] Epoch: 70 , batch: 472 , training loss: 4.339961\n",
      "[INFO] Epoch: 70 , batch: 473 , training loss: 4.283571\n",
      "[INFO] Epoch: 70 , batch: 474 , training loss: 4.070049\n",
      "[INFO] Epoch: 70 , batch: 475 , training loss: 3.943996\n",
      "[INFO] Epoch: 70 , batch: 476 , training loss: 4.320066\n",
      "[INFO] Epoch: 70 , batch: 477 , training loss: 4.455235\n",
      "[INFO] Epoch: 70 , batch: 478 , training loss: 4.442595\n",
      "[INFO] Epoch: 70 , batch: 479 , training loss: 4.439429\n",
      "[INFO] Epoch: 70 , batch: 480 , training loss: 4.557671\n",
      "[INFO] Epoch: 70 , batch: 481 , training loss: 4.432984\n",
      "[INFO] Epoch: 70 , batch: 482 , training loss: 4.535041\n",
      "[INFO] Epoch: 70 , batch: 483 , training loss: 4.375350\n",
      "[INFO] Epoch: 70 , batch: 484 , training loss: 4.168713\n",
      "[INFO] Epoch: 70 , batch: 485 , training loss: 4.285915\n",
      "[INFO] Epoch: 70 , batch: 486 , training loss: 4.180410\n",
      "[INFO] Epoch: 70 , batch: 487 , training loss: 4.174362\n",
      "[INFO] Epoch: 70 , batch: 488 , training loss: 4.342393\n",
      "[INFO] Epoch: 70 , batch: 489 , training loss: 4.253716\n",
      "[INFO] Epoch: 70 , batch: 490 , training loss: 4.312077\n",
      "[INFO] Epoch: 70 , batch: 491 , training loss: 4.243254\n",
      "[INFO] Epoch: 70 , batch: 492 , training loss: 4.198280\n",
      "[INFO] Epoch: 70 , batch: 493 , training loss: 4.376577\n",
      "[INFO] Epoch: 70 , batch: 494 , training loss: 4.276661\n",
      "[INFO] Epoch: 70 , batch: 495 , training loss: 4.443830\n",
      "[INFO] Epoch: 70 , batch: 496 , training loss: 4.305605\n",
      "[INFO] Epoch: 70 , batch: 497 , training loss: 4.348342\n",
      "[INFO] Epoch: 70 , batch: 498 , training loss: 4.335656\n",
      "[INFO] Epoch: 70 , batch: 499 , training loss: 4.384064\n",
      "[INFO] Epoch: 70 , batch: 500 , training loss: 4.569158\n",
      "[INFO] Epoch: 70 , batch: 501 , training loss: 4.866638\n",
      "[INFO] Epoch: 70 , batch: 502 , training loss: 4.889553\n",
      "[INFO] Epoch: 70 , batch: 503 , training loss: 4.506459\n",
      "[INFO] Epoch: 70 , batch: 504 , training loss: 4.650107\n",
      "[INFO] Epoch: 70 , batch: 505 , training loss: 4.620488\n",
      "[INFO] Epoch: 70 , batch: 506 , training loss: 4.593735\n",
      "[INFO] Epoch: 70 , batch: 507 , training loss: 4.647168\n",
      "[INFO] Epoch: 70 , batch: 508 , training loss: 4.558119\n",
      "[INFO] Epoch: 70 , batch: 509 , training loss: 4.396779\n",
      "[INFO] Epoch: 70 , batch: 510 , training loss: 4.485053\n",
      "[INFO] Epoch: 70 , batch: 511 , training loss: 4.398034\n",
      "[INFO] Epoch: 70 , batch: 512 , training loss: 4.481439\n",
      "[INFO] Epoch: 70 , batch: 513 , training loss: 4.755519\n",
      "[INFO] Epoch: 70 , batch: 514 , training loss: 4.393709\n",
      "[INFO] Epoch: 70 , batch: 515 , training loss: 4.638509\n",
      "[INFO] Epoch: 70 , batch: 516 , training loss: 4.456126\n",
      "[INFO] Epoch: 70 , batch: 517 , training loss: 4.412032\n",
      "[INFO] Epoch: 70 , batch: 518 , training loss: 4.369004\n",
      "[INFO] Epoch: 70 , batch: 519 , training loss: 4.205992\n",
      "[INFO] Epoch: 70 , batch: 520 , training loss: 4.457098\n",
      "[INFO] Epoch: 70 , batch: 521 , training loss: 4.430035\n",
      "[INFO] Epoch: 70 , batch: 522 , training loss: 4.508365\n",
      "[INFO] Epoch: 70 , batch: 523 , training loss: 4.447928\n",
      "[INFO] Epoch: 70 , batch: 524 , training loss: 4.725818\n",
      "[INFO] Epoch: 70 , batch: 525 , training loss: 4.612564\n",
      "[INFO] Epoch: 70 , batch: 526 , training loss: 4.369947\n",
      "[INFO] Epoch: 70 , batch: 527 , training loss: 4.423465\n",
      "[INFO] Epoch: 70 , batch: 528 , training loss: 4.441450\n",
      "[INFO] Epoch: 70 , batch: 529 , training loss: 4.424105\n",
      "[INFO] Epoch: 70 , batch: 530 , training loss: 4.282765\n",
      "[INFO] Epoch: 70 , batch: 531 , training loss: 4.410751\n",
      "[INFO] Epoch: 70 , batch: 532 , training loss: 4.324464\n",
      "[INFO] Epoch: 70 , batch: 533 , training loss: 4.448978\n",
      "[INFO] Epoch: 70 , batch: 534 , training loss: 4.452016\n",
      "[INFO] Epoch: 70 , batch: 535 , training loss: 4.442339\n",
      "[INFO] Epoch: 70 , batch: 536 , training loss: 4.314161\n",
      "[INFO] Epoch: 70 , batch: 537 , training loss: 4.309090\n",
      "[INFO] Epoch: 70 , batch: 538 , training loss: 4.386129\n",
      "[INFO] Epoch: 70 , batch: 539 , training loss: 4.507208\n",
      "[INFO] Epoch: 70 , batch: 540 , training loss: 4.995988\n",
      "[INFO] Epoch: 70 , batch: 541 , training loss: 4.836198\n",
      "[INFO] Epoch: 70 , batch: 542 , training loss: 4.714496\n",
      "[INFO] Epoch: 71 , batch: 0 , training loss: 3.690665\n",
      "[INFO] Epoch: 71 , batch: 1 , training loss: 3.559929\n",
      "[INFO] Epoch: 71 , batch: 2 , training loss: 3.682344\n",
      "[INFO] Epoch: 71 , batch: 3 , training loss: 3.595712\n",
      "[INFO] Epoch: 71 , batch: 4 , training loss: 4.006678\n",
      "[INFO] Epoch: 71 , batch: 5 , training loss: 3.628470\n",
      "[INFO] Epoch: 71 , batch: 6 , training loss: 4.030914\n",
      "[INFO] Epoch: 71 , batch: 7 , training loss: 3.967408\n",
      "[INFO] Epoch: 71 , batch: 8 , training loss: 3.610764\n",
      "[INFO] Epoch: 71 , batch: 9 , training loss: 3.889699\n",
      "[INFO] Epoch: 71 , batch: 10 , training loss: 3.861846\n",
      "[INFO] Epoch: 71 , batch: 11 , training loss: 3.761658\n",
      "[INFO] Epoch: 71 , batch: 12 , training loss: 3.696278\n",
      "[INFO] Epoch: 71 , batch: 13 , training loss: 3.696413\n",
      "[INFO] Epoch: 71 , batch: 14 , training loss: 3.623126\n",
      "[INFO] Epoch: 71 , batch: 15 , training loss: 3.853558\n",
      "[INFO] Epoch: 71 , batch: 16 , training loss: 3.642842\n",
      "[INFO] Epoch: 71 , batch: 17 , training loss: 3.783003\n",
      "[INFO] Epoch: 71 , batch: 18 , training loss: 3.759134\n",
      "[INFO] Epoch: 71 , batch: 19 , training loss: 3.523335\n",
      "[INFO] Epoch: 71 , batch: 20 , training loss: 3.525566\n",
      "[INFO] Epoch: 71 , batch: 21 , training loss: 3.632343\n",
      "[INFO] Epoch: 71 , batch: 22 , training loss: 3.514957\n",
      "[INFO] Epoch: 71 , batch: 23 , training loss: 3.723297\n",
      "[INFO] Epoch: 71 , batch: 24 , training loss: 3.569509\n",
      "[INFO] Epoch: 71 , batch: 25 , training loss: 3.699008\n",
      "[INFO] Epoch: 71 , batch: 26 , training loss: 3.540399\n",
      "[INFO] Epoch: 71 , batch: 27 , training loss: 3.533233\n",
      "[INFO] Epoch: 71 , batch: 28 , training loss: 3.708106\n",
      "[INFO] Epoch: 71 , batch: 29 , training loss: 3.534937\n",
      "[INFO] Epoch: 71 , batch: 30 , training loss: 3.546198\n",
      "[INFO] Epoch: 71 , batch: 31 , training loss: 3.668174\n",
      "[INFO] Epoch: 71 , batch: 32 , training loss: 3.597412\n",
      "[INFO] Epoch: 71 , batch: 33 , training loss: 3.642770\n",
      "[INFO] Epoch: 71 , batch: 34 , training loss: 3.636115\n",
      "[INFO] Epoch: 71 , batch: 35 , training loss: 3.561132\n",
      "[INFO] Epoch: 71 , batch: 36 , training loss: 3.694074\n",
      "[INFO] Epoch: 71 , batch: 37 , training loss: 3.553772\n",
      "[INFO] Epoch: 71 , batch: 38 , training loss: 3.608609\n",
      "[INFO] Epoch: 71 , batch: 39 , training loss: 3.460622\n",
      "[INFO] Epoch: 71 , batch: 40 , training loss: 3.630471\n",
      "[INFO] Epoch: 71 , batch: 41 , training loss: 3.640472\n",
      "[INFO] Epoch: 71 , batch: 42 , training loss: 4.116418\n",
      "[INFO] Epoch: 71 , batch: 43 , training loss: 3.864707\n",
      "[INFO] Epoch: 71 , batch: 44 , training loss: 4.172191\n",
      "[INFO] Epoch: 71 , batch: 45 , training loss: 4.131665\n",
      "[INFO] Epoch: 71 , batch: 46 , training loss: 4.114682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 71 , batch: 47 , training loss: 3.648779\n",
      "[INFO] Epoch: 71 , batch: 48 , training loss: 3.710926\n",
      "[INFO] Epoch: 71 , batch: 49 , training loss: 3.909215\n",
      "[INFO] Epoch: 71 , batch: 50 , training loss: 3.648517\n",
      "[INFO] Epoch: 71 , batch: 51 , training loss: 3.837903\n",
      "[INFO] Epoch: 71 , batch: 52 , training loss: 3.640787\n",
      "[INFO] Epoch: 71 , batch: 53 , training loss: 3.807535\n",
      "[INFO] Epoch: 71 , batch: 54 , training loss: 3.786585\n",
      "[INFO] Epoch: 71 , batch: 55 , training loss: 3.846004\n",
      "[INFO] Epoch: 71 , batch: 56 , training loss: 3.703142\n",
      "[INFO] Epoch: 71 , batch: 57 , training loss: 3.610433\n",
      "[INFO] Epoch: 71 , batch: 58 , training loss: 3.686702\n",
      "[INFO] Epoch: 71 , batch: 59 , training loss: 3.730376\n",
      "[INFO] Epoch: 71 , batch: 60 , training loss: 3.641569\n",
      "[INFO] Epoch: 71 , batch: 61 , training loss: 3.777334\n",
      "[INFO] Epoch: 71 , batch: 62 , training loss: 3.644519\n",
      "[INFO] Epoch: 71 , batch: 63 , training loss: 3.825909\n",
      "[INFO] Epoch: 71 , batch: 64 , training loss: 4.021470\n",
      "[INFO] Epoch: 71 , batch: 65 , training loss: 3.748111\n",
      "[INFO] Epoch: 71 , batch: 66 , training loss: 3.601191\n",
      "[INFO] Epoch: 71 , batch: 67 , training loss: 3.630399\n",
      "[INFO] Epoch: 71 , batch: 68 , training loss: 3.779844\n",
      "[INFO] Epoch: 71 , batch: 69 , training loss: 3.712213\n",
      "[INFO] Epoch: 71 , batch: 70 , training loss: 3.914662\n",
      "[INFO] Epoch: 71 , batch: 71 , training loss: 3.754627\n",
      "[INFO] Epoch: 71 , batch: 72 , training loss: 3.803759\n",
      "[INFO] Epoch: 71 , batch: 73 , training loss: 3.799761\n",
      "[INFO] Epoch: 71 , batch: 74 , training loss: 3.864536\n",
      "[INFO] Epoch: 71 , batch: 75 , training loss: 3.739072\n",
      "[INFO] Epoch: 71 , batch: 76 , training loss: 3.869316\n",
      "[INFO] Epoch: 71 , batch: 77 , training loss: 3.803097\n",
      "[INFO] Epoch: 71 , batch: 78 , training loss: 3.888578\n",
      "[INFO] Epoch: 71 , batch: 79 , training loss: 3.726357\n",
      "[INFO] Epoch: 71 , batch: 80 , training loss: 3.944198\n",
      "[INFO] Epoch: 71 , batch: 81 , training loss: 3.867841\n",
      "[INFO] Epoch: 71 , batch: 82 , training loss: 3.829638\n",
      "[INFO] Epoch: 71 , batch: 83 , training loss: 3.932039\n",
      "[INFO] Epoch: 71 , batch: 84 , training loss: 3.909399\n",
      "[INFO] Epoch: 71 , batch: 85 , training loss: 3.963166\n",
      "[INFO] Epoch: 71 , batch: 86 , training loss: 3.898866\n",
      "[INFO] Epoch: 71 , batch: 87 , training loss: 3.863770\n",
      "[INFO] Epoch: 71 , batch: 88 , training loss: 4.005477\n",
      "[INFO] Epoch: 71 , batch: 89 , training loss: 3.773530\n",
      "[INFO] Epoch: 71 , batch: 90 , training loss: 3.862224\n",
      "[INFO] Epoch: 71 , batch: 91 , training loss: 3.794001\n",
      "[INFO] Epoch: 71 , batch: 92 , training loss: 3.862668\n",
      "[INFO] Epoch: 71 , batch: 93 , training loss: 3.971514\n",
      "[INFO] Epoch: 71 , batch: 94 , training loss: 4.046040\n",
      "[INFO] Epoch: 71 , batch: 95 , training loss: 3.865351\n",
      "[INFO] Epoch: 71 , batch: 96 , training loss: 3.843456\n",
      "[INFO] Epoch: 71 , batch: 97 , training loss: 3.770610\n",
      "[INFO] Epoch: 71 , batch: 98 , training loss: 3.713408\n",
      "[INFO] Epoch: 71 , batch: 99 , training loss: 3.842299\n",
      "[INFO] Epoch: 71 , batch: 100 , training loss: 3.748413\n",
      "[INFO] Epoch: 71 , batch: 101 , training loss: 3.761601\n",
      "[INFO] Epoch: 71 , batch: 102 , training loss: 3.915024\n",
      "[INFO] Epoch: 71 , batch: 103 , training loss: 3.721610\n",
      "[INFO] Epoch: 71 , batch: 104 , training loss: 3.685209\n",
      "[INFO] Epoch: 71 , batch: 105 , training loss: 3.926159\n",
      "[INFO] Epoch: 71 , batch: 106 , training loss: 3.949166\n",
      "[INFO] Epoch: 71 , batch: 107 , training loss: 3.775015\n",
      "[INFO] Epoch: 71 , batch: 108 , training loss: 3.732310\n",
      "[INFO] Epoch: 71 , batch: 109 , training loss: 3.625136\n",
      "[INFO] Epoch: 71 , batch: 110 , training loss: 3.798110\n",
      "[INFO] Epoch: 71 , batch: 111 , training loss: 3.923813\n",
      "[INFO] Epoch: 71 , batch: 112 , training loss: 3.832707\n",
      "[INFO] Epoch: 71 , batch: 113 , training loss: 3.816747\n",
      "[INFO] Epoch: 71 , batch: 114 , training loss: 3.808860\n",
      "[INFO] Epoch: 71 , batch: 115 , training loss: 3.792870\n",
      "[INFO] Epoch: 71 , batch: 116 , training loss: 3.696069\n",
      "[INFO] Epoch: 71 , batch: 117 , training loss: 3.920988\n",
      "[INFO] Epoch: 71 , batch: 118 , training loss: 3.941185\n",
      "[INFO] Epoch: 71 , batch: 119 , training loss: 4.021389\n",
      "[INFO] Epoch: 71 , batch: 120 , training loss: 4.051447\n",
      "[INFO] Epoch: 71 , batch: 121 , training loss: 3.909956\n",
      "[INFO] Epoch: 71 , batch: 122 , training loss: 3.822297\n",
      "[INFO] Epoch: 71 , batch: 123 , training loss: 3.799004\n",
      "[INFO] Epoch: 71 , batch: 124 , training loss: 3.916529\n",
      "[INFO] Epoch: 71 , batch: 125 , training loss: 3.743170\n",
      "[INFO] Epoch: 71 , batch: 126 , training loss: 3.716215\n",
      "[INFO] Epoch: 71 , batch: 127 , training loss: 3.723437\n",
      "[INFO] Epoch: 71 , batch: 128 , training loss: 3.884757\n",
      "[INFO] Epoch: 71 , batch: 129 , training loss: 3.807959\n",
      "[INFO] Epoch: 71 , batch: 130 , training loss: 3.829435\n",
      "[INFO] Epoch: 71 , batch: 131 , training loss: 3.825685\n",
      "[INFO] Epoch: 71 , batch: 132 , training loss: 3.872117\n",
      "[INFO] Epoch: 71 , batch: 133 , training loss: 3.800410\n",
      "[INFO] Epoch: 71 , batch: 134 , training loss: 3.580334\n",
      "[INFO] Epoch: 71 , batch: 135 , training loss: 3.658588\n",
      "[INFO] Epoch: 71 , batch: 136 , training loss: 3.927766\n",
      "[INFO] Epoch: 71 , batch: 137 , training loss: 3.871222\n",
      "[INFO] Epoch: 71 , batch: 138 , training loss: 3.903986\n",
      "[INFO] Epoch: 71 , batch: 139 , training loss: 4.476770\n",
      "[INFO] Epoch: 71 , batch: 140 , training loss: 4.263150\n",
      "[INFO] Epoch: 71 , batch: 141 , training loss: 4.025318\n",
      "[INFO] Epoch: 71 , batch: 142 , training loss: 3.765748\n",
      "[INFO] Epoch: 71 , batch: 143 , training loss: 3.890779\n",
      "[INFO] Epoch: 71 , batch: 144 , training loss: 3.729531\n",
      "[INFO] Epoch: 71 , batch: 145 , training loss: 3.780874\n",
      "[INFO] Epoch: 71 , batch: 146 , training loss: 4.039622\n",
      "[INFO] Epoch: 71 , batch: 147 , training loss: 3.670000\n",
      "[INFO] Epoch: 71 , batch: 148 , training loss: 3.661017\n",
      "[INFO] Epoch: 71 , batch: 149 , training loss: 3.719647\n",
      "[INFO] Epoch: 71 , batch: 150 , training loss: 3.952975\n",
      "[INFO] Epoch: 71 , batch: 151 , training loss: 3.826621\n",
      "[INFO] Epoch: 71 , batch: 152 , training loss: 3.857443\n",
      "[INFO] Epoch: 71 , batch: 153 , training loss: 3.920436\n",
      "[INFO] Epoch: 71 , batch: 154 , training loss: 3.977155\n",
      "[INFO] Epoch: 71 , batch: 155 , training loss: 4.202552\n",
      "[INFO] Epoch: 71 , batch: 156 , training loss: 3.907380\n",
      "[INFO] Epoch: 71 , batch: 157 , training loss: 3.863760\n",
      "[INFO] Epoch: 71 , batch: 158 , training loss: 3.988189\n",
      "[INFO] Epoch: 71 , batch: 159 , training loss: 3.898955\n",
      "[INFO] Epoch: 71 , batch: 160 , training loss: 4.147011\n",
      "[INFO] Epoch: 71 , batch: 161 , training loss: 4.138029\n",
      "[INFO] Epoch: 71 , batch: 162 , training loss: 4.126951\n",
      "[INFO] Epoch: 71 , batch: 163 , training loss: 4.335560\n",
      "[INFO] Epoch: 71 , batch: 164 , training loss: 4.259895\n",
      "[INFO] Epoch: 71 , batch: 165 , training loss: 4.213264\n",
      "[INFO] Epoch: 71 , batch: 166 , training loss: 4.092156\n",
      "[INFO] Epoch: 71 , batch: 167 , training loss: 4.064092\n",
      "[INFO] Epoch: 71 , batch: 168 , training loss: 3.833397\n",
      "[INFO] Epoch: 71 , batch: 169 , training loss: 3.830778\n",
      "[INFO] Epoch: 71 , batch: 170 , training loss: 3.977748\n",
      "[INFO] Epoch: 71 , batch: 171 , training loss: 3.469764\n",
      "[INFO] Epoch: 71 , batch: 172 , training loss: 3.688556\n",
      "[INFO] Epoch: 71 , batch: 173 , training loss: 4.008731\n",
      "[INFO] Epoch: 71 , batch: 174 , training loss: 4.445549\n",
      "[INFO] Epoch: 71 , batch: 175 , training loss: 4.713698\n",
      "[INFO] Epoch: 71 , batch: 176 , training loss: 4.370991\n",
      "[INFO] Epoch: 71 , batch: 177 , training loss: 4.005568\n",
      "[INFO] Epoch: 71 , batch: 178 , training loss: 4.010109\n",
      "[INFO] Epoch: 71 , batch: 179 , training loss: 4.067288\n",
      "[INFO] Epoch: 71 , batch: 180 , training loss: 3.994331\n",
      "[INFO] Epoch: 71 , batch: 181 , training loss: 4.306825\n",
      "[INFO] Epoch: 71 , batch: 182 , training loss: 4.232622\n",
      "[INFO] Epoch: 71 , batch: 183 , training loss: 4.221031\n",
      "[INFO] Epoch: 71 , batch: 184 , training loss: 4.111502\n",
      "[INFO] Epoch: 71 , batch: 185 , training loss: 4.071059\n",
      "[INFO] Epoch: 71 , batch: 186 , training loss: 4.217235\n",
      "[INFO] Epoch: 71 , batch: 187 , training loss: 4.311604\n",
      "[INFO] Epoch: 71 , batch: 188 , training loss: 4.292547\n",
      "[INFO] Epoch: 71 , batch: 189 , training loss: 4.213118\n",
      "[INFO] Epoch: 71 , batch: 190 , training loss: 4.251404\n",
      "[INFO] Epoch: 71 , batch: 191 , training loss: 4.362207\n",
      "[INFO] Epoch: 71 , batch: 192 , training loss: 4.208111\n",
      "[INFO] Epoch: 71 , batch: 193 , training loss: 4.295404\n",
      "[INFO] Epoch: 71 , batch: 194 , training loss: 4.246377\n",
      "[INFO] Epoch: 71 , batch: 195 , training loss: 4.177312\n",
      "[INFO] Epoch: 71 , batch: 196 , training loss: 4.035015\n",
      "[INFO] Epoch: 71 , batch: 197 , training loss: 4.117767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Epoch: 71 , batch: 198 , training loss: 4.054200\n",
      "[INFO] Epoch: 71 , batch: 199 , training loss: 4.199848\n",
      "[INFO] Epoch: 71 , batch: 200 , training loss: 4.067529\n",
      "[INFO] Epoch: 71 , batch: 201 , training loss: 3.976644\n",
      "[INFO] Epoch: 71 , batch: 202 , training loss: 3.971895\n",
      "[INFO] Epoch: 71 , batch: 203 , training loss: 4.120359\n",
      "[INFO] Epoch: 71 , batch: 204 , training loss: 4.168664\n",
      "[INFO] Epoch: 71 , batch: 205 , training loss: 3.800377\n",
      "[INFO] Epoch: 71 , batch: 206 , training loss: 3.732292\n",
      "[INFO] Epoch: 71 , batch: 207 , training loss: 3.720130\n",
      "[INFO] Epoch: 71 , batch: 208 , training loss: 4.044847\n",
      "[INFO] Epoch: 71 , batch: 209 , training loss: 4.038990\n",
      "[INFO] Epoch: 71 , batch: 210 , training loss: 4.022948\n",
      "[INFO] Epoch: 71 , batch: 211 , training loss: 4.021272\n",
      "[INFO] Epoch: 71 , batch: 212 , training loss: 4.123607\n",
      "[INFO] Epoch: 71 , batch: 213 , training loss: 4.075883\n",
      "[INFO] Epoch: 71 , batch: 214 , training loss: 4.138929\n",
      "[INFO] Epoch: 71 , batch: 215 , training loss: 4.358628\n",
      "[INFO] Epoch: 71 , batch: 216 , training loss: 4.057745\n",
      "[INFO] Epoch: 71 , batch: 217 , training loss: 4.018467\n",
      "[INFO] Epoch: 71 , batch: 218 , training loss: 3.977419\n",
      "[INFO] Epoch: 71 , batch: 219 , training loss: 4.097092\n",
      "[INFO] Epoch: 71 , batch: 220 , training loss: 3.938724\n",
      "[INFO] Epoch: 71 , batch: 221 , training loss: 3.938088\n",
      "[INFO] Epoch: 71 , batch: 222 , training loss: 4.091670\n",
      "[INFO] Epoch: 71 , batch: 223 , training loss: 4.224132\n",
      "[INFO] Epoch: 71 , batch: 224 , training loss: 4.240677\n",
      "[INFO] Epoch: 71 , batch: 225 , training loss: 4.143308\n",
      "[INFO] Epoch: 71 , batch: 226 , training loss: 4.252916\n",
      "[INFO] Epoch: 71 , batch: 227 , training loss: 4.235940\n",
      "[INFO] Epoch: 71 , batch: 228 , training loss: 4.232897\n",
      "[INFO] Epoch: 71 , batch: 229 , training loss: 4.087926\n",
      "[INFO] Epoch: 71 , batch: 230 , training loss: 3.962312\n",
      "[INFO] Epoch: 71 , batch: 231 , training loss: 3.827999\n",
      "[INFO] Epoch: 71 , batch: 232 , training loss: 3.970853\n",
      "[INFO] Epoch: 71 , batch: 233 , training loss: 4.004991\n",
      "[INFO] Epoch: 71 , batch: 234 , training loss: 3.693733\n",
      "[INFO] Epoch: 71 , batch: 235 , training loss: 3.792747\n",
      "[INFO] Epoch: 71 , batch: 236 , training loss: 3.892918\n",
      "[INFO] Epoch: 71 , batch: 237 , training loss: 4.108564\n",
      "[INFO] Epoch: 71 , batch: 238 , training loss: 3.929205\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "print('[INFO] train tang poem...')\n",
    "run_training('./poem_generator_500_epoch') # 训练模型\n",
    "#训练模型时间比较长，训练模型完成后每次生成诗歌的时，不需要再次训练 ，可以注销上面的 run_training()。生成部分执行速度很快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "print('[INFO] write tang poem...')\n",
    "poem2 = gen_poem('洋','./poem_generator')# 生成诗歌\n",
    "print(\"#\" * 25)\n",
    "pretty_print_poem(poem2)\n",
    "print('#' * 25)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
