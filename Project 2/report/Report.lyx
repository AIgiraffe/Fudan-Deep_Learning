#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass ctex-article
\begin_preamble
\usepackage{amsmath}
\usepackage{bm}
\usepackage{ctex}

\usepackage{color}
\usepackage{fancyvrb}
\usepackage{CJKutf8}
\usepackage[overlap, CJK]{ruby}
\newenvironment{SChinese}{%
\CJKfamily{gbsn}%
\CJKtilde
\CJKnospace}{}
\newcommand{\cntxt}[1]{\begin{CJK}{UTF8}{}\begin{SChinese}#1\end{SChinese}\end{CJK}}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
% set default figure placement to htbp
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language chinese-simplified
\language_package default
\inputencoding utf8-plain
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\font_cjk gbsn
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
用RNN网络模型写唐诗
\end_layout

\begin_layout Author
Linyang He
\end_layout

\begin_layout Standard
本文以写唐诗这个情景为例，重点关注了RNN模型（以及LSTM,GRU模型）在时间序列问题上的应用。本文先会讲述各个RNN模型，再描述了唐诗生成的过程。接着展示了
以“日 、 红 、 山 、 夜 、 湖、 海 、 月”等字为首字的诗歌。最后是实验的总结，并提出了一些自己的思考。
\end_layout

\begin_layout Section
Background
\end_layout

\begin_layout Subsection
RNN
\end_layout

\begin_layout Standard
当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列。为了解决一些这样类似的问题，能够更好的处理序列的信息，RNN就
诞生了。递归神经网络（Recurrent Neural Networks，RNN）对于时序信息的处理有着天然的优异性。RNN的特点在于，会把上个时序的hidde
n_state信息作为这个时序的输入，这样，就实现了不同时序上的信息的沟通。下图是一个单独的RNN Unit示意图。
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig1.png
	scale 20

\end_inset


\end_layout

\begin_layout Standard
可以发现，隐藏层中间有个w在循环的输入输出，这便是hidden_state。如果展开来看，可以得到如下图像，这也是平时我们常见的RNN图像。
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig2.png
	scale 25

\end_inset


\end_layout

\begin_layout Standard
具体而言，假设在时刻
\begin_inset Formula $t$
\end_inset

时，网络的输入为
\begin_inset Formula $x_{t}$
\end_inset

，隐层状态（即隐层神经元活性值）为
\begin_inset Formula $h_{t}$
\end_inset

 不仅和当前时刻的输入
\begin_inset Formula $x_{t}$
\end_inset

 相关，也和上一个时刻的隐层状态
\begin_inset Formula $h_{t-1}$
\end_inset

 相关。 
\begin_inset Formula $f(·)$
\end_inset

是非线性激活函数，通常为logistic函数或tanh 函数，
\begin_inset Formula $U$
\end_inset

 为状态-状态权重矩阵，
\begin_inset Formula $W$
\end_inset

 为状态-输入权重矩阵, 那么，RNN的运算逻辑通常可以表示成：
\begin_inset ERT
status open

\begin_layout Plain Layout

$$h_t = f(Wx_t + Uh_{t−1})$$
\end_layout

\end_inset

.
 总之，链式的特征揭示了 RNN 本质上是与序列和列表相关的。他们是对于这类数据的最自然的神经网络架构。
\end_layout

\begin_layout Subsection
LSTM
\end_layout

\begin_layout Standard
RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是
真的可以么？答案是，还有很多依赖因素。 有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着
预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，
相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。
\end_layout

\begin_layout Standard
但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France...
 I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前
位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。 不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远
的信息的能力。在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这
些知识。Bengio, et al.
 (1994)等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。 然而，幸运的是，LSTM 并没有这个问题。
\end_layout

\begin_layout Standard
LSTM是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM 通过刻意的设计来避免长期依赖问题。不同于普通RNN的单一神经网络层，LSTM
 unit中有许多其他门控（包括遗忘门、记忆门等），以一种非常特殊的方式进行交互。这四个门控是：
\end_layout

\begin_layout Enumerate
Input gate (current cell matters)
\end_layout

\begin_layout Enumerate
Forget (gate 0, forget past)
\end_layout

\begin_layout Enumerate
Output (how much cell is exposed)
\end_layout

\begin_layout Enumerate
New memory cell
\end_layout

\begin_layout Standard
对应的四个门控的输出是：
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig3.png
	scale 60

\end_inset


\end_layout

\begin_layout Standard
此时，LSTM Unit的示意图为下图，从左至右分别为Input gate, forget, new memory cell, 以及output。
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig4.png
	scale 45

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 因此，最后的两个输出分别为：
\end_layout

\begin_layout Enumerate
Final memory cell（位于上方的）
\end_layout

\begin_layout Enumerate
Final hidden state （位于下方和h_t).
\end_layout

\begin_layout Standard
对应的公式分别为：
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig5.png
	scale 50

\end_inset


\end_layout

\begin_layout Subsection
GRU
\end_layout

\begin_layout Standard
门控循环单元（Gated Recurrent Unit，GRU）网络是一种比LSTM网络更加简单的循环神经网络。与LSTM类似，都是在RNN上增加了不同的门控产
生的高级的RNN。在LSTM网络中，输入门和遗忘门是互补关系，用两个门比较冗余。GRU将输 入门与和遗忘门合并成一个门：更新门。同时，GRU也不引入额外的记忆单
元， 直接在当前状态
\begin_inset Formula $h_{t}$
\end_inset

 和历史状态
\begin_inset Formula $h_{t-1}$
\end_inset

 之间引入线性依赖关系。GRU有三个门：
\end_layout

\begin_layout Enumerate
Update gate
\end_layout

\begin_layout Enumerate
Reset Gate
\end_layout

\begin_layout Enumerate
New memory content.
\end_layout

\begin_layout Standard
三个门控分别对应的输出是：
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig6.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard
这样，我们得到新的
\begin_inset Formula $h_{t}$
\end_inset

是：
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig7.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard
GRU的图示：
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig8.png
	scale 60

\end_inset


\end_layout

\begin_layout Section
Experiment
\end_layout

\begin_layout Subsection
Language Model
\end_layout

\begin_layout Standard
语言模型是生成唐诗的重要背景知识。实际上，唐诗能写成，本质上就是利用RNN网络训练了一个基于唐诗语料库的语言模型。如果把语料库换成其他的歌词，也就可以写成其他风
格的诗作了。这里简要介绍一下语言模型。统计语言模型是一个单词序列上的概率分布，对于一个给定长度为m的序列，它可以为整个序列产生一个概率 
\begin_inset ERT
status open

\begin_layout Plain Layout

$P(w_1,w_2,…,w_m)$ 
\end_layout

\end_inset

。理论上我们需要根据一句话中所有的历史词汇来计算当前词汇的概率，但是这样计算太过复杂。通常，我们根据马尔可夫性假设当前词的概率与前面的n个词有关系。于是，语言模
型：
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig9.png
	scale 40

\end_inset


\end_layout

\begin_layout Standard
可以看到，我们实际中在应用统计语言模型的时候，只能根据前n个词汇来计算，过长的依赖就不能得到解决。而这也正是RNN神经语言模型能够解决的问题。只需要把上面提到的
各种RNN模型的输入变成语料库中的lexicon序列，训练这个模型便可以得到神经语言模型了。
\end_layout

\begin_layout Subsection
Peom Generation
\end_layout

\begin_layout Standard
本文使用的是tensorflow版本的代码，这里分析一下各个函数的作用，以此来说明唐诗生成的过程。
\end_layout

\begin_layout Subsubsection
数据预处理部分
\end_layout

\begin_layout Standard
经过对函数process_poems的分析，我们可以得处三个返回值poems_vector, word_int_map, words分别为：存着诗（字符串）的v
ector索引集；每个字和索引的字典；诗歌语料库中出现的所有的字。
\end_layout

\begin_layout Subsubsection
rnn_lstm model
\end_layout

\begin_layout Standard
这部分代码定义了我们的RNN模型，我们使用的是MultiRNNCell，表示我们的模型是多层RNN堆叠而成的。同时，我们利用的是dynamic_rnn得到输出，
说明有多个时序被同时计算，这样提升了模型的训练效率。
\end_layout

\begin_layout Subsubsection
训练模型部分
\end_layout

\begin_layout Standard
这部分代码是清晰易懂的。唯一个值得关注的点是该程序将参数保存checkpoint中。如果训练被中断了，下次训练时可以直接从中断部分继续实验，极大地提升了效率。
\end_layout

\begin_layout Subsubsection
生成诗歌部分
\end_layout

\begin_layout Standard
2.2.3在代码上是训练模型，而生成诗歌则是验证模型。此外，要注意我们需要导入已经训练好的模型。注意如果需要训练好的参数能够成功导入，那么就要让网络模型等和训练的时
候的一致。而具体到诗歌生成，原理其实很简单，此时，实际上利用的就是语言模型的思想，给出一个begin_word，利用神经语言模型，预测出最可能的下一个word，
以此类推直到遇到了end_token为止，而这些预测出来的word组合起来也就是所谓的唐诗了。
\end_layout

\begin_layout Subsubsection
主函数
\end_layout

\begin_layout Standard
将上述的代码组合起来，按照预处理、训练、验证的流程进行实现。
\end_layout

\begin_layout Section
Result
\end_layout

\begin_layout Standard
本次实验中，我们训练了三个模型，情况如下，其中trraning_loss是最后一次迭代的值。
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="5">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
模型
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
learning_rate
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
epochs
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
training_loss
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
RNN
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.01
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
50
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
4.11
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
LSTM
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.001
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
112
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.73
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
LSTM
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
C
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.001
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
32
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3.52
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
LSTM
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
生成的诗歌分别为：
\end_layout

\begin_layout Standard
\align left
\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
C
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig10.png
	scale 60

\end_inset


\begin_inset Graphics
	filename fig11.png
	scale 60

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig12.png
	scale 60

\end_inset


\begin_inset Graphics
	filename fig13.png
	scale 60

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Graphics
	filename fig14.png
	scale 60

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
结果分析：
\end_layout

\begin_layout Enumerate
我们发现在同样取learning_rate为0.001的时候，epoch多了之后，反而training_loss变高，这说明本身这个模型可能设计的就不太好，导致不
能完全收敛到全局最优解。
\end_layout

\begin_layout Enumerate
总体上而言，当training_loss变小之后，生成的诗歌的句子的长度也减小了。这说明神经语言模型得到了一定的提升，语言模型会较早的发现end_token，而
不会陷入生成很多很长且没有多少道理的诗句。
\end_layout

\begin_layout Standard
这些诗句中，还是有一些比较惊艳的，这里摘录出来，以飧读者：
\end_layout

\begin_layout Itemize
何当见君子，不见白头翁。
\end_layout

\begin_layout Itemize
日暮花如雪，春风入夜深。
\end_layout

\begin_layout Itemize
云门春色上，花落一枝香。
\end_layout

\begin_layout Itemize
莫问东风景，何人更有期。
\end_layout

\begin_layout Itemize
海上山头白，春风吹雨声。
\end_layout

\begin_layout Itemize
归来不可见，日暮一声风。
\end_layout

\begin_layout Itemize
欲待春光里，还随白露姿。
\end_layout

\begin_layout Itemize
相逢不可见，何处有残阳。
\end_layout

\begin_layout Section
Others
\end_layout

\begin_layout Standard
对唐诗生成这个任务的一些思考：
\end_layout

\begin_layout Enumerate
本文对字的嵌入是将所有的字编号来了个索引，但是字与字之前在语义上的相似性，并没有通过这个编号本身表现出来。如果换作是Chinese Character的embe
dding向量，效果会不会更好一些。利用word2vec的原理，只不过这里的word是每一个字。然后还是用这个古诗的语料库进行训练。
\end_layout

\begin_layout Enumerate
本文的方法基本指利用了语言模型，我们知道古诗词有严格的韵脚、对仗等限制，未来也许可以添加这些条件，让诗歌看起来更工整。
\end_layout

\begin_layout Enumerate
一般而言，生成任务都有一些evaluation来评测我们的模型是好是坏，但是在诗歌生成的案例中，似乎并没有这样的一个评判方法。可否考虑大量的人工评分数据作为ev
aluation值得思考。艺术作品的好与坏，事实上，是很难用机器去定量评价的。艺术本就闪耀着人性的光辉，人性的东西应该回归由人性自身去评价，
\end_layout

\end_body
\end_document
