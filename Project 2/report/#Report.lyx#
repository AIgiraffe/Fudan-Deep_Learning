#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass ctex-article
\begin_preamble
\usepackage{amsmath}
\usepackage{bm}
\usepackage{ctex}

\usepackage{color}
\usepackage{fancyvrb}
\usepackage{CJKutf8}
\usepackage[overlap, CJK]{ruby}
\newenvironment{SChinese}{%
\CJKfamily{gbsn}%
\CJKtilde
\CJKnospace}{}
\newcommand{\cntxt}[1]{\begin{CJK}{UTF8}{}\begin{SChinese}#1\end{SChinese}\end{CJK}}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\newenvironment{Shaded}{}{}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{#1}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
% set default figure placement to htbp
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language chinese-simplified
\language_package default
\inputencoding utf8-plain
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\font_cjk gbsn
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
用RNN网络模型写唐诗
\end_layout

\begin_layout Author
Linyang He
\end_layout

\begin_layout Standard
本文以写唐诗这个情景为例，重点关注了RNN模型（以及LSTM,GRU模型）在时间序列问题上的应用。本文先会讲述各个RNN模型，再描述了唐诗生成的过程。接着展示了
以“日 、 红 、 山 、 夜 、 湖、 海 、 月”等字为首字的诗歌。最后是实验的总结，并提出了一些自己的思考。
\end_layout

\begin_layout Section
Background
\end_layout

\begin_layout Subsection
RNN
\end_layout

\begin_layout Standard
当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列。为了解决一些这样类似的问题，能够更好的处理序列的信息，RNN就
诞生了。递归神经网络（Recurrent Neural Networks，RNN）对于时序信息的处理有着天然的优异性。RNN的特点在于，会把上个时序的hidde
n_state信息作为这个时序的输入，这样，就实现了不同时序上的信息的沟通。下图是一个单独的RNN Unit示意图。
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig1.png
	scale 20

\end_inset


\end_layout

\begin_layout Standard
可以发现，隐藏层中间有个w在循环的输入输出，这便是hidden_state。如果展开来看，可以得到如下图像，这也是平时我们常见的RNN图像。
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig2.png
	scale 25

\end_inset


\end_layout

\begin_layout Standard
具体而言，假设在时刻
\begin_inset Formula $t$
\end_inset

时，网络的输入为
\begin_inset Formula $x_{t}$
\end_inset

，隐层状态（即隐层神经元活性值）为
\begin_inset Formula $h_{t}$
\end_inset

 不仅和当前时刻的输入
\begin_inset Formula $x_{t}$
\end_inset

 相关，也和上一个时刻的隐层状态
\begin_inset Formula $h_{t-1}$
\end_inset

 相关。 
\begin_inset Formula $f(·)$
\end_inset

是非线性激活函数，通常为logistic函数或tanh 函数，
\begin_inset Formula $U$
\end_inset

 为状态-状态权重矩阵，
\begin_inset Formula $W$
\end_inset

 为状态-输入权重矩阵, 那么，RNN的运算逻辑通常可以表示成：
\begin_inset ERT
status open

\begin_layout Plain Layout

$$h_t = f(Wx_t + Uh_{t−1})$$
\end_layout

\end_inset

.
 总之，链式的特征揭示了 RNN 本质上是与序列和列表相关的。他们是对于这类数据的最自然的神经网络架构。
\end_layout

\begin_layout Subsection
LSTM
\end_layout

\begin_layout Standard
RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是
真的可以么？答案是，还有很多依赖因素。 有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着
预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，
相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。
\end_layout

\begin_layout Standard
但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France...
 I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前
位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。 不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远
的信息的能力。在理论上，RNN 绝对可以处理这样的 长期依赖 问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这
些知识。Bengio, et al.
 (1994)等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。 然而，幸运的是，LSTM 并没有这个问题。
\end_layout

\begin_layout Standard
LSTM是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM 通过刻意的设计来避免长期依赖问题。不同于普通RNN的单一神经网络层，LSTM
 unit中有许多其他门控（包括遗忘门、记忆门等），以一种非常特殊的方式进行交互。这四个门控是：
\end_layout

\begin_layout Enumerate
Input gate (current cell matters)
\end_layout

\begin_layout Enumerate
Forget (gate 0, forget past)
\end_layout

\begin_layout Enumerate
Output (how much cell is exposed)
\end_layout

\begin_layout Enumerate
New memory cell
\end_layout

\begin_layout Standard
对应的四个门控的输出是：
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig3.png
	scale 60

\end_inset


\end_layout

\begin_layout Standard
此时，LSTM Unit的示意图为下图，从左至右分别为Input gate, forget, new memory cell, 以及output。
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig4.png
	scale 45

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
noindent
\end_layout

\end_inset

 因此，最后的两个输出分别为：
\end_layout

\begin_layout Enumerate
Final memory cell（位于上方的）
\end_layout

\begin_layout Enumerate
Final hidden state （位于下方和h_t).
\end_layout

\begin_layout Standard
对应的公式分别为：
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig5.png
	scale 50

\end_inset


\end_layout

\begin_layout Subsection
GRU
\end_layout

\begin_layout Standard
门控循环单元（Gated Recurrent Unit，GRU）网络是一种比LSTM网络更加简单的循环神经网络。与LSTM类似，都是在RNN上增加了不同的门控产
生的高级的RNN。在LSTM网络中，输入门和遗忘门是互补关系，用两个门比较冗余。GRU将输 入门与和遗忘门合并成一个门：更新门。同时，GRU也不引入额外的记忆单
元， 直接在当前状态
\begin_inset Formula $h_{t}$
\end_inset

 和历史状态
\begin_inset Formula $h_{t-1}$
\end_inset

 之间引入线性依赖关系。GRU有三个门：
\end_layout

\begin_layout Enumerate
Update gate
\end_layout

\begin_layout Enumerate
Reset Gate
\end_layout

\begin_layout Enumerate
New memory content.
\end_layout

\begin_layout Standard
三个门控分别对应的输出是：
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig6.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard
这样，我们得到新的
\begin_inset Formula $h_{t}$
\end_inset

是：
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig7.png
	scale 50

\end_inset


\end_layout

\begin_layout Standard
GRU的图示：
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig8.png
	scale 60

\end_inset


\end_layout

\begin_layout Section
Experiment
\end_layout

\begin_layout Subsection
Language Model
\end_layout

\begin_layout Standard
语言模型是生成唐诗的重要背景知识。实际上，唐诗能写成，本质上就是利用RNN网络训练了一个基于唐诗语料库的语言模型。如果把语料库换成其他的歌词，也就可以写成其他风
格的诗作了。这里简要介绍一下语言模型。统计语言模型是一个单词序列上的概率分布，对于一个给定长度为m的序列，它可以为整个序列产生一个概率 
\begin_inset ERT
status open

\begin_layout Plain Layout

$P(w_1,w_2,…,w_m)$ 
\end_layout

\end_inset

。理论上我们需要根据一句话中所有的历史词汇来计算当前词汇的概率，但是这样计算太过复杂。通常，我们根据马尔可夫性假设当前词的概率与前面的n个词有关系。于是，语言模
型：
\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename fig9.png
	scale 40

\end_inset


\end_layout

\begin_layout Standard
可以看到，我们实际中在应用统计语言模型的时候，只能根据前n个词汇来计算，过长的依赖就不能得到解决。而这也正是RNN神经语言模型能够解决的问题。只需要把上面提到的
各种RNN模型的输入变成语料库中的lexicon序列，训练这个模型便可以得到神经语言模型了。
\end_layout

\begin_layout Subsection
Peom Generation
\end_layout

\begin_layout Standard
本文使用的是tensorflow版本的代码，这里分析一下各个函数的作用，以此来说明唐诗生成的过程。
\end_layout

\begin_layout Subsubsection
数据预处理部分
\end_layout

\begin_layout Standard
经过对函数process_poems的分析，我们可以得处三个返回值poems_vector, word_int_map, words分别是存这
\end_layout

\end_body
\end_document
